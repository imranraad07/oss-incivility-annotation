,project_name,issue_id,comment_id,comment_body,created_at,user_id
0,rails/rails,352750519,352750519,"Per https://twitter.com/dhh/status/1032050325513940992, I'd like for Rails to set a good example and tone by using better terminology when we can. An easy fix would be to replace our use of whitelist with allowlist and blacklist with denylist.

We can even just use them as verbs directly, as we do with the former terms. So something is allowlisted or denylisted.

I took a quick look and it seems like this change is mostly about docs. We only have one piece of the code that I could find on a search that uses the term whitelist with `enforce_raw_sql_whitelist`. Need to consider whether we need an alias and a deprecation for that.",2018-08-21T23:48:12Z,2741
1,rails/rails,352750519,414870475,"Good intentions, but I doubt there's any relation of the origin of the terms blacklist/whitelist to race. There are many idioms and phrases in the English language that make use of colours without any racial backstories.
I haven't met any black person (myself included) who was ever offended by the use of ""blacklist"". Frankly, a good number find it patronising to make this kind of change.",2018-08-22T00:57:07Z,14361073
2,rails/rails,352750519,414873068,At least [one source from a search](https://www.etymonline.com/word/blacklist) suggests the word had its origins around union members. ,2018-08-22T01:11:49Z,10137
3,rails/rails,352750519,414873738,"Regardless of origin, allow/deny are simply clearer terms that does not require tracing the history of black/white as representations of that meaning. We can simply use the meaning directly.",2018-08-22T01:15:48Z,2741
4,rails/rails,352750519,414875618,"1. [etymology is quite important](
https://www.quora.com/Is-the-term-blacklist-racist?share=1). in the end, we might consider plain words „black“ and „white“ racist and enter the realms of newspeak which i figure you especially, @dhh, are familiar with.

2. “allow/deny are simply clearer terms” — now that’s an actual, technically useful argument.

3. can we please stop jumping onto political bandwagons? i am here for the sanity.",2018-08-22T01:26:22Z,201135
5,rails/rails,352750519,414877006,"The terms **Blocklist** and **Clearlist** are sometimes used in place of Blacklist and Whitelist.
",2018-08-22T01:33:59Z,905505
6,rails/rails,352750519,414886139,I'm gonna go ahead and get started on this... 😄 ,2018-08-22T02:23:21Z,19339870
7,rails/rails,352750519,414893315,"One could also argue that in color theory, black is the absence of color (photons which make up the spectrum) and white is the accumulation of all colors. Thus a blacklist is a list which contains elements that are to be absent and a whitelist to be allowed...",2018-08-22T03:03:42Z,396486
8,rails/rails,352750519,414893935,"I think this is a great idea, I have proposed internally at multiple companies I've worked at changing master/slave and blacklist/whitelist to leader/replica and allowlist/denylist, if only because in an industry with poor representation it feels incredibly overt to be standing in a room full of mostly white people using these terms outloud.

It doesn't matter what the origin or intent of this was, or whether people can find a narrow lens through which to see it as not a problem.  Consider using these terms in a coding or systems interview with someone you have just met.

I understand that some black people may not consider these terms offensive, but I would rather someone not want to work with me because they think I am too politically correct than because they think I am too insensitive and blind.

@minaslater thanks for beating me to the first PR on this. :) 💯 ",2018-08-22T03:07:33Z,526479
9,rails/rails,352750519,415013013,"@bitmonk I respectfully disagree with your opinion that your (or my) future colleagues would impose that kind of cultural bias on words that exist in the [English](https://www.merriam-webster.com/dictionary/white%20list) [language](https://www.merriam-webster.com/dictionary/blacklist) as well as are well-defined [on](https://en.wikipedia.org/wiki/Whitelisting) [Wikipedia](https://en.wikipedia.org/wiki/Blacklist_(disambiguation)), of which `blacklist` has far more far reaching disciplines than just comp-sci. 

That said and since I think this issue will garner overwhelming support in this community, I do support a compromise suggested by @rafaelfranca's review to replace the actual words with their definition or some other phrase that is more fluid than a single term.",2018-08-22T12:21:31Z,396486
10,rails/rails,352750519,415040253,"I think the question here should be: is replacing `whitelist` and `blacklist` with `allowlist` and `denylist` a better option? If presented with the word `blacklist` and `denylist`, which is most likely self-explanatory as to the action to be performed?",2018-08-22T13:55:35Z,8125356
11,rails/rails,352750519,482582749,A quick search and replace in the codebase shows that all entries for those words were already changed. I'll close this issue.,2019-04-12T13:52:44Z,47848
12,ruby-i18n/i18n,503451343,503451343,"I'm not sure that you'll like it, but I like to split code into dependencies instead of duplicating code in projects.

I've mentioned this change in https://github.com/ruby-i18n/i18n/pull/424#issuecomment-406851149.",2019-10-07T13:25:51Z,6510020
13,ruby-i18n/i18n,503451343,539187469,Thanks for the patch but I would rather keep the dependencies simple for this gem.,2019-10-07T20:21:26Z,2687
14,ruby-i18n/i18n,503451343,539191928,"> Thanks for the patch but I would rather keep the dependencies simple for this gem.

Can you somehow to argument this point? It looks like ""Thank you, I'll not use I18n (or ActiveSupport, or Ruby on Rails, or Ruby), I'll write and maintain my own code instead"".",2019-10-07T20:33:01Z,6510020
15,ruby-i18n/i18n,503451343,539210257,I got out of OSS originally because of such petty arguments. Can we not?,2019-10-07T21:22:57Z,2687
16,ruby-i18n/i18n,503451343,539236511,"> I got out of OSS

When? I see pretty green GitHub table, but it can't be filtered only with OSS.

> originally because of such petty arguments.

Can I look at discussions? I can read your opinion without repeating.

> Can we not? 

Yes.",2019-10-07T22:47:26Z,6510020
17,ruby-i18n/i18n,503451343,539239864,The table is green on Monday to Friday mainly. Hmmm perhaps I use GitHub for work?,2019-10-07T22:59:34Z,2687
18,ruby-i18n/i18n,503451343,539432465,"> Hmmm perhaps I use GitHub for work?

Probably, I didn't exclude this, but this is not an answer for any of two my questions. OK.",2019-10-08T09:31:02Z,6510020
19,sparklemotion/nokogiri,305814776,305814776,"
am trying to make a script to search by user by entering in there first name it should display the detail of that user this interacts with the file email.xml please help me !!! 
i can get it to read the xml doc and display all users i just need someone to give some exmaple code or show me how the hell i am supposed to do this thanks :)",2018-03-16T06:22:07Z,37433469
20,sparklemotion/nokogiri,305814776,373737684,"Hello!

Thanks for asking this question! Your request for assistance using
Nokogiri will not go unanswered!

However, Nokogiri's Github Issues is reserved for reporting bugs or
submitting patches. If you ask your question on the mailing list, Team
Nokogiri promises someone will provide you with an answer in a timely
manner.

If you'd like to read up on Team Nokogiri's rationale for this policy,
please go to http://bit.ly/nokohelp.

Thank you so much for understanding! And thank you for using Nokogiri.
",2018-03-16T14:52:28Z,8207
21,sparklemotion/nokogiri,305814776,375953557,"uuhhhh there was a bug with nokogiri its a scuffed gem ???? and annoying
and not very user friendly for something that claims to be user friendly


On Sat, Mar 17, 2018 at 1:52 AM, Mike Dalessio <notifications@github.com>
wrote:

> Closed #1736 <https://github.com/sparklemotion/nokogiri/issues/1736>.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/sparklemotion/nokogiri/issues/1736#event-1525697311>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AjswfZK3YznF_VG0fzwDk6mYwB3YziqTks5te9GxgaJpZM4StTWR>
> .
>
",2018-03-25T08:20:32Z,37433469
22,sparklemotion/nokogiri,286791320,286791320,"## If you're having trouble installing Nokogiri ...

**Have you tried following [the installation tutorial][tutorial]?**

yes

**What is the output of `gem install`?**

```
PS C:\Users\replaced> gem install nokogiri -v 1.8.1
ERROR:  Error installing nokogiri:
The last version of nokogiri (= 1.8.1) to support your Ruby & RubyGems was 1.8.1. Try installing it with `gem install nokogiri -v 1.8.1`
 nokogiri requires Ruby version < 2.5, >= 2.2. The current ruby version is 2.5.0.
```

**What are the contents of the `mkmf.log` file?**

Didnt get so far

**What operating system are you using?**

Windows 10 x64",2018-01-08T15:54:00Z,4162281
23,sparklemotion/nokogiri,286791320,356673546,Same OS and same problem... ,2018-01-10T17:22:47Z,10075337
24,sparklemotion/nokogiri,286791320,357411525,"Same here, and I really would like it if downgrading is not my only option
",2018-01-13T05:30:07Z,14236367
25,sparklemotion/nokogiri,286791320,357495804,I am currently having the same issue,2018-01-14T08:19:22Z,24515337
26,sparklemotion/nokogiri,286791320,357499822,Same problem,2018-01-14T09:47:12Z,1104431
27,sparklemotion/nokogiri,286791320,357632186,"Windows 7 x64 same error msg.

```
>gem install nokogiri -v 1.8.1
ERROR:  Error installing nokogiri:
        The last version of nokogiri (= 1.8.1) to support your Ruby & RubyGems w
as 1.8.1. Try installing it with `gem install nokogiri -v 1.8.1`
        nokogiri requires Ruby version < 2.5, >= 2.2. The current ruby version i
s 2.5.0.

```

",2018-01-15T09:48:16Z,5774448
28,sparklemotion/nokogiri,286791320,357677494,"The fix is already merged, although a new release is pending. See:  https://github.com/sparklemotion/nokogiri/pull/1704  & https://github.com/sparklemotion/nokogiri/issues/1706

Until then, it's better to stick with Ruby 2.4.2 on Windows.

The Nokogiri team removed the GEMSPEC from sources, to disallow people building from master branch, since it is unstable and probably would contain bugs. You can read more about it here:
https://github.com/sparklemotion/nokogiri/blob/master/Y_U_NO_GEMSPEC.md",2018-01-15T13:06:03Z,3650296
29,sparklemotion/nokogiri,286791320,358909080,Any word on support for 2.5.0 on OS X?,2018-01-19T09:16:06Z,7992264
30,sparklemotion/nokogiri,286791320,358927731,@leehanslim OSX should use the normal gems that also work on *nix. I had no problems with rbenv to install nokogiri on OSX.,2018-01-19T10:31:57Z,4162281
31,sparklemotion/nokogiri,286791320,359342644,Same problem.,2018-01-22T07:15:46Z,1590359
32,sparklemotion/nokogiri,286791320,359778086,any news?,2018-01-23T12:39:02Z,10075337
33,sparklemotion/nokogiri,286791320,359783631,I am wondering how this happens. Its kind of a tradition ruby makes a point release around christmas every year. the dev version was around for weeks. Ignoring Windows blames the users who are bound to this OS for what ever reason,2018-01-23T13:02:34Z,4162281
34,sparklemotion/nokogiri,286791320,359829700,"@SimonHoenscheid nokogiri-core is always welcoming of contributors who would like to help us support Windows. We've asked for help repeatedly over the years, as we've all got jobs that distract us from supporting open source software, despite our best efforts and intentions.

I'd like to ask that you pay attention to the tone of your message above. ""Ignoring Windows"" is clearly an untrue statement, given the number of hours I've personally spent, the number of hours @larskanis has spent, the amount of time we've spent providing [automated testing tooling][1] for [Nokogiri on Windows][2], etc., etc.

Windows support in Nokogiri is first class. We provide precompiled DLLs for users who want a fast, simple installation; and we provide support for users who want to compile Nokogiri and libxml using DevKit. Saying that we're ignoring Windows users is absurd and a little insulting.

Currently I'm blocking a release that supports Ruby 2.5 on windows on getting Ruby 2.5 supported in our test pipelines. I accept some fault for not being more transparent about this as the blocking factor. But now I'm trying to make amends by communicating exactly what's going on behind the scenes to properly support you and the platform that you use. Not ignoring you or blaming you, as you claim.

I'm going to lock this issue because I really don't want to do a whole back-and-forth. We'll release support for Ruby 2.5 on Windows as soon as we can, just as we always do, and we appreciate your patience and empathy as we work.

 [1]: https://github.com/flavorjones/windows-ruby-dev-tools-release
 [2]: https://ci.nokogiri.org/teams/nokogiri-core/pipelines/nokogiri/jobs/win-ruby-2.4-devkit/builds/10",2018-01-23T15:39:12Z,8207
35,sparklemotion/nokogiri,286791320,360782174,"Remaining work to be done on our CI pipeline at https://ci.nokogiri.org/

* [x] [upgrade concourse to 3.8.0](https://concourse.ci/downloads.html#v380)
* [x] test Ruby 2.5 support just added to [windows-ruby-dev-tools-release](https://github.com/flavorjones/windows-ruby-dev-tools-release/tree/flavorjones-add-ruby-25) and cut a release
* [x] add a Ruby 2.5 test job to the pipeline (updating [concourse-gem)](https://github.com/flavorjones/concourse-gem/blob/master/lib/concourse.rb#L10)
* [x] get Ruby 2.5 tests to pass
* [ ] cut a Nokogiri release that adds Ruby 2.5

This is probably a good time to remind watchers that Ruby 2.2 is approaching EOL on  2018-03-31, and so the first release after that will likely remove Ruby 2.2 binaries from the fat gem.
",2018-01-26T13:18:19Z,8207
36,sparklemotion/nokogiri,286791320,360836510,"Windows Ruby 2.5 build is up and running:

> https://ci.nokogiri.org/teams/nokogiri-core/pipelines/nokogiri/jobs/win-ruby-2.5-devkit/builds/1

",2018-01-26T16:40:18Z,8207
37,sparklemotion/nokogiri,286791320,361064638,Note that commit bf94cf5 was added to master to make windows tests less brittle.,2018-01-28T13:53:28Z,8207
38,sparklemotion/nokogiri,286791320,361243041,"Nokogiri 1.8.2 has shipped with Ruby 2.5 support in the ""fat binary"" windows gems. Thanks everyone for your patience.",2018-01-29T13:20:37Z,8207
39,FreeRADIUS/freeradius-server,732267627,732267627,"After hours of struggling with getting this right, I thought I would post this config as the default one since the current one is wrong, or at least doesn't work.

When building from source, the paths are wrong. Also, systemd PID shouldn't be run in `/var/run` (`usr/local/var/run`), it should be run in `/run/folder`.

This PR solves all that so that the paths are correct, and the actual PID is run from the correct directory.

Also added logging for systemd instead of logging to syslog.",2020-10-29T12:22:22Z,4511254
40,FreeRADIUS/freeradius-server,732267627,718754729,Which system did you try this on?  We've used this on multiple Debian / Ubuntu systems.  Packages on http://packages.networkradius.com are using these.,2020-10-29T13:32:23Z,17612
41,FreeRADIUS/freeradius-server,732267627,718758006,"From looking at the changes, it appears you build the system via `./configure / make / make install`.  The defaults used there are for generic Unix-style operating systems.  Then, you've used the Debian-specific `systemd` file.  Since Debian / Ubuntu changes the paths where files go, this `systemd` file is specifically for use on Debian / Ubuntu systems.  Not for use on ""generic Unix-style"" operating systems.

If you plan on using FreeRADIUS on Debian / Ubuntu, then use the build system for that: `make deb`.  That will build a `.deb` package which will work.

Or, use the pre-built packages from http://packages.networkradius.com",2020-10-29T13:37:46Z,17612
42,FreeRADIUS/freeradius-server,732267627,718805007,"Thanks for the pointers! This was on Ubuntu 20.04 Server.

> Or, use the pre-built packages from http://packages.networkradius.com

I would if I could, but they don't work with Ubuntu Focal 20.04.

Already spent three days on this. Now the service starts and freeradius reports OK, but still I can't connect from our Unifi AP. Using `radiusd -X` works fine though. I'm going nuts soon. 

",2020-10-29T14:51:06Z,4511254
43,FreeRADIUS/freeradius-server,732267627,718823253,"@alandekok Hey, just tried `make deb` and that failed as well since dependencies required are only available in Bionic.

Please make Focal work so that we can use a recent LTS release instead of using something that will end support soon.

I give up now. ",2020-10-29T15:18:32Z,4511254
44,FreeRADIUS/freeradius-server,732267627,718902779,"@enoch85 FreeRADIUS is an open source project.  If you want to see progress on a particular issue, then you are expected to contribute, _especially_ with something fairly trivial like this, where you could easily determine the correct dependencies for 20.04 and submit a PR.

",2020-10-29T17:22:59Z,791758
45,FreeRADIUS/freeradius-server,732267627,719361159,"@arr2036 The repo that I maintain only has [620 stars](https://github.com/nextcloud/vm/), but yeah. I'm well familiar with the concept. Thanks for the friendly reminder. 👍 

Everything is easy if you know how to do it. I can tell you almost everything about Nextcloud, but I've never worked with FreeRADIUS before. 

As a maintainer of a project I see myself as the main contributor. My philosophy is to make it as easy as possible for people using what I produce. In my case, [install Nextcloud on any computer/server](https://help.nextcloud.com/t/appreciation-for-a-job-well-done/92140) with 0 Linux experience. I'm sorry to see that this isn't the thing here. Focal was released in April 2020, and still it's not supported.",2020-10-30T07:42:58Z,4511254
46,FreeRADIUS/freeradius-server,732267627,719486183,"I fixed up the Debian control file and added a Dockerfile for Focal a week after it was released. The Dockerfile works by building packages and then installing them, so `make deb` definitely works on Focal as long as the correct build dependencies have been installed.

If you've got Docker installed then you can use it to build the packages if it helps to avoid having to install the build dependencies on the host system:
```
docker build -t fr-ubuntu20-build --target=build scripts/docker/ubuntu20
C=$(docker create fr-ubuntu20-build)
docker cp $C:/usr/local/src/repositories/ ./deb
docker rm $C
```
Packages for the main site is on my to do list, but sadly so are a lot of other things.",2020-10-30T10:59:25Z,1319836
47,FreeRADIUS/freeradius-server,732267627,719488024,"@enoch85  If you're familiar with Open Source, then you should be familiar with the concept that ""we don't work for you"".  You should also be familiar with the idea of ""you have zero right to demand that we do what you want"".  You should also be familiar with the idea of ""please supply patches, as we can't do everything"".

We are _very_ sensitive to the all-too common practice of ""you do what I want, because I can't be bothered to do it myself"".  We have very little sympathy for such poor attitude.

We don't expect everyone to be experts in FreeRADIUS.  We do expect that people's contributions will be productive, and polite.

The current release (3.0.21) does not have Focal support because Focal was released after we released 3.0.21.  As Matthew points out, Focal is supported in the v3.0.x branch.  Which will become 3.0.22 any time now.

To close this out, we're sorry that the software was imperfect.  But your approach is to take offence at straightforward, factual, comments.  We suggest this is entirely the wrong approach.",2020-10-30T11:03:40Z,17612
48,FreeRADIUS/freeradius-server,732267627,719495679,"@mcnewton 

> as long as the correct build dependencies have been installed.

I don't remember the exact name now but some libxxx-c2 and libxxx-c3 weren't possible to install from Ubuntu Focals own repos (clean ISO from their site), hence `make deb` failed. I did `apt-get install libxxx*` (1500 MB of packages) just to don't miss out on anything, but still - no success.

If the docker solution works, then great!

@alandekok 

> ""please supply patches, as we can't do everything"".

I'm sure familiar with that, and I understand 100% - but putting things IN CAPITAL LETTERS when you could do **bold** or *cursive* instead just seemed offensive to me. 

> ""you do what I want, because I can't be bothered to do it myself""

Well, at least I tried. I hope you didn't found my efforts (though small) as not bringing anything to the table.

> We suggest this is entirely the wrong approach.

Noted, and sorry for that.

",2020-10-30T11:20:49Z,4511254
49,FreeRADIUS/freeradius-server,732267627,719496255,"And, looking forward to try 3.0.22. 😄 ",2020-10-30T11:21:57Z,4511254
50,FreeRADIUS/freeradius-server,732267627,719498681,"Ah, from the control file: `libjson0 | libjson-c2 | libjson-c3 | libjson-c4 | libjson-c5,` - you need one of those (`|` = or), not all of them. I think Focal will be `libjson-c5`, which I added just before the Dockerfile.

See ""Install build dependencies"" in the Dockerfile which will install them all correctly for you using `mk-build-deps`.",2020-10-30T11:27:17Z,1319836
51,FreeRADIUS/freeradius-server,732267627,719660952,"> As a maintainer of a project I see myself as the main contributor. My philosophy is to make it as easy as possible for people using what I produce. 
In my case, [install Nextcloud on any computer/server](https://help.nextcloud.com/t/appreciation-for-a-job-well-done/92140) with 0 Linux experience. I'm sorry to see that this isn't the thing here. 

Nextcloud and FreeRADIUS target very different technical communities.  Users of FreeRADIUS are expected to have basic Linux/BSD experience, networking knowledge, and have an idea of how RADIUS and its authentication protocols function.  FreeRADIUS is used in installations with millions of subscribers, it's the primary authentication and proxy server for the Eduroam federation, It's shipped with DDWRT, and embedded in many manufacturer's wireless solutions.  Organisations should not be having junior dev/devops install and configure FreeRADIUS instances, if those instances are to be used as a basis for a critical service.

Your main complaints appear to be Debian or Ubuntu messed up the permissions on automatically generated certificates (which is an issue which should have been reported to them, not the FreeRADIUS project), ~and that we haven't resolved a minor dependency issue in our Debian packaging.  One which you yourself do not seem willing to submit fixes for.~ This has already been fixed and will in the next official release.

The point I was making was seeing that you have a build environment already established, it would take literally a couple of minutes to figure out the correct dependencies.  Even if you had zero experience with debian packaging, a recursive grep through the source would have shown where libjson was listed as a dependency. 

I strongly dislike the sense of entitlement some people exhibit when using Open Source software.  This is not paid-for commercial software, its software developed by a community.  I have contributed to many projects outside of FreeRADIUS, and I would never come in with the attitude you've shown here if I wanted a positive outcome.
",2020-10-30T16:37:13Z,791758
52,FreeRADIUS/freeradius-server,732267627,719788968,"After locking this PR, enoch85 has taken to emailing us privately complaining that were are bullies, and praising his own abilities.

As a result of his ongoing pattern of abuse and hyper-sensitivity, he has been blocked from the FreeRADIUS organization.  We also note that this block will also protect his delicate ego from further ""abuse"".",2020-10-30T20:45:11Z,17612
53,jekyll/jekyll,569316203,569316203,"## Summary
As discussed in #4182, I'd like to request the addition of the ""override_ext"" front matter tag.

## Motivation
I'm using php code on my website, like many others, too. I can create a ""test.php"" file, and the generated file will have the "".php""-extension, all right. However, I want to use markdown for my blog posts, and these "".md"" files are automatically generated to "".html"" files! And because the posts have a .""php"" layout with php code, but the file ends up as html, the site does not work. OK, I could create a permalink as suggested in the issue linked above, but that's a bad workaround because I don't want to hardcode the file name and directory.
A similar request has been discussed already, but I find the [excuses](https://github.com/jekyll/jekyll/issues/5646#issuecomment-357329996) quite weak. As of now, it's hardcoded for markdown files to be converted to html files, and that's simply not right. Give us an option here!

Edit: OK, it's not really needed for other files where I can already set my custom extension manually, this would just add more confusion. But for all files which jekyll automatically converts into a hardcoded format, this should be changeable - most likely in config.yml.",2020-02-22T10:38:44Z,48808497
54,jekyll/jekyll,569316203,617711650,"
This issue has been automatically marked as stale because it has not been commented on for at least two months.

The resources of the Jekyll team are limited, and so we are asking for your help.

If this is a **bug** and you can still reproduce this error on the latest <code>3.x-stable</code> or <code>master</code> branch, please reply with all of the information you have about it in order to keep the issue open.

If this is a **feature request**, please consider building it first as a plugin. Jekyll 3 introduced [hooks](http://jekyllrb.com/docs/plugins/#hooks) which provide convenient access points throughout the Jekyll build pipeline whereby most needs can be fulfilled. If this is something that cannot be built as a plugin, then please provide more information about why in order to keep this issue open.

This issue will automatically be closed in two months if no further activity occurs. Thank you for all your contributions.
",2020-04-22T11:10:18Z,6166343
55,jekyll/jekyll,569316203,626236630,"This is already possible based on answers here.

https://stackoverflow.com/questions/14119772/how-to-change-extension-of-files-generated-by-jekyll

An arbitrary extension or no-extension file will be parsed with liquid if it has frontmatter and then jekyll outputs as .html
And if you set the permalink in the file you can force the extension to be .json, .php , etc.
Maybe set the permalink pattern globally in the config as a collection. You can set permalink for post but the response there was that it doesn't work in this case I think",2020-05-09T21:18:21Z,18750745
56,jekyll/jekyll,569316203,626574978,"Yeah, that's the problem: I don't want to hardcode the path with permalinks, I just want to be able to adjust the file extension...",2020-05-11T09:08:19Z,48808497
57,jekyll/jekyll,569316203,626575842,"Also, @DirtyF: It would be nice to at least give an explanation before just closing an issue, this way nothing will get resolved,",2020-05-11T09:09:50Z,48808497
58,jekyll/jekyll,393681341,393681341,"Plase, add a step on the Step by Step Tutorial to demonstrate how to theme the just finished demo app.

## Motivation

1. Theming is not an obvious task for a newbie like me. Also, doc speaks about gem based and 'regular' file themes. It's confusin  a bit.
2. At the end of demo app it's more beautiful to leave in the han of the user a well looking demo

## Idea

Please include istructions on 
- how to enable default gem base theme
- how to customized if possible a gem theme
- how to switch to a regular file based theme
- idem, how to customize it

... or ...

Create a guide on how to start theming AND then include previous steps into the new tutorial.

Thanks for this excellent piece of software !!!!",2018-12-22T15:23:14Z,1268699
59,jekyll/jekyll,393681341,449582031,"On how to start theming for Jekyll, I'd recommend @daviddarnes articles:

1. https://www.siteleaf.com/blog/making-your-first-jekyll-theme-part-1/
2. https://www.siteleaf.com/blog/making-your-first-jekyll-theme-part-2/",2018-12-22T16:31:30Z,103008
60,jekyll/jekyll,393681341,449763207,"Thanks, but it's better have an official doc, and not links to external resources. IMHO",2018-12-24T18:51:08Z,1268699
61,jekyll/jekyll,393681341,450029809,If it's validation you're looking for then I can 100% endorse these tutorials ✨. However if you want this set in stone then maybe we could link to these tutorials in the official docs? Maybe in https://jekyllrb.com/tutorials/home/ or https://jekyllrb.com/resources/. What do you day @DirtyF?,2018-12-26T21:36:33Z,1177460
62,jekyll/jekyll,393681341,450209070,"Oooh, I like @desiredpersona’s idea.",2018-12-27T18:49:58Z,28698431
63,jekyll/jekyll,393681341,450211638,"IMHO, Documentation is one thing and a (sponsored) community blog is an entirely different league altogether. They should not be intermingled.
Documentation is translating ""the code base"" into layman lingo. (*Point-of-View: Maintainers -> Users*)
Community blog is about sharing experience with the code base. (*Point-of-View: One User -> Other Users*)
Bringing the sponsors into the mix, is just complicating things, unnecessarily.........",2018-12-27T19:06:28Z,12479464
64,jekyll/jekyll,393681341,450236747,Sorry but ... To rest in topic... A simple step added to main step by step intro is so wrong?,2018-12-27T21:46:45Z,1268699
65,jekyll/jekyll,393681341,450398231,"I don’t see what is wrong with referring to a tutorial on another site? Yes the tutorial was paid for, but tutorials of that size take time to create. Maybe we could add some more detail on the points people are getting stuck on in the docs?",2018-12-28T17:40:59Z,1177460
66,jekyll/jekyll,393681341,450413054,"If I see a tutorial on an external website it soon or later will become obsolete.

If I see a tutorial on main site I think that must be ok, tested, and updated. 

Simply. Close this issue

A company that does not understand the need of a full doc do not will offer a serious support in the long time

Bye bye.",2018-12-28T19:20:18Z,1268699
67,jekyll/jekyll,393681341,450416440,"> A company that does not understand the need of a full doc do not will offer a serious support in the long time

Jekyll is an open-source project **freely maintained by volunteers**, it has been around for 10 years now and powers hundred of thousands of websites around the world. 

Jekyll has a great [community](https://talk.jekyllrb.com/) and docs are continuously improved with the help of the contributors.

Jekyll themes _are_ [documented](https://jekyllrb.com/docs/themes/#understanding-gem-based-themes). I'll see if I can add some links to point to theming at the end of the step-by-step guide.",2018-12-28T19:44:12Z,103008
68,jekyll/jekyll,316610337,316610337,":wave: Hello everyone! Summer is coming, and so is the implementation period for Jekyll 4.0. That's right, it's time to get some breaking changes in. To accommodate for this, we're opening this issue to collect interesting ideas that people would like to see implemented in 4.0. __Keep in mind that even if an idea receives a lot of support, there's no guarantee that it'll get implemented__ — that depends on if someone is free to actually implement it. We're all volunteers here, keep that in mind.

Feel free to revive old feature requests, too, just not something that we've explicitly rejected.

For an organized view of how we're consolidating ideas and features, check out our Project board: https://github.com/jekyll/jekyll/projects/2",2018-04-22T20:25:31Z,10137
69,jekyll/jekyll,316610337,383451465,"I vote for #6293 Markdown links bidi support, in `<>`, `[]()` by default.",2018-04-23T04:32:02Z,183966
70,jekyll/jekyll,316610337,383690460,"Adding fetch / get for datafiles to core (or as an ""official"" addon/plugin) would be great, see the unloved / public domain source @ <https://github.com/18F/jekyll-get>  The code itself is about 40 lines, see <https://github.com/18F/jekyll-get/blob/master/_plugins/jekyll_get.rb>.",2018-04-23T19:15:49Z,53281
71,jekyll/jekyll,316610337,383692192,"Using the quik library for scaffolding. If you scaffold a new jekyll theme or plugin now all the code is ""hard-coded"" / ""hard-wired"". The scaffolding code itself is also ""hard-wired""  / ""hard-coded"", that is, not (re)usable for other projects. Using a (simple) ""generic"" scaffolding library such as quik you can turn any git(hub) repo (or directory/folder or zip archive) into a parametrized and scripted template scaffold. See the [Jekyll Quick Starter Template / Scaffold - Build Your Own (Gem-Packaged) Theme](https://github.com/quikstart/jekyll-starter-theme) as an example. 

PS: Background / References - Talk Notes - [Quik - The Missing Project Scaffolder (Library) for Ruby - Quick Start Your Ruby Gems, Jekyll Websites, Jekyll Themes 'n' More](https://github.com/geraldb/talks/blob/master/quik.md)",2018-04-23T19:21:52Z,53281
72,jekyll/jekyll,316610337,383720956,Custom HTTP headers compatibility for Github Pages (Webrick isn't planned to support https://github.com/github/pages-gem/issues/415),2018-04-23T20:59:25Z,183966
73,jekyll/jekyll,316610337,383729613,@laukstein Let’s keep this restricted to Jekyll features. We don’t have any control over GitHub Pages.,2018-04-23T21:30:21Z,251545
74,jekyll/jekyll,316610337,383754782,"I'd love to see automatic generation of tag and category archive pages in core (plugins like [jekyll-archives](https://github.com/jekyll/jekyll-archives/) do this, but none are whitelisted in GitHub Pages). I wrote up #6952 with my detailed rationale and a proposal of how it could work.

Here's the TL;DR:

> Having vanilla Jekyll auto-generate archive pages for every tag and category is probably a good idea now, and I think I've come up with a good, clean way to do it while staying true to Jekyll's philosophy.

Code-wise, this snippet gets most of it across:

```yaml
collections:
  tags:
    output: true
    permalink: /tags/:name/
  categories:
    output: true
    permalink: /categories/:name/
```


",2018-04-23T23:31:32Z,4627002
75,jekyll/jekyll,316610337,383799753,"I know that this is markdown and not necessarily jekyll related, but my life would have been a thousand times easier if jekyll supported inline footnotes like so  `[^Footnote, p. 123]` as apposed to the more tedious:

```
This is first reference[^one], this is the second[^two]

[^one]: Footnote, p.1
[^two]: Footnote, p.2

```",2018-04-24T04:28:25Z,7410291
76,jekyll/jekyll,316610337,383857231,@mbrav This depends on the Markdown engine. Please ask  on [Kramdown](https://github.com/gettalong/kramdown) or [CommonMark](http://commonmark.org/).,2018-04-24T08:58:22Z,103008
77,jekyll/jekyll,316610337,384153347,"First would like to say thank you to the Jekyll team for all the great work you all do. Jekyll is one my favorite frameworks to work with and I really do appreciate what you guys do! 
Ok on to the request it would be great if there was a built-in way to paginate a list Jekyll collection items on a page. I know there is [jekyll-paginate-v2 gem](https://github.com/sverrirs/jekyll-paginate-v2) but it would be nice if there was a built in pagniation that worked for both post and collection items.",2018-04-25T03:55:37Z,4413018
78,jekyll/jekyll,316610337,384634593,"Hello all, my first post so go easy. I want to just say that I love Jekyll, I'm now using it on a lot of my web builds. One thing that has bothered me though, is that I've not been able to take advantage of the latest Jekyll features because a lot of the 3rd party CMS solutions I rely on for my clients use (cloudcannon, siteleaf and forestry) simply don't run the latest versions of Jekyll themselves. This forces me to have my folder structures in a really messy state and not take advantage of the latest features when developing locally. So if anyone at Jekyll could bend a few arms to get them to update that would be fab.",2018-04-26T13:08:07Z,4135530
79,jekyll/jekyll,316610337,384651313,"@nativerez It alwyas take some time to update to the latest version for services like GitHub Pages, Forestry, CloudCannon or Siteleaf because they need to run tests and adapt their tools. There's nothing Jekyll's core team can do about it.",2018-04-26T14:00:09Z,103008
80,jekyll/jekyll,316610337,385148351,"I would like to see Nested Collections. I know there are alternatives to do this. But still if the Jekyll team can make it easier, that would be awesome.!",2018-04-28T07:21:01Z,7797813
81,jekyll/jekyll,316610337,385313083,I would like to see [reading metadata in separate files](https://github.com/jekyll/jekyll/issues/1082). ,2018-04-30T04:52:45Z,26921196
82,jekyll/jekyll,316610337,385407165,"@hosnas Great suggestion. I'll add a use case:
- Create a collection of static image files
- Add metadata (like alt text, captions, dimensions) in sidecar files
- Loop through them to create a rich image gallery

Jekyll CMS services like [Siteleaf](https://siteleaf.com) could make great use of this feature. Siteleaf uploads static assets to an `_uploads` collection. cc @sskylar ",2018-04-30T14:02:39Z,4627002
83,jekyll/jekyll,316610337,385857693,I would like to see the tags of collections included in site.tags. Thanks!,2018-05-02T03:54:51Z,32698567
84,jekyll/jekyll,316610337,386162200,Please please support for i18n. At least 2 languages. Many plug-ins break or don't work with ghpages.,2018-05-03T00:33:39Z,24360355
85,jekyll/jekyll,316610337,386622387,build in support for content blocks and components,2018-05-04T14:39:21Z,1388742
86,jekyll/jekyll,316610337,386652799,@shivajivarma Can you elaborate? Do you mean something like includes?,2018-05-04T16:22:16Z,10137
87,jekyll/jekyll,316610337,386685852,"I'd very much love to have native support for Coffeescript in much the same manner as Sass. Thus, we could define in `_config.yml`

```
# Support Coffeescript
coffeescript:
  coffeescript_dir: _coffee
  output: compressed
```

This way one could create partials of `.coffee` in well-structured folders with all of them compiling minified to one `.js` file, even same way as you have `css/style.sass` with all the imports of individual Sass files.",2018-05-04T18:00:44Z,10137
88,jekyll/jekyll,316610337,386769747,"it would be great to have more interactions with github, like pulling profile information (contributions, avatar, ), releases data - and convert it for proper publication. this would be vital for software website to create team and download pages that would not require multiple code changes. currently working with current available plugins, this solution lacks stability since there's too many plugins that should be constantly updated. would be glad to help if this will go further!",2018-05-05T01:29:38Z,26132682
89,jekyll/jekyll,316610337,386777055,also background section could get some improvements - building website that will look good on regular displays and retina quiet challenging. creating multiple backgrounds for could be the solution to decrease loading time. or middle state like `page loading` to comfort browsing,2018-05-05T04:00:11Z,26132682
90,jekyll/jekyll,316610337,386793771,@charlesrocket GitHub data should be part of GitHub-metadata plugin not Jekyll-core,2018-05-05T09:49:57Z,103008
91,jekyll/jekyll,316610337,386797765,"@DirtyF sorry, i just started with jekyll a week ago) this repo is on the list. at this time, i guess that this info could be pulled by `metadata`, but shouldn't jekyll convert it for better (more universal) usage?",2018-05-05T11:08:20Z,26132682
92,jekyll/jekyll,316610337,386798166,"@charlesrocket This is off-topic, but FYI GitHub-metadata already provides access to these data through `site.github` namespace.",2018-05-05T11:16:45Z,103008
93,jekyll/jekyll,316610337,386798575,@DirtyF my bad. guess i misfired with backgrounds as well? ,2018-05-05T11:23:48Z,26132682
94,jekyll/jekyll,316610337,386798905,"@charlesrocket It looks like so, as Jekyll is fully agnostic when it comes what's get generated in the front-end, it just transforms files into HTML. You can use plugins like `jekyll-assets` and/or `jekyll-cloudinary` to help you deal with responsive images.",2018-05-05T11:30:02Z,103008
95,jekyll/jekyll,316610337,386799085,"@DirtyF thanks for the input! im looking for every jekyll plugin i can find, put majority is out of date and has no support, so it pushed me to propose these functions into core (not counting jekyll-originated plugins) should i delete my comments? don't want to overload the tread since im thinking it will grow and going tho useless comments would add some discomfort",2018-05-05T11:33:31Z,26132682
96,jekyll/jekyll,316610337,386800657,(I) really love this ability to minimize comments on GitHub.. :sparkles:,2018-05-05T12:01:38Z,12479464
97,jekyll/jekyll,316610337,386803866,"I would like to see an automated tool like Webpack integrated into Jekyll to handle Sass and Javascript. 
Some of the biggest reasons are auto-prefixing for Sass and the ability to use ES6 syntax that can automatically be polyfilled with babel. I think having an automated tool handle the sass and javascript bits may speed up the compile time of Jekyll. ",2018-05-05T12:56:09Z,4413018
98,jekyll/jekyll,316610337,386804048,"> I think having an automated tool handle the sass and javascript bits may speed up the compile time of Jekyll

How is that..??",2018-05-05T12:59:41Z,12479464
99,jekyll/jekyll,316610337,386833978,A big reason for me when doing frontend work and the Sass folder has many nested folders and files Jekyll sass can be slow. I have found letting web pack handle the sass instead of Jekyll improves speed and developer happiness. ,2018-05-05T20:50:22Z,4413018
100,jekyll/jekyll,316610337,386843679,"I'd love to see Docker volume mounts work with Jekyll and Docker for Windows.

Rails, Flask, Phoenix and Node applications that I've worked with all work fine in Docker for Windows. You volume mount in the code, make a change, and in milliseconds your change is reflected and ready to be seen in a browser.

With Jekyll 3.8, this does not happen. The file change is reflected inside of the container, but the `jekyll serve` command doesn't regenerate the file. Using `--force_polling` also doesn't work.

Everything works fine under WSL (without Docker) but Jekyll is the only reason I have Ruby installed on my machine, mainly because every other app from every other language / framework works wonderfully with Docker.

Call me crazy but I don't think 4.0 should ship until this is addressed. Not being able to Dockerize a Jekyll site for development with Docker for Windows is a huge bummer.",2018-05-06T00:07:38Z,813219
101,jekyll/jekyll,316610337,386856025,"> I have found letting web pack handle the sass instead of Jekyll improves speed

Jekyll Core doesn't handle pre-processing css and javascript. We have two plugins (both are written in Ruby): `jekyll-sass-converter` (included in `jekyll` by default) and `jekyll-coffeescript`.

The speed improvements are probably due to the difference between `ruby-sass` and `node-sass`",2018-05-06T05:48:15Z,12479464
102,jekyll/jekyll,316610337,387056265,"@oe : something like this 

```html
<html>
  <head>
    {% contentblock scripts %}
  </head>
  <body>
    <div class=""main"">
      {{ content }}
    </div>
    <div class=""sidebar"">
      {% contentblock sidebar %}
    </div>
  </body>
</html>
```

```html
---
layout: default
---

Here is my post content.

{% contentfor sidebar %}
* Some content
* in a markdown list
* with some {{ 'liquid' }} tags too!
{% endcontentfor %}
```

https://github.com/rustygeldmacher/jekyll-contentblocks
",2018-05-07T12:58:28Z,1388742
103,jekyll/jekyll,316610337,387239592,"When I see the [repo](https://github.com/rustygeldmacher/jekyll-contentblocks), I though GitHub pages. It has similarity between. `_sidebar.md`, `_footer.md` etc..",2018-05-07T23:42:18Z,24360355
104,jekyll/jekyll,316610337,387647384,"I would love to see an offically supported page generator from data files. With the stellar rise of ""headless"" (= API only) SaaS CMS systems [static site generators ](url)have become the tool of choice for types of sites that they have not been considered for in the past. 

e.g. [contentful.com](https://contentful.com)  provides an [official jekyll plugin](https://www.contentful.com/ecosystem/jekyll/) that pulls all the CMS content into the Jekyll _data folder as yaml.  You can then use that content inside pages and include you've already created, but you can't actively create new pages from the SaaS CMS.  This is possible with other generators, but I'd like to stick with Jekyll due to your maturity and ecosystem. 

e.g. the site I'm maintaining is a mix of content maintained by techies directly in the jekyll git and content written and uploaded by less technical people in contentful. contentful triggers the redeploy on [netlify.com](https://www.netlify.com/)  via  webhook when a new article was published.    

Overall a great stack, just that we have to rely on a somewhat unmaintained hacky ""page generator"" jekyll plugin to stitch it together. 
",2018-05-09T07:29:15Z,4006139
105,jekyll/jekyll,316610337,387790643,"One more suggestions (I'd be happy to help along) - The Jekyll Documention is fantastic and outstanding. But, what's wrong (or better how to make it more awesome)?

Currently the documentation is part of the jekyll repo <https://github.com/jekyll/jekyll/tree/master/docs> and the layouts, styling, build scripts etc. are part of the documentation too - it's all mixed together. What's wrong with that?

I'd say with its own dedicated markdown files in a repo it would be easier to contribute / edit / retarget etc. and with a ""proper"" docu (remote) theme  in its own repo it would be easy to reuse and change the theme too.   A while ago (dare I say years) I started to ""clean up"" as an example the documentation to use ""plain vanilla"" markdown files in the manuscripts (book/documentation) format. See the Hyde Press Bookshelf live <http://hydepress.github.io> and all the source repos <https://github.com/hydepress>. For the suggestion for Jekyll 4.0 it would just be a  new theme repo and a new docs repo (with markdown only). Would be great to see some booklets / guides / tutorials that could be made easily from the new docs repo using some (selected) pages etc.   Let me (us) know what you think. ",2018-05-09T16:06:10Z,53281
106,jekyll/jekyll,316610337,387794045,"I'd like an additional YAML file that defines site-wide variables so other programs can generate the file safely (instead of appending to `_config.yml`).

It could be better if we can have something like `{% include vars.yml %}` in `_config.yml` but I don't think it's necessary. Just allow loading extra variables from another file, even if the file name is hard-coded, would be enough.

A typical use case would be when I write a custom script that pre-processes some file and show in the information generated site with something like `{{ site.my_preprocessor.info }}`. My current approach of appending to `_config.yml` would pollute the file. With another file enabled, I can keep my configuration file safe by ignoring the dedicated variable YML in `.gitignore`.",2018-05-09T16:17:35Z,7273074
107,jekyll/jekyll,316610337,387878154,Make internal linking easier. Like generating a JSON of internals links including anchors.,2018-05-09T21:16:03Z,29690918
108,jekyll/jekyll,316610337,388162247,"@iBug If I understand correctly, you could do this sort of thing using two (or more) config files. You can use this [build command flag](https://jekyllrb.com/docs/configuration/#build-command-options) to specify multiple config files:

```
--config FILE1[,FILE2,...]
```

> Settings in later files override settings in earlier files.

Granted, this option is only available as a command line flag, so it wouldn't work if you're using GitHub Pages to build your site.",2018-05-10T19:38:38Z,4627002
109,jekyll/jekyll,316610337,388232674,@letrastudio Thanks for the response. Command line flags is surely not a problem for me because I use Travis CI to build my site.,2018-05-11T01:29:04Z,7273074
110,jekyll/jekyll,316610337,388521708,"The ability to specify that collection items with the attribute `published: false` should still have their YAML data exposed in the backend `site.my_collection` object. Presently, setting `published: false` makes the data entirely inaccessible, even on the backend.",2018-05-12T01:44:25Z,6486411
111,jekyll/jekyll,316610337,389244841,"Would love to see a standardized implementation of a service worker built into Jekyll utilizing the methods in [The offline cookbook](https://jakearchibald.com/2014/offline-cookbook/). A way of configuring it so that you could easily designate what the 'shell app' is so that those files load from offline cache first, and then designating dynamic content (such as blog posts) that load from network first and only cache so many entries (since caching 100 blog posts is ridiculous).

I realize these things can be done now, but finding a way to configure it to work perfectly with Jekyll would be amazing and save a lot of people the struggle of figuring out how to properly implement a service worker for it.",2018-05-15T17:16:44Z,18449360
112,jekyll/jekyll,316610337,389414999,I would be grateful if you could resolve #6410.,2018-05-16T06:57:17Z,1114146
113,jekyll/jekyll,316610337,389420714,It would be really beneficial for Jekyllers around the world if you guys could implement some sort of semi-automatic generation on `.yml` files based on a folder structure. This would be a massive improvement and a great way to document distributed services. ,2018-05-16T07:21:50Z,39187473
114,jekyll/jekyll,316610337,389716084,"I would like some nice to have plugin improvements:

- Include all enabled plugins in site.plugins, including those available from the Gemfile.
- Something like `{% if plugin ""foo"" %}...` for code is that is conditional on having certain plugins available. 
- Don't parse these conditional blocks unless the plugin is available, so code inside doesn't crash if the plugin isn't available.

",2018-05-17T01:30:47Z,163478
115,jekyll/jekyll,316610337,389749422,"> Include all enabled plugins in site.plugins, including those available from the Gemfile.

Can you add a bit more detail here? How is this different from the current state?",2018-05-17T05:31:54Z,251545
116,jekyll/jekyll,316610337,389797224,"> Don't parse these conditional blocks unless the plugin is available, so code inside doesn't crash if the plugin isn't available.

@alzeih This cannot be implemented because Liquid is parsed and rendered by the `liquid` gem. Jekyll only provides the means to use and extend Liquid constructs for use in template files.",2018-05-17T09:02:39Z,12479464
117,jekyll/jekyll,316610337,389803714,">> Include all enabled plugins in site.plugins, including those available from the Gemfile.

> Can you add a bit more detail here? How is this different from the current state?

Plugins using the group `:jekyll_plugins` in a `Gemfile` are not visible in liquid as `site.plugins`. Only plugins in the `_config.yml` are visible in `site.plugins`. 

This is useful for conditional behaviour like
```
  {% if site.plugins contains ""foo"" %}
     ...
  {% endif %}
```",2018-05-17T09:25:01Z,163478
118,jekyll/jekyll,316610337,389805827,"> @alzeih This cannot be implemented because Liquid is parsed and rendered by the liquid gem. Jekyll only provides the means to use and extend Liquid constructs for use in template files.

Ah, I see.

This mentions using partials as a workaround: https://github.com/Shopify/liquid/issues/704#issuecomment-188321757 

Having all plugins available in `site.plugins` would allow for this.

",2018-05-17T09:32:06Z,163478
119,jekyll/jekyll,316610337,389873052,"> > Don't parse these conditional blocks unless the plugin is available, so code inside doesn't crash if the plugin isn't available.
>
> @alzeih This cannot be implemented because Liquid is parsed and rendered by the liquid gem. Jekyll only provides the means to use and extend Liquid constructs for use in template files.

This would be such a useful feature, I wonder if we could find a workaround. Maybe instead of using the regular `{% if %}` tag, we create a new tag that acts like the `{% raw %}` tag (so Liquid does not parse the contents), and then _Jekyll_ will render the contents if the plugin is installed.

This is just an ideas list. Let’s not let implementation details get in the way at this stage. We are programmers; _anything_ is possible.",2018-05-17T13:50:17Z,251545
120,jekyll/jekyll,316610337,389876958,"@pathawks Wow!! That's an awesome idea!!!
We can easily create a new tag that is a combination of both `raw` and `if` tags... 
:heart: :tada: ",2018-05-17T14:01:13Z,12479464
121,jekyll/jekyll,316610337,389986861,"What do you think for [SanitizeHelper](https://stackoverflow.com/questions/50340509/escape-javascript-with-jekyll-redcarpet), @ashmaroli and @pathawks ",2018-05-17T19:46:20Z,24360355
122,jekyll/jekyll,316610337,390288255,"Because Jekyll is focused to create mostly frontend-only sites... it would be awesome to have a better way to organize our `Javascript` in partials and get an `assets/main.min.js` as a result.

This way we could place all our partials in a `js_dir` (defaults could be maybe `<source>/_js`) and specify the output style with a style option in the `_config.yml` file like:

```YAML
javascript:
    style: compressed
```

I know we have this with [CoffeScript](https://jekyllrb.com/docs/assets/#coffeescript), but [it is now fading away and `ES6` rising and rising](https://trends.google.com/trends/explore?date=today%205-y&q=coffescript,ES6) due to the fact that [`ES6` is an excellent modern CoffeeScript alternative](https://robots.thoughtbot.com/replace-coffeescript-with-es6).

# TL;DR

It'll be very cool to see **GitHub Pages sites with minified ES6 created from well structured javascript partials** ❤️ . 
",2018-05-18T18:09:36Z,24572406
123,jekyll/jekyll,316610337,391164424,"I would like to see a plugin that lets you loop through any directory's files. This could let you list, display, or include the files in the directory. This might look like

```
{% assign files = directory: 'some/path/to/stuff' %}
{% for file in files %}
  {{file.name}}
{% endfor %}

{% for file in files %}
  {% include_relative file.path %}
{% endfor %}
```",2018-05-22T22:45:37Z,421960
124,jekyll/jekyll,316610337,391171987,"@merlinpatt You can already do this by looping over `site.static_files`

I wrote up [an example of doing this to build an image gallery](https://mademistakes.com/til/static-files/), but it could be adapted to access any static files.",2018-05-22T23:23:19Z,1376749
125,jekyll/jekyll,316610337,391175752,"@mmistakes The issue with that is that the files have to then be outputted into the main site.

My main use case here is that I would like to be able to loop over files in a directory that won't be outputted or accessible to the public.

For example, I have a slide deck split into various parts, which I then use `include_relative` to put back in the main file.

### Current method - repeated includes

Directory structure

```
react/
  react_includes/
    part1.html
    part2.html
    ...
  index.html
```

react/index.html

```
{% include_relative react_includes/part1.html %}
{% include_relative react_includes/part2.html %}
...
```

_config.yml

```
exclude: ['react/react_includes']
```

### Preferred method - loop over directory

But I have a lot of parts, and add to it / move it around a lot, so I'd rather do something like this:

```
{% assign files = directory: 'react_includes/' %}
{% for file in files %}
  {% include_relative file.path %}
{% endfor %}
```

By using this method, I could reorganize my files, add new ones, or delete them and I wouldn't have to change the slide deck itself.",2018-05-22T23:46:11Z,421960
126,jekyll/jekyll,316610337,391220248,"@tyler-insight, I have a PR [open][] for this in Minima, the default theme you get if you run `jekyll new`. In that case, I based my cache off of `lib/site_template`, although I probably should go through the Cookbook to make sure I've covered all bases.

[open]: https://github.com/jekyll/minima/pull/234",2018-05-23T04:56:37Z,1534882
127,jekyll/jekyll,316610337,393325414,"[I proposed this in the old Jekyll 4 wishlist thread](https://github.com/jekyll/jekyll/issues/5307#issuecomment-358385811), but it'd be really nice to be able to have schemas for data, collections, frontmatter, etc. ",2018-05-30T21:28:06Z,72919
128,jekyll/jekyll,316610337,394292023,"I'd like to see Multiple Outputs baked in so for each .md file we can output the usual html as well as say amp and json files too. For example:  `layout: page, page-amp, page-json` referencing `page.html`, `page-amp.html`, and `page.json.json` layout files respectively.

It might also be nice to to be able to specify to output the same html page at different locations/paths.",2018-06-04T09:28:36Z,12131727
129,jekyll/jekyll,316610337,396959302,"My two biggest wishlist items give two new degrees of freedom/flexibility:

1. Multiple Outputs, see: #3041
2. Multiple Content Sections, see: #246

Currently setting out to implement manually! 😨",2018-06-13T14:32:50Z,30758921
130,jekyll/jekyll,316610337,397028665,"User comments please, filterable and maybe with optional moderation.",2018-06-13T17:55:39Z,6044
131,jekyll/jekyll,316610337,397043183,"@asierra You can use Disqus or some similar software for that, it's way outside the scope of Jekyll as a _static_ site generator.",2018-06-13T18:40:35Z,10137
132,jekyll/jekyll,316610337,398275126,"Could you consider supporting multi-language & language switching by processing files in po format, or by some other means? ",2018-06-19T05:04:27Z,36896360
133,jekyll/jekyll,316610337,398303328,"Hello Jekyll team!

I love working with collections, but the current option `output: true|false` enforces generation of either all or none of the pages. Having more control about which item of the collection is output would be appreciated.

The current way of putting `published: false` suppresses output, but also makes the document inaccessible (it does not show up while looping through the collection).

Thank you for the good work so far!",2018-06-19T07:33:12Z,925528
134,jekyll/jekyll,316610337,399762508,"I'd say: **making it easier to filter content by date**.

At the moment, my understanding is posts/collection `date` field is coerced into a Ruby `Date` object. Comparison with `where_exp` is impossible without workaround (casting a variable in `_config.yml`). There is a bit of background about this in #6581.

```
{% assign max_date = '2018-06-24' | date %}
{% assign posts = site.posts | where_exp: 'p', 'p.date < max_date' %}
{% for post in posts %}
  ...
{% endfor %}
```

or

```
{% assign posts = site.posts | where_exp: 'p', 'p.date < ""2018-06-24""' %}
{% for post in posts %}
  ...
{% endfor %}
```

I see this from a Jekyll user standpoint and I don't know whether this sits better as a `where_exp` logic or as a `date` operator logic. 

Thanks for making Jekyll such a nice tool :-)

----

A +1 comment on @geraldb proposal: I find remote data to be quite useful to avoid monolithic Jekyll setup. My use case would be to reference sub-projects on a main Jekyll websites, from remote JSON file and Atom feed.",2018-06-24T14:54:13Z,138627
135,jekyll/jekyll,316610337,400253497,"Oh, and: the only thing that keeps me continually considering whether I should switch to Hugo is speed. Can we find ways to make Jekyll:
* Much faster at generating the site? (why is Hugo so many times faster? - IDK)
* Much better at doing it incrementally? (perhaps with manual dependency hints)",2018-06-26T10:04:27Z,30758921
136,jekyll/jekyll,316610337,400266532,@Convincible Hugo is much faster because it's written in natively compiled Go rather than interpreted Ruby. We're not planning to change that.,2018-06-26T10:55:57Z,10137
137,jekyll/jekyll,316610337,400398025,"@oe Ah, fair enough – but then, within the confines of interpreted Ruby, speed is on my wishlist! Mainly an issue when you get to 1,000+ page blogs/sites... I'd love to keep using Jekyll even here and beyond.",2018-06-26T17:28:57Z,30758921
138,jekyll/jekyll,316610337,400453666,"Speaking of Hugo … something like Shortcodes ([in Hugo](https://gohugo.io/content-management/shortcodes/#readout) or [in WordPress](https://codex.wordpress.org/Shortcode)) would writing Markdown easier to me, because I can defer HTML constructs to another file (for example, generating HTML from JSON via a given partial, but denote where it should show up in my Markdown).

Downside in my experience is, that the HTML looks broken once the plugin for the shortcode is removed … (because the shortcode is interpreted as text).

So Jekyll should offer _hooks_ for shortcodes. The implementation should happen in plugins.",2018-06-26T20:37:06Z,3097194
139,jekyll/jekyll,316610337,400456299,"Oh, and this wishlist reminded me on an „argument” I had with @pathawks over in https://github.com/jekyll/jekyll-sitemap/issues/88#issuecomment-361295866 a couple of months ago.

Would be handy to have a way to support internationalisation! (Including [hreflang](https://developer.mozilla.org/de/docs/Web/HTML/Element/link)s for SEO purposes)",2018-06-26T20:45:46Z,3097194
140,jekyll/jekyll,316610337,403271910,"Add the ability to place the post and its files in the same folder.
```
_posts
├─┬ 2018-07-08-post
│ ├── 2018-07-08-post.md
│ ├── img.png
│ ├── img2.jpg
│ └── file.pdf
└─┬ 2018-07-08-post2
  ├── index.md
  ├── img.png
  └── document.doc
```",2018-07-08T08:31:22Z,2528139
141,jekyll/jekyll,316610337,403422656,@Harrix Why can't you do this at present?,2018-07-09T09:45:24Z,30758921
142,jekyll/jekyll,316610337,403427196,"@Convincible At present, static files within a `_posts` directory are ignored..",2018-07-09T10:01:40Z,12479464
143,jekyll/jekyll,316610337,403454731,"@Harrix FYI there's a plugin you can use that allows you to place static files in the same folder as the post.

https://nhoizey.github.io/jekyll-postfiles/",2018-07-09T11:58:42Z,1376749
144,jekyll/jekyll,316610337,403996610,"@oe @Convincible On the hardware side of things, when there is a complaint about Jekyll taking a long time to generate pages, one could respond by saying “get a processor with more cores”. But from what I understand, Jekyll does not seem to leverage well multi-core processing when it comes to generating files. It’s a petty since taking advantage of many cores could decrease page generation by 4-8X considering that many personal computers these days have processors with about that many cores/threads. 

So leveraging the multi-thread capabilities of modern processors when generating pages is the suggestion.",2018-07-10T23:24:14Z,7410291
145,jekyll/jekyll,316610337,404034566,"> So leveraging the multi-thread capabilities of modern processors when generating pages is the suggestion.

@mbrav We can't leverage multiple cores because Ruby (the language Jekyll is written in), itself doesn't leverage them.",2018-07-11T03:35:56Z,12479464
146,jekyll/jekyll,316610337,404035287,"@ashmaroli Ruby certainly does support [threads](https://ruby-doc.org/core-2.3.0/Thread.html). The trick is making sure that we only render things in parallel that **do not** depend on each other, and how can you know that two pages do not interfere with each other without rendering them?",2018-07-11T03:41:59Z,251545
147,jekyll/jekyll,316610337,404035661,"@pathawks Yes, Ruby does support threads. But because of the `Global Interpreter Lock (GIL)`, threads in Ruby are never actually run in parallel,..

*edit: I meant never run simultaneously..*",2018-07-11T03:44:39Z,12479464
148,jekyll/jekyll,316610337,404036202,"> threads in Ruby are never actually run in parallel

TIL 😭:sob::sob:",2018-07-11T03:48:32Z,251545
149,jekyll/jekyll,316610337,404036651,"What if we made templates pure, or at least atomic?",2018-07-11T03:52:00Z,927220
150,jekyll/jekyll,316610337,404036758,@pathawks what's an example where two pages depend on each other? I can see this being an issue for includes or layouts but pages themselves should be independent of each other,2018-07-11T03:52:54Z,421960
151,jekyll/jekyll,316610337,404036779,"> templates pure, or at least atomic

Do elaborate further @nickmccurdy ",2018-07-11T03:53:04Z,12479464
152,jekyll/jekyll,316610337,404036986,"@merlinpatt For example, an `index.md` could have a `{% link random-page.md %}`",2018-07-11T03:54:30Z,12479464
153,jekyll/jekyll,316610337,405090985,Proper i18n support. There are many plug-ins but none of them is supported by GitHub Pages.,2018-07-15T13:08:57Z,28626829
154,jekyll/jekyll,316610337,405106851,@mohsenkhanpour https://github.com/jekyll/jekyll/issues/6948#issuecomment-386162200,2018-07-15T17:49:58Z,24360355
155,jekyll/jekyll,316610337,406811511,"PWA support out of the box. 
There are some Jekyll plug-ins that do this already and their workaround and implemention is dead simple.

These are my arguments on why to add this feature:
1. Jekyll is already doing great on many factors on benchmark tests:
This is the Lighthouse score for a fresh website with these characteristics:

    Built with Jekyll
    Uses Hyde as a theme
    Hosted on GitHub Pages

<img width=""841"" alt=""github-pages-lighthouse"" src=""https://user-images.githubusercontent.com/28626829/43038964-a12cb7f2-8d38-11e8-9f97-da4b3b7c718c.png"">

As you can see Jekyll is already doing great in Performance, Accessibility, and SEO with 99, 95, 100 out of 100 respectively. But it is not doing good in PWA. 

2. PWA is a new web standard accepted by many international web organizations ( Read more: [W3C](https://www.w3.org/TR/appmanifest/), [MDN](https://developer.mozilla.org/en-US/docs/Web/Manifest)).
I personally think with a new major release, Jekyll has to standardize itself.
It would be a huge advantage for a static site generator to support the latest methods available.

3. Replying to @pathawks question regarding the looks: PWA let's you assign values like `theme_color`, `orientation`, `icons`, `display`, and `background_color` for example:
```
""background_color"": ""red""
 \\ This value repeats what is already available in the site’s CSS, but can be used by browsers to draw the background color of a shortcut when the manifest is available before the stylesheet has loaded. 

""dir"": ""rtl"",
""lang"": ""ar"",
 \\ i18n stuff.

""display"": ""standalone""
 \\ This hides the browser's user-agent.

""orientation"": ""portrait-primary""
 \\ It's clear what it does IMHO.

""theme_color"": ""aliceblue""
 \\ Defines the default theme color for an application. This sometimes affects how the OS displays the site (e.g., on Android's task switcher, the theme color surrounds the site). 
```
PWA is useful and these are only values related to the looks.

4. It won't break Jekyll. It is totally compatible.
5. Some of the information related to `manifest.json` are already available in `_config.yml`, e.g. `""name""` , `""start_url""`, `""description""`.
6. It is easy to implement. We can automate a task which reads `_config.yml` and if there is something like the below code it generates the `manifest.json` at the root of the published `_site`:
```
PWA:
   name: something
   theme_color: aliceblue
```
Workaround suggestions:
Give this a read: [Using a Service Worker with Jekyll](https://jamesiv.es/jekyll/amp/2017/05/09/serviceworkers-with-jekyll.html)",2018-07-21T17:30:35Z,28626829
156,jekyll/jekyll,316610337,406812195,@mohsenkhanpour What would a Jekyll site look like as a Progressive Web App? Can you elaborate?,2018-07-21T17:42:55Z,251545
157,jekyll/jekyll,316610337,406816602,@pathawks I updated the initial comment/request with my thoughts on why this is a good thing to implement in the upcoming Jekyll 4.0 with some reference on how it affects the looks.,2018-07-21T18:56:33Z,28626829
158,jekyll/jekyll,316610337,406817981,To benefit from PWA features I'd recommend the use of [jekyll-pwa](https://github.com/lavas-project/jekyll-pwa ) plugin  based on workbox 3 by Google. You'll score 100/100 in LightHouse if you add a manifest.json.,2018-07-21T19:19:17Z,103008
159,jekyll/jekyll,316610337,406820149,"@DirtyF [jekyll-pwa](https://github.com/lavas-project/jekyll-pwa) is more of a Service Worker thingy than `manifest.json` thingy. 😂 
I am talking about vanilla `manifest.json` and not Service Workers.",2018-07-21T19:59:28Z,28626829
160,jekyll/jekyll,316610337,406854122,"The ability to render includes only once. This [question on Stack](https://stackoverflow.com/questions/30523270/jekyll-generate-an-include-once-and-include-it-to-all-pages) demonstrates the idea. To demonstrate why this is a great idea, here is a scenario that I have: say you want to have a tag cloud in every one of your posts. You have `tag-cloud.html` in your `_includes`, which allows you to generate a tag cloud on every post so the user can have a better time navigating the site. Instead of generating a tag cloud for *every* post, it will be nice to have the ability to generate an include *only once* and then have it included in every post. In my case, this could decrease the generation of my blog from 1 minute to 4 seconds. Curent solutions include plugins and hacks, but this does not apear to be hard to implement natively in Jekyll.",2018-07-22T09:54:26Z,7410291
161,jekyll/jekyll,316610337,406866903,"@mbrav The plugin mentioned in the StackOverflow you linked to, `jekyll-include-cache` is the best option available out there.
However, there is a related proposal for getting similar support in Core: #7108 ..
..and another (very distantly) related proposal at #7136 

You may *subscribe* to the above PRs to stay notified about any developments on the proposals",2018-07-22T13:31:53Z,12479464
162,jekyll/jekyll,316610337,412993142,"I was reading over the thoughts about Open Collective in the blog post [HERE](https://jekyllrb.com/news/2018/08/01/jekyll-sponsoring/)

Moving towards a means to maintain the list of plugins better might be to move those lists to yaml files and/or individual yaml files.

If each entry in a section of the list here: [Available Plugins](https://jekyllrb.com/docs/plugins/#available-plugins)
Then we can make it easier for maintenance. You would simply add a yaml file in the appropriate location that corresponds to your section. The file would have the fields needed to be listed. This would be `name`, `url`, `description`... 
Part of why that helps things is no merge conflicts even become possible. Refactoring that page becomes less impacted by people adding to the lists, and we can add in additional fields, like `repository_url` that can be used to track the repository of the project in question. When a new breaking change comes into a PR, we can potentially use a bot/script to pull in all those files and if the repository url exists, that it checks for a given pattern (and ambitiously, opens a PR for an automated fix). 

All this can potentially also be achieved with a yaml file per section, but we dont gain as much in terms of potential merge conflicts, but this is a fairly straightforward data set at that point, so this might not even be a concern. Opening hundreds of yaml files does have the potential of adding on time to generate the site. 

Side note: This also makes it extremely easy to keep the list alphabetically ordered.

I know this is not exactly a solution to the idea: ""Create a comprehensive official plugin and theme directory site"" but it does cause a shift in how we store the data we already have thats more inline with recommendations Jekyll users would be used to. It also makes it a lot easier to pull that data out into some other repository to make a dedicated official plugin directory whenever that task might be taken on.

EDIT: Apologies for my poor wording....I regret not having proofed this better. ",2018-08-14T19:47:01Z,10980567
163,jekyll/jekyll,316610337,414676728,"I would love to see better environment support.

**Update:** See also https://github.com/jekyll/jekyll/issues/6948#issuecomment-415073688

_For example site.url:_ Right now site.url has an implicit env pattern that is hard coded for development ([Source](https://github.com/jekyll/jekyll/pull/6270)). This is very implizit, hard to understand and remember and not extendable. #5142

I would love to see this extended. Example:

```yaml
url:
  production: 'https://www.example.com'
  development: 'http://www.example.test' # this could have the a fallback like today, see ""source"" above
  staging: 'https://staging.example.com' # this would be possible
```

The pattern could be:
When jekyll calls a variable, it checks for `<var-name>.<env-name>`. The fallback is `<var-name>`. So it does not matter if I configure just `url` order `url.<env-name>`.

(This is a copy of https://github.com/jekyll/jekyll/issues/5307#issuecomment-360737818)",2018-08-21T13:38:09Z,111561
164,jekyll/jekyll,316610337,414783157,"@tordans You can set up all kinds of fallbacks by using multiple config files, as I mentioned in [this comment](https://github.com/jekyll/jekyll/issues/6948#issuecomment-388162247).

I've come across this use case myself — I use `_config.yml` for production values, and an additional `_config_dev.yml` for development overrides. When building locally I run:

```
jekyll serve --config _config.yml,_config_dev.yml
```
",2018-08-21T18:52:21Z,4627002
165,jekyll/jekyll,316610337,415056402,"Is anyone working on i18n support? I noticed @oe's enthusiasm for it, and [a lot of interest](https://github.com/jekyll/jekyll/issues/6948#issuecomment-386162200). I think I can make a significant contribution at the proposal/spec stage.

I’ve been working on a multilingual Jekyll site, and I’ve come up with a pretty good plugin-free solution! It’s up at [openhousemacau.com](https://www.openhousemacau.com), hosted on GitHub Pages with Jekyll 3.7.3. It’s still in active development, but the i18n part is fully functional, and can support any number of locales (not just the two it has now).

It’s quite a complete solution, featuring sane (DRY) content management with fallbacks between locales, string translation, and date format localization. It even supports permalink localization, though we ended up not using it for this site. It works by using:

- Parallel collections for everything (`_posts` and `_posts_zh`, for example)
- A `locales` key in `_config.yml`, set up in a similar way to `collections`
- [Scope path glob patterns](https://jekyllrb.com/docs/configuration/#glob-patterns-in-front-matter-defaults) in front matter defaults
- Localized strings in `site.data`
- A liquid include for localizing dates
- Another liquid include for setting up a bunch of relationships and variables

The biggest issue is that the last liquid include gets called a lot (especially when looping through documents), so site builds get slow quickly. While I can’t open source the entire site, I could open source the i18n system by making a demo site, if there’s interest.

Throughout development I’ve kept Jekyll 4.0 in mind, trying to think about how a native solution could eliminate the problems I’ve come up against. I think I’m pretty qualified at this point to submit a complete top-to-bottom specification proposal for discussion. Should I do it?",2018-08-22T14:41:07Z,4627002
166,jekyll/jekyll,316610337,415061483,"So This is just a thought, I can try and make an issue for it with more details and thought through functionality, but I have noticed a potential gap in functionality.

If you use the `_drafts` folder, or have a post that is `unpublished` in your `_posts` folder, then you still need to move things around for your content, like images, to not get published to the generated site.

Since a lot of users use `/assets/imgs/` folders for the beginning position of images associated to a post, I was thinking that we could make a new special key to use in posts that you add to the frontmatter.

The key would be something like `asset_paths:` or `asset_path:` that we merge to an array, and it can define any of the assets that should not be published if they do not match up to any other posts that are being published. 

So if I have the following in my frontmatter:
```yaml
asset_path: /assets/imgs/vacation/July/4
```

Then the post when in the drafts folder would not copy over any of that folder located at ` /assets/imgs/vacation/July/4`.
However, if a post is in `_posts` that contains the same frontmatter key/value, then it would get published as it was included in a post that is getting published. This could be done for individual file levels too. 

The last part would be that unless included and exclusively used for an unpublished post, that the folder/files in question would perform /be acted on exactly as they already are now, where they will be included or excluded based on the `_config.yml` settings.",2018-08-22T14:55:18Z,10980567
167,jekyll/jekyll,316610337,415073688,"In Reply to https://github.com/jekyll/jekyll/issues/6948#issuecomment-414783157

@letrastudio I am aware of that solution, but it only causes new problems.

**The Szenario is:**
I have a config-files with 100 lines of config.
One, maybe two line of this config needs to be changed for different environments.

With your solution, I need to duplicate 99 lines in three files (dev, staging, production) and manually sync them every time I make a change. **This is bound to fail!** 

People will forget to sync the changes and the main idea of a staging system (to behave like the production system) will be lost.

**Other solutions would be**
a. The one I describe in https://github.com/jekyll/jekyll/issues/6948#issuecomment-414783157
b. Allowing more than one config, one general-config-file for all env, and one config-file for each env. This is similar to the way rails does it. The env-config will overwrite the general-config in case that is needed.
c. Your solution, but with the ability to ""include"" or ""reference"" other config files inside the one I call with the jekyll build command. For this I would say ""build with staging-config"" and inside staging config ""use this 1 line and include/use all other lines from this other config-file"".
d. …?",2018-08-22T15:28:17Z,111561
168,jekyll/jekyll,316610337,415078430,"@tordans You don't have to duplicate 99 lines for each environment specific config file. When you specify multiple config files at build Jekyll daisy chains them from left to right.

Meaning you could have a production config with all settings, and then for your staging and dev configs just add the lines that are unique or change. Jekyll will use everything from the first config, and override whatever comes next. For example, say you have

**_config.yml** (production settings with all configs)

```
title: My Awesome Site
description: Powered by Jekyll
url: https://prod-domain.com
```

and then a staging specific config (e.g. `_config.staging.yml`

```
url: https://staging-domain.com
```

When you run `bundle exec jekyll build` you'd get the prod `url`, if you run:

```
bundle exec jekyll build --config _config.yml,_config.staging.yml
```

You'd get the staging `url` along with all the other variables set in `_config.yml`.
",2018-08-22T15:41:41Z,1376749
169,jekyll/jekyll,316610337,415087105,"Wanted to add a comment about what I noted in [Comment](https://github.com/jekyll/jekyll/issues/6948#issuecomment-415061483) that this is essentially a solution for what @Harrix requested, but a bit more segregated to the standards that we expect blogs to keep for folder/file structure. Instead of making a change to store the content with the post, this just allows you to specify it directly. ",2018-08-22T16:06:19Z,10980567
170,jekyll/jekyll,316610337,415097650,"@letrastudio So the more I've looked into implementing i18n in Core, the less feasible it became practically. There's huge performance drawbacks, even for sites that only use one language, the time spent rewriting a huge part of the Core internals would very likely be better spent working on other features, and we've garnered feedback from some big users of Jekyll that for them, performance improvements are more important than a huge new breaking feature. Obviously this shouldn't influence our decision as a project all that much, but it did provide us with insight into why a full i18n implementation in Core wouldn't be feasible. What I think we __could__ do is provide baseline APIs upon which a (maybe officially supported) plugin could operate. If you have any such ideas, feel free to shoot them my direction!",2018-08-22T16:39:29Z,10137
171,jekyll/jekyll,316610337,415246857,"@oe I understand, and I feel the same way about performance. However, I am not completely discouraged! Wouldn't performance impact depend largely on how the feature is designed? I _think_ that my proposed solution wouldn't significantly impact performance for single-language sites (and it wouldn't break Jekyll 3 sites at all). Big disclaimer though: I am not a Ruby programmer so I could be completely wrong.

I think I'll write up my ideas anyway and post them as a new issue — even if they're not feasible for Core, I hope they'll contribute to the discussion, maybe even inspire some intrepid plugin developer. Or they might help spark some ideas for the baseline APIs you suggested (such low-level discussion is probably a bit out of my league).

And still, I've found that i18n is quite doable with vanilla Jekyll 3. IMHO content management can actually be better than with existing plugins, though it's a bit complex to work with on the Liquid side. Some small tweaks to make that easier might be worth implementing; I'd happily settle for making it feel like less of a hack.",2018-08-23T01:30:32Z,4627002
172,jekyll/jekyll,316610337,419266066,What are the plans for (Dart) Sass support in Jekyll 4.0 now that the Ruby implementation is deprecated? I'm _very_ interested to hear the responses from the maintainers. ,2018-09-06T22:48:01Z,95672
173,jekyll/jekyll,316610337,419272431,Could libsass be used instead?,2018-09-06T23:21:28Z,927220
174,jekyll/jekyll,316610337,419290933,🤷‍♂️ ,2018-09-07T01:23:47Z,95672
175,jekyll/jekyll,316610337,419360450,"@kevinSuttle We'll rely on sassc implementation, that is currently [removing dependency to Ruby sass](https://github.com/sass/sassc-ruby/pull/85)",2018-09-07T08:14:51Z,103008
176,jekyll/jekyll,316610337,419414624,Awesome. Thank you @DirtyF!,2018-09-07T11:45:46Z,95672
177,jekyll/jekyll,316610337,419921211,"It would be great if `include` would take preference over `exclude`. So that you could `exclude: *`, and include `include: [ homepage.html ]` and do fast iterations over a single page. This is also how e.g. `rsync` and other unix tools operate, and more useful than the (current) other way around. This was also reported in ticket https://github.com/jekyll/jekyll/issues/4791 but stalled as it would be a breaking change. Seems like 4.0 would be a perfect moment. What do you say @envygeeks? (cced as he was planning to work on it)",2018-09-10T13:56:37Z,26752
178,jekyll/jekyll,316610337,419928579,"> It would be great if `include` would take preference over `exclude`

@kvz I've submitted a PR that should handle what you're looking for, as a side-effect.. Would you be able to give that branch a test-run..?
```ruby
# Gemfile
gem 'jekyll', git: 'https://github.com/jekyll/jekyll.git', ref: 'refs/pull/7188/head'
```
Feedback invited at the [PR's url](https://github.com/jekyll/jekyll/pull/7188)",2018-09-10T14:16:31Z,12479464
179,jekyll/jekyll,316610337,420374494,"@kvz:
> It would be great if `include` would take preference over `exclude`

@ashmaroli:
> I've submitted a PR that should handle what you're looking for

Just wanted to share that we confirmed that [that PR](https://github.com/jekyll/jekyll/pull/7188#issuecomment-420336100) solves the `include` vs `exclude` issue, so it would be very neat if that could land in 4.0 💟 

",2018-09-11T18:30:48Z,26752
180,jekyll/jekyll,316610337,422570076,"(I suggest this without understanding what would actually be involved in doing it)

I'd love to see performance work done regarding [glob patterns in frontmatter defaults](https://jekyllrb.com/docs/configuration/front-matter-defaults/#glob-patterns-in-front-matter-defaults). I think it's a tremendously useful feature, but it's hampered by the fact that the more useful it is for a project, the more detrimental it is to build times.

I use this feature heavily in scenarios where I have a collection of documents which represents different variations of a type of content. For example, in the [Sentry marketing resources](https://sentry.io/_/resources/) ""resources"" is a collection, which is broken up into folders representing podcasts, videos, and pdfs. Each of those folders has it's own defaults that are appropriate for the given medium.

In smaller collections there isn't much of an issue, but on our docs site, where a collection is 250+ pages, using a wildcard added 2s to a .8s build.",2018-09-18T21:55:03Z,72919
181,jekyll/jekyll,316610337,422573399,"Thanks to everyone for the feedback, that's already a lot of food on our plate, we won't be able to implement *all* your requests, but we'll definitively do our best to fulfill your expectations. We're currently focusing on performance. As we will need your help, we might pick some priority issues and offer rewards thanks to our Open Collective sponsors. Stay tuned.",2018-09-18T22:06:40Z,103008
182,jekyll/jekyll,304422453,304422453,"Hi,

Jekyll syntax highlighting is broken with Twig. Consider the following code block containing a perfectly valid Twig syntax:

```
{% highlight twig %}
{% raw %}
{% set a = 'b' %}
{% endraw %}
{% endhighlight %}
```

It outputs the following HTML:

```
<code class=""language-twig"" data-lang=""twig"">
    <span class=""cp"">{%</span> 
    <span class=""k"">set</span> 
    <span class=""nv"">a</span> 
    <span class=""err"">=</span>
    <span class=""s1"">'b'</span> 
    <span class=""cp"">%}</span>
</code>
```

Notice the **err** class attributed to the equal sign.

## Steps to reproduce

* Follow the official quick-start guide: https://jekyllrb.com/docs/quickstart/
* Replace the content of the post created by the installation with this:

```
---
layout: post
title:  ""Welcome to Jekyll!""
categories: jekyll update
---

{% highlight twig %}
{% raw %}
{% set a = 'b' %}
{% endraw %}
{% endhighlight %}

```

",2018-03-12T15:34:02Z,125664
183,jekyll/jekyll,304422453,372357726,"How is the resulting output if you were to use triple-backticks instead?

    ```twig
    {% raw %}
      {% set a = 'b' %}
    {% endraw %}
    ```",2018-03-12T15:47:43Z,12479464
184,jekyll/jekyll,304422453,372361724,"@ashmaroli, same.",2018-03-12T15:58:33Z,125664
185,jekyll/jekyll,304422453,372362713,So you see the issue is not with Jekyll but rather with Rouge that `highlight` and the triple-backticks block uses to highlight code.,2018-03-12T16:01:19Z,12479464
186,jekyll/jekyll,304422453,372374996,"No, the problem also happens with the `{% highlight %}` syntax.",2018-03-12T16:35:16Z,125664
187,jekyll/jekyll,304422453,372378503,"Yes, that's because the `highlight` tag uses `Rouge` for syntax-highlighting by default
https://github.com/jekyll/jekyll/blob/86d86258a8bc912c906776d8f2f9a58b3d376519/lib/jekyll/tags/highlight.rb#L38-L46

If you want to use `pygments` instead of `rouge` as your site's highlighter, add the following to your `_config.yml`:
```yml
highlighter: pygments
```",2018-03-12T16:44:29Z,12479464
188,jekyll/jekyll,304422453,372381451,This sounds like an issue with https://github.com/jneen/rouge rather than Jekyll. Jekyll has no knowledge of syntax of any language.,2018-03-12T16:52:22Z,251545
189,jekyll/jekyll,304422453,372383292,"@pathawks, why did you close this? It's not fixed and I'm not the one explicitely using Rouge. The maintainers of the project are using a dependency that is buggy, they should take care of this. What do you want me to do? I don't even know what Rouge is!",2018-03-12T16:57:14Z,125664
190,jekyll/jekyll,304422453,372397547,"> What do you want me to do? I don't even know what Rouge is!

I’ve provided a link to the repository so that you can open an issue there and explain the problem you are having.

I do not know what “Twig” is, so it would not make sense for me to be the one to explain what needs changing in Rogue.

There is nothing in Jekyll’s code that can be changed to fix this issue; the fix will have to come from Rogue.

Here is a link to Rogue’s Twig lexar: https://github.com/jneen/rouge/blob/master/lib/rouge/lexers/twig.rb",2018-03-12T17:37:54Z,251545
191,jekyll/jekyll,304422453,372400893,"> There is nothing in Jekyll’s code that can be changed to fix this issue; the fix will have to come from Rogue.

That's not my point. You are the one using Rouge to implement a feature that you advertise explitely on your docs! That's your responsibility to take care of things that don't work as expected in the dependencies of your project. As a consumer of your product, I expect it to work as advertised:

https://jekyllrb.com/docs/templates/#code-snippet-highlighting

You are advertising syntax highlighting, you are supposed to deliver! And if you don't, you are supposed to take care of whatever is needed to have your product work as expected.


",2018-03-12T17:45:35Z,125664
192,jekyll/jekyll,304422453,372406665,"@ericmorand We're sorry that you're facing issues while using Jekyll.
I agree that you as an end-user shouldn't concern yourself about bugs in dependencies.

One of the maintainers will get in touch with the developers at Rouge and sort things out for you.
",2018-03-12T18:01:54Z,12479464
193,jekyll/jekyll,304422453,372407373,"@ashmaroli, thanks a lot. I already created an issue, maybe a maintainer could comment on it if the issue is not clear enough or if some things can be added:

https://github.com/jneen/rouge/issues/881",2018-03-12T18:03:52Z,125664
194,jekyll/jekyll,304422453,372407740,"@ericmorand Please take a step back and consider that this is an entirely volunteer-run project. We're not contractually obligated to work on every bug and answer every question, seeing as we simply don't have enough resources. So our apologies if some things take too long, or don't end up happening, but it's wrong to blame the maintainers for this.",2018-03-12T18:04:57Z,10137
195,jekyll/jekyll,304422453,372412152,"@oe, I totally understand that and i can relate: maintaining open source projects is very time-consuming. But I'm not having that discussion because I want to see that bug fixed immediately, to be honest this is a low priority bug even by my own standards. My point is that if, when a bug happens, maintainers blame a dependency, close the issue and ask for the reporter to open an issue elsewhere, that could go that way:

Jekyll

Oh, sorry this is a bug with Rouge, go open an issue there.

 -> Rouge

Oh, sorry this is a bug with Ruby, go open an issue there.

-> Ruby

Oh, sorry this is a bug with GCC, go open an issue there.

...and so on. At one point, my issue will be invalid because I won't even know what and how to report the bug. Already, Rouge maintainers could totally close my issue as invalid because I'm giving a way to reproduce that imply Jekyll - it would be legitimate for them to say that it's a Jekyll bug or that they want a reproducible example using only Rouge.",2018-03-12T18:18:17Z,125664
196,jekyll/jekyll,304422453,372413773,"@ericmorand The way I see it, this issue could pop up in any software that uses Rouge, and is therefore not specific to Jekyll. I agree however that it could have been better communicated before it was closed, sorry about that.",2018-03-12T18:23:12Z,10137
197,jekyll/jekyll,304422453,372415507,"@ericmorand I re-opened the issue to convey that we have not abandoned this report straight away..
Do know that I've kept a tab on the issue-ticket at Rouge and will follow its proceedings as time permits..",2018-03-12T18:28:30Z,12479464
198,jekyll/jekyll,304422453,401623176,"
This issue has been automatically marked as stale because it has not been commented on for at least two months.

The resources of the Jekyll team are limited, and so we are asking for your help.

If this is a **bug** and you can still reproduce this error on the <code>3.3-stable</code> or <code>master</code> branch, please reply with all of the information you have about it in order to keep the issue open.

If this is a **feature request**, please consider building it first as a plugin. Jekyll 3 introduced [hooks](http://jekyllrb.com/docs/plugins/#hooks) which provide convenient access points throughout the Jekyll build pipeline whereby most needs can be fulfilled. If this is something that cannot be built as a plugin, then please provide more information about why in order to keep this issue open.

This issue will automatically be closed in two months if no further activity occurs. Thank you for all your contributions.
",2018-07-01T18:11:11Z,6166343
199,mojolicious/mojo,340595516,340595516,"* Mojolicious version: 7.85
* Perl version: 5.28.0
* Operating system: macos

### Steps to reproduce the behavior
Just just the delay helper. 

### Expected behavior
A deprecation warning with *explaination* what to do, alternative options. How can i replace the helper code with what ? 

### Actual behavior
Warnings that delay helper is DEPRECATED. Problem: warning without alternative options. User just panics that the code will stop to work one day.
",2018-07-12T11:19:18Z,847081
200,mojolicious/mojo,340595516,404479635,Please use our official support channels. https://mojolicious.org/perldoc#SUPPORT,2018-07-12T11:22:56Z,30094
201,mojolicious/mojo,340595516,404490887,"This discussion is meant to either change/introduce documentation or code. You can mark it as a suggestion, but closing it is not very helpful. For instance, Net::OAuth2::AuthorizationServer tests upon installation produce many of these warnings, just as an example. It affects other CPAN authors along the way, and web searches should end up here, i suppose. ",2018-07-12T12:10:59Z,847081
202,mojolicious/mojo,340595516,404493264,"You did not propose any specific changes that could be discussed here. Until you reach that stage, please use our official support channels for discussions. https://mojolicious.org/perldoc/Mojolicious/Guides/Contributing#CONTRIBUTING-DOCUMENTATION",2018-07-12T12:21:03Z,30094
203,mojolicious/mojo,340189202,340189202,"* Mojolicious version: 7.87
* Perl version: v5.26.1
* Operating system: ubuntu

### Steps to reproduce the behavior
perl -MMojo::File -le 'Mojo::File->new(q(a.txt))->spurt(qq(\x{100}))'

### Expected behavior
a.txt written with 2 bytes c4 80

### Actual behavior
get error: Wide character in syswrite at /home/dk/perl5/perlbrew/perls/perl-5.26.1/lib/5.26.1/x86_64-linux/IO/Handle.pm line 483.
a.txt is zero bytes

",2018-07-11T10:47:31Z,50233
204,mojolicious/mojo,340189202,404127826,"Mojo::File::spurt is documented to take bytes, but ```qq(\x{100})``` is a character. Encode it to your favourite encoding first.",2018-07-11T10:51:15Z,604816
205,mojolicious/mojo,340189202,404129197,">     Write all data at once to the file.

Doesn't seem like bytes to me",2018-07-11T10:56:06Z,50233
206,mojolicious/mojo,340189202,404129409,"https://metacpan.org/pod/Mojo::File#spurt
```$path = $path->spurt($bytes);```",2018-07-11T10:56:53Z,604816
207,mojolicious/mojo,340189202,404133396,"Yes, write all bytes from the scalar.",2018-07-11T11:14:26Z,50233
208,mojolicious/mojo,340189202,404133702,"Wow, just ""closed the issue"" as a non-issue? That's mature :D",2018-07-11T11:15:50Z,50233
209,mojolicious/mojo,340189202,404135563,The answer @pink-mist gave was 100% correct. So this is not a bug. If you require additional help please use our official support channels. https://mojolicious.org/perldoc#SUPPORT,2018-07-11T11:24:15Z,30094
210,mojolicious/mojo,340189202,404162820,"Well it might be correct, but the documentation ""Write all data at once to the file"" doesn't say anything about the byte/character distinction, and $bytes can be understood as whatever bytes are there in the scalar, which I would expect. So that's at least a case to either fix the documentation or the attitude.",2018-07-11T13:09:23Z,50233
211,mojolicious/mojo,340189202,404168986,"I don't see this discussion end constructively, therefore i'm locking this issue.",2018-07-11T13:28:51Z,30094
212,jasmine/jasmine,333693227,333693227,"326 / 600 tests executed, then hangs.

```
19 06 2018 15:54:19.133:WARN [Firefox 52.0.0 (Windows 8.1 0.0.0)]: Disconnected (1 times)
Firefox 52.0.0 (Windows 8.1 0.0.0) ERROR
  Disconnected
```

Tried with Chrome - same thing. 

```
19 06 2018 16:11:34.594:WARN [Chrome 67.0.3396 (Windows 8.1 0.0.0)]: Disconnected (1 times)
Chrome 67.0.3396 (Windows 8.1 0.0.0) ERROR
  Disconnected
```


Current config:

```js
    browsers: ['Firefox'],
    singleRun: true,
    autoWatch: false,
    /**
     * How long will Karma wait for a message from a browser before disconnecting from it (in ms).
     */
    browserNoActivityTimeout: 100000,
    /**
     * maximum number of tries a browser will attempt to reconnect in the case of a disconnection
     */
    browserDisconnectTolerance: 2,
    browserDisconnectTimeout: 210000,
    captureTimeout: 60000,
    retryLimit: 5,
    concurrency: Infinity,

    frameworks: ['browserify', 'jasmine'],

    client: {
      mocha: {
        /*
        @see https://github.com/karma-runner/karma-phantomjs-launcher/issues/126
        */
        timeout: 20000
      }
    },

    reporters: ['mocha', 'junit', 'html', 'coverage'],

    plugins: [
      'karma-junit-reporter',
      'karma-jasmine',
      'karma-chrome-launcher',
      'karma-firefox-launcher',
      'karma-html-reporter',
      'karma-browserify',
      'karma-coverage',
      'karma-babel-preprocessor',
      'karma-mocha-reporter',
    ],
```

hardware:
* CPU: Intel Xeon E5-1620 v3 @ 3.5GHz
* RAM: 128 GB
* GPU: Nvidia Quadro K2200 4GB RAM

environment:
* OS:  win 8.1 x64
* node: v10.0.0
* npm: 5.6.0

modules:
```js
    ""jasmine-core"": ""2.99.1"",
    ""karma"": ""2.0.3"",
    ""socket.io"": ""1.4.5""
    ""jasmine-ajax"": ""3.4.0"",
    ""karma-babel-preprocessor"": ""6.0.1"",
    ""karma-browserify"": ""5.3.0"",
    ""karma-chrome-launcher"": ""2.0.0"",
    ""karma-coverage"": ""1.1.1"",
    ""karma-firefox-launcher"": ""1.1.0"",
    ""karma-html-reporter"": ""0.2.7"",
    ""karma-jasmine"": ""1.1.2"",
    ""karma-junit-reporter"": ""1.2.0"",
    ""karma-mocha-reporter"": ""2.2.0"",
    ""babel-polyfill"": ""6.23.0"",
    ""babel-preset-es2015"": ""6.16.0"",
    ""babel-preset-es2016"": ""6.22.0"",
    ""babelify"": ""7.3.0"",
    ""browserify"": ""14.4.0"",
    ""browserify-istanbul"": ""2.0.0"",
    ""eslint"": ""3.19.0"",
    ""eslint-config-airbnb-base"": ""11.1.3"",
    ""eslint-plugin-import"": ""2.2.0"",
    ""gulp"": ""3.9.1"",
    ""gulp-babel"": ""6.1.2"",
    ""gulp-clean"": ""0.3.2"",
    ""gulp-connect"": ""5.0.0"",
    ""gulp-notify"": ""2.2.0"",
    ""gulp-param"": ""1.0.3"",
    ""gulp-replace"": ""0.5.4"",
    ""gulp-sonar"": ""3.0.1"",
    ""gulp-sourcemaps"": ""1.6.0"",
    ""gulp-util"": ""3.0.7"",
    ""isparta"": ""4.0.0"",
    ""istanbul"": ""0.4.5"",
    ""node-notifier"": ""4.6.1"",
    ""proxyquireify"": ""3.2.1"",
    ""shelljs"": ""0.7.8"",
    ""vinyl-buffer"": ""1.0.0"",
    ""vinyl-source-stream"": ""1.1.0"",
    ""watchify"": ""3.9.0"",
```


Notable observations:
* Browsers tab saturates 1 CPU core usage (~100%)
* Memory usage keeps growing, 
  * from about 100mb to 750 in 1 minute on FF
  * from about 60 to 180 on Chrome
* tab is unresponsive entirely until gets killed by karma or crashes with crash report (only on FF).
* karma guys say it's not their fault
* jasmine guys say it's not their fault
* karma-jasmine guys say it's not their fault

related issues:
https://github.com/jasmine/jasmine/issues/1327
https://github.com/karma-runner/karma/issues/1788
https://github.com/karma-runner/karma/issues/762
https://github.com/karma-runner/karma-phantomjs-launcher/issues/126
https://github.com/karma-runner/karma-phantomjs-launcher/issues/55
https://github.com/karma-runner/karma-phantomjs-launcher/issues/126

May this help those who come after us fare better :wink:",2018-06-19T14:18:37Z,5939063
213,jasmine/jasmine,333693227,398585348,"If your suite always failing in the same spot, it might have something to do with your tests themselves, and how they interact with the various test frameworks you're using. Take a look at [Steve's comment](https://github.com/jasmine/jasmine/issues/1327#issuecomment-379574146) in #1327 for some options on how to troubleshoot this error. I would also suggest updating to Jasmine 3.1 (and corresponding karma-jasmine updates) ensure this hasn't already been fixed in the 3.0 or later releases.

We (Jasmine maintainers) haven't been able to reproduce this case, as it seems to require a fairly complex async suite.

Hope this helps. Thanks for using Jasmine!",2018-06-20T00:20:39Z,512724
214,jasmine/jasmine,333693227,398721311,"@slackersoft 

> Take a look at Steve's comment in #1327 for some options on how to troubleshoot this error. 

Thank you for your response. I have linked that issue too, and I have read and tried everything that would not require node version change or break our build pipeline that was proposed in all of the posts on all of those issues.

My post is not really a call for help, I have spent sufficient time here to be convinced that it's not worth more investment from our side. Instead I hope for this to be a good overview for others, and perhaps it will help others some of the time that I have invested.

> We (Jasmine maintainers) haven't been able to reproduce this case, as it seems to require a fairly complex async suite.

Your choice of words is not very accurate. You didn't try to reproduce this issue. Your reasoning is on point though - outlined configuration is much too complex and has only started failing after certain tests were added (added tests run 100% fine, but others don't and the whole thing freezes).",2018-06-20T11:47:34Z,5939063
215,jasmine/jasmine,333693227,398938639,"> My post is not really a call for help,

Then it doesn't really sound like it belongs as an issue here. We want to keep this space clear for bugs and issues in Jasmine itself.

I would also urge you to be careful and considerate when telling others what they have and haven't done. We've invested a significant amount of time in trying to eliminate various timing issues with Jasmine, but this one seems to come primarily from the suite being executed.

Thanks for using Jasmine!",2018-06-21T00:27:24Z,512724
216,gitextensions/gitextensions,249811390,249811390,"This PR is created in response to https://github.com/jbialobr/gitextensions/pull/3
It addresses some issues that bother me in https://github.com/jbialobr/gitextensions/pull/3 
These are:
1. WorkingPathProvider depends on Directory, AppSettings and GitModule static methods. These are widely used across the app. I would not like to pass them to various objects through a multiparam constructor.

2. Turning static methods into instance methods: https://github.com/jbialobr/gitextensions/pull/3/files#diff-05956b8e9b35344894a0ffe609cf32b9L376 

3. Incompleteness of arranging the exterior. Unconfigured calls return default(T), which may cause false positive tests results. I would prefer to be notified that unit test referenced not configured call. 

It does not address the separation CommandLineArgs concern https://github.com/jbialobr/gitextensions/pull/3/files#diff-05956b8e9b35344894a0ffe609cf32b9L376
This is only a sketch - there is no proper class per file and file per project separation. I am interested in discussing about the general idea of this solution.

The general idea of this solution is:
1. Wrap static methods in Gateway classes.
2. Gateway classes are singletons, they provide default implementation through public virtual members.
3. In unit tests Gateway classes are substituted in Setup method.
4. After the arrange phase unconfigured calls on substituted Gateways are not allowed.

",2017-08-12T13:11:55Z,1003909
217,gitextensions/gitextensions,249811390,321984432,@jbialobr Do you have a link to the original discussion describing the underlying problem that both you and @RussKie were trying to solve?,2017-08-12T14:23:36Z,1408396
218,gitextensions/gitextensions,249811390,321987243,"Sure, I added this PR https://github.com/gitextensions/gitextensions/pull/3881
@RussKie would like to have tests for it and made a sketch PR https://github.com/jbialobr/gitextensions/pull/3 showing how he would see this done.",2017-08-12T15:12:47Z,1003909
219,gitextensions/gitextensions,249811390,322019136,"To set the context.
The current codebase features heavy use of inheritance and static methods.

OOP in general and inheritance in particular were all the rage about 10-15-20 years ago. As a concept it does have benefits of reuse, but it also carries downsides of maintenance overhead, complicated abstraction layers, difficult testability, side effects and surprises in behavioural changes... 
These days stateless interface-based implementations have become quite popular (especially with raise of functional languages) where each class has a single responsibility (or as close to that as possible) and gets all the dependencies injected via a constructor. This allows to write much cleaner and maintainable code, easier to unit test (as opposed to inheritance based classes where you would more likely be writing integration tests) and easier to alter behavior by substituting dependencies.
Another benefit of using interface-based architecture that you could plug in IoC container (e.g. autofac).

Over the years I have experience both sides, I was a big proponent of OOP but I have learnt the hard way that it makes it much harder to maintain such code, especially in large projects.

As I observed, a lot of bugs stem from implementations with overloaded concerns and lack of unit tests. As a result the code is quite confusing, inflexible and hard maintain.
In my opinion, one of the ways we could start reducing the tech debt and improving the codebase - start extracting separate concerns into individual classes and compose them as required. Gradually we would be able migrate most of the codebase. 

Static methods have their use too, but as any tool they have to be used appropriately.",2017-08-13T03:15:56Z,4403806
220,gitextensions/gitextensions,249811390,322023873,"My main concern with DI is it often leads to the use of mocking in tests. Mocks have a strong tendency to deviate from the documented pre- and post-conditions of the original interfaces, leading to the following problems:

1. Tests are no longer executing against reality. Sometimes it's close, but it means tests are prone to both false positives (failed due to incorrect mock) and false negatives (passed even though the situation intended to be tested will fail at runtime).
2. Mocks are effectively a very clumsy DSL where logic must be manually reimplemented. Every mock necessarily reduces the long-term maintainability of the project, as well as the ability of developers to make behavior-preserving changes.
3. Mocks are not a substitute for properly covered tests. You can test code without mocks, or you can test code both with and without mocks, but mocks never save you from writing tests you would otherwise have to write.

As long as there is a clear understanding that mocks are only to be considered in the total absence of the ability to write a test using another approach, then I don't have a particular problem with either using or not using DI.",2017-08-13T06:00:49Z,1408396
221,gitextensions/gitextensions,249811390,322024064,"My early testing recommendation would be to enable codecov.io (or similar) service as soon as possible. Code coverage tooling which is tightly integrated with both the code review process on GitHub as well as the supporting CI system enables developers to write mid-level tests (against API boundaries) without sacrificing overall implementation coverage. Tests written at this level, which I call ""units of behavior tests"", have these advantages:

1. They are typically easy to understand
2. They demonstrate the intended use of APIs at module boundaries (public and internal), including edge cases
3. Failure of the test is typically easy to relate to a sequence of steps a user could take to produce incorrect behavior at runtime
4. They are typically easier to design in relation to anticipated use of the application
5. When coupled with code coverage tooling, they help reveal edge cases in API usage scenarios

Note that I consider user input/interaction, file system and network activity, and operating system interaction as API boundaries that can be tested against. APIs internal to the application can be added to this set in any number of ways.",2017-08-13T06:06:54Z,1408396
222,gitextensions/gitextensions,249811390,322029529,"> My main concern with DI is it often leads to the use of mocking in tests. Mocks have a strong tendency to deviate from the documented pre- and post-conditions of the original interfaces

I am not sure if completely I understand your point.
If each class has a single responsibility (SR), then DI should not add any ambiguity - for each use case you set dependencies to imitate boundary conditions, but you **only** exercise and validate functionality of the class under test.
Any changes to dependencies should not have any profound effect because the class is built against a contract and not a particular implementation. Certainly a degree of coupling is possible, but it typically it will be lose coupling.
Also a SR class (usually) define a limited number of methods (accessible via an interface). As a result you test the public surface. Generally there is a limited need to test internal or private methods (due to their absence).
Another benefit of smaller classes with SR - you know exactly what each class depends on. For example right now `GitModule` spans across 3,400+ lines of code carrying hundreds of methods. And we inject this monstrosity to classes which may need to access only one method from the lot...

If a class has multiple responsibilities and/or initialises its own concrete dependencies then it is virtually impossible to perform unit testing - instead you will be writing integration tests. 
Any changes to dependencies with high degree of probability will necessitate changes to the class, because there is a tight coupling to concrete implementations.


Wrt: code coverage - whilst it is a very useful metric, it shouldn't be viewed as an absolute. 100% CC can be very misleading.
Personally when I write tests I usually strive for 100% coverage taking in consideration various edge cases. I use NCrunch, it helps me to visualise what cases I may have missed.",2017-08-13T08:40:28Z,4403806
223,gitextensions/gitextensions,249811390,322029719,">Mocks have a strong tendency to deviate from the documented pre- and post-conditions of the original interfaces

Yes, that's true. Regardless of the tools we use there is always a way to use them inappropriately.
I like to be pragmatic. For me it means: achieve maximum benefits at reasonable cost with decent safety.
Weighing those factors we have to take into account the existing code base and resources we have.
Thank you for pointing to codecov.io - it should help to find which pre-condition we lack in the test set (though 100% codecov coverage does not imply 100% pre-conditions coverage). Implementing codecov into GitExt project will show us all the most obvious pre-conditions against which should we write  ""units of behavior tests"".
Then we have to write these tests, to do it, we have to weigh the factors I mentioned and decide whether to mock or to configure the exterior. I think we can't make this decision at a general level. Besides, one approach does not have to fit all concerns.
Here we have 3 dependencies, how would you see ideally written [this test](https://github.com/gitextensions/gitextensions/pull/3898/files#diff-67c45fbdcf3a92b53649d84f9f401c3eR26)?
```csharp
        public void ReturnsRecentDirectory_if_RecentDirectory_IsValidGitWorkingDir()
         {
             //arange
             DirectoryGateway.Inst.CurrentDirectory.Returns(string.Empty);  <-- mocked
             _ext.StartWithRecentWorkingDir = true; <-- mocked
             string unitTestRecentWorkingDir = ""unitTestRecentWorkingDir"";
             _ext.RecentWorkingDir = unitTestRecentWorkingDir;
             GitModuleGateway.Inst.IsValidGitWorkingDir(unitTestRecentWorkingDir).
                 Returns(true);<-- mocked
             //act
             string workingDir = _workingPathProvider.GetWorkingDir(new string[0]);
             //assert
             workingDir.Should().Be(unitTestRecentWorkingDir);
         }
```
",2017-08-13T08:45:23Z,1003909
224,gitextensions/gitextensions,249811390,322050109,"@russkie. Thoughts of mef vs aurofac?

https://stackoverflow.com/questions/15572302/difference-between-mef-and-ioc-containers-like-unity-autofac-smap-ninject-w

On Sun, Aug 13, 2017, 11:47 AM Janusz Białobrzewski <
notifications@github.com> wrote:

> *@jbialobr* commented on this pull request.
> ------------------------------
>
> In GitExtensions/StaticDependencyInjection.cs
> <https://github.com/gitextensions/gitextensions/pull/3898#discussion_r132845408>
> :
>
> > +    }
> +
> +    public interface IInstanceFactory
> +    {
> +        T CreateInstance<T>() where T : class, new();
> +    }
> +
> +    public class NewInstanceFactory : IInstanceFactory
> +    {
> +        public T CreateInstance<T>() where T : class, new()
> +        {
> +            return new T();
> +        }
> +    }
> +
> +    public class StaticDI
>
> Thanks, it is always better to be more thorough.
> It seems that I false assumed that the 3 dependencies are
> static/stateless. The all three services depends on IFileSystem. Changing
> this assumption, my concerns change as follow:
>
>    1. The dependencies should be passed down the tree through a
>    multiparam constructor starting from the Main proc.
>    2. We should create a separate service GitWorkingDirService to which
>    FindGitWorkingDir and IsValidGitWorkingDir should be moved.
>    3. It should be handled at the Nsubstitute level.
>
> I would not introduce the ServiceLocator as it is in conflict with 1)
>
> I will update this PR to comply with the updated assumption.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/gitextensions/gitextensions/pull/3898#discussion_r132845408>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ADdhsRmqOyDUF78xYjEAV-J2TwxGgYuxks5sXxqYgaJpZM4O1aRu>
> .
>
",2017-08-13T15:54:44Z,3629489
225,gitextensions/gitextensions,249811390,322060262,">>My main concern with DI is it often leads to the use of mocking in tests. Mocks have a strong tendency to deviate from the documented pre- and post-conditions of the original interfaces

>I am not sure if completely I understand your point.

One problem I see in using mocks/spies is it often tests the implementation instead of expected behavior.
A typical unit test arranges the before state (internal and external), performs an action and asserts that the after state is equal to the expected state. Because mocked methods do not modify any external state we can not verify the after state, only asserting the expected sequence of calls on the dependencies. Changing the implementation breaks such an unit test. 
",2017-08-13T18:57:10Z,1003909
226,gitextensions/gitextensions,249811390,322075338,"> Thoughts of mef vs aurofac?

I haven't worked with MEF. Until you mentioned it I am not sure I was even acutely aware of it (though I may have heard the name). 

On paper MEF has benefits but as well it has a number of trade offs. 
To me, one of the trade offs is that it doesn't seem to be widely embraced by the industry, so taking a dependency on it carries an inherent risk coupling to a technology which may become instantly obsolete (like Silverlight).
This leads to another problem - MEF requires a lot of additional ceremony (decorations). In event we want to migrate from MEF we won't be able to do so easily. If we use IoC switching containers is relatively trivial task and doesn't require re-architecture the whole app. I've just migrated our big solution from Funq to Autofac within two days and the footprint of the change was minimal. 
Also from what I've read MEF seems to be opaque in how it handles types, where IoC may be more transparent.",2017-08-13T23:50:30Z,4403806
227,gitextensions/gitextensions,249811390,322075518,"> One problem I see in using mocks/spies is it often tests the implementation instead of expected behavior.
> A typical unit test arranges the before state (internal and external), performs an action and asserts that the after state is equal to the expected state. Because mocked methods do not modify any external state we can not verify the after state, only asserting the expected sequence of calls on the dependencies. Changing the implementation breaks such an unit test.

This is only a problem if your implementations are stateful.
I am a big proponent of stateless implementations (""pure"" methods as they call it in functional land). In stateless world you test behaviors which do not depend on state in any way.
",2017-08-13T23:55:00Z,4403806
228,gitextensions/gitextensions,249811390,322081292,"> To me, one of the trade offs is that it doesn't seem to be widely embraced by the industry

It's been the primary container for Visual Studio since 2010. Over the years it's received a lot of love to meet the performance demands of that application, though not all of those benefits have made their way to the normal distributions of it *yet*. I would classify it as having fewer features, but within the bounds of its capabilities excels in ease of distribution and being generally well-understood.",2017-08-14T01:27:28Z,1408396
229,gitextensions/gitextensions,249811390,322081554,"Yep. I'm developing a smartsheet to TFS MVC website. The website handles
the communication with smartsheet and it uses mef plugins to react to the
webhook calls from smartsheet.

On Sun, Aug 13, 2017, 9:27 PM Sam Harwell <notifications@github.com> wrote:

> To me, one of the trade offs is that it doesn't seem to be widely embraced
> by the industry
>
> It's been the primary container for Visual Studio since 2010. Over the
> years it's received a lot of love to meet the performance demands of that
> application, though not all of those benefits have made their way to the
> normal distributions of it *yet*. I would classify it as having fewer
> features, but within the bounds of its capabilities excels in ease of
> distribution and generally well-understood.
>
> —
> You are receiving this because you commented.
>
>
> Reply to this email directly, view it on GitHub
> <https://github.com/gitextensions/gitextensions/pull/3898#issuecomment-322081292>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ADdhsTGGLts65YkEkkC2d0bCYikelDQgks5sX6KCgaJpZM4O1aRu>
> .
>
",2017-08-14T01:30:41Z,3629489
230,gitextensions/gitextensions,249811390,322096168,">This is only a problem if your implementations are stateful.
I am a big proponent of stateless implementations (""pure"" methods as they call it in functional land). In stateless world you test behaviors which do not depend on state in any way.

But it is a real problem:

https://github.com/gitextensions/gitextensions/pull/3865/files#diff-dce953c015443e09345315dc6cdf8452R59

https://github.com/gitextensions/gitextensions/blob/0b9a310019965c8183599e9afa3a722df03ae1f4/UnitTests/GitCommandsTests/Remote/GitRemoteControllerTests.cs#L239

https://github.com/jbialobr/gitextensions/pull/3/files#diff-7ca5aacd7a7769eef82fb4b8f4d3d9ddR46",2017-08-14T04:13:47Z,1003909
231,gitextensions/gitextensions,249811390,322116105,"I'm not sure what you perceive as a problem in attached links, I'm sorry.

The classes under tests are stateless - they do not have any settable fields or properties (outside the container) nor they access any objects besides those passed via the container. The external dependencies are interface-based, and as such they are stateless too. So there are no side effects and there are external context mutations - so I can assert the correctness of my implementation.

The unit tests exercise my class implementations and check the flow of execution by checking how far each use case progresses and what methods within the method under test were called. 
There is no magic here :)

Of course there is an arrangement phase, you can't not have it. But the arrangement only sets a possible return from the underlying dependency (boundary conditions). My duty as a class developer write unit tests to account for these boundary conditions and facilitate stable predictable response.

 **EDIT:**
Just realised some classes may be accessing `AppSettings` which arguably belongs to the external context, but for the most part we can presume AppSettings to be immutable. 
They should be abstracted too.",2017-08-14T07:15:23Z,4403806
232,gitextensions/gitextensions,249811390,322117357,"> > To me, one of the trade offs is that it doesn't seem to be widely embraced by the industry
>
> It's been the primary container for Visual Studio since 2010. Over the years it's received a lot of love to meet the performance demands of that application, though not all of those benefits have made their way to the normal distributions of it yet. I would classify it as having fewer features, but within the bounds of its capabilities excels in ease of distribution and being generally well-understood.

My point that it is still contained within Microsoft.
Given that we know how Microsoft (and other big players) drop support for their products overnight I would be very cautious investing into tech stack which is difficult to deprecate.
We can certainly consider MEF for dealing with plugins but I feel very uneasy committing to it for the core of GE.",2017-08-14T07:23:29Z,4403806
233,gitextensions/gitextensions,249811390,322123092,">I'm not sure what you perceive as a problem in attached links, I'm sorry.

""... it often tests the implementation instead of expected behavior.""
The following test tests implementation.
```csharp
        public void SetRemoteState_should_call_ToggleRemoteState(string remoteName, bool remoteDisabled)
        {
            var sections = new List<IConfigSection> { new ConfigSection(""-remote.name1"", true), new ConfigSection(""remote.name2"", true) };
            _configFile.GetConfigSections().Returns(x => sections);

            _controller.ToggleRemoteState(remoteName, remoteDisabled);

            _configFile.Received(1).GetConfigSections();
            _module.Received(remoteDisabled ? 1 : 0).RemoveRemote(remoteName);
            _configFile.Received(remoteDisabled ? 0 : 1).RemoveConfigSection($""{GitRemoteController.DisabledSectionPrefix}{GitRemoteController.SectionRemote}.{remoteName}"");

            _configFile.Received(1).AddConfigSection(sections[remoteDisabled ? 1 : 0]);
            _configFile.Received(1).Save();
        }
    }
```
I would see the assert part along the lines of this:
```csharp
        public void ToggleRemoteState_ChangesDisabledValue(string remoteName, bool remoteDisabled)
        {
//arrange
            var sections = new List<IConfigSection> { new ConfigSection(""-remote.name1"", true), new ConfigSection(""remote.name2"", true) };
            _configFile.GetConfigSections().Returns(x => sections);
//act
            _controller.ToggleRemoteState(remoteName, remoteDisabled);
//assert
           _controler.GetRemote(remoteName).Disabled.Should().Be(!remoteDisabled);
        }
    }
```
This way we don't care about the implementation details, we only deal with pre and post conditions.
If the implementation changes from `remove then add` to `rename`, the first test will fail while the second will not.

>The external dependencies are interface-based, and as such they are stateless too.

Apparently we have a different understanding of ""stateless"". The config file holds a state. Invoking its methods changes this state.",2017-08-14T07:53:58Z,1003909
234,gitextensions/gitextensions,249811390,322159927,"> The following test tests implementation.

Yes, because as a class developer I need to ensure the order of execution to ensure the expected behavior.
As a class consumer you don't care about it - you expect by calling a method something happens and that something happens in expected predictable manner.
Any changes to my class behaviors should only really affect my class. And for a consumer of the class the changes should be transparent. 

For example, as a consumer by calling `_repo.GetRecord(id: 1)` I expect to get record with id=1 - whether the record is in a database, xml file or a remote service I don't care. If repo implementaion is migrated from a  database to a remote service or an xml file - it is an implementation detail for the repository implementation and I shouldn't change my implementation.
On the other hand, as a repository developer in its implementation I would probably want to make sure that the database connection is established before I attempt to interact with the database. Naturally in the repository tests I would expect to see something like `_db.Received(1).Open()`.

. . .

> Apparently we have a different understanding of ""stateless"". The config file holds a state. Invoking its methods changes this state.

Indeed we are :smile: 
As a consumer I interact with an interface - I don't know nor expected to care about the implementation detail of the instance given to me at runtime. By virtue of this the dependency is treated as stateless.

. . .

Btw `GitRemoteController` isn't a stateless implementation, it has a bunch of side effects and it maintains the state in `Remotes` list. So it is probably not the best example to use.

To sum it up:
We probably won't be able to make everything stateless, not from the get go anyway:
- the codebase isn't ready for it yet, and
- UI layer is typically stateful.

If we go with interface-based implementations it is much easier to write smaller SR implementations which can be easily tested. It is much easier to make implementations stateless too. 
You get benefits of declarative composition - it is easy to see objects connections and dependencies. As well as DI and substitutions - i.e. you could have Windows- and Linux-specific implementations without random `if running on Windows do this else do that`.

",2017-08-14T10:49:14Z,4403806
235,gitextensions/gitextensions,249811390,322166724,">Yes, because as a class developer I need to ensure the order of execution to ensure the expected behavior.

I see it totally different. As a class developer you need to ensure the expected behavior. The order of execution and the content of execution is subject of change in contrast to the expected behavior.
When you want to optimize your implementation you change it and run unit test to see if everything is ok.
When unit tests test the implementation you get tests failure. ",2017-08-14T11:29:19Z,1003909
236,gitextensions/gitextensions,249811390,322168805,"> Yes, because as a class developer I need to ensure the order of execution to ensure the expected behavior.

This view makes sense. I would lean towards agreeing with it¹ on paper on both technical merits and practicality merits. However, reality of developing over time tells a very different story.

This is not only not correct, but every time I have seen it followed it's had a disastrous impact on the project. The possibility of someone with this mindset working on tests is the reason I raised my original concern above.

¹ I was impressed with the approach joining another project that leveraged it. I only changed views after the problems were proven beyond any shadow of a doubt; fixing the tests was a long, hard road. But at the end every metric we came up with for measuring test quality was improved.",2017-08-14T11:42:11Z,1408396
237,gitextensions/gitextensions,249811390,322169847,">As a class consumer you don't care about it - you expect by calling a method something happens and that something happens in expected predictable manner.

I don't buy it. You write a method that calls:
...
 dep.Proc1; 
... 
dep.Proc2;
...
dep.Proc3;  

Then you write a unit test that ensures that `dep` received these calls in the exact order. This way you verify that you implemented it the way you think it should be implemented. However if the order should be Proc3, Proc2, Proc1 your unit test does not fail. Another thing is that reading a such specified assert you don't know what is the expected state after the method execution - you only know how test writer imagine it should be implemented. I think that tests should document expected behavior not implementation.",2017-08-14T11:48:22Z,1003909
238,gitextensions/gitextensions,249811390,322171827,"Ok, let consider this method https://github.com/gitextensions/gitextensions/blob/master/GitCommands/Remote/GitRemoteController.cs#L190-L205

```
        public string RemoveRemote(GitRemote remote)
        {
            if (remote == null)
            {
                throw new ArgumentNullException(nameof(remote));
            }

            if (!remote.Disabled)
            {
                return _module.RemoveRemote(remote.Name);
            }

            var sectionName = $""{DisabledSectionPrefix}{SectionRemote}.{remote.Name}"";
            _module.LocalConfigFile.RemoveConfigSection(sectionName, true);
            return string.Empty;
        }
```

1. Suppose there is a refactor gone wrong and a developer accidentally deletes ""!"" on line 197.
To ensure the correct handling of disabled remotes I would write a test something like: 

```
[Test]
public void Ensure_disabled_remotes_dont_get_removed() 
{
    // ...
    _module.DidNotReceive().RemoveRemote(Arg.Any<string>());
}
```

2. I want to ensure the correct section name is passed to the config manager.
In the test I can expect to make a call

```
[Test]
public void Ensure_removing_correct_section() 
{
    // ...
    _module.LocalConfigFile.Received(1).RemoveConfigSection(""...."", true|false);
}
```

How would you write tests to ensure the correct behavior? ",2017-08-14T11:59:53Z,4403806
239,gitextensions/gitextensions,249811390,322173149,"I would add a disabled remote to a configuration file, and then remove one of the other remotes. At the end of the test I verify that the disabled remote is still in the file.

If a test written in this form fails, there is no question about what it was testing or why the failure indicates bad behavior. There are many reasons why a test using the expected call sequence could fail. For example, if someone deletes the file because they thought they were removing the last item, the test passes even though the specific failure it was written to detect certainly occurred.",2017-08-14T12:07:16Z,1408396
240,gitextensions/gitextensions,249811390,322175234,"> I would add a disabled remote to a configuration file, and then remove one of the other remotes. At the end of the test I verify that the disabled remote is still in the file.

This won't be a unit test but a integration test since the class now needs to know how to write or read from a file. Which breaks SR principle.",2017-08-14T12:18:33Z,4403806
241,gitextensions/gitextensions,249811390,322176701,"```csharp
[Test]
public void Ensure_disabled_remotes_dont_get_removed() 
{
   //arragne
    _module.AddRemote(name);
   _module.DisableRemote(name);
//act
    _controller.RemoveRemote(name);
//assert
   assert(_module.GetRemote(name).Disabled == true);
}
```",2017-08-14T12:26:16Z,1003909
242,gitextensions/gitextensions,249811390,322179260,"If `_module` isn't an interface you are too writing integration tests...
testing this class as well how module class works

On 14/08/2017 10:26 PM, ""Janusz Białobrzewski"" <notifications@github.com>
wrote:

> [Test]
> public void Ensure_disabled_remotes_dont_get_removed()
> {
> //arragne
> _module.AddRemote(name);
> _module.DisableRemote(name);
> //act
> _module.RemoveRemote(name);
> //assert
> assert(_module.GetRemote(name).Disbaled == true);
> }
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/gitextensions/gitextensions/pull/3898#issuecomment-322176701>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AEMyXtMUnIlC8RAW-FInPnC9jRS9rtDeks5sYDzrgaJpZM4O1aRu>
> .
>
",2017-08-14T12:39:46Z,4403806
243,gitextensions/gitextensions,249811390,322199770,"> This won't be a unit test but a integration test since the class now needs to know how to write or read from a file. Which breaks SR principle.

I prefer to describe tests as ""fast""/""slow"", or as ""stable""/""unstable"". A test as I described would be characterized as ""fast"", ""stable"", and also particularly well-suited to detecting regressions related to a specific scenario. When you start describing tests as ""unit"" or ""integration"", it's easy to lose sight of the fundamental purpose of testing.

The fundamental purpose of testing is to detect cases where the observable behavior of an application deviates from the expected behavior for some user scenario. I find this condition is reasonably relaxed to detecting changes in behavior under a theoretically-reachable scenario.

""Unit"" tests are a sub-optimal approximation of this that resulted from several limitations:

1. It is difficult for developers to reason about the line-level behavior of high-level testing.
1. Code coverage tooling was incomplete and/or slow, such that it was not practical to adopt as part of a team code review process.
1. High-level testing tends to be slow, unreliable, and/or difficult to stand up (e.g. only runs under a specific non-standard system configuration, or requires specialized software installations).

Code coverage tooling now performs very well, and eliminates the problems typically associated with the first item. We found that by moving away from ""unit tests"" as a goal, we were able to write tests that better represented reality. The focus instead was on test speed, stability, and independence from system configuration. The result was something between what would normally be classified as ""unit"" or ""integration"" tests - we used fakes¹ to eliminate external dependencies, and tested primarily against well-defined module boundaries². When possible, we fed inputs to the application in the same way it would actually occur for a user:

1. When testing the behavior of a subsystem, we tested using the same API that other code would be using to drive that subsystem. Code which we *couldn't* cover with this approach often meant the code was dead code under real-world conditions.
1. When testing UI controls, we actually created the control and interacted with it as a user would (yes this can be fast and reliable).
1. When testing file system, we created files and folders and tested against them. We used ACLs or chmod to force access errors and held handles open to force sharing violations.

¹ Typically stub executables or our own implementations of interfaces in code
² We applied a `[ServiceInterface]` attribute to the boundary interface, which carried special requirements. These interfaces were required to be threading agnostic, safe for concurrent use without regard for the caller (new unknown callers could appear at will), idempotent, and only ever obtained from the single service container instance. Mocking was *forbidden* for any object which was not marked with `[ServiceInterface]`. For cases where one or more of these conditions could not be met, we used `[ServiceInterface(Restricted = true)]`, and required that the restriction (often a limitation on who is allowed to call it) be documented - use of this form was seen as technical debt, but used occasionally because the correct solution was too costly at the time.",2017-08-14T14:07:39Z,1408396
244,gitextensions/gitextensions,249811390,322358334,"> The order of execution and the content of execution is subject of change in contrast to the expected behavior.

This is a contradictory statement - the _order of execution_ is one of the key factors for _expected behaviour_.
And the _content of execution_ should not matter at all.",2017-08-15T02:13:59Z,4403806
245,gitextensions/gitextensions,249811390,322384024,"Thank you @sharwell for taking time and sharing your experience. I find your approach very pragmatic.
I think that at this point enough has been said and we should stop to write down the fundamental goals the tests should serve and the criteria they should meet. Only after that we can discuss how to achieve these goals and meet these criteria. 

These are mine:
**Goals:**
1. Ensure the expected behavior of the app. By saying expected behavior I mean that having given pre conditions the tested unit transforms the existing state into a state described by the post conditions. To be clear we should assert on the expected state not on the steps that lead to this state.
2. Document the expected behavior. 

**Criteria**
1. Independent from the environment - a test should arrange the environment in the way that makes it invariant regardless of changes made to the environment outside of the test.
2. Stable - a test should always produce the same result for the same starting conditions.
3. Easy to understand the pre and post conditions.",2017-08-15T06:00:15Z,1003909
246,gitextensions/gitextensions,249811390,322384442,">When testing UI controls, we actually created the control and interacted with it as a user would (yes this can be fast and reliable).

@sharwell Is there any publicly available project that uses this approach - I would love to see this in action.",2017-08-15T06:03:54Z,1003909
247,gitextensions/gitextensions,249811390,322403149,"@sharwell great write up, thanks.

It looks like we come from different schools of thought and practice. 
Like yours my position comes from practical experience solving real engineering challenges.

I'm part of a small team building RAD framework and reusable components where pretty much every component can be substituted by a consumer. We have a number of major and minor releases concurrently powering hundreds of apps in production, and we provide full runtime backwards compatibility.

We can assume very little about consumers, their habits and requirements. In turn we must provide a great deal of flexibility but ensure stability of the framework core and consumer applications (to a degree). This is specifically important in security-related areas.
All of this dictates how we architect and engineer the framework, how we deal with dependencies, how we deploy and how we test.
Until we re-engineered the codebase to consist of interface based stateless single responsibility classes we had a number of challenges meeting the requirements (including production incidents). We also couldn't reason about stability and predictability of provided behaviors.
Now we have 1,800+ .NET unit tests, hundreds of JS tests and probably another few thousand of BDD tests. Some tests, especially security-related, test the order of execution - without that we can't reason about predictability, nor we can release new versions or provide backwards compatibility.

Smaller interface-based single responsibility implementations allow us to achieve 
* isolation (all dependencies are injected), 
* testability (all dependencies are mocked), 
* stability (via predictability), 
* flexibility (consumers only concern themselves with replacing a specific implementation instead of worrying about inter-dependencies). 
And as a bonus - our APIs are easy to register in an arbitrary IoC container.",2017-08-15T07:59:22Z,4403806
248,gitextensions/gitextensions,249811390,322450186,"@RussKie Your last post really resonates with me. One of the goals I had when I was pushing so hard to switch away from mocks (on a former project) was based on the type of application being shipped to customers. As you are well aware, anything considered public API surface area has special requirements related to versioning. These requirements constrain the development team substantially, but are a great asset from the customers perspective. However, in an application where the final product is the application (as opposed to a publicly-consumable API), these same requirements *burden* the development team. I didn't like the fact that when we wanted to make a change to our implementation, we kept running into pain points (leading to delays) resembling those faced by developers trying to make changes to public APIs.

Another application (this time a library) I've contributed to over time is StringTemplate. While it has a few known extension points built in, it was never designed to be arbitrarily reconfigurable for end users. Over time it's led to a frustrating experience for a small subset of users trying to do arguably normal things, just things we didn't think of from the start. For example, by failing to abstract the file system (location and reading) away from the template loader, it's not possible to use *some* features of StringTemplate when the templates are stored e.g. in a database, or stored using a compression mechanism we didn't think of from the start. Better adherence to SRP would likely improve the long term durability and flexibility of this library.

It sounds to me like GitExtensions is unlikely to suffer from most of the problems related to reusable components, and more likely to benefit from a testing approach that focuses on application behavior.

> > When testing UI controls, we actually created the control and interacted with it as a user would (yes this can be fast and reliable).
>
> @sharwell Is there any publicly available project that uses this approach - I would love to see this in action.

Not to the extent of the one I've been referring to. I've started to add tests for UI related bug reports to PerfView, but these are quite limited and not particularly elegant currently.

When I built a system like this in the past, I started with the following:

1. Identify all mutable static state in the application. We will need a way to reset these to known default values between each test, ensuring that the outcome of any given test cannot influence the behavior of another (with a corollary that a test failure uniquely and deterministically identifies the culprit, which can then be executed in isolation to reproduce the failure):
    * Mutable `static` fields
    * File read and written
    * Registry keys/values read/written
2. Identify all asynchronous operations. We will need a way to ensure that a test is not considered complete while one of these is executing (or scheduled for later execution, in the case of timers or delays).
3. Identify external dependencies for which the concrete implementation is likely problematic for high-speed deterministic testing. We will need a way to substitute the behavior of these items during testing. The method of substitution must guarantee that during the execution of any given test, all executing code is using the same implementation (either the original or the substituted one, but never a mix and never multiple substitutes).
    * One obvious case is invocations of *git.exe* for many types of tests
    * Loading resources from a network

Once you address the above, most tests are still written against module interface boundaries. However, when actually testing UI behaviors it's not particularly challenging to write those tests against the controls. Most of the problems typically associated with testing UI behavior are really just failures to address one of the above underlying concerns.",2017-08-15T12:11:37Z,1408396
249,gitextensions/gitextensions,249811390,322455965,"I would go as far as proposing moving away from statics altogether.
There are merits for extensions, but like any tool they have to be used sparingly to be used correctly.",2017-08-15T12:43:10Z,4403806
250,gitextensions/gitextensions,249811390,323526732,"Erik Dietrich of NDepend writes insightful articles discussing different aspects of code architecture and design. This is one of them along the lines of this discussion, worth reading https://blog.ndepend.com/singleton-pattern-costs/",2017-08-19T14:34:37Z,4403806
251,gitextensions/gitextensions,249811390,323562594,"The article is somewhat related, but the single-instance services I would advocate for behave very differently from the classical singleton pattern. Most of the differences were created to directly address one or more costs listed in that article. ",2017-08-20T04:17:08Z,1408396
252,gitextensions/gitextensions,249811390,323570842,"Indeed, but it raises points how statics introduce side effects and hinder testabiltiy.",2017-08-20T08:15:49Z,4403806
253,gitextensions/gitextensions,249811390,323713364,">If `_module` isn't an interface you are too writing integration tests...
testing this class as well how module class works

I am happy to do that. I don't need unit tests, I need the app to be thoroughly tested. I expressed my needs here https://github.com/gitextensions/gitextensions/pull/3898#issuecomment-322384024
I don't want to say that asserting on implementation is always wrong. It can be helpful, but it should not be overused. I think it should be strictly limited and explicitly described when it can be used. I see to many corner cases in my mind when unit tests asserting on implementation go green and the app is not working as expected. There are even more cases I see when changing the implementation turns such unit tests into red ones. These of many assertions in a single test are the most likely to fail and the hardest to correct (I am ignoring here the fact that we should not have to correct tests for which the pre and post conditions have not changed). 
I don't mind writing tests that ensure a developer well understand description of the used interfaces and its operations. For example when resetting unstaged changes, I prefer to have a set of tests like:
  1. Ensure staged files are untouched
  2. Ensure ignored files are untouched
  3. etc.

to have a test like:

1. Ensure the `git reset --hard` is executed.


>>The order of execution and the content of execution is subject of change in contrast to the expected behavior.

>This is a contradictory statement - the order of execution is one of the key factors for expected behaviour. And the content of execution should not matter at all.

Perhaps my English skills are too poor to express it correctly. 
I prefer to verify that the chosen by a developer sequence of execution leads to the expected state than to verify that the chosen by a developer sequence of execution is the sequence he chose. 
The order is indeed one of the key factors but there may exist many orders that lead to the expected behavior. 

",2017-08-21T10:54:08Z,1003909
254,gitextensions/gitextensions,249811390,326885447,"Lets look at this test: https://github.com/gitextensions/gitextensions/commit/9fb22f58727408d73baaea03ecf6cb69a66f997a#diff-3a91b3e9edc1faa5190d69a2fec75400R81
It is a copy-paste from the implementation. It is really a [test double](https://en.wikipedia.org/wiki/Test_double). It doubles the implementation, it doubles the false assumption that the path separator is `\`. While unit tests are useful, they should not replace integration tests.
Nevertheless, the architecture should allow to write both kinds of tests. ",2017-09-04T07:23:00Z,1003909
255,gitextensions/gitextensions,249811390,640770442,"Locking this discussion for archival purposes, since I use it as a reference for an in-depth discussion from opposing viewpoints in other contexts.",2020-06-08T17:34:55Z,1408396
256,gitextensions/gitextensions,249811390,641020383,"Sam, I'd be interested in learning gained insights and outcomes of those discussions.",2020-06-09T04:27:35Z,4403806
257,rack/rack,318673916,318673916,This suggested addition is the result of a collaboration between @boazsegev of Iodine fame and myself. Both Agoo and Iodine will support these suggested additions. At this point I don't think any other servers are supporting WebSockets and SSE so this would set the standard for others to follow.,2018-04-28T22:41:04Z,118856
258,rack/rack,318673916,385656965,"Interesting.

I already support completely transparent asynchronous web sockets in [falcon](https://github.com/socketry/falcon). It uses `rack.hijack` just fine. An example is given here: https://github.com/socketry/async-websocket/blob/master/examples/chat/config.ru

Of course with this model supporting SSE is also trivial.

Regarding the event driven model which is being exposed via `rack.upgrade`: I don't think exposing an event driven API is a great way to make systems asynchronous. It's kind of limiting.

Regarding the overall design - careful thought needs to be given to how HTTP/2 fits into this picture. Ideally, SSE, WebSockets, and other ""interactive"" streams, transit via reading from the request body and writing to the response body. That's the model which I feel works best with HTTP/2, where each request/response is encapsulated into a full duplex stream.",2018-05-01T12:09:57Z,30030
259,rack/rack,318673916,385664722,"Bare in mind that `async-websocket` doesn't support web sockets over HTTP/2, this is something I'll investigate in the coming months.

This is also one of the challenges of implementing `rack.hijack` for HTTP/2. You need to be careful of what it means, exactly, to hijack a connection (or stream in the case of HTTP/2).",2018-05-01T12:57:24Z,30030
260,rack/rack,318673916,385668006,"I think the HTTP/2 layer is something that's handled only by the server. It shouldn't change the `rack.upgrade` semantics since it isn't one of the protocols / behaviors that should be handled by the application.

I think the idea is great because it decouples the application from any network logic.

While `hijack` solutions remove the server, changing the relationship from `network<=>server<=>application` to `network<=>application`, this approach keeps the application away from the network, keeping the initial (and desired) relationship intact.

**EDIT**:

I also published [a blog post about this PR](https://bowild.wordpress.com/2018/05/01/rubys-rack-push-decoupling-the-real-time-web-application-from-the-web/) and tried to explain this difference on a [Reddit thread](https://www.reddit.com/r/ruby/comments/8g5fpe/rubys_rack_push_decoupling_the_realtime_web/dy9jawr/). I'm not sure how well I did explaining my thought on the matter, but I think it's a wonderful approach.
",2018-05-01T13:15:22Z,870622
261,rack/rack,318673916,385669300,"@boazsegev While I see where you are coming from, I think that your idea of a semantic model is much higher than what is exposed by HTTP.

Given the direction of HTTP/2, it's clear to me that the base HTTP semantic model is a full-duplex un-buffered stream of chunks. Whether you implement this with multiple HTTP/1 connections or HTTP/2 streams (which to me is the network layer), is irrelevant to the application.

The application simply sees a full duplex stream of chunks. That's the semantic model for the application. On top of this you can implement your desired web sockets, sse, and other interesting things, like streaming responses, and so on.

Your semantic model has a tight coupling with your desired application level protocols, but I think that's a very limiting design. For example, you can't implement general streaming responses like https://github.com/socketry/falcon/blob/master/examples/beer/config.ru (the fact it uses `async` is irrelevant, you could just as easily do it with threads).",2018-05-01T13:21:57Z,30030
262,rack/rack,318673916,385669706,I think I should mention this discussion here since it seems relevant: https://github.com/rack/rack/issues/1148,2018-05-01T13:24:04Z,30030
263,rack/rack,318673916,385672209,I'd like to point out that Rack is event based as are the WebSocket and SSE API/specs. The Rack hijack offers a streaming back door that is not addressed by the spec. This PR attempts to formalize an approach to using WebSocket and SSE that is consistent with the use pattern common in modern web applications with JavaScript front ends. It is not an attempt to get rid of the hijack option. There is certainly reason to continue support for hijacking but that should not be a reason to reject this PR.,2018-05-01T13:36:26Z,118856
264,rack/rack,318673916,385673011,"@ioquatix , you're totally right, I can't implement that code... nobody can unless they implement `Async::Task` and add that as a Rack dependency.

As for streaming solutions: SSE **is** a streaming solution.

If you refer to video streaming or binary data requirements, WebSockets are perfect for that.

If you meant old-school HTTP streaming... yes, that's true. I'm not even sure that I should, but I'm aware that I don't. In fact, I'm pretty sure [the Rack model wouldn't be ideal for streaming](http://blog.plataformatec.com.br/2012/06/why-your-web-framework-should-not-adopt-rack-api/) anyway. ",2018-05-01T13:40:33Z,870622
265,rack/rack,318673916,385673490,"> I'd like to point out that Rack is event based

[I'm not convinced by this](https://github.com/rack/rack/search?utf8=✓&q=on_&type=)

If anything, rack is based on composable middleware. There is no event handling.

> The Rack hijack offers a streaming back door that is not addressed by the spec

Sorry, but this is complete dump trucks: https://github.com/rack/rack/blob/154ac5255323021ef74039e11a0a7376af889eaa/SPEC#L87-L96

> This PR attempts to formalize an approach to using WebSocket and SSE that is consistent with the use pattern common in modern web applications with JavaScript front ends

This is the guts of the matter. Why is this necessary? It's clearly already possible to do it - so we have to ask the question, do we need to formalise this in rack, or is there something simpler that we could formalise which allows more possibilities. We should accept or reject this PR based on the merits of what it adds to Rack, as a specification. I think there are better, more generic, options.
",2018-05-01T13:42:55Z,30030
266,rack/rack,318673916,385674547,"> @ioquatix , you're totally right, I can't implement that code... nobody can unless they implement Async::Task and add that as a Rack dependency.

I don't think I proposed that anywhere. I just said, with the current Rack spec, you can do it, and it scales well. We don't need to change the rack spec to have a great implementation of web sockets.

> As for streaming solutions: SSE is a streaming solution.

It only works in one direction and it only works in a very specific context. It's not general response streaming, as in `body.each {|chunk| ... peer.write(chunk) ...}`

> If you refer to video streaming or binary data requirements, WebSockets are perfect for that.

> If you meant old-school HTTP streaming... yes, that's true. I'm not even sure that I should, but I'm aware that I don't. In fact, I'm pretty sure the Rack model wouldn't be ideal for streaming anyway.

The Rack model is great for streaming responses. That's the whole point of the response body responding to `each`.",2018-05-01T13:47:57Z,30030
267,rack/rack,318673916,385674938,"By the way, I'm not trying to shit all over your PR, although it probably feels like it. Congratulations for making something, and implementing it and proposing it. Basically, I just don't see the logic of it though. I think there are better things in Rack that need to be addressed, and I think the direction of HTTP/2 confirms this.",2018-05-01T13:49:43Z,30030
268,rack/rack,318673916,385675990,"Of course Rack is event based. The trigger for all Rack code is the `#call(env)` callback where `env` is the encapsulation of the event that has occurred. That event being an HTTP request. After that you are correct, the handling is based on a middleware model.",2018-05-01T13:54:33Z,118856
269,rack/rack,318673916,385676197,"I'm paraphrasing slightly to focus on your question, forgive me and correct me if I misunderstood:

> Why is this necessary? ... is there something **simpler** that we could formalise which allows more possibilities.

I believe this is necessary because `hijack` is complex.

There's a difference between simple and easy. `hijack` is easy - but not simple.

It might seem simple for Rack and maybe even simple in the server, but it's a super complex solution that requires applications to add IO handling code.

The `hijack` pattern is an anti-DRY and anti-SOLID pattern that breaks away from object oriented approaches and requires applications to become familiar with the networking layers.

Sure, we might need to keep `hijack` around because some things are just impossible without it, but that doesn't make it the best solution for everything. 

On the other hand, the callback approach in this PR is **simple**, but it's clearly not **easy**.

Servers will have to put in some work (less work than applications need to put in for `hijack`, but still).

However, the solution is simple and follows both DRY and SOLID principles.

The callback approach isn't a generic solution, it's just good for things that lend themselves to using callbacks, but this is the advantage of this approach.

IMHO, This is a perfect fit for everything callback related and it's extendible (if WebSockets 2.0 come around, nothing needs to be changed on the Rack/Application side, just the server).",2018-05-01T13:55:27Z,870622
270,rack/rack,318673916,385679604,"@ioquatix , I was looking through the falcon repo and you seem to have done great work there.

I understand your concerns and your wish to have a better solution. It seems to me that you yourself put in a lot of time and effort into improving the sated of affairs and making things easier and better for everyone.

I appreciate your approach and I find myself slightly surprised by the challenges you raise against this PR.

It seems to me that you're using similar semantics with different method names (i.e., your WebSocket `next_message` seems to block the current thread, allowing other threads to handle other server events until a new message becomes available... or maybe I misunderstood).

Sure, my implementation is evented and your implementation is multi-threaded, but both lend themselves to the same approach - application side ""callbacks"".

Your implementation could just as easily remain threaded and hide the `next_event` loop away so it calls the correct callback.

I'm not sure I understand your reservations.",2018-05-01T14:09:44Z,870622
271,rack/rack,318673916,385684707,"> I believe this is necessary because hijack is complex.

Those are all really good points and I appreciate your thinking.

To be honest, in falcon, implementing your proposed API wouldn't be more than about 100 lines of code, perhaps less. So, it's not complicated to implement, because falcon already has a well defined and good concurrency model. That's the context in which I'm thinking about your PR, by the way.

Whether the complexity is in the server, or the application code, is an interesting point and one that I can agree with to a certain extent. I don't think there is a strong argument either way.

> @ioquatix , I was looking through the falcon repo and you seem to have done great work there.

Thanks, I can tell we are going to get along well now.

> I understand your concerns and your wish to have a better solution. It seems to me that you yourself put in a lot of time and effort into improving the sated of affairs and making things easier and better for everyone.

Yes, but I still haven't made a 1.0 release yet. It's coming soon hopefully.

> I appreciate your approach and I find myself slightly surprised by the challenges you raise against this PR.

Fair enough.

> It seems to me that you're using similar semantics with different method names (i.e., your WebSocket next_message seems to block the current thread, allowing other threads to handle other server events until a new message becomes available... or maybe I misunderstood).

That's almost right. Async doesn't use threads, it uses fibers which are cooperatively scheduled. They have much less overhead than threads.

> Sure, my implementation is evented and your implementation is multi-threaded, but both lend themselves to the same approach - application side ""callbacks"".

Async is event driven. When a fiber performs an operation that would block, it yields back to the reactor which resumes once the operation can continue.

I deliberately try to avoid callbacks because in my experience they lead to callback hell. The difference is that with callbacks you need to use variables to track state, and each time the callback is invoked you have to process that state to figure out what to do next. With fibers, you naturally resume where you were with your stack intact. You can implement complicated state machines with ease.

> Your implementation could just as easily remain threaded and hide the next_event loop away so it calls the correct callback.

Yes, this is feasible.

> I'm not sure I understand your reservations.

I like this PR from the point of view that it tries to provide some generic interface for web sockets and server sent events. Making a generic interface for that is both simple and difficult. I admire the you've done it and implemented it.

My main reservation is that your proposed API is incomplete. The rack specification as it stands, is really simple and allows for a lot of flexibility. What you've proposed is an extension which fundamentally encodes a model for concurrency into Rack, something that it hasn't had except in a very precise location (response body `#each`).

To look at it another way, `falcon` passes the rack linter, yet it implements real-time streaming requests and responses. So, within the current confines of Rack, we can enjoy these advanced features.

Your proposed API exposes an incomplete model for concurrency and the phrase that comes to my mind is ""thar be the dragons"". What happens if you stick a `RestClient.get` into `on_message` - does it stall the entire server? How do you communicate between web sockets (in your PR I assume you'd need a shared backend like redis)? Whether or not you agree with it, the node model of single process, multiple connections is one that works very well - you can have an array of web socket connections, send messages to all of them, etc. This model can work well with Rack as shown by `falcon` (e.g. https://github.com/socketry/async-websocket/tree/master/examples/chat).

Whatever way you cut it, Ruby has a pretty frustrating model for concurrency right now (and IO too, but it's slowly getting better). People are working on it, myself, others (Eric, https://bugs.ruby-lang.org/issues/13618), ko1 (guilds) and I have no doubt that good things are coming to this space.

But, Rack, right now, has to be a specification that works with Ruby as it is today. If `Thribers` become part of Ruby (I personally hope not), the way servers should implement web-sockets should be totally different. It might be possible to make a MVP, like this PR, but I think it's loading Rack up with too much complexity.",2018-05-01T14:30:46Z,30030
272,rack/rack,318673916,385693709,"@ioquatix interesting, thanks for taking the time to explain.

I love the fiber approach, as it hides away the event reactor and make things more comfortable to manage.

> My main reservation is that your proposed API is incomplete.  

Yes, you are right that there are concerns related to the proposed approach. It doesn't deal with IPC or client<=>client communications. It also allows developers to violate multi-threading best practices.

However, this proposal seems pretty balanced. It doesn't enforce a huge change on existing servers and the ""missing"" pieces are easily complemented by existing solutions (Redis being just one of them).

Personally, I implement pub/sub in iodine to ""fill the gap"". However, I'm not sure this should be part of the specification since other developers might wish to ""fill the gap"" using a different approach, such as limiting the server to a single process and using an array.

> What you've proposed is an extension which fundamentally encodes a model for concurrency into Rack...

I'm not sure I understand this part.

The server can be a single threaded server, a thread-per connection server, a fiber based server and practically any type of concurrency model can be implemented.

I'm not sure I see where the PR requires a specific concurrency model.

> What happens if you stick a `RestClient.get` into `on_message` - does it stall the entire server?

That really depends on the server, I guess.

What happens if I call `sleep(10)` while handling an HTTP request (except getting fired, that is)?

It's probably the same answer.

Iodine, for example, supports multi-threading and cluster mode (pretty much the same model as Puma). Agoo (I think) supports multi-threading.

For both of these servers I would recommend avoiding blocking calls within the thread, but they both offer some level of protection before experiencing DoS.

I don't use fibers so I'm not sure how they would react to this scenario. Hopefully they will react better.

Either way, I doubt the question of having fibers replace threads is part of the discussion here.

> Whether or not you agree with it, the node model of single process, multiple connections is one that works very well...

I agree with it, but I don't see why that matters.

Even the node model needs to be adjusted when scaling horizontally.

All models have properties that define their strengths and weaknesses.

IMHO, this PR proposes a mechanism that's independent of model used by the server, making it flexible enough for everyone to implement.",2018-05-01T15:04:37Z,870622
273,rack/rack,318673916,385816162,"> However, this proposal seems pretty balanced. It doesn't enforce a huge change on existing servers and the ""missing"" pieces are easily complemented by existing solutions (Redis being just one of them).

How do servers support your proposal? By pulling in `websocket-driver`? It's a large surface area, and it does get pretty tricky. I wouldn't call it balanced - it's heavily biased towards implementing WebSockets.

> I'm not sure I understand this part.
> The server can be a single threaded server, a thread-per connection server, a fiber based server and practically any type of concurrency model can be implemented.
> I'm not sure I see where the PR requires a specific concurrency model.

Invoking a Rack app is for the most part trivial. It's a function call that returns a result. No real model for concurrency is needed for this basic definition. You scale up by executing the function on different CPU cores, but fundamentally you can't change it to a non-linear event driven callback model or some other asynchronous model (`async` provides transparent inversion of control back to the reactor, but it doesn't change or require changes in flow control).

This proposal embeds non-linear flow control into the Rack spec. What I mean is, it's impossible to implement the given proposal without some kind concurrency. I'm not saying that any particular model for concurrency is being encoded, but just that by your approach a model for currency is now necessary.

This has a huge down-stream effect, since all code that depends on Rack now has to be aware of and capable of asynchronous execution. For example, how would you change `rack-test` to support this PR?

I think this equally applies to `rack.hijack`, and I also don't like that approach, but it's pretty much set in stone now. For example, how do you implement `rack.hijack` with HTTP/2? It's probably not possible in the general sense.

> What happens if I call sleep(10) while handling an HTTP request (except getting fired, that is)?

In `async` provided you call `task.sleep(10)` the fiber defers for 10 seconds. It doesn't block the server. In `puma` it would block the worker thread.

Have you tried using ActiveRecord in your `on_message` callback? How did that scale for you? My experience is that the ActiveRecord `ConnectionPool` design is very poor for highly concurrent workloads. These are very tricky issues to get right (although you can do it as shown here: https://github.com/socketry/async-postgres and it does scale up pretty well).

> [regarding node] I agree with it, but I don't see why that matters.
> Even the node model needs to be adjusted when scaling horizontally.

Because with your proposed API, implementing a basic Node style server like this is not possible. Right from the get go you need additional machinery to do pub/sub or other kinds of communication. Even the situation with streaming responses is not improved without additional work. It's a very specific proposal designed for a very specific kind of scalability. It's far too specific IMHO.

What we need is a proposal that better aligns with HTTP/2 streams since it's clear to me that it should be the future of HTTP and Rack. It should have a clear model for concurrency that fits with the existing multiprocess/multithread/worker implementations - i.e. reading from `rack.input` might block the request and writing to the response body might block the request due to buffering. On top of that, as already demonstrated, you can implement highly scalable asynchronous servers.

Streaming request and response bodies in `falcon` directly improve the latency of existing apps with no changes. Here is an implementation of `rack.input` which streams (and buffers) the input: https://github.com/socketry/falcon/blob/master/lib/falcon/adapters/input.rb

But this PR requires significant changes to existing apps for any kind of benefit. Not only that, but it only supports a very specific kind of scalability with an under-specified concurrency model (i.e. what happens if you block in `on_message`).

Let me finish with the following question: Do even think there is a future for WebSockets? https://datatracker.ietf.org/doc/draft-hirano-httpbis-websocket-over-http2/ hasn't been touched since 2014. There is an interesting write up here: https://daniel.haxx.se/blog/2016/06/15/no-WebSockets-over-http2/ - WebSockets are something which has never been a good fit for the request/response paradigm and that's something which fundamentally underpins Rack (assuming that `rack.hijack` is a poorly thought out addition to the spec :).",2018-05-01T23:05:24Z,30030
274,rack/rack,318673916,385825312,"Hi all! Websockets and rack again, let's do this!

Up front, last time this came up, I prototyped it in puma (https://github.com/puma/puma/pull/1054). It was an experiment we didn't merge in, but it totally works and we might revisit it.

The API of on_* methods for events is totally fine, no issue there.

Extending the object to decorate it with a write method is pretty ugly, from both an implementation statement as well as a performance one. Passing an object that implements #write to the on_* methods to write back is less code and performs better.

If frameworks want to take that object and use extend to make it's methods available on the handler, that's totally fine and up to the framework. Rack should not do that on it's own, it should provide a lower requirement.",2018-05-02T00:01:11Z,7
275,rack/rack,318673916,385826050,"You should be able to opt to the ""type"" of upgrade you are after

I can already see 2 use cases that this protocol does not help at all

1. https://github.com/discourse/discourse/blob/master/lib/hijack.rb 

With the current example use of:

https://github.com/discourse/discourse/blob/master/app/controllers/user_avatars_controller.rb#L71-L78

2. Messagebus also controls transport via: https://github.com/SamSaffron/message_bus/blob/master/lib/message_bus/client.rb and there is no clean protocol here for a ""chunked encoding"" http req short of SSE (which happens to fit sort of except for demanding `""text/event-stream""` as the content type

I would recommend

```
class Raw
  def type
      :raw  # or :sse or :socket
   end

  def on_open(io)
      # custom header support
      io.write_headers({  }) 
     
      # transparent chunked encoding support
      io.write_chunk(""a chunk"") 
  end
end

 env['rack.raw'] = Raw.new 
```

I am not super happy about `rack.upgrade` as a name cause it very tied to web sockets kind of terminology",2018-05-02T00:05:03Z,5213
276,rack/rack,318673916,385826927,"Rather than `env['rack.raw'] = Raw.new`, why not just `return [200, {}, Raw.new]` and have servers do different things depending on `#type`.",2018-05-02T00:10:43Z,30030
277,rack/rack,318673916,385827000,"Another thing that makes me somewhat uneasy here is that it still suffers from the very ugly trait that deep down the middleware stack someone returns rubbish to all the rest of the pieces that then gets thrown in bin.

So... to mitigate this very bad problem I would recommend:

```
  # only gets called AFTER everything walked down the middleware stack. 
  def on_open(io, env, response)
      status, headers, result = response

      # custom header support
      io.write_headers({  }) 
     
      # transparent chunked encoding support
      io.write_chunk(""a chunk"") 
  end
```

Overall I am leaning toward not even mucking with rack long term for a lot of this stuff and moving to fiber servers that can pause and resume more cleanly, multiplexing tons of connections cleanly. A big blocker there though is the MRI does not let you ship fibers between threads now which is not ideal. 

That said this works ... so ... yay ?https://github.com/SamSaffron/performance/blob/master/fiber_server/server.ru

",2018-05-02T00:11:11Z,5213
278,rack/rack,318673916,385827745,"Also... one extra big question here is ... do we want to bloat all of our Rack servers out there with all this extra logic when simply adding a single dependency to middlware that implements this today is already feasible on top of hijack? 

should we not start with `gem install magic_generic_web_socket_support_middleware` prior to pushing on implementers to all implement the same thing? ",2018-05-02T00:15:31Z,5213
279,rack/rack,318673916,385827949,"> A big blocker there though is the MRI does not let you ship fibers between threads now which is not ideal.

Fibers are actually bound to the thread that creates them by definition.

Some designs use a more generic coroutine structure (e.g. green threads) which you can move between threads. If you are interested in this, you might find https://bugs.ruby-lang.org/issues/13618 interesting.

If you are interested in Fiber based servers, check out falcon. https://github.com/socketry/falcon/blob/master/examples/beer/config.ru and https://github.com/socketry/ which is a complete stack of asynchronous components.

> Also... one extra big question here is ... do we want to bloat all of our Rack servers out there with all this extra logic when simply adding a single dependency to middlware that implements this today is already feasible on top of hijack?

Yes, I strongly agree with this, and the answer IMHO is no.",2018-05-02T00:16:48Z,30030
280,rack/rack,318673916,385829111,@evanphx I appreciate you opinions and it is good to have a different perspective than what we had. I can't agree that the performance would be any different in one case over the other though. The other points are certainly worth considering.,2018-05-02T00:24:14Z,118856
281,rack/rack,318673916,385829346,"yes, I am across Eric's work there and hope we get something in MRI, I also think Koichi is open to having a protocol for allowing to move Fibers between threads, and I want to see proper green threads back. 

Also seen Falcon, I agree that a fiber based server is very very appealing for a bunch of workloads it simplifies so much of the mess we have now with pure threaded servers and slow requests, especially cause you can walk down the middleware stack properly in the right time. ",2018-05-02T00:25:34Z,5213
282,rack/rack,318673916,385829465,"> should we not start with gem install magic_generic_web_socket_support_middleware prior to pushing on implementers to all implement the same thing?

TBH I'm OK with it if it's an opt-in.  This proposal seems simple enough that someone could implement a ""hijack"" based approach using a middleware.  The upside of formalizing it is that if webservers want to implement a high performance version of `magic_generic_web_socket_support_middleware` they can.  I think of it as similar to `X-Sendfile`: yes we have a middleware that can do it, but webservers can implement an accelerated version since we have the standard.",2018-05-02T00:26:12Z,3124
283,rack/rack,318673916,385830066,"@SamSaffron in regard to on_open taking an io, env, and repsonse. By the time `on_open` is called the connection has already been established. Since it is either WebSocket or SSE a response and env are no longer relevant. The intent was to allow middleware to decide if a connection shoulc be established based on the return status in the `#call(env)` method that asked for the upgrade.",2018-05-02T00:30:35Z,118856
284,rack/rack,318673916,385830427,"@tenderlove  My issue though is that encourages a whole bunch of code duplication... say Puma takes this on now it needs to ship with a websocket protocol as a strong dependency, or worst still it will carry its own duplicate websocket protocol thingy. Chunked encoding is easy enough but there is a fair amount of code to do websocket upgrade and encoding depending on how far you want to take it and how many protocol variants you want to support. 

",2018-05-02T00:32:50Z,5213
285,rack/rack,318673916,385830745,"I only think `on_open` should take an IO. It should be the responsibility of the instance to store the IO IMHO. This way you can call `@io.write` at any time, e.g. from a timer.",2018-05-02T00:34:45Z,30030
286,rack/rack,318673916,385830968,"> I only think on_open should take an IO. It should be the responsibility of the instance to store the IO IMHO. This way you can call @io.write at any time, e.g. from a timer.

I'd rather they all take the IO object.  You can save to an ivar if you want.",2018-05-02T00:36:16Z,3124
287,rack/rack,318673916,385831122,"> I'd rather they all take the IO object.

What's the reasoning behind that? Is it more efficient?",2018-05-02T00:37:20Z,30030
288,rack/rack,318673916,385831324,"To address @SamSaffron's ""junk data"" comment, I totally agree.  Maybe we could use headers to indicate to the webserver that the body object is ""special"" (it knows websockets):

```ruby
class WSBody
  def each
    yield ""Webserver said it does WS, but lied""
  end

  def on_open io
  end

  # ... more on_* methods
end

# App

app = lambda do |env|
  if env['rack.supports_ws?']
    [ 200, { ""X-Websocket"" => ""on"" }, WSBody.new ]
  else
    [ 404, {}, [ ""This was supposed to be a WS response"" ] ]
  end
end

# Webserver

status, headers, body = app.call(""rack.supports_ws?"" => true)
if headers[""X-Websocket""]
  # do websocket stuff with the body
  body.on_open io
else
  # do the normal `each` thing
end
```

I think it would eliminate questions about what to do WRT middleware (this type of API would insist that the thing responding to Websockets *not* forward to another middleware)",2018-05-02T00:38:35Z,3124
289,rack/rack,318673916,385831381,"@SamSaffron code duplication on the server implementation is really a matter left up to the server author. I don't think that is relevant to the proposed spec addition, which is proposed to be optional.

@ioquatix using an io object or the extended handler itself is pretty much the same in regard to some external writer loop. The example code on Agoo demonstrates that. Thats not to say that using an io object might be preferred fro other reasons though.",2018-05-02T00:39:04Z,118856
290,rack/rack,318673916,385831441,"@SamSaffron 

> My issue though is that encourages a whole bunch of code duplication

I think code duplication is one of the concerns this PR will solve.

Rright now we have tons of applications each implementing their own WebSocket protocol (there's only one standard protocol, so historical variants aren't necessary).

On top of that, all these application have their own implementation of IO handling logic (their own reactor, `nio4r` or some other approach).

Consolidating the dependency in the server actually minimizes code bloat as most of the code is already there. Sure, adding the WebSocket parser might be new code, but the network and IO handling logic is already there.",2018-05-02T00:39:27Z,870622
291,rack/rack,318673916,385831630,"> What's the reasoning behind that? Is it more efficient?

It means that you can implement a stateless object.  I prefer to avoid maintaining state when possible.  If the methods take the IO object as a parameter, I am allowed to implement a stateless object, but it is *my* choice.  Insisting that only `on_open` takes the IO object forces me to maintain state (I no longer have a choice).",2018-05-02T00:40:40Z,3124
292,rack/rack,318673916,385831801,"> Since it is either WebSocket or SSE a response and env are no longer relevant. 

I think chunked encoding responses are just as good as a candidate, they are more widely supported than SSE anyway. In fact I can see almost zero reason to support SSE cause it is pretty much a novelty protocol. https://caniuse.com/#feat=eventsource compared to https://caniuse.com/#feat=xhr2 which is supported on IE/edge. 

env is very relevant for chunked encoding you may want to add headers deep in your middleware for CORS and various things like that. ",2018-05-02T00:41:52Z,5213
293,rack/rack,318673916,385832184,"@tenderlove in you code example you seem to be assuming WebSocket and SSE calls are triggered by an event from the browser. Maybe I misread the code. My apologies if I did. Anyway, I think a common use case will be to push event from Ruby to the browser. Do foresee a more request response pattern initiated from the browser using WebSockets instead of using HTTP?",2018-05-02T00:44:59Z,118856
294,rack/rack,318673916,385832303,"> @tenderlove in you code example you seem to be assuming WebSocket and SSE calls are triggered by an event from the browser. Maybe I misread the code. My apologies if I did. Anyway, I think a common use case will be to push event from Ruby to the browser. Do foresee a more request response pattern initiated from the browser using WebSockets instead of using HTTP?

Derp, yes, sorry. It's been a long day. 😞",2018-05-02T00:45:59Z,3124
295,rack/rack,318673916,385832446,"@tenderlove regarding stateless objects, I think there is some merit to what you say. The principles of the API remains the same but adding the io object does allow some additional options. It certainly doesn't hurt.",2018-05-02T00:47:15Z,118856
296,rack/rack,318673916,385832502,"@SamSaffron 

> I think chunked encoding responses are just as good as a candidate

I'm not sure if the PR is supposed to be a `hijack` replacement.

It's definitely a step towards retiring `hijack`, but not necessarily a complete replacement.

The idea is that response that passes through the middleware is actually sent by the server.

i.e., notice the header in the response (place in `config.ru` and run with `iodine`):

```ruby
# Place in config.ru
RESPONSE = [200, { 'Content-Type' => 'text/html',
          'Content-Length' => '12' }, [ 'Hello World!' ] ]
# a Callback class
class MyCallbacks
  def initialize env
     @name = env[""PATH_INFO""][1..-1]
     @name = ""unknown"" if(@name.length == 0)
  end
  def on_open
    subscribe :chat
    publish :chat, ""#{@name} joined the chat.""
  end
  def on_message data
    publish :chat, ""#{@name}: #{data}""
  end
  def on_close
    publish :chat, ""#{@name} left the chat.""
  end
end
# The actual Rack application
APP = Proc.new do |env|
  if(env['rack.upgrade?'] == :websocket)
    env['rack.upgrade'] = MyCallbacks.new(env)
    [200, { ""X-Header"" => ""This is sent"" }, []]
  else
    RESPONSE
  end
end
# The Rack DSL used to run the application
run APP
```

Yes, I suppose long polling can be added to this list, using chunked encoding, but this means that the headers are sent immediately, since this is part of the design's premise.
",2018-05-02T00:47:43Z,870622
297,rack/rack,318673916,385832518,"> Sure, adding the WebSocket parser might be new code, but the network and IO handling logic is already there.

This is my point though... What are @evanphx @FooBarWidget and Eric going to do here? They are pretty much the people who need to be convinced this is good.

- Add a hard dependency to ""standard websocket parser gem"" (I give it zero % chance this will happen for Unicorn, but don't know where Evan and Hongli stand) 

- Add ""websocket_encoder.rb"" file and ""websocket_upgrader.rb"" and ""magic_rack_protocol.rb"" they hand curate and is duplicated 3 times between the 3

- Add a soft dependency we need to add to our Gemfile to ""activate"" 

I don't see any clear cut good answer here. ",2018-05-02T00:47:52Z,5213
298,rack/rack,318673916,385833274,"This `env['rack.upgrade?'] == :websocket` is a big concern for me protocol wise, if `/some/route` is meant to be SSE, and `/some/ws` is meant to be web sockets, how is the server going to decide this upfront. Not allowing the router any say here is a problem imo. 

`if env['rack.can_upgrade?`]` seems far more flexible.  ",2018-05-02T00:53:22Z,5213
299,rack/rack,318673916,385833417,@SamSaffron I'm surprised we are considering rejecting the PR because it might be hard for some of the current server gems to implement. I would have thought Rack was intended to make life easier for developers/users and not for the server implementers.,2018-05-02T00:54:23Z,118856
300,rack/rack,318673916,385833652,"> if /some/route is meant to be SSE, and /some/ws is meant to be web sockets, how is the server going to decide this upfront. 

The server doesn't.

The server lets the application know what the user asked for (if the user asks for WebSockets or SSE).

The application is free to refuse the connection if the route doesn't match.

The application is also free to implement both solutions on the same route.
",2018-05-02T00:55:56Z,870622
301,rack/rack,318673916,385833803,@SamSaffron `env[`rack.upgrade`] can be tested for `nil` or for s specific value. Hiding the fact that the upgrade is either WebSocket or SSE is limiting. Providing more information only help make the decision easier for more specialized applications.,2018-05-02T00:57:01Z,118856
302,rack/rack,318673916,385834361,"@tenderlove , @SamSaffron 

I'm not sure I understand what you mean by ""junk data"".

As I understand the PR, the `env` object performs the roundtrip through the middleware and afterwards the server tests for the callback object. At what point is there a risk for junk data?",2018-05-02T01:00:25Z,870622
303,rack/rack,318673916,385834948,"My personal goal here would be adoption, and if we are introducing a protocol here that can not easily be adopted (or will be rejected by the big 3) then chances of faye,message_bus and family all moving to it are very low. 

Also it places unfair pressure. ""Look passenger did it why don't you?""

That said, since this can be implemented today in middleware it is not the end of the world if the big 3 reject it. 

> The server lets the application know what the user asked for (if the user asks for WebSockets or SSE).

I follow so you are looking at `Upgrade: websocket` header, then the server knows it can implement it so it sets `['rack.upgrade?']` to websocket. But ... SSE ship `Accept: text/event-stream` and chunked encoding ships nothing. and in all cases they could potentially use this protocol. The terminology is very websocket centric. ",2018-05-02T01:04:33Z,5213
304,rack/rack,318673916,385835007,"I don't know if this comment is helpful to the discussion at large, so pardon me if this feels as an intervention. There are legitimate cases where hijack is used because it offers access to the socket. For example, we use it to call `sendfile` multiple times in sequence to server very large binary responses, and it works well.

The ""downgrade"" in convenience that `hijack` brings is well compensated by the interesting things you can do to it. Imagine with this proposal one of the use cases would be _not_ using SSE (and not using WebSockets) but instead stuffing large binary responses into the socket from the filesystem (as we do in https://github.com/WeTransfer/fast_send). Would such an ""upgrade wrapper"" facility help that use case in any capacity? For example, would it give me a way to ""yield back control"" from a sending loop - which I ideally would have synchronous or in a Fiber - back to the webserver?

In practical terms, using Hijack was a bit tricky in the first place because it was not clear _which exact subset_ of an `IO` the given socket-ish object supports, and I feel this might be the case for this proposal too. Personally I would love to see that IO specified somewhat tighter (""here are the methods that _must_ be available, here are the methods that _may_ be available if the server decides to""). I don't know if having something so tightly scoped to be a WebSockets support library is the answer to ""hijack API is obtuse"" TBH.",2018-05-02T01:04:58Z,16446
305,rack/rack,318673916,385835632,"> In practical terms, using Hijack was a bit tricky in the first place because it was not clear which exact subset of an IO the given socket-ish object supports, and I feel this might be the case for this proposal too. 

Yes absolutely, we don't even know which type of exception this object can raise and the list goes on. I would love to see this thing properly documented. 

Also another missing practical item is that you have no way of setting a callback to happen after the server finished dealing with the hijacked request which is a huge hole and can lead to concurrency issues and so on. eg: `env['after_hijacked] = lambda{|r| ...}` 

> I don't know if having something so tightly scoped to be a WebSockets support library is the answer to ""hijack API is obtuse"" TBH.

I agree with this. I would like to see hijack better specified. ",2018-05-02T01:10:10Z,5213
306,rack/rack,318673916,385836295,"@SamSaffron regarding `env['rack.upgrade?']`, the server has already looked at the headers and determined the requested upgrade. There is no need for the application to look at the headers to determine the upgrade type. It was provided as a convenience so the application did not need to look at the various headers that were used to determine the upgrade requested. It also leaves the API open for future additions if some new event based connection protocol is supported.",2018-05-02T01:15:07Z,118856
307,rack/rack,318673916,385837083,"@julik @SamSaffron I think there is huge value in cleaning up / specifying the hijack API.  If either of you want to take a stab at it I would really appreciate it.  (Also, if we're going to rev the spec I think it's time we remove the ""rewindable body"" requirement)",2018-05-02T01:20:40Z,3124
308,rack/rack,318673916,385837888,"@SamSaffron 

> I follow so you are looking at Upgrade: websocket header, then the server knows it can implement it so it sets ['rack.upgrade?'] to websocket. But ... SSE ship Accept: text/event-stream and chunked encoding ships nothing. and in all cases they could potentially use this protocol. The terminology is very websocket centric.

Yes, the idea is that the `env['rack.upgrade?']` value is set according to the headers client send (i.e., `Accept: text/event-stream` or `Upgrade: websocket`, for now).

If there was a way to detect XHR, it would probably be easy enough to manage... but there isn't. This might remain in the `hijack` realm for now.

As @ohler55 [mentioned](https://github.com/rack/rack/pull/1272#issuecomment-385836295), this allows the application to ignore the HTTP headers and distance itself from the HTTP protocol.

As a side-note:

I loved XHR when I didn't have WebSockets. I hardly ever use SSE or XHR for push notifications (I do use XHR for REST calls). However, I can see HTTP/2 bringing a strong movement towards SSE, so it might be in my future.
",2018-05-02T01:25:58Z,870622
309,rack/rack,318673916,385840091,"@tenderlove , @ohler55 , I feel that the static object support is a very strong argument in favor of an `io` object (although I find the name `io` misleading, maybe `client` more befitting).

I feel emotional resistance against this added complexity, but I can understand the logical benefits.

However, retaining a copy of the `env` object is a major memory concern as a lot of data has to be kept ""alive"" and moves into long-term memory storage... even though this might shrink the number of cases where static objects could be used.

This will make `on_message(client, data)`, `on_close(client)`, etc' less enticing, but it will work.",2018-05-02T01:39:51Z,870622
310,rack/rack,318673916,385842908,@boazsegev I know we discussed having an `io` object or what ever the label is. I don't have a strong opinion able the label. I don't see the need for passing around the `env` object though. I'm in agreement with you. The data in the `env` is stale by the time it is used in any of the callbacks. It is only relevant on the initial invocation of the `#call(env)` method.,2018-05-02T02:00:04Z,118856
311,rack/rack,318673916,385851416,"@ohler55 , yes, you were the one who suggested the `io` object during our discussion and I was the one with the strong feelings against the idea.

It's an implementation perspective. The extra object is somewhat of a headache and increases performance costs...

...but I'd rather have something everyone agrees to implement than have nothing.

Besides, if we all write amazing servers, I'll retire iodine in favor of whichever server is the best, so the extra implementation and maintenance cost is avoidable as far as I'm concerned ;-)",2018-05-02T03:00:22Z,870622
312,rack/rack,318673916,385945342,"Let me make an attempt to summarize. There are some who are completely against changing the Rack spec and feel hijacking is the best way forward if WebSocket and SSE is to be used with Rack. For those that haven't rejected the PR outright the following apply.

1. The consensus is that an `io` or `client` object be used instead of extending the handler.
2. There may still be open issues around how to indicate an upgrade is pending and how to pass the handler to the server.

I'll update the proposed changes to reflect not extending the handler today.",2018-05-02T11:23:06Z,118856
313,rack/rack,318673916,386284278,"I've been mucking around with a sample implementation of this proposal.

I can implement it entirely as middleware. So, in my mind, it could be something independent of Rack. I didn't implement it exactly because I just reused what I've already done with `async-websocket`.

# Specific feedback regarding the proposal.

With reference to https://developer.mozilla.org/en-US/docs/Web/API/WebSocket

## `rack.upgrade?`

It's okay. It's trivial enough to implement.

It's not clear if the user should return anything. In my case, I use hijack, so no return is necessary (or desirable).

## `on_open`

Makes sense. Do you mean the connection at the socket level or the connection at the web-socket level?

## `on_message(client, message)`

Makes sense. Would it make more sense to call this `on_text` which is similar to how `websocket-driver` names things? (then `write` could be `text`). Alternatively, call this `on_receive` and have a `send` method?

## `on_shutdown(client)`

Not exactly sure where this fits in. It's similar to `on_close`.

## `on_close`

Do you mean when the websocket connection has been closed by the remote end only (and gracefully?) i.e. when ready state of web socket is changed to close? What about EOF?

## `on_drained(client)`

This seems very asynchronous model dependent. I don't know how to implement this. My model is completely asynchronous so when you call `write` it won't return until the write is complete, but it's non-blocking. Remove this or make it completely optional (as in, it should still work if unimplemented).

## `write(message)`

Pretty crappy name IMHO, because it directly clashes with the WebSocket::Driver implementation's use of `#write` (https://github.com/faye/websocket-driver-ruby#usage). If you have to wrap the underlying driver, it's another object allocation. Bleh. Rename it `#send` or `#text`?

It would be pretty damn convenient if `#send` would convert it to JSON first. Perhaps if `Accept: application/json` is set, it can be done automatically. We could also support other formats. So, user doesn't need to worry about underlying transport to send complex objects (e.g. hash, array, and so on).

## `close()`

If user calls this, what is the sequence of events afterwards? `on_close`? `on_shutdown`? Nothing?

## `open?()`

In distributed systems, it's hard to know if anything is truely open. But you can usually tell with certainty if something is closed. Rename to `closed?` - it's typical Ruby `IO#closed?` `Socket#closed?` and so on.

## `pending()`

Again, this is very async model specific. Remove?

# Summary

I still stand by my original assessment. But, I think it's an interesting idea. This proposal implements something which can be done entirely in middleware with minimal overhead if your server has a half decent concurrency model.. I don't see a clear benefit to making Rack more complex in this regard and encoding a concurrency model into the `SPEC`. However, I can see value in making some kind of standard layer. I just don't feel like it belongs in the server to implement it, middleware would make more sense. It's also more flexible for changes in the future. The `rack.hijack` API might be simple, but it does solve this problem elegantly and allows servers to expose whatever concurrency model makes sense.

I still believe streaming requests/response bodies to be the right way forward, but I'm still learning about how this all fits together with HTTP/2 and whether it's possible to use `XMLHTTPRequest` in a similar way to WebSockets in the browser. It's simpler to set up, multiplexes better (when running on HTTP/2), and has a simpler model - basically read chunk from stream, write chunk to stream, and it works with existing request/response model without any real changes.

It would be interesting to see if the WebSocket draft for HTTP/2 is ever implemented by anyone.

Anyway that's my 2c. Feel free to play around with the implementation :)",2018-05-03T12:49:46Z,30030
314,rack/rack,318673916,386295227,"An addendum, a concurrent `rack.hijack` IO should be possible for most servers with very minimal surface area (it's what `falcon` does). It would still require some kind of `rack.upgrade` approach since to make a worker truely async you'd need to expose part of the concurrency model, as in ""Your rack.upgrade object will be run in it's own thread with blocking IO"" as the base requirement, but with more advanced servers able to provide better guarantees (e.g. rack.multithread could be false, you can assume an `async` like model if things are running concurrently). Again, it starts to get pretty hairy, in terms of how you define that model, and whether that should really be in Rack, but it's probably a more generic approach and it would work with HTTP/2 as well (in general, rack.hijack with an IO like API should map directly to HTTP/2 streams, and it's the next thing I want to try in `falcon`).. It wouldn't even preclude you from implementing web sockets on top of it with whatever you currently use and a default implementation as a middleware which really does just make a thread per request is entirely possible.",2018-05-03T13:28:01Z,30030
315,rack/rack,318673916,386296756,"I appreciate you taking the PR seriously enough to devote time to exploring implementation options. Thank you. Nice summary as well. I'd like to point out that all the method are optional. If the handler does not respond to the method then it is not called.

`rack.upgrade` :heavy_check_mark: 

`on_open` indicated the connection has beenn upgraded to WebSocket or SSE and is ready for sending messages.

`on_message` :heavy_check_mark: 

`on_shutdown` I agree, it does not seem necessary.

`on_close` Yes, called when the socket is closed. Ideally gracefully but that assumes the client did a graceful close which may not be the case. Basically the socket is no longer write able and nothing else will be received from it. Since both WebSocket and SSE are message based the notion of EOF doesn't really come into play as it is a streaming concept. Best case is EOF is the same as closed or maybe another way to look at it is if the server is reading a message and gets an EOF error then it assumes the socket is closed and tells the application that the connection is classed with the `on_close` callback. That is the way Agoo handles it anyway.

`on_drained` indicates no more messages are waiting to be sent. Depending on the implementation this could indicate the previous message was sent and another can be sent or the backlog of messages has dropped to zero. It is meant as a way for the application to meter itself and detecting slow consumers. In my opinion it is nice to have to avoid slow consumer attack vectors.

`write` could certainly be renamed but `send` would conflict in the base Ruby Object `#send` so something different. `#write` is the second best choice. You seem to have agreed since you used it in Faye which of course is where the conflict lies. How about `push`?

`close` when called initiates a graceful close of the connection. Once closed the `on_close` callback should be called.

`open?` flipping the call to `closed?` makes sense. Agreed.

`pending` is async specific. It could be removed and everything would still be fine. It was added to address the slow client issues and allow and application to address slow client attacks. How about return some value that indicates it is not supported?

Thanks again for the well thought out response.",2018-05-03T13:33:08Z,118856
316,rack/rack,318673916,386298090,I like the idea of using `rack.multithread` as an indicator to the application as to what it can expect in regard to the async model especially since it is already part of the spec and seems to be a perfect fit.,2018-05-03T13:37:24Z,118856
317,rack/rack,318673916,386299251,"> write could certainly be renamed but send would conflict in the base Ruby Object #send so something different. #write is the second best choice. You seem to have agreed since you used it in Faye which of course is where the conflict lies. How about push?

Don't get me started with `send` and Ruby. I had no end of problems with it several years ago, when I first started playing with Ruby's IO. Fortunately, now `__send__` exist in Ruby. I sometimes wonder if we are writing Python :p

To me, send/recv are socket level, but they resonate for higher level network IO too.

read/write make me think of buffered IO.

As this is a message based protocol, and you chose `on_message`, why not then `send_message`?

> pending and on_drained  ...

I understand where you are coming from.

In my client code, the following happens:

```ruby
connection.send_message(....) -> @driver.text -> @socket.write(protocol level data) -> Socket#write_nonblock -> okay, :wait_writable or EPIPE
connection.send_message(....) -> @driver.text -> @socket.write(protocol level data) -> Socket#write_nonblock -> okay, :wait_writable or EPIPE
connection.send_message(....) -> @driver.text -> @socket.write(protocol level data) -> Socket#write_nonblock -> okay, :wait_writable or EPIPE
```

You literally can't send messages faster than they would be written to the socket, because you'll yield on `:wait_writable` and `select(io, writable)`. Once the data is written to OS socket, how do you know `pending` or not? I mean, eventually that buffer will fill up. But do we get receipt of messages in WebSockets? I don't know enough about it, I only used `websocket-driver` and left the rest up to that gem. Can we get pending acknowledged count from that implementation?




",2018-05-03T13:41:14Z,30030
318,rack/rack,318673916,386299976,"In an implementation where messages are buffered some how, where do you propose to handle EPIPE? Or should it never occur and `on_close` should be invoked? or do we need `on_error`? To me, implementing something like `on_error` to handle EPIPE?

If your concurrency model is more event driven, I can imagine you would buffer the message and actual writing might occur on a different call stack.",2018-05-03T13:43:29Z,30030
319,rack/rack,318673916,386301122,"In my implementation, when you have a loop like `while event = @driver.next_event`, if you receive EOF, event is nil. It's not an exception. Then, it would call `on_close`. However, sometimes it might be caused by ECONNRESET. This is really an error condition. should it be `on_close(error = nil)`?

but if you call send_message, and remote end is gone, you make get EPIPE, Errno::ECONNRESET, or on macOS even EPROTOTYPE (https://bugs.ruby-lang.org/issues/14713). It will blow up in user's code.",2018-05-03T13:47:05Z,30030
320,rack/rack,318673916,386303835,"You bring up some interesting points. For Agoo the write are indeed on a different call stack but I like the idea of providing more information to the application about the reason for a close. Maybe the callback should be `on_close(err)` where `err` is `nil` for a graceful close. Yes, we are thinking along the same lines,

I would expect a write to fail with some kind of exception if the connection has been closed. Agoo raises an IOError.
",2018-05-03T13:55:18Z,118856
321,rack/rack,318673916,386448690,"@ioquatix, it is my humble understanding that this specification allows us to abstract away the network layer as much as we can.

This allows all persistent connections to share the same API regardless of their type (WebSockets, SSE, raw TCP/IP, etc').

It's my understanding that EOF and other network details should be handled by the server. They aren't application related events.

The reverse is also true. JSON is as application concern, it doesn't fit every persistent connection out there (i.e., binary transmissions in WebSocket data).

It's my recommendation that the server require a String object and let the application deal with making sure the data formatted as a String.

Network details could be inferred by the server (UTF-8 for text encoded messages and otherwise the server should probably assume binary data, which some connection objects might not support).


### `on_shutdown`

> `on_shutdown` I agree, it does not seem necessary.

I humbly disagree.

Maybe I'm assuming but I think applications can benefit from the knowledge that the client is closing because the server is shutting down.

There are three possible disconnection causes:

1. The connection is deemed unnecessary (this could be a proxy timeout due to inactivity).
1. The server shutting down (the client might be mid transmission).
1. Network errors (which we can do nothing about).

True, `on_shutdown` is only called when the server is shutting down (not for every connection), but it's a good way to prevent communication errors and keep the disconnection polite - especially when using server clusters or load balancing.

This could be use by load-balanced micro-services as well to indicate a node is going away (and perhaps send a list of alternate nodes in a gossip based system).

### `on_drained` / pending()

@ioquatix , since you're using a blocking model, where the method returns only after `write` was complete, `pending` can easily return `0` - no pending `write`, indicates that `on_drained` isn't about to be called (maybe ever).

Do note that this blocking design might be risky, if - for example - a DataBase connection was checked out of a connection pool or the `write` method was called within a critical section (i.e., looping over all clients).


### `write`

> `write` - Pretty crappy name IMHO...

Actually, just because other implementations use this amazing name (which is also the name of an underlying system call), doesn't mean that the specification should choose a poor name...

...in fact, it just supports the choice as a good one - developers will have a good idea as to what happens when they call this method.

IMHO `send` conflicts with the Ruby object `send` which might prevent an error from being detected (i.e., if `send` is called on another object, it will not return a ""missing method `write`, but rather a long gibberish message).

`push` seems (IMHO) less intuitive and more protocol specific (SSE). It also implies objects rather than data.

IMHO, `write` describes exactly what's happening. It might behave slightly differently from an IO `write` (no partial `write` allowed), but it performs the same function.

### `open?`

> open? flipping the call to closed? makes sense. Agreed.

Actually I think that `closed?` will make application code harder to maintain. It's too easy to mixup `close` for `closed?` and vice verses.

Using `open?` is safer, even if the server's information regarding the connection is never fail proof.

### `on_close(err)` / `on_error(err)`

I really think network / connection errors aren't an application concern, they are a server concern.

However, if we are to report an error, I would wonder if it would make sense to place the error information in a different callback, preventing the need for the obvious `if(err)` block of code in `on_close`.
",2018-05-03T21:52:37Z,870622
322,rack/rack,318673916,386449668,"@ioquatix 

> In my implementation, when you have a loop like while event = @driver.next_event, if you receive EOF, event is nil. It's not an exception. Then, it would call on_close. However, sometimes it might be caused by ECONNRESET. This is really an error condition. should it be on_close(error = nil)?

`ECONNRESET` is a perfect example for an error that the application really shouldn't receive. Why should the application have any information about network errors? What would it do with it?

IMHO, this is a Server related concern and it should be handled by the server.",2018-05-03T21:56:54Z,870622
323,rack/rack,318673916,386452844,"I have less of an issue passing some error back to the application with `on_close`. The application can't do anything about it but it isn't harmful either.

@boazsegev you make some good points I was a little too willing to compromise on.

I'm not sure where we go from here. I assume we need a 'member' to step in again. How about it @tenderlove , how should we proceed on this PR?",2018-05-03T22:11:50Z,118856
324,rack/rack,318673916,386460920,"@ioquatix 

> In my client code, the following happens... 
>
> You literally can't send messages faster than they would be written to the socket...
>... Once the data is written to OS socket, how do you know pending or not?

I think the question isn't about data pending **after** performing `write` to the socket. It's about data pending **before** `write` is performed (if applicable).

In your  (fiber-per-connection) model `write` always completes. But consider the node.js model (which iodine emulates in this regard):

* An application calls `connection.write`.
* The server places the data in the connection's queue and adds a `flush_connection` event to the event loop.
* `connection.write` returns.

    Note that _**no network actions were performed**_. At this point `pending()` would probably return `1` (or perhaps a higher value).

* The application code ends and the server handles the ""flush"" event, flushing the connection's buffer using the system call `write` (or `socket.write`).
* Once all the data was written to the socket (which might require a number of events) the connection's `on_drained` event is scheduled.

   This doesn't guarantee that the data actually reached the browser/client (a network error might have occurred, the OS might be waiting on a timeout).

   This only guarantees that the server won't call `write` (or `sock.write`) until more data is scheduled to be sent - the data is out of the server's control.

",2018-05-03T22:54:15Z,870622
325,rack/rack,318673916,386463025,Agoo uses a similar approach.,2018-05-03T23:06:27Z,118856
326,rack/rack,318673916,386481308,"**`write` - exception vs. `true`/`false`?**

Reading through the thread I noticed how hard it might be to predict a `write` failing.

This means that user code must use `begin`/`rescue` blocks to address network issues.

Since a failing `write` is a predictable and possible result in the normal flow, wouldn't it be better if the server handled any exceptions and simply returned `true` if the data was (or scheduled to be) written to the socket and `false` otherwise?

I think this will improve performance and also prevent the obvious nesting of exception handling code (on the server and the app).

Thoughts?",2018-05-04T01:06:19Z,870622
327,rack/rack,318673916,386483449,"I had assumed raising an exception was the natural behavior but maybe that is not the best approach. Even returning true or false does not necessarily indicate the message was delivered. I think the real question is, what information would be useful to the app. Maybe nothing. Callbacks exist for closing which lets the app know writes will no longer be delivered. Is an acknowledged protocol desired on top of WebSockets? I think that might be a step too far.",2018-05-04T01:22:24Z,118856
328,rack/rack,318673916,386488115,"I think the next steps are:

- To finish several implementations of the proposed spec, this will expose any obvious design issues with the spec.
- To implement several practical applications, and test interop (including periodic/timer events, pub/sub, and upstream interaction, e.g. database/redis) - if we can't make useful applications that work on different servers (i.e. if you start relying on iodine's pub/sub which can't be implemented in puma, it's going to be a problem.
- To prepare a finalised spec which takes into account the above and then have a formal vote, probably only involving actual stake-holders: Ruby server implementors and Rack contributors.",2018-05-04T02:01:04Z,30030
329,rack/rack,318673916,386490033,"There are currently two implementations of the originally proposed spec, Agoo and Iodine. I'd think that if proof that the spec is workable.

Waiting for several applications to be implemented against a spec that has not been approved seems rather unlikely. Does that pretty much means not changes can be made to the spec because there is no spec to follow. Rather circular logic.

So the purpose of this PR was to refine the proposal and then get a vote which would be result in this PR being either approved or rejected. How do we get to that phase?",2018-05-04T02:18:29Z,118856
330,rack/rack,318673916,386490611,"> Waiting for several applications to be implemented against a spec that has not been approved seems rather unlikely.

I don't think so. Making some sample apps is a great way to document the API, provides a great starting point for new users, etc. And finally, if the sample apps can't work between servers, what's the point of a shared SPEC?",2018-05-04T02:22:39Z,30030
331,rack/rack,318673916,386491161,"> There are currently two implementations of the originally proposed spec, Agoo and Iodine. I'd think that if proof that the spec is workable.

I think we need to see a working implementation in at least Puma & Passenger as a baseline since that is what is very common for Ruby development and production.",2018-05-04T02:26:56Z,30030
332,rack/rack,318673916,386492282,I believe the suggested bar was several practical applications. There are already sample applications in the Agoo and Iodine repositories. ,2018-05-04T02:35:45Z,118856
333,rack/rack,318673916,386492566,"> To implement several practical applications

Something more practical than ""Hello World"". For example, a shared real-time Todo list :p

I think we should be involving some database activity in the callback, probably using ActiveRecord or Redis.

If you've already got several sample applications, once we finalise a draft SPEC and we all implement it, we should be able to all run those apps on our respective servers, right? That's the whole point of having a shared SPEC.",2018-05-04T02:38:32Z,30030
334,rack/rack,318673916,386493067,"I still don't see a statement about whether `write` can block.

I'm also unconvinced that pending/drained is an effective solution for non-blocking writes.

I remain of the opinion that this should remain a draft / small-e extension while real-world implementations adopt it and fully exercise it.

Easy-mode for convincing me would be a PR to Rails adapting Action Cable to this API -- which would also allow like-for-like benchmarking.",2018-05-04T02:43:40Z,1034
335,rack/rack,318673916,386513684,"@matthewd ,

Although I understand the suggestion (adding a Rails PR), I find that quite unfair.

I can program a server in C and I can wrap in a gem for Ruby MRI... but I know nothing about ActionCable, it's inner workings or the Client<=>Application protocol used above the WebSocket layer.

The learning curve and time spent on a Rails patch would only be viable if the PR is conceptually acceptable and we all intend to move forward with the proposal and implement it.

I really doubt if pushing a PR to Rails is something that should happen before we agree that the PR is conceptually acceptable (i.e., contingent on benchmarks, or whatever).

As a side note, I also don't think this should enter a Rails 5.x update release. This PR has the potential to retire the Rails IO handling layer (the one based on `nio4r`) and reduce the existing codebase significantly - at least where WebSockets and SSE are concerned (and long polling could be achieved by [SSE Javascript's polyfills](https://github.com/Yaffle/EventSource), retiring `hijack` altogether).
",2018-05-04T06:15:31Z,870622
336,rack/rack,318673916,386514494,"> I still don't see a statement about whether write can block.

@matthewd , I assumed (since there's no statement) that it would be implementation defined.

I'm assuming that blocking servers will be avoided by applications that are effected by the behavior.

But I totally understand if you believe there should be a concurrency model requirement regarding `write` and `close` not blocking. After all, if `write` blocks, pub/sub performance might grind to a halt.",2018-05-04T06:21:40Z,870622
337,rack/rack,318673916,386583565,@matthewd would a statement leaving the blocking behavior up the the server be enough or maybe a method to ask if blocking would block other threads?,2018-05-04T12:17:16Z,118856
338,rack/rack,318673916,387379194,"I have an initial development implementation of the updated draft, placed in [iodine's 0.6.0 version branch](https://github.com/boazsegev/iodine/tree/0.6-stripped) (installable from source).

There's some features being re-written (such as pub/sub extensions, including Redis integration), but the core WebSocket/SSE and pub/sub API will work.

I think this would be enough to author a test application using a monkey-patched Rails...

@matthewd , any final changes / requests / tips before we start woking on a ""like-for-like benchmarking"" example app? How do you want to benchmark this? Should we use the [websocket-shootout approach](https://github.com/hashrocket/websocket-shootout/tree/master/ruby/action-cable-server)?

(it might not test what we're looking at, as memory consumption and performance will still depend heavily on the pub/sub system rather than the connection system...)",2018-05-08T12:00:22Z,870622
339,rack/rack,318673916,387410220,The Agoo develop branch is also compatible with the spec as described in the current state of this PR. Should make a versioned release later this week once the named pub-sub feature is completed.,2018-05-08T13:54:13Z,118856
340,rack/rack,318673916,387612175,I cleaned up the formatting of the SPEC around the upgrade section. An HTML version is at [http://www.ohler.com/agoo/rack/file.SPEC.html](http://www.ohler.com/agoo/rack/file.SPEC.html).,2018-05-09T03:51:35Z,118856
341,rack/rack,318673916,388965736,"@evanphx , how is the Puma adaptation going?

So far, the API iodine implements for the `client` object looks [like this](https://www.rubydoc.info/gems/iodine/0.6.0/Iodine/Connection).

It includes a number of methods not listed in the specification besides the Pub/Sub extensions (the `timeout`, `timeout=` and `protocol` method).

I also added the `env` accessor so I could avoid making objects just to keep the `env` alive. This is something I need for an upcoming plezi.io release and I'm not sure if it should be adopted (as it adds longevity to all the objects in the `env` Hash, which is bound to increase memory fragmentation).

Anyway, I hope to see this move forward. Iodine has been serving clients like this for the last couple of year or so (albeit using `extent` rather than a client object) and so far the feedback is positive and the performance is promising.",2018-05-14T21:16:49Z,870622
342,rack/rack,318673916,388993015,If you need extensions to make anything useful they need to be part of the spec IMHO.,2018-05-14T23:19:46Z,30030
343,rack/rack,318673916,389001235,"@ioquatix , this was my thought as well, which is why I wrote it here. Perhaps these added methods (`timeout`, `timeout=` and `protocol`) should be part of the specification.

On the other hand, I don't have to have the `env` variable, I just as easily could have create a callback object (using a class instance) instead of static module... it was more of a design preference that I thought I should mention.

**EDIT** I should note that the `timeout` methods aren't required to make WebSockets / SSE work. It's just that iodine also supports raw TCP/IP connections and these connections require access to timeout management.",2018-05-15T00:08:43Z,870622
344,rack/rack,318673916,389005949,"The entire pub-sub pattern can be implemented by keeping an Array of connections. A publish would then call write on each one. The reason pub-sub was not included in the PR was that it added another new use pattern to the spec that would take even more effort for servers to comply with.

Like @boazsegev and Iodine Agoo includes a pub-sub implementation that is compatible to a large degree with Iodine. In then end we wanted something basic that could be used with a minimum of changes in Rack and the way developers use it.

The current PR focuses just on Push without also introducing pub-sub. Maybe a future PR can propose the addition of pub-sub. One step at a time.",2018-05-15T00:40:04Z,118856
345,rack/rack,318673916,389015687,"> The entire pub-sub pattern can be implemented by keeping an Array of connections. A publish would then call write on each one. The reason pub-sub was not included in the PR was that it added another new use pattern to the spec that would take even more effort for servers to comply with.

This wouldn't work with falcon in forked mode, since it forks one process per CPU core and they all handle connections independently.

Even in threaded mode, some kind of synchronisation is required. I believe the same problem would affect puma in cluster mode, for example.

My opinion is that this spec needs to be feature complete, as in, it's possible within the confines of the spec to implement actual useful websocket applications. Otherwise, what's the point? If every server supports the core spec but provides their own incompatible models on top of that, this isn't going to work out very well.",2018-05-15T01:43:24Z,30030
346,rack/rack,318673916,389020040,"The PR, as it stands, is feature complete for WebSocket and SSE. Sure, extending the PR to include pub-sub would be a problem for servers that don't share state but that has nothing to do with WebSocket and SSE support proposed here.

@ioquatix, Slamming @boazsegev example code that is meant to demonstrate not only the proposed Rack additions but also other features of Iodine and then claiming that example invalidates this PR is ludicrous. The PR offers a clean and simple way for even new developers to take advantage of WebSockets and SSE. Some of the comments such as those from @tenderlove were constructive and genuinely aimed toward enhancing the current Rack spec with the addition of WebSockets and SSE. You have fought tooth and nail against it from the start with an extreme passion to block any variation on the PR. I don't understand why you feel so threatened by this PR. Can you explain why you have such strong feelings about offering Rack users an simple to use approach to WebSocket and SSE?",2018-05-15T02:10:02Z,118856
347,rack/rack,318673916,389022780,"Okay, this conversation clearly needs to go back on ice for a bit.

I don't agree that pub/sub is within scope, but if we can't do each other the courtesy of assuming genuine opinions and positive/constructive intentions, then we're not going to get anywhere useful.",2018-05-15T02:27:52Z,1034
348,rack/rack,318673916,389285061,"TBH I'm kind of worried about committing to an API without a real world app using it.  The reason is because I want to make sure we don't forget anything we need to implement actual apps.  If we could get Action Cable running on top of a server that used this API, then I'd be satisfied to merge.",2018-05-15T19:27:55Z,3124
349,rack/rack,318673916,582273156,"There has been some great discussion here. Thanks everyone for their effort and passion. I am going to close this issue, because I don't think we reached a consensus.

That being said, further discussion is required, backed by concrete implementations. Specifically, I think more emphasis should be placed on HTTP/2 (and HTTP/3) and union of the semantic model when HTTP/1 is also included in the mix.

We welcome proposals for a better stream model for Rack 3.0 and we are currently considering how to make `rack.hijack` a first class citizen: https://github.com/rack/rack/issues/1550",2020-02-05T07:15:11Z,30030
350,chef/ohai,608513361,608513361,We can do better here.,2020-04-28T18:10:10Z,1015200
351,rubygems/rubygems.org,1156931157,1156931157,"Hi Team, thank you for all the work you do for the RubyGems to be accessible by everyone!
I'm from Ukraine and Russian army is bombarding our cities and killing our children, doing acts of terror.
And Russian IT stays neutral because ""it's not them who kill people"".  
Please help us to raise their awareness by signing this petition and restricting any traffic from Russia for some time:
https://github.com/stop-war-in-ukraine/stop-russia-it",2022-03-02T09:47:25Z,1910505
352,rubygems/rubygems.org,1156931157,1057758785,It violates OSS philosophy. Rubygems.org have no plan to ban Russia. ,2022-03-03T07:38:14Z,12301
353,capistrano/capistrano,13258430,13258430,"If you set up some task this way:

before 'deploy', 'some_task', 

And in some_task you simply read the ""latest_release"" variable (in a puts line for instance), then:

Current results:
a) If it's the first deploy ever in capistrano (the www folder doesn't exist), capistrano breaks completely, it cannot deploy.
b) If it's not the first deploy, then it will make capistrano change its behaviour with regards to the ""current"" symlink, because it will point to the previous release instead of the last one (after the deploy happened).

Expected results:
a) It should work.
b) It should point current to the latest release.

This is a big fuckup IMHO.
",2013-04-16T17:39:09Z,331303
354,capistrano/capistrano,13258430,16461106,"> This is a big fuckup IMHO.

Profanity aside, you may have a point. I'll take a patch.
",2013-04-16T18:02:11Z,18952
355,capistrano/capistrano,13258430,16461191,"Why do you close the issue?
",2013-04-16T18:03:13Z,331303
356,capistrano/capistrano,13258430,16461276,"Because you don't offer a patch, and profanity really pisses me off.
",2013-04-16T18:04:33Z,18952
357,capistrano/capistrano,13258430,16461378,"Mutable programming pisses me off even more.

Anyway, the point of github issues is to have a bug tracker.

The contributions comes in the pull-requests tab.
",2013-04-16T18:06:12Z,331303
358,capistrano/capistrano,13258430,16461583,"Indeed, in short - if there's no release, you are doing something wrong if you are calling `latest_release`. It's not safe for use when it's a cold deploy. _It's that simple._

There are other variables which are safe for use throughout the deploy process.

Capistrano, the 2.0.x branch mainline is effectively on the verge of being deprecated, send a patch or things won't be fixed. And even if you do send a patch, changing the behaviour of core variable lookups is not something I approach lightly.

I'd rather invest that time into the v3 branch which will coincide with the Rails 4 release in the coming weeks, and have a stable version which has fewer insane design decisions behind it.
",2013-04-16T18:09:36Z,18952
359,capistrano/capistrano,13258430,16461645,"As for your second point, you may be onto something there, but it's too late to change it, it's unreasonable overhead to call back to the server to calculate that every time, the results are cached for a reason. If you populate the cache with bad data, then you can't really effectively clear it. I accept that it's confusing, but I won't change it.
",2013-04-16T18:11:02Z,18952
360,capistrano/capistrano,13258430,16461791,"I understand that latest_release may not have the proper value when it's queried at the wrong time... but CHANGING BEHAVIOUR???? Sorry for the caps, but really finding out this bug has put me on my nerves.

In regards to your backwards-compatibility policy: fair enough, I understand if you prefer to accept only a patch to fix this in master rather than the 2.0.x branch, but:

a) I never proposed to fix it in a branch.
b) This bug is not fixed yet on any branch, so it should be reopened. Otherwise people confused about it will not be able to find it.

This is, my friend, how bug trackers work.
",2013-04-16T18:13:30Z,331303
361,capistrano/capistrano,13258430,16462166,"I won't fix it, and it's been the way it is for five years without anyone running into problems. I can't afford the time to test it, fix it and make sure it's safe before releasing it. That's what it boils down to. Those variables are a source of a lot of confusion, and they are fragile, and have different meanings wherever you call them, anyway.

I'd like to be able to fix it, your profane issue report aside; but the reality is, I simply can't.
",2013-04-16T18:20:44Z,18952
362,capistrano/capistrano,13258430,16462334,"Then leave this issue open until the end of times.
",2013-04-16T18:23:41Z,331303
363,capistrano/capistrano,13258430,16462422,"Otherwise it would be the first time in my entire life in which I see a bug closed as WONTFIX.

I mean, I've seen feature requests closed as WONTFIX status. But bugs? It's like denying to recognize that there is a bug.
",2013-04-16T18:25:27Z,331303
364,capistrano/capistrano,13258430,16462602,"Just for the sake of people being able to find the issue (and be enlightened by the work-around: ""not read a variable""), this issue should remain open.
",2013-04-16T18:28:48Z,331303
365,capistrano/capistrano,13258430,16462723,"Agreed. I've re-ragged with v2.
",2013-04-16T18:30:49Z,18952
366,capistrano/capistrano,13258430,27621011,"Why did you close this? People are getting here from this SO question: http://stackoverflow.com/questions/3141454/deploysymlink-on-capistrano-points-the-current-directory-to-previous-release/16043844#16043844 and if they find this issue closed, they might be misled into thinking that it is fixed.
",2013-11-02T12:34:21Z,331303
367,capistrano/capistrano,13258430,27625808,"@knocte did you reproduce it on v3?
",2013-11-02T16:44:26Z,522155
368,capistrano/capistrano,13258430,27630493,"Oh, when was that released?
",2013-11-02T19:30:24Z,331303
369,capistrano/capistrano,13258430,27630538,"See rubygems, early september
On 2 Nov 2013 20:30, ""Andres G. Aragoneses"" notifications@github.com
wrote:

> Oh, when was that released?
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/capistrano/capistrano/issues/440#issuecomment-27630493
> .
",2013-11-02T19:32:49Z,18952
370,capistrano/capistrano,13258430,27630609,"Ok, then I have not tested that. And I'm afraid I will not be able to test it very soon.

Just one advice: if you want to close issue in order to request feedback from users about new versions, add a comment explaining why you are closing the issue.
Thanks
",2013-11-02T19:36:30Z,331303
371,capistrano/capistrano,13258430,27642117,"We have more than 100 obsolete issues  to close, and past experience
suggests the OPs never reapond
On 2 Nov 2013 20:36, ""Andres G. Aragoneses"" notifications@github.com
wrote:

> Ok, then I have not tested that. And I'm afraid I will not be able to test
> it very soon.
> 
> Just one advice: if you want to close issue in order to request feedback
> from users about new versions, add a comment explaining why you are closing
> the issue.
> Thanks
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/capistrano/capistrano/issues/440#issuecomment-27630609
> .
",2013-11-03T10:33:11Z,18952
372,capistrano/capistrano,13258430,27737223,"That doesn't respect the Robust principle ;) http://en.wikipedia.org/wiki/Robustness_principle
",2013-11-05T00:31:44Z,331303
373,capistrano/capistrano,13258430,286312317,"People are still upvoting my stackoverflow answer linked above, which seems to hint that this bug is still present in 3.x.",2017-03-14T03:33:32Z,331303
374,capistrano/capistrano,13258430,286354139,thanks for checking the so answer upvote rates @knocte ,2017-03-14T08:32:31Z,18952
375,capistrano/capistrano,13258430,378936727,"Running into this issue currently I think.

How does one call the current release path that is right now being built?

I have a task running at `after :updated, :build do` and I'm trying to change directories and then run composer install, but changing directories using release_path is giving me the previous release, not the one currently being built.

Is that this issue? If not I'm sorry but the stackoverflow question led me here and I'm going kind of crazy trying to figure out how to change directories and then run a command, without it doing it in the release that's about to be replaced.

",2018-04-05T13:31:54Z,4382195
376,capistrano/capistrano,13258430,378988513,"This issue is more than five years old, and refers to a long obsolete version of Capistrano, please open a new issue.",2018-04-05T16:05:02Z,18952
377,capistrano/capistrano,13258430,534820283,"Can someone link to this ""new"" issue?",2019-09-25T02:13:32Z,5461429
378,capistrano/capistrano,13258430,535014772,"This issue is very old and the discussion was not very productive, so I will lock it. If you are a Capistrano v3 user running into an error using the `latest_release` setting please open a new issue.",2019-09-25T13:10:53Z,189693
379,capistrano/capistrano,13258430,535014941,"@batmanbury there is no ""new"" issue; please open one if you are running into this problem.",2019-09-25T13:11:18Z,189693
380,django-extensions/django-extensions,488965754,488965754,"[root@iZ25qp2d64mZ EHT]# python3 manage.py runserver_plus --cert server.crt 127.0.0.1:7000
Traceback (most recent call last):
  File ""manage.py"", line 10, in <module>
    execute_from_command_line(sys.argv)
  File ""/usr/local/lib/python3.5/site-packages/django/core/management/__init__.py"", line 338, in execute_from_command_line
    utility.execute()
  File ""/usr/local/lib/python3.5/site-packages/django/core/management/__init__.py"", line 330, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File ""/usr/local/lib/python3.5/site-packages/django/core/management/base.py"", line 390, in run_from_argv
    self.execute(*args, **cmd_options)
  File ""/usr/local/lib/python3.5/site-packages/django/core/management/base.py"", line 441, in execute
    output = self.handle(*args, **options)
  File ""/usr/local/lib/python3.5/site-packages/django_extensions/management/utils.py"", line 62, in inner
    ret = func(self, *args, **kwargs)
  File ""/usr/local/lib/python3.5/site-packages/django_extensions/management/commands/runserver_plus.py"", line 262, in handle
    self.inner_run(options)
  File ""/usr/local/lib/python3.5/site-packages/django_extensions/management/commands/runserver_plus.py"", line 305, in inner_run
    self.check_migrations()
AttributeError: 'Command' object has no attribute 'check_migrations'",2019-09-04T06:23:48Z,35052597
381,django-extensions/django-extensions,488965754,527760429,PS:My django version is 1.8.2,2019-09-04T06:24:33Z,35052597
382,django-extensions/django-extensions,488965754,527764303,come on~ I need help~ ,2019-09-04T06:38:15Z,35052597
383,django-extensions/django-extensions,488965754,527770339,"Please look at; https://github.com/django-extensions/django-extensions#support

Ask politely and respectfully both for the people and their time.

`Django 1.8.2` is not supported anymore in `Django Extensions` and the `check_migrations` method is not available for commands in older `Django` versions. So this regretfully will not work.

Please upgrade your `Django` version.",2019-09-04T07:00:02Z,65552
384,celery/celery,1527268155,1527268155,"Celery sucks by simply don't processing jobs in a gevent pool, jobs simply dangling forever, there is no fail-safe mechanism at all. Task simply block themselves until they die in the queue blocking the whole worker, especially long-running I/O tasks. I'm really done with this crappy project, have a nice day.",2023-01-10T11:59:48Z,43114687
385,celery/celery,1527268155,1377153592,"Hey @venomone :wave:,
Thank you for opening an issue. We will get back to you as soon as we can.
Also, check out our [Open Collective](https://opencollective.com/celery) and consider backing us - every little helps!

We also offer priority support for our sponsors.
If you require immediate assistance please consider sponsoring us.
",2023-01-10T11:59:50Z,46743201
386,celery/celery,1527268155,1377155803,"Not to forget dying sql connection you have to fix with a hack like this:

```
def possible_reconnect_required():
    try:
        for name, config in settings.DATABASES.items():
            module = importlib.import_module(config[""ENGINE""] + "".base"")

            def ensure_connection(self):
                if self.connection is not None:
                    try:
                        with CursorWrapper(self.create_cursor(), self) as cursor:
                            cursor.execute(""SELECT 1"")
                        return
                    except Exception:
                        pass

                with self.wrap_database_errors:
                    self.connect()

            module.DatabaseWrapper.ensure_connection = ensure_connection
        connection.ensure_connection()
    except django.db.utils.OperationalError:
        connection.close()
        connection.connect()
```

Otherwise, tasks don't keep the connection to the DB except they are running just for a couple of seconds ...",2023-01-10T12:01:47Z,43114687
387,celery/celery,1527268155,1377157774,@venomone avoid doing this any further. you can gracefully open a discussion in github forum.,2023-01-10T12:03:23Z,6212603
388,celery/celery,1527268155,1377165142,@venomone you will get blocked if you this again ever in any open source projects.,2023-01-10T12:09:12Z,6212603
389,apache/couchdb,613337048,613337048,"## Summary

In some scenarios it can be easier/safer to pass JWT tokens to clients through cookies (secure, sameSite, httpOnly, etc) so that they don't store those in localStorage/have to add those themselves to requests to the back-end.

Having CouchDB be able of extracting a JWT from a Cookie in addition/as an alternative to the Authorization header would be beneficial for such scenarios.

## Desired Behaviour

Being able to configure CouchDB to extract JWT tokens from cookies (in addition or as an alternative to the Authorization header).

## Additional context

My project uses a fairly classical high level architecture:
* Back-end system exposing a RESTful API
* Front-end single-page application

The back-end RESTful API issues self-signed JWT tokens and provides those to the SPA through secure/samesite/httpOnly cookies so as to simplify management and keep a good level of security. 

This works fine in our scenario since there's a single origin. When the SPA issues requests against the RESTful API, the cookie containing the JWT token is attached transparently by the user agent.

If that same JWT token could also be trusted by CouchDB (which is now possible with 3.1.0 as far as I understand) and if CouchDB could extract it from the cookie, then our SPA clients could seamlessly authenticate against CouchDB.

For the longer term, OAuth support would indeed be nice, but I can imagine that it is already somewhere on CouchDB's roadmap.
",2020-05-06T13:51:40Z,89887
390,apache/couchdb,613337048,1616316141,"**I second this motion.** 

Parsing HTTP Cookies is trivial. CouchDB would simply need to specify the key, or the name, of the cookie that contains the JWT.

**I recommend cookie shall have a configurable key value which can be set under the `[chttpd]` section. This delivers maximum flexibility.**

It is said to be best practice to issue JWT's as HTTPOnly cookies meaning that they are not available to client/browser JavaScript. In other words, _HTTPOnly_ cookies make it impossible to build a custom header aligning with CouchDB's interface, namely `Authorization: Bearer <JWT token>`

https://docs.couchdb.org/en/stable/api/server/authn.html#jwt-authentication

RFC 6265, HTTP State Management Mechanism

> [Section 4.1.2.6.  The HttpOnly Attribute](https://datatracker.ietf.org/doc/html/rfc6265#section-4.1.2.6)

>    The HttpOnly attribute limits the scope of the cookie to HTTP
>    requests.  In particular, the attribute instructs the user agent to
>    omit the cookie when providing access to cookies via ""non-HTTP"" APIs
>    (such as a web browser API that exposes cookies to scripts).",2023-07-02T02:45:20Z,12751611
391,apache/couchdb,613337048,1616628878,"Hi,

you can use JWT tokens without requesting sessions via `_/session` API.",2023-07-02T12:19:58Z,9985963
392,apache/couchdb,613337048,1616765804,I need to be able to have CouchDB read cookies in order for me to not expose the JWT to client side JavaScript.,2023-07-02T18:28:23Z,12751611
393,apache/couchdb,613337048,1616770080,What are you trying to achieve? Session cookies and JWT tokens are seperate things. A client can request a session with username/pasword or can authenticate with a JWT token.,2023-07-02T18:44:30Z,9985963
394,apache/couchdb,613337048,1616772698,"I want to authenticate with JWT tokens. I have it working.

Couch requires the JWT be passed in a specific _client request_ HTTP header, namely `Authorization: Bearer <JWT token>`. Right?

How can I set this _client request header_ on the server?",2023-07-02T18:50:27Z,12751611
395,apache/couchdb,613337048,1616773782,"> How can I set this _client request header_ on the server?

Which server do you mean, the CouchDB server?",2023-07-02T18:56:04Z,9985963
396,apache/couchdb,613337048,1616774650,"No the web server. In my case Node.js, or PHP, any web server.",2023-07-02T18:59:44Z,12751611
397,apache/couchdb,613337048,1616778006,"To summarize this and get this correct. Your problem is to add this `Authorization: Bearer <JWT token>` header to your client request, because CouchDB expects this as a http request header to authenticate via JWT token?",2023-07-02T19:14:18Z,9985963
398,apache/couchdb,613337048,1616779364,"Precisely. The JWT is stored in an _HHTPOnly_ cookie on the browser/client. So, how do I set the clients HTTP `Authorization` header server side?",2023-07-02T19:17:51Z,12751611
399,apache/couchdb,613337048,1616785732,"You can't. You can, however, set client HTTP cookies server side. So, we need Couch to look for the JWT in an HTTP cookie named _`<configurable>`_.

The idea is that it is a security measure to _not_ expose the JWT to client side scripting.",2023-07-02T19:26:52Z,12751611
400,apache/couchdb,613337048,1616795761,"> Precisely. The JWT is stored in an _HHTPOnly_ cookie on the browser/client. So, how do I set the clients HTTP `Authorization` header server side?

The JWT token has nothing to do with a session or a cookie. You send it with your request to get access (if correct). How you (securly) store them on your client is a completely different question.",2023-07-02T19:38:06Z,9985963
401,apache/couchdb,613337048,1616796887,"Add support for authentication/authorization using cookies containing JWTs is the issue.

We need Couch to look for JWT's in a cookie.

How to securely store JWT's on your client includes using HTTPOnly cookies which makes it impossible to authenticate to Couch (unless you expose the JWT to client side scripting).",2023-07-02T19:41:48Z,12751611
402,apache/couchdb,613337048,1616801953,"> We need Couch to look for JWT's in a cookie.

How should this work? The server can't send the client the credentials to obtain access. It's the same as when the server would sends the client a username/password combination. 
The client needs to provide the server credentials to authenticate and not the server.

If you want a cookie (httponly - with a JWT token) from the server, how do you authenticate against it and/or with what credentials?",2023-07-02T19:58:18Z,9985963
403,apache/couchdb,613337048,1616804922,"CouchDB does not issue JWT's. CouchDB _verifies_ JWTs (vis a vis the `[jwt-keys]` that it is configured with.

CouchDB supports Authentication via JWT so long as the JWT is sent in an HTTP header titled `Authorization`.

CouchDB SHOULD support authentication via JWT sent as an HTTP cookie.

Why? Bcs cookies are more secure in that they can be stored on a client as _HTTPOnly_. ",2023-07-02T20:10:54Z,12751611
404,apache/couchdb,613337048,1616808031,"> CouchDB does not issue JWT's. CouchDB _verifies_ JWTs (vis a vis the `[jwt-keys]` that it is configured with.
> 

I know 😉

> CouchDB supports Authentication via JWT so long as the JWT is sent in an HTTP header titled `Authorization`.
> 

Correct.

> CouchDB SHOULD support authentication via JWT sent as an HTTP cookie.
> 

Ok, and how do you request that cookie? 

",2023-07-02T20:23:04Z,9985963
405,apache/couchdb,613337048,1616809073,"CouchDB does request cookies. CouchDB receives cookies.

Another server issues the cookie to the client/browser. See https://developer.mozilla.org/en-US/docs/Web/HTTP/Cookies#define_where_cookies_are_sent",2023-07-02T20:27:13Z,12751611
406,apache/couchdb,613337048,1616812242,"> CouchDB does request cookies. CouchDB receives cookies.
> 

I know, you didn’t answer my question. How do you request your cookie?
",2023-07-02T20:41:28Z,9985963
407,apache/couchdb,613337048,1616814153,"Cookies are issued to browsers/clients by servers. Cookies are not requested, they are received.

A server sends a browser a cookie with a JWT as the value. The browser includes that cookie (that JWT) in _every_ HTTP call to all servers on a domain or by path (configurable)

<img width=""1160"" alt=""image"" src=""https://github.com/apache/couchdb/assets/12751611/1b23ae80-0bda-4966-bdda-12afedd6177e"">
",2023-07-02T20:50:20Z,12751611
408,apache/couchdb,613337048,1616819266,"CouchDB sends a cookie if you request a session. To get a session, you need to provide a username/password. If you are authenticated successfully, you will get this session cookie (httponly), which can be used for further requests.

If you have an JWT token, you can send this with a header. If this token is correct, you get access to the system. You have to send this token with every request.

If you have a username/password you don’t need a JWT token for requests or vice-versa. ",2023-07-02T21:09:10Z,9985963
409,apache/couchdb,613337048,1616820026,This issue seems elusive to you. I'll let someone else run with it.,2023-07-02T21:12:53Z,12751611
410,apache/couchdb,613337048,1616825174,"> This issue seems elusive to you.

No, but you don’t tell me how you want request your cookie (with your token), because you need an authorization before to get the cookie, but when you have an authorization before you don’t need a JWT token anymore, because you can already authenticate against CouchDB.

> I'll let someone else run with it.
🍀",2023-07-02T21:26:15Z,9985963
411,apache/couchdb,613337048,1616829422,"How I get my JWT is not the issue and has nothing to do with CouchDB. I think this is where you might be missing me?

![image](https://github.com/apache/couchdb/assets/12751611/4990bc20-4c3e-456e-b2ee-ee1b3f41cbc2)
",2023-07-02T21:34:32Z,12751611
412,apache/couchdb,613337048,1616861362,Maybe [proxy auth](https://docs.couchdb.org/en/stable/api/server/authn.html#proxy-authentication) is more suitable for you?,2023-07-02T22:13:17Z,9985963
413,apache/couchdb,613337048,1617028737,"Proxy auth does not remedy this issue and is not related to this issue.

We don't want to expose JWT's to client/browser JavaScript. CouchDB accepting JWT via cookies is the remedy.",2023-07-03T00:20:15Z,12751611
414,apache/couchdb,613337048,1617664814,"RFC 6750, § 5.3,

> Don't store bearer tokens in cookies:  Implementations MUST NOT store
      bearer tokens within cookies that can be sent in the clear (which
      is the default transmission mode for cookies).  Implementations
      that do store bearer tokens in cookies MUST take precautions
      against cross-site request forgery.

",2023-07-03T08:56:42Z,47223
415,apache/couchdb,613337048,1617668261,Unless someone can point to an RFC or other authoritative source I don't think CouchDB will support receiving JWT's in a cookie. We require them to be sent as bearer tokens.  ,2023-07-03T08:58:58Z,47223
416,apache/couchdb,613337048,1618642521,"CouchDB itself uses authentication tokens as _HTTPOnly_ cookies. The `AuthSession` cookie below is from Couch.

Somebody on the CouchDB development team understands the need for implementing accepting JWT's via cookies.

<img width=""706"" alt=""image"" src=""https://github.com/apache/couchdb/assets/12751611/5cbdb113-ca9a-47b4-ab58-8b8cc9ba1899"">
",2023-07-03T15:24:10Z,12751611
417,apache/couchdb,613337048,1618947557,"There is no ""need"" for us to do so. We are well-versed in the various cookie properties, including HttpOnly.",2023-07-03T17:54:09Z,47223
418,apache/couchdb,613337048,1618977062,I'm not understanding why you refuse to allow app developers to take advantage of http cookies as bearers of auth tokens just like Couch does?,2023-07-03T18:22:11Z,12751611
419,apache/couchdb,613337048,1622515799,"""CouchDB supports cookie authentication"" [[source](https://docs.couchdb.org/en/stable/intro/security.html#cookie-authentication)] **_unless you use JWT authentication_**. SHOULD be added to documentation.",2023-07-05T21:09:49Z,12751611
420,apache/couchdb,613337048,1624021970,Such documentation clarifies that CouchDB team understands the use case and benefit(s) for cookie authentication and explicitly refuses to extend these benefits to the JWT handler [Issue# 2873](https://github.com/apache/couchdb/issues/2873).,2023-07-06T17:09:34Z,12751611
421,apache/couchdb,613337048,1624041986,enough of this.,2023-07-06T17:23:23Z,47223
422,GoldenCheetah/GoldenCheetah,869840472,869840472,"Hi,

I've imported my rides from Garmin Connect to add power estimates and I've noticed that the normalised Power is often well overstated, the average power looks sensible but for example on one ride the avg watts was 157 and the normalised was 262, this is massively overstated I'd expect maybe 165 to 170.  I can provide the TCX file if it helps.

Many Thanks,

Ralph.

Issue tracker is **only** for Bugs and Features, please don't open issues for questions or technical support. Before to open a new issue please read the contributing guidelines (link below).

If you have questions, please read the FAQs and User's/Developer's Guide:
* FAQs - https://github.com/GoldenCheetah/GoldenCheetah/wiki/FAQ
* User's Guide - https://github.com/GoldenCheetah/GoldenCheetah/wiki/UG_Main-Page_Users-Guide
* Developer's Guide - https://github.com/GoldenCheetah/GoldenCheetah/wiki/Developers-guide

If you need help or technical support please use the forums:
* Users - https://groups.google.com/forum/#!forum/golden-cheetah-users
* Developers - https://groups.google.com/forum/#!forum/golden-cheetah-developers
",2021-04-28T11:28:04Z,83346436
423,GoldenCheetah/GoldenCheetah,869840472,828409105,"Hi,

This is a bug, I've been writing software for over 30 years and certainly in my experience when said software produces incorrect results it generally has a bug, admittedly Garmin could possibly be at fault if their TCX data is wrong but I highly doubt that so my money is very much on a bug in the Golden Cheetah software. 

Totally unhelpful response from the moderator.",2021-04-28T12:21:13Z,83346436
424,GoldenCheetah/GoldenCheetah,869840472,828502233,"We have 2 forums for that kind of discussions, use whatever you find more convenient.",2021-04-28T14:28:51Z,1444784
425,GoldenCheetah/GoldenCheetah,548445007,548445007,"Strava Sync funktioniert nicht mit Version 3.5 RC2X. Habe alles versucht, neu authentiziert, etc.
Token in GC ist nicht der gleiche wie in Strava
Fehler: ""Zeitüberschreitung bei Socketoperation""
",2020-01-11T15:47:22Z,56628276
426,GoldenCheetah/GoldenCheetah,548445007,573337228,see https://groups.google.com/d/msg/golden-cheetah-users/TIXYuun6ntY/vkz5AIJ1BwAJ,2020-01-11T17:35:42Z,1444784
427,GoldenCheetah/GoldenCheetah,548445007,573342309,Dies funktioniert aber nicht !,2020-01-11T18:30:14Z,56628276
428,GoldenCheetah/GoldenCheetah,548445007,573343114,"Please, use the forum to get help.",2020-01-11T18:38:28Z,1444784
429,GoldenCheetah/GoldenCheetah,548445007,573343614,i think this is the forum,2020-01-11T18:45:01Z,56628276
430,GoldenCheetah/GoldenCheetah,548445007,573355133,There’s a link to the forum in the 1st answer.,2020-01-11T21:16:10Z,1444784
431,GoldenCheetah/GoldenCheetah,548445007,573394727,"so i've done everything as described in the link and also reinstalled GC.
It is still not working, so it must be a false code!!!!",2020-01-12T08:50:23Z,56628276
432,GoldenCheetah/GoldenCheetah,548445007,573395673,"I just tried it and it works. Authorise with Strava via add account, then sync (make sure you set the date range to a range that includes some activity).

If the code was broken there would be a lot more reports of issues, believe me. We have a large user base. 

Make sure you are using the latest dev build from here:
https://github.com/GoldenCheetah/GoldenCheetah/releases/tag/V3.5-RC2X

Also, the full release is due in a week or so.",2020-01-12T09:06:03Z,142876
433,GoldenCheetah/GoldenCheetah,548445007,573397136,"So i tried it again, nothing works. After login  in strava via Facebook and allow Golden cheetah, the fault ""socket Operation (4)"" appears. ",2020-01-12T09:28:18Z,56628276
434,GoldenCheetah/GoldenCheetah,548445007,573397310,How should i set a Data Range?,2020-01-12T09:30:51Z,56628276
435,GoldenCheetah/GoldenCheetah,548445007,573405524,"If you post in the users forum may be other user with the same problem can explain what he did i to solve it.
GitHub issues are used to track bugs and features only, as a FOSS we don’t have the resources to give case by case support here.",2020-01-12T11:26:23Z,1444784
436,GoldenCheetah/GoldenCheetah,540979735,540979735,"Hello, 

TSS edit on Details/Metric tab is not working (running V3.5-RC2X) although it was working properly in previous version. A number can be edited but when clicked outside of the box it goes to zero automatically. I made a search but did not find a similar topic.

Thanks.
",2019-12-20T12:50:02Z,56796799
437,GoldenCheetah/GoldenCheetah,540979735,567951688,"TSS is being deprecated in v3.5, there is a compatibility metric named TSS to enable pre-existing charts to continue working but it has several limitations. I recommend to add a Data Field in preferences similar to your existing TSS but named BikeStress and use it for overrides, it will impact on your TSS dependent charts and it is a better solution for moving forward.",2019-12-20T14:53:30Z,1444784
438,GoldenCheetah/GoldenCheetah,540979735,567955734,"> TSS is being deprecated in v3.5, there is a compatibility metric named TSS to enable pre-existing charts to continue working but it has several limitations. I recommend to add a Data Field in preferences similar to your existing TSS but named BikeStress and use it for overrides, it will impact on your TSS dependent charts and it is a better solution for moving forward.

Many thanks for your response. I have manually edited BikeScore in the Details/Metrics tab of the activity, but it does not impact on TSS unfortunately. I searched under chart settings to add Bike Stress but there is no option. :(",2019-12-20T15:05:26Z,56796799
439,GoldenCheetah/GoldenCheetah,540979735,567980877,"BikeScore is an independent metric, the new name for TSS is BikeStress and you can add it to Details tab with an entry like this in Options/Preferences > Data Fields:
![BikeStressOverride](https://user-images.githubusercontent.com/1444784/71268127-f6891900-232a-11ea-8306-7352eea9f0f3.png)
",2019-12-20T16:18:09Z,1444784
440,GoldenCheetah/GoldenCheetah,540979735,568192681,"> BikeScore is an independent metric, the new name for TSS is BikeStress and you can add it to Details tab with an entry like this in Options/Preferences > Data Fields:
> ![BikeStressOverride](https://user-images.githubusercontent.com/1444784/71268127-f6891900-232a-11ea-8306-7352eea9f0f3.png)

![GC options 2](https://user-images.githubusercontent.com/56796799/71310520-b787c000-241d-11ea-8ba9-b25d5dc13ed5.png)

Unfortunately there is no BikeStress here. Please see attached file.",2019-12-21T16:15:01Z,56796799
441,GoldenCheetah/GoldenCheetah,540979735,568193695,"For new users it is created automatically, existing users need to add it using the (+) button and the arrows to order.",2019-12-21T16:30:27Z,1444784
442,GoldenCheetah/GoldenCheetah,540979735,568268229,"> For new users it is created automatically, existing users need to add it using the (+) button and the arrows to order.

I click the (+) button but there is no choice to add anything. :( 
![GC opt3](https://user-images.githubusercontent.com/56796799/71323124-71dbfd80-24d8-11ea-84a8-af8af4d5a685.png)
",2019-12-22T14:31:09Z,56796799
443,GoldenCheetah/GoldenCheetah,540979735,568269390,"A new field was added at the first row, a simple click allows to change the Screen Tab or Field part, the arrows at the bottom left allows to position the selected field were you want.",2019-12-22T14:45:31Z,1444784
444,GoldenCheetah/GoldenCheetah,540979735,568271817,"> A new field was added at the first row, a simple click allows to change the Screen Tab or Field part, the arrows at the bottom left allows to position the selected field were you want.

Many thanks for your help, I appreciate it! I thought a drop down menu would appear, but the solution is to manually write ""BikeStress"" :) ",2019-12-22T15:17:09Z,56796799
445,GoldenCheetah/GoldenCheetah,540979735,589971105,"I tried this method a few weeks ago and could still not get it to work properly,does not give you a tick box next to bikestress in the edit metrics tab and does not overwrite imported details because of this. Have tried deleting the TSS field and just leaving bikestress there and that does not work either",2020-02-22T16:15:09Z,61348882
446,GoldenCheetah/GoldenCheetah,540979735,589982864,"> I tried this method a few weeks ago and could still not get it to work properly,does not give you a tick box next to bikestress in the edit metrics tab and does not overwrite imported details because of this. Have tried deleting the TSS field and just leaving bikestress there and that does not work either

Because it’s BikeStress, not bikestress",2020-02-22T18:07:40Z,1444784
447,expressjs/express,814421264,814421264,"Hi!

As an example Node.js and https://nodejs.org/api/fs.html is starting to recommend promise-based versions foremost. (compare with earlier versions that has the promised based versions last: https://nodejs.org/docs/latest-v14.x/api/fs.html).

Express 5.0 has added automatic handling in routes of async/await routes that will automatically invoke the Express error handler if a an async function throws. There are current workarounds to create a `wrapAsync()` function around routes, but in bigger projects some users miss this and it's also not-so-nice-looking (and confusing for some).

I am merely wondering if Express 5.0 could be released, and current items on https://github.com/expressjs/express/milestone/11 could be moved to a 6.0 release? I am very respectful of the fact that you voluntarily work on this, and these kind of questions are frowned upon. https://github.com/expressjs/express/pull/2237 was created 7 years ago now, so I at least think it should be ok to ask for this without in any way not being grateful of this project already and its release cadence.

I guess what I am saying is that there are some nice things already on the main branch for Express 5.0 that works good with latest versions of Node.js 15, so a release would be amazing.",2021-02-23T12:31:48Z,30827238
448,expressjs/express,814421264,799237275,yeah thats a great idea bro,2021-03-15T08:52:15Z,73113145
449,expressjs/express,814421264,799475691,"@enyoghasim I am no maintainer of this repo and do not have the power to do releases, so I am unsure why I was added as asignee? Considering you are part of the express organization, is there any chance you could rally your peers to look into this?",2021-03-15T14:43:05Z,30827238
450,expressjs/express,814421264,842343520,"@dougwilson do you have anything to add to this? You are certainly doing a great job with maintenance of all the subdependencies etc of Express. But if Express 5.0 alpha 8 has not received any complaints of serious bugs, what are you feeling about this topic?

Also the answer by @enyoghasim concerns me a bit: I am not sure if it is sarcasm or what, but yeah.",2021-05-17T13:53:36Z,30827238
451,expressjs/express,814421264,842347633,"Hi @thernstig I'm sorry for one of the comments in the thread; I went ahead a hid it for now.

As for your idea, that is pretty much what the TC decided, even though there is still a lot of flac from the community about deferring things like http/2 to express 6. Really, there is only one outstanding issues for express 5, and it is related to async/await in the router. There is an open PR to address this, but the author has kind of faded away. Ideally that's really all that needs to be fixed if the idea is for express 5's main goal to be async/await.",2021-05-17T13:58:31Z,67512
452,expressjs/express,814421264,842509115,"@dougwilson thank you very much for answering, I know you have a very busy schedule.

Any chance you could champion taking over that PR to get it merged?

I agree with you and the TC. I think it is perfectly fine to release 5.0 and then later HTTP/2. There is always some ""can you just add this as well?"". I personally do not think there is anything stopping release 6 being a much smaller release with just having HTTP/2 as the main focus. That way it really should not matter much to users who want HTTP/2 get an ""intermediate"" 5.0 release.

Articles like https://dev.to/romainlanz/why-you-should-drop-expressjs-in-2021-711 make me sad because 1) much of the article is not true 2) Express is being actively worked on but some people do not explore enough to be aware it is.",2021-05-17T17:39:09Z,30827238
453,expressjs/express,814421264,842519269,@dougwilson  @thernstig  i am sorry for the hidden comment ,2021-05-17T17:55:45Z,73113145
454,expressjs/express,814421264,842573572,"**tl;dr** HTTP/2 is probably less important than potential instability in a codebase due to unhandled promises.

Completely anedoctal, but I tested a migration of our API to Koa and Fastify. They're both perfectly fine and have some nice advantages over Express. But you know, I'm back to Express anyway. I like the stability of the project.

That said, I find that working around `UnhandledPromiseRejection` has become a nuisance. It's just less reliable than knowing my framework of choice is battle tested to handle that for me.

Having Express 5 without HTTP/2 is, for us, a no-brainer 🤷 ",2021-05-17T19:24:37Z,8460502
455,expressjs/express,814421264,843830618,"I feel the same way as @filipecatraia, having tried Koa but moved back to express for similar reasons. I would also prefer not to have to wrap all of my request handlers in `asyncHandler`.

@dougwilson you said:
> Really, there is only one outstanding issues for express 5 and it is related to async/await in the router.

Would you mind linking to the issue in this thread for anybody else following along who might like to take a shot at it?",2021-05-19T07:34:15Z,1271216
456,expressjs/express,814421264,851864639,"@dougwilson, could you share the only issue holding off the release?",2021-06-01T06:41:05Z,30827238
457,expressjs/express,814421264,855432900,"Leaving a few useful links here:

- Pending items for v5 release: https://github.com/expressjs/express/pull/2237
- Open PRs related to 5.x: https://github.com/expressjs/express/pulls?q=is%3Aopen+is%3Apr+label%3A5.x
- Open issues related to 5.x: https://github.com/expressjs/express/issues?q=is%3Aopen+is%3Aissue+label%3A5.x",2021-06-06T17:25:19Z,22199259
458,expressjs/express,814421264,855450914,"@aravindvnair99 those links do not help much I am afraid. @dougwilson stated there was only one outstanding issue for 5.0. Meaning it seems the rest could be postponed.

It is clear @dougwilson is very active on Github, and seeing there is only one outstanding issue for the release, it feels natural to focus on that PR i.e. take over what the original author of that PR started. The focus seems to be on updating other things, which maybe could be held of till that one thing stopping Express 5.0?

The community has been waiting for many years quite patiently for the release, and has helped along the way with PRs and issues. It would be great to finally after these years make the community happy with a new release 😃 ",2021-06-06T19:36:23Z,30827238
459,expressjs/express,814421264,855455583,"@thernstig I agree. My intention though was just to mention the links here for anyone else who stumbles upon this issue tracker.

I must admit I too am excited for a v5 release. Express has always been about stability in my opinion, but yes I look forward to v5. It's long been in the development stage.

And as for what @dougwilson was referring to, I'm not sure which issue that is exactly. My guess are on this one: https://github.com/expressjs/express/pull/4321",2021-06-06T20:08:56Z,22199259
460,expressjs/express,814421264,872144540,"@dougwilson, could we get some feedback from you? Asking with my nicest voice possible, is there no chance you could fix the last outstanding problem if you know what to do? It feels every time this is brought up lots of other things are fixed instead of this. I am unsure why, it almost feel like releasing 5.0 is shunned for some reason 😞 ",2021-07-01T11:04:30Z,30827238
461,expressjs/express,814421264,872219273,"The author of the PR is back with us and was just working on it the other day. When someone is working on aomething, I don't feel it would be appropriate if I just did it myself and then pushed my own code and then closed their PR, would you? I don't think that would be a great way to keep people contributing. Since they are now interacting with the PR to finiah it, I think that is fine.

@aravindvnair99 is a triager and even provided various links here. We have the triagees so, among other things, I don't have to respond to every single comment across all the repos and can do other tasks.",2021-07-01T12:50:42Z,67512
462,expressjs/express,814421264,1043120656,"A beta.1 has been released with the referenced router changes 🎉 . Please check it out http://expressjs.com/en/guide/migrating-5.html

Since this changed the route path syntax a bit, I'll let it get maybe a month for feedback before official release.",2022-02-17T16:01:51Z,67512
463,expressjs/express,577744765,577744765,"Refs: https://github.com/expressjs/discussions/issues/98
Refs: https://github.com/expressjs/discussions/issues/106#issuecomment-595342919
co-authored-by: Wes Todd @wesleytodd",2020-03-09T08:47:07Z,6447530
464,expressjs/express,577744765,601556168,"@dougwilson , @wesleytodd : we are ready to land this, isn't it? what is blocking this?",2020-03-20T06:36:55Z,6447530
465,expressjs/express,577744765,602557717,"@dougwilson / @wesleytodd -  can you either remove the red-X or clarify what your concern is? If it is just that one is waiting for other's approval, please state so, so that we can move forward.

/cc @expressjs/express-tc - FYI, and important PR for defining project captains for repos. Please chime in if you have comments, or else complete review to help progress!",2020-03-23T12:21:24Z,6447530
466,expressjs/express,577744765,602595333,"There is a simple issue here I was going to fix on merge, but didn't get a chance to this weekend with what is going on in my country. After work today I will at least comment on it so anyone can make the change if I cannot.",2020-03-23T13:40:45Z,67512
467,expressjs/express,577744765,602988230,"@dougwilson - I have addressed all the review comments, mostly following your suggestions `AS IS`. Please have a look and do the needful!",2020-03-24T03:15:36Z,6447530
468,expressjs/express,577744765,609009441,it has been 11 days since the last update. can we take this forward?,2020-04-04T10:38:16Z,6447530
469,expressjs/express,577744765,611259730,@dougwilson - I just squashed all the commits into one,2020-04-09T00:20:57Z,6447530
470,expressjs/express,577744765,611260523,"Thanks @gireeshpunathil ! As we discussed on the TC meeting, it should be in the format as the current commit on `master` and our guide: the message on the first line, a blank line, a reference to the PR in the form ""`closes #PR`"". I can make the said changes if you need. LMK",2020-04-09T00:23:24Z,67512
471,expressjs/express,577744765,611262649,@dougwilson - made the changes to the commit message as per the guidelines; PTAL.,2020-04-09T00:31:26Z,6447530
472,expressjs/express,577744765,613784052,"Ok... It still does not seem to be in the form from my comment... ? I can just update if you need, let me know. If I can better clarify how the message should be constructed let me know as well. I believe it was clear, and provided an example of a previous comment message, but if I can provide better guidance for the format, I certainly can.

I am circling around on this because I would like this to get merged, but I recall that you wanted to do the commit editing so I could just do the fast-forward merge, but it's not in the correct state for that. I can make those edits if you like, but don't want to force push on your branch without you being aware as per our previous conversation :)",2020-04-15T02:47:39Z,67512
473,expressjs/express,577744765,613787817,"@dougwilson - 
 - thanks - yes, I prefer to be advised over edits if possible, towards improving my own knowledge on those areas.

this is the current commit message:

```
doc: add section for project captains
    
closes https://github.com/expressjs/discussions/issues/98
Co-Authored-By: Wes Todd <wes@wesleytodd.com>
```

this is the rule:
`the message on the first line, a blank line, a reference to the PR in the form ""closes #PR"".`

 - the message is on the first line
 - there is a blank line
 - there is a reference to the PR against `closes` verb

the two additional things are:
 - there is a subsystem (doc) in the message line (following the existing conventions)
 - there is a co-author field (attribution of authorship, many of the original wordings came from Wes)

please advise where is the change required?
",2020-04-15T03:01:41Z,6447530
474,expressjs/express,577744765,613789010,"1. The subsystem is ""docs"" and not ""doc""
2. The closes should be `closes #4210`
3. We do not include co-author fields, but we can if desired to amend our guidelines, in which this commit would wait for that to happen

It should just be something like
```
docs: add project captains to contribution

closes #4210
```

Then as discussed in the TC meeting, the committer on the commit would need to be the person performing the merge itself (myself unless you would like to wait for another to do that).",2020-04-15T03:05:39Z,67512
475,expressjs/express,577744765,613796332,"@dougwilson - adjusted the commit message accordingly, PTAL!
@wesleytodd - I had earlier kept you as a `co-author` for this PR, but looks like we do not include that as per the current process. Can you please review and provide your concurrence? If you have concerns pls let me know, we would need to go the route of amending guidelines.",2020-04-15T03:34:03Z,6447530
476,expressjs/express,577744765,613836449,"So based on the conversation going on in https://github.com/expressjs/discussions/issues/121 it seems like this is not ready to land as-is, as it seems that there may be either additional discussion to have on the goal here and/or updating to the wording in this pull request to reflect it. Specifically in regards to what falls under repo ownership.

It would come to reason to me that landing pull requests would be a ""day-to-day"" task for said project captains, and their particular merging strategies, commit message formats, etc. would serve the repo on a technical front, as outlined in this document.

I think part of the goal of project captains is to empower them to manage their repos, and commit messages seems to fall directly in that category to me. For example, if a project captain wants to use something like standard-release to automate releases and history file generation, they may need specific structure to commit messages. We should strive to empower the owners to do what they think will improve their workflow.

But maybe I'm misunderstanding on that point. Especially if I'm misunderstanding, that definately means we need greater clarity in the document, as we don't want to have everyone interpreting it differently, I presume. I'm happy to provide suggested edits on the wording, but likely need input from the original author @wesleytodd to sync on the interpretation so I can help clarify 👍 ",2020-04-15T06:07:52Z,67512
477,expressjs/express,577744765,613841426,"@dougwilson - it seems strange (and funny) to me that you closed https://github.com/expressjs/discussions/issues/121 with no action on it, and then quoting its content as a reason for further work here!",2020-04-15T06:24:05Z,6447530
478,expressjs/express,577744765,613842539,Hi @gireeshpunathil that issue is orthogonal to this issue.,2020-04-15T06:27:27Z,67512
479,expressjs/express,577744765,613858249,"> But maybe I'm misunderstanding on that point. Especially if I'm misunderstanding, that definately means we need greater clarity in the document, as we don't want to have everyone interpreting it differently, I presume. 

here is the sequence of events, for better clarity for everyone:

 - the commit message did not follow this repo's convention
 - more specifically, I added `Co-authored-by` field, but not not an established practice in this repo
 - @dougwilson pointed out that, I promptly removed it, but sought verbal consent from the co-author
 - I opened an issue to discuss that: specifically, implementing a guideline pan-project
 - that is closed as inappropriate in that repo, and to be dealt in individual repos.
 - now back here, @dougwilson says it is causing misunderstanding!

This is becoming a surprise to me! where is the mis-understanding? If you assert that no org wide guidelines are present, so be it, and I will seek change proposal with captains, period! where is it that I interpreted this document at all? leave alone interpreted differently?

I suggest we take a step back; 
 - retire the discussion in https://github.com/expressjs/discussions/issues/121 as it is already closed and I agreeing to your proposal to take anything further with individual repo captians
 - and go ahead with this proposal
",2020-04-15T07:08:07Z,6447530
480,expressjs/express,577744765,615077688,"Hi @gireeshpunathil please let me know if there is some time in which you are available for a voice chat; I would like to proceed with landing this PR, but I am honestly unsure at this point how to interact with your pull requests that will not come off in bad faith, and would like a chance to have a discussion with you so we are on the same page so we can make progress landing.",2020-04-17T06:56:27Z,67512
481,expressjs/express,577744765,615093519,@dougwilson - I sent you a mail with my time preferences.,2020-04-17T07:36:48Z,6447530
482,expressjs/express,577744765,650777405,"in general, this can have more reviews!",2020-06-28T15:11:44Z,6447530
483,expressjs/express,577744765,664036654,"I don't have any real comments on this PR, but once the discussion is resolved and it lands, we should make sure the change is reflected on the website; I opened https://github.com/expressjs/expressjs.com/issues/1184 to track that.",2020-07-26T20:32:51Z,2925364
484,expressjs/express,577744765,664091288,"I believe I have addressed all the outstanding comments. pinging @expressjs/express-tc , to seek some reviews or take it towards landing.",2020-07-27T02:51:14Z,6447530
485,expressjs/express,577744765,664091631,I am locking this PR until the above issue with the interaction between myself and @gireeshpunathil is resolved.,2020-07-27T02:52:51Z,67512
486,zeromq/libzmq,907232677,907232677,This solves issue #4200,2021-05-31T08:24:44Z,426015
487,zeromq/libzmq,907232677,851357832,Please add a relicense statement as described in https://github.com/zeromq/libzmq/tree/master/RELICENSE - thanks,2021-05-31T09:30:20Z,782193
488,zeromq/libzmq,907232677,851359117,"> Please add a relicense statement as described in https://github.com/zeromq/libzmq/tree/master/RELICENSE <https://github.com/zeromq/libzmq/tree/master/RELICENSE> - thanks
> 
Do you really consider a 9 lines diff in CMakeLists.txt a code contribution that needs a license?

",2021-05-31T09:32:26Z,426015
489,zeromq/libzmq,907232677,851360791,">> Please add a relicense statement as described in https://github.com/zeromq/libzmq/tree/master/RELICENSE <https://github.com/zeromq/libzmq/tree/master/RELICENSE> - thanks
>> 
> Do you really consider a 9 lines diff in CMakeLists.txt a code contribution that needs a license?
> 

After all I do not (yet?) hold or claim any copyrights in an ZeroMQ code.  This was a mere bugfix.

Different story are my Lua bindings at https://github.com/arcapos/luazmq <https://github.com/arcapos/luazmq> and https://github.com/arcapos/mqlua <https://github.com/arcapos/mqlua>, but then these are not part of the ZeroMQ project.

",2021-05-31T09:35:04Z,426015
490,zeromq/libzmq,907232677,851372951,"> Please add a relicense statement as described in https://github.com/zeromq/libzmq/tree/master/RELICENSE <https://github.com/zeromq/libzmq/tree/master/RELICENSE> - thanks
> Do you really consider a 9 lines diff in CMakeLists.txt a code contribution that needs a license?

(probably?) not, but Github makes it very obvious when a first contribution is made, so it's easier to ask for it at that point, rather than tracking who sent it and who didn't if you then later submit a larger PR. I know it's annoying, but believe me, trying to get this relicensing sorted is orders of magnitude more work.",2021-05-31T09:54:39Z,782193
491,zeromq/libzmq,907232677,851395118,"

> Am 31.05.2021 um 11:54 schrieb Luca Boccassi ***@***.***>:
> 
> ﻿
> Please add a relicense statement as described in https://github.com/zeromq/libzmq/tree/master/RELICENSE https://github.com/zeromq/libzmq/tree/master/RELICENSE - thanks
> Do you really consider a 9 lines diff in CMakeLists.txt a code contribution that needs a license?
> 
> (probably?) not, but Github makes it very obvious when a first contribution is made, so it's easier to ask for it at that point, rather than tracking who sent it and who didn't if you then later submit a larger PR. I know it's annoying, but believe me, trying to get this relicensing sorted is orders of magnitude more work.
> 

Well, I have no plans to contribute code, I am a mere (long time) user of ZeroMQ, and if it does not build, I invest some time trying to find out what is wrong. Maybe WITH_OPENPGM should be added to the automated tests.
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or unsubscribe.
",2021-05-31T10:32:50Z,426015
492,zeromq/libzmq,907232677,852035632,Are you not going to merge this?,2021-06-01T11:04:53Z,426015
493,zeromq/libzmq,907232677,852089096,"Please add a relicense statement first as described in https://github.com/zeromq/libzmq/tree/master/RELICENSE - it's a few lines of text to copy/paste, it takes less than a minute. Thanks.",2021-06-01T12:36:23Z,782193
494,zeromq/libzmq,907232677,852095381,"Not for a 9 line change in Makefile.

> Am 01.06.2021 um 14:36 schrieb Luca Boccassi ***@***.***>:
> 
> ﻿
> Please add a relicense statement first as described in https://github.com/zeromq/libzmq/tree/master/RELICENSE - it's a few lines of text to copy/paste, it takes less than a minute. Thanks.
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or unsubscribe.
",2021-06-01T12:46:29Z,426015
495,zeromq/libzmq,907232677,1578712239,"no permission given to distribute under MPL2, the relicensing is complete so this cannot be merged, closing",2023-06-06T12:52:46Z,782193
496,zeromq/libzmq,907232677,1578719516,"So stupid… And obviously not interested in solving problems…Am 06.06.2023 um 14:53 schrieb Luca Boccassi ***@***.***>:﻿
no permission given to distribute under MPL2, the relicensing is complete so this cannot be merged, closing

—Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you authored the thread.Message ID: ***@***.***>",2023-06-06T12:57:50Z,426015
497,zeromq/libzmq,907232677,1578724314,"The codebase is now under MPL 2.0 and you clearly and explicitly refused to work under it, so it's not 'stupid' at all, it's simply how copyright and licenses work, it's not optional or voluntary.",2023-06-06T13:00:13Z,782193
498,ohmyzsh/ohmyzsh,1616936787,1616936787,"### Describe the bug

When I enter a command, that returns an error, the line with that command is marked with a green arrow.
The next line, valid or not, gets marked red, which is confusing, since it was valid in my case.

### Steps to reproduce

Example: Installing something via apt without using sudo:

❯ apt-get install mosh
E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)
E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?
❯ sudo apt-get install mosh
[sudo] password for user:

The first line has a green arrow, while the thirds one is red.

### Expected behavior

Color the arrow red where the command actually failed.

### Screenshots and recordings

![image](https://user-images.githubusercontent.com/46635221/224000635-d131e0d5-e84d-4acd-ac73-8c6a982c5688.png)


### OS / Linux distribution

Debian 11 (Bullseye) (using Windows Subsystem for Linux)

### Zsh version

5.8

### Terminal emulator

Windows Terminal

### If using WSL on Windows, which version of WSL

WSL2

### Additional context

I am using Oh My ZSH with powerlevel10k",2023-03-09T10:46:27Z,46635221
499,ohmyzsh/ohmyzsh,1616936787,1461802658,"After a command is run, the new prompt shows the status of the previous exit status. 

It is at the new prompt you eyes are looking where you need feedback on what just happened. 

Changing the previous prompt makes no sense as many commands would’ve likely scrolled it off the screen. ",2023-03-09T10:57:04Z,15944841
500,ohmyzsh/ohmyzsh,1616936787,1461814285,"It is just really confusing to me.
I agree with you that many commands will not be displayed anymore after executing.

However, it is not ""sudo apt-get install... "" that failed. It was the command above.
I personally think that this is misleading if you dont know what exactly happens. The current behaviour makes the arrow (for me) kind of obsolete.
",2023-03-09T11:04:17Z,46635221
501,ohmyzsh/ohmyzsh,1616936787,1461852501,"@Gandalf1783 there are a lot of different themes that doesn't bundle this feature! Personally, I think this feature is quite useful to know if `$?` is `0` or not!",2023-03-09T11:29:33Z,66907184
502,ohmyzsh/ohmyzsh,1616936787,1461891768,"I really like the idea. It is really useful. Thats also why I have the return-value indicator on the right side of the terminal (the number with the red x or the checkmark) configured for my prompt.

In my eyes, you can see the cli now as a kind of table.
At the beginning, you got your indicator, if a command returned with 0 or not. However, it is not related to the command right next to it.

I know what you mean, since of course you have the return value AFTER a command or programm finished executing, so it gives you feedback after something has completed (successfully or not).

But for commands like git, dhclient, cat, tail, cp, ls, mkdir, chown, chmod etc... which dont have a long console output (usually), the way its implemented does not make sense to me.

I dont want to push y'all into changing this, especially since people are using this for quite some time now. 

I just see e.g. the arrows to be a thing of ""pointing"" and conveying meaning. It looked to me as a ""this line produced an error"", similar to what an IDE would do if theres a bug, error that occured on a specific line of code. And since that line didnt threw the error, but the one above did, this seems wrong.",2023-03-09T11:52:08Z,46635221
503,ohmyzsh/ohmyzsh,1616936787,1461926722,"I see your point, just a matter of preference!",2023-03-09T12:12:28Z,66907184
504,ohmyzsh/ohmyzsh,1616936787,1461955754,"And the difficulty in trying to accomplish it. The prompt cmd  would have to know where the previous prompt was displayed and then go back to rewrite it, without affecting anything else. ",2023-03-09T12:28:29Z,15944841
505,ohmyzsh/ohmyzsh,1616936787,1462287769,"Hm, I don't think thats an issue.
Count the lines since the last command, write into the buffer at that location, done.

Applications like htop are constantly changing their buffer to provide ""real-time"" data, so a single rewrite in the buffer to display an icon shouldn't be hard.",2023-03-09T15:45:59Z,46635221
506,ohmyzsh/ohmyzsh,1616936787,1462304502,"htop is using ncurses.

Try your method, then submit it once you have it working with both static and scrolling output.  I'm looking forward to it.

How do you count the lines?
What buffer are you talking about?
",2023-03-09T15:54:51Z,15944841
507,ohmyzsh/ohmyzsh,1616936787,1464620629,"> Hm, I don't think thats an issue. Count the lines since the last command, write into the buffer at that location, done.
> 
> Applications like htop are constantly changing their buffer to provide ""real-time"" data, so a single rewrite in the buffer to display an icon shouldn't be hard.

As trivial as this is, I'm surprised you haven't submitted a patch yet.  If you're stuck on something, post it and maybe someone can help you out.",2023-03-10T23:06:17Z,15944841
508,ohmyzsh/ohmyzsh,1616936787,1465202429,"> Hm, I don't think thats an issue. Count the lines since the last command, write into the buffer at that location, done.
> 
> Applications like htop are constantly changing their buffer to provide ""real-time"" data, so a single rewrite in the buffer to display an icon shouldn't be hard.

@Gandalf1783 Given your lack of response, I'm assuming you just wanted to gaslight someone (me in this case) without any actual knowledge of the problem?  You just wanted to sound like you knew more than someone else without knowing anything at all?

No program has enough knowledge (currently) to pull off what you think is trivial.

The terminal doesn't know the purpose of the text it displays, it can't do it.

zsh/omz/p10k doesn't know what happens in-between the prompts.  They can control what is displayed on the current prompt lines.   They can query where on the terminal they are, but once another program has run with unknown amount of text output and the screen scrolls, all for that previous knowledge is useless.  Terminal queries for row number only apply to the displayed screen, not relative to the top of the scroll back buffer.  

To be fair, I too was annoyed by this some time ago and solved it for myself by displaying green/red lines across the entire terminal width before the next prompt is displayed.  The color line is the status for everything above the line.  I've left the prompt symbol in, but rarely look it.  Here's an example:

<img width=""363"" alt=""image"" src=""https://user-images.githubusercontent.com/15944841/224548171-55aae1cb-8d04-4feb-af7a-20a90993c3ad.png"">

Now, if you'll notice, iTerm2 does indeed update its prompt marker (which is outside the terminal text area).",2023-03-12T13:37:43Z,15944841
509,ohmyzsh/ohmyzsh,1616936787,1465207542,"Excuse me for not answering.

I haven't had a look on this for the last few days.

First of all, what I didn't notice is that this is written in shell scripts. I thought this was a C application that extended the zsh shell/passed all inputs to it and gave the user its own custom shell with custom themes.

In regards to the ""No program has the knowledge..."": I just said it was trivial to count lines, wait for the exit code to return and edit the first character (the prompt symbol) to the according color. Like I said, I wasn't aware that this is a shell script. Of course, without any external help this cant be done.

I've taken a look, and since e.g. Github-Repos and their branch can be displayed (e.g. master), I have noticed the extra github-helper program thats started in the background.

I assume that it is possible to pass the stdout/stdin from the session to another program, count lines/return code and just tell zsh what should happen or just modify the buffer directly.

I would've forked the repo and ran my own tests/integrations, but due to the lack of knowledge in shell-scripting im basically through the topic. I cant really contribute anything here, I was just curious if there was some option I was missing, since I only found some posts online that were interested in a similar solution / were bothered by this.",2023-03-12T14:00:36Z,46635221
510,ohmyzsh/ohmyzsh,1616936787,1465211622,"> Excuse me for not answering.
> 
> I haven't had a look on this for the last few days.
> 
> First of all, what I didn't notice is that this is written in shell scripts. I thought this was a C application that extended the zsh shell/passed all inputs to it and gave the user its own custom shell with custom themes.

So you didn't even know what the topic was about?
 
> In regards to the ""No program has the knowledge..."": I just said it was trivial to count lines, wait for the exit code to return and edit the first character (the prompt symbol) to the according color. Like I said, I wasn't aware that this is a shell script. Of course, without any external help this cant be done.

Even that isn't trivial.  Count linefeeds?  What about lines that wrap to the next line?  Output from a PAGER, such as as less?  Output from htop?  How do you count those lines?

> I've taken a look, and since e.g. Github-Repos and their branch can be displayed (e.g. master), I have noticed the extra github-helper program thats started in the background.
> 
> I assume that it is possible to pass the stdout/stdin from the session to another program, count lines/return code and just tell zsh what should happen or just modify the buffer directly.

It would have to duplicate everything the terminal is already doing to accurately determine how much screen space has been used.  Not impossible, but hardly trivial.

> I would've forked the repo and ran my own tests/integrations, but due to the lack of knowledge in shell-scripting im basically through the topic. I cant really contribute anything here, I was just curious if there was some option I was missing, since I only found some posts online that were interested in a similar solution / were bothered by this.

Good for you telling others how trivial something is.
",2023-03-12T14:17:36Z,15944841
511,ohmyzsh/ohmyzsh,1616936787,1465213511,"It has been established already that this is not possible, and any further discussion is pointless.

Richard, I'm concerned about your tone. If you're not able to have a technical argument without getting offended, just don't engage and forget about it. I'd much rather that than have this kind of vitriol on our project.",2023-03-12T14:26:09Z,1441704
512,ohmyzsh/ohmyzsh,1616936787,1465217142,"To be extra clear, the implementation of this is not feasible because of the *technical* reasons that Richard mentioned. He is 100% correct on that. It is difficult to understand why it's not just ""counting lines"" when you don't have a full understanding of how a terminal works. But my point about the tone still stands. End of discussion.",2023-03-12T14:43:03Z,1441704
513,gradle/gradle,436467857,436467857,"why the hell did you haphazardly change the initial value of properties in this commit???
https://github.com/gradle/gradle/commit/dbb85f50bb184cc128949d4eec039b9ed30fc170

why the hell you break my code programmed against gradle 4.10 in the **very first** release after that???

why didn't you respect you own mandates and added a new method `.emptyProperty(Class)` and deprecated the one you had instead of needlessly changing the damn semantics of the API? why was my code was intentionally broken on gradle 5.0 when it should have been good though at least gradle 6.0?

why didn't you even document this brain-dead breaking change in the 4.10 -> 5.0 migration guide here:
https://docs.gradle.org/current/userguide/upgrading_version_4.html#changes_5.0
and could you please at least document it now???

i can't believe you guys, this is completely irresponsible. and don't tell me you needed to do this to support property conventions, you didn't. what a waste of time.",2019-04-24T02:28:30Z,3977813
514,gradle/gradle,436467857,486104049,"I understand you are unhappy with the change in commit https://github.com/gradle/gradle/commit/dbb85f50bb184cc128949d4eec039b9ed30fc170. The change is part of the stabilizing process that every [Incubating API](https://docs.gradle.org/current/userguide/feature_lifecycle.html#sec:incubating_state) goes through. The summary of the contract is when an API is tagged as incubating, the users should be expecting breaking change without notice. You are welcome to submit a PR for this breaking change.

On another note, it would be preferable for your next issue to follow the issue template as well as reading over the [Code of Conduct](https://gradle.org/conduct/).",2019-04-24T07:31:14Z,22181740
515,gradle/gradle,339498894,339498894,"when we have storage OOM gradle doesn't sygnalize that, instead proper info we have a bullshit :)

`$ gradle clean`

>FAILURE: Build failed with an exception.
>
>* What went wrong:
>Unable to start the daemon process.
>This problem might be caused by incorrect configuration of the daemon.
>For example, an unrecognized jvm option is used.
>Please refer to the user guide chapter on the daemon at >https://docs.gradle.org/4.7/userguide/gradle_daemon.html
>Please read the following process output to find out more:`
>-----------------------
>* Try:
>Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log >output. Run with --scan to get full insights.

Unable to start the daemon process... **when explicity run with false flag in props file**

the next try maybe gradle didn't read the prop file proper .. **so explicity set no deamon arg**

`$ gradle --info --no-daemon clean`

>Initialized native services in: /opt/gradle/ceph3us/native
To honour the JVM settings for this build a new JVM will be forked. Please consider using the daemon: https://docs.gradle.org/4.7/userguide/gradle_daemon.html.
Starting process 'Gradle build daemon'. Working directory: /opt/gradle/ceph3us/daemon/4.7 Command: /opt/jdk1.8/bin/java -XX:+AggressiveOpts -XX:+UseG1GC -Xmn512m -XX:MaxMetaspaceSize=1g -XX:SurvivorRatio=40 -XX:+UseCompressedOops -XX:+UseCompressedClassPointers -XX:-OmitStackTraceInFastThrow -XX:SoftRefLRUPolicyMSPerMB=100 -XX:-HeapDumpOnOutOfMemoryError -Xms512m -Xmx3g -Dfile.encoding=UTF-8 -Duser.country=PL -Duser.language=pl -Duser.variant -cp /opt/gradle/lib/gradle-launcher-4.7.jar org.gradle.launcher.daemon.bootstrap.GradleDaemon 4.7
Successfully started process 'Gradle build daemon'
An attempt to start the daemon took 1.005 secs.

FAILURE: Build failed with an exception.

>* What went wrong:
Unable to start the daemon process.
This problem might be caused by incorrect configuration of the daemon.
For example, an unrecognized jvm option is used.
Please refer to the user guide chapter on the daemon at https://docs.gradle.org/4.7/userguide/gradle_daemon.html
Please read the following process output to find out more:
-----------------------


>* Try:
Run with --stacktrace option to get the stack trace. Run with --debug option to get more log output. Run with --scan to get full insights.

>* Get more help at https://help.gradle.org

still some daemon shit WTF ??? one more try brings same results of bulshit doeasnt reveal the TRUE CAUSE for BUILD FAILED 

'$ gradle --debug --no-daemon clean'

>{ unrelated sensitive data cut}
>16:50:14.639 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Changing state to: STARTING
16:50:14.640 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Waiting until process started: Gradle build daemon.
16:50:14.655 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Changing state to: STARTED
16:50:14.656 [INFO] [org.gradle.process.internal.DefaultExecHandle] Successfully started process 'Gradle build daemon'
16:50:14.656 [DEBUG] [org.gradle.launcher.daemon.client.DefaultDaemonStarter] Gradle daemon process is starting. Waiting for the daemon to detach...
16:50:14.657 [DEBUG] [org.gradle.process.internal.ExecHandleRunner] waiting until streams are handled...
16:50:14.659 [DEBUG] [org.gradle.launcher.daemon.bootstrap.DaemonOutputConsumer] Starting consuming the daemon process output.
16:50:15.611 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Changing state to: DETACHED
16:50:15.611 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Process 'Gradle build daemon' finished with exit value 0 (state: DETACHED)
16:50:15.611 [DEBUG] [org.gradle.launcher.daemon.client.DefaultDaemonStarter] Gradle daemon process is now detached.
16:50:15.613 [INFO] [org.gradle.launcher.daemon.client.DefaultDaemonStarter] An attempt to start the daemon took 0.982 secs.
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Unable to start the daemon process.
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] This problem might be caused by incorrect configuration of the daemon.
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] For example, an unrecognized jvm option is used.
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Please refer to the user guide chapter on the daemon at https://docs.gradle.org/4.7/userguide/gradle_daemon.html
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Please read the following process output to find out more:
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] -----------------------
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Try:
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Run with --stacktrace option to get the stack trace.  Run with --scan to get full insights.
16:50:15.621 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 
16:50:15.621 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Get more help at https://help.gradle.org

**pleaes add OOM storage watcher during task execution that will throw some sort of StorageOOM exception**

**distracts from the fact that there was no memory left at the start of the gradle .. which should be signalized apriori any taskl start / evaluate**

tneet to consider the SPACE 
-- WHERE PROJECT IS EXECUTED (build output)   
-- WHEN GRADLE WRITES  (cache, etc) 

**those places should be checked during start gradle (LOW MEM WARNING) and monitored as mentioned above**


i have wasted 5 min befor i did to know the real cause.. ",2018-07-09T15:29:20Z,10565704
516,gradle/gradle,339498894,403610103,Closing due to abusive language. Please read our code of conduct and open an issue using appropriate language.,2018-07-09T20:30:35Z,1191797
517,gradle/gradle,339498894,405948813,"@oehme  you are funny :) you can learn a bit about semantics, grammar and the meanings of words - essentialy about language and usage :) 
can you tell me who was here offended ?

**maybe better was to use here a euphemisms so that the dumb people didn't think they were being offended when in fact nobody was...**  if you fell so then it is your right to feel as you want to :) 

understanding is key to everything.. so **try to understand it does not hurt!**


cipa, kutas, jebnij, pierdolenie 

best 
ceph3us",2018-07-18T14:23:11Z,10565704
518,gradle/gradle,339498894,405954317,"If this is your way of communicating, please stay away from our community.",2018-07-18T14:38:23Z,1191797
519,npm/npm,340715954,340715954,"#### What's the feature?

IMO `npm install` should only install exact packages unless explicitly told not to via a flag e.g. `--allow-package-upgrades`.

#### What problem is the feature intended to solve?

- Would make CI builds repeatable by default
- Would make it impossible to accidentally, silently upgrade to a compromised package e.g. #21202 ",2018-07-12T16:43:58Z,495495
520,npm/npm,340715954,404576380,Please open this at <https://npm.community/c/ideas>.,2018-07-12T16:45:52Z,733364
521,npm/npm,340715954,404577187,"Thanks, @KenanY. I'm also going to lock this since it's likely to draw attention from #21202.

As a note: this feature request is covered by `package-lock.json` as of `npm@5`. Please upgrade to the latest npm. The feature, as described, would also not make CI builds repeatable by default, since that would involve all transitive dependencies having been published with this default, and that would likely take an immense amount of time and never be quite done, considering the scale of the registry, so I don't consider it a very good solution.

There are other things in the work to address issues with unintended dependency updates, but now's not the time to talk about them. Have a good one!",2018-07-12T16:48:38Z,17535
522,npm/npm,340599374,340599374,"Please see the following issue:
https://github.com/eslint/eslint-scope/issues/39

`eslint-scope` 3.7.2 has been published an hour ago which is a hacked version that steals the NPM accounts or something.

Please pull the version 3.7.2 from the npm and freeze the account so this does not get propagated.

As a matter of fact, there is no release tag for 3.7.2 on Github, so I think it would be great to consider double checking with Github repository before publishing any code. 

This would at least limit the possibility of uploading the malicious code to NPM without having Github credentials to tag the release/version. 

Additionally I believe it's possible to check if the release was signed and somehow enforce all tagged commits to be signed. I think Github returns such information via their API, at least you can see the verified commits via Github's web interface so there must be a way. Developer may be able to opt-in for this extra security via some `.rc` file stored in Git repo.",2018-07-12T11:32:55Z,704044
523,npm/npm,340599374,404502145,3.7.2 seems to have been unpublished now. Would love to hear what actually happened.,2018-07-12T12:54:34Z,58669
524,npm/npm,340599374,404507723,"@akx shortly, the npm credentials have been stolen and the malicious release has been made directly to npm (not GitHub). 

The malicious code ran upon `npm install` and attempted to transfer the npm authentication token from `.npmrc` to remote server. 

Surprisingly, the tampered code only contained a bootstrap script that downloaded and executed the main script (with `eval`) from pastebin and the pastebin script had a syntax error in it so that's how it revealed itself. 

All of what happened is something to think about, why didn't npm spot the calls to `eval` and reject the update. I know many packers use `eval` as well but come on, it's not 1999 again and there are simple measures and protocols that can be implemented in order to strengthen the security of npm ecosystem.",2018-07-12T13:14:09Z,704044
525,npm/npm,340599374,404519128,"The first question that came to mind after seeing this issue is: why is there a difference between the published code on npm, and the code on github? I know why, because when you run `npm publish`, you publish from a local directory.

I think npm can increase security by **not letting people publish code from their local directories but only download code from a public repo** like github’s (ala server-to-server). It would of course only be possible for public modules/repositories. That way all published code always has a history.

It increases security because to publish malicious code you need to:
1. steal both npm credentials and github credentials
2. commit and push malicious code to github
3. pass security a project's build pipeline (and possible security scans) 

I'm probably not the first with this idea, but has it been considered by the npm team yet?

I can also imagine that the world would be a safer place if `npm publish` would require an npm account with 2FA enabled.",2018-07-12T13:51:28Z,172579
526,npm/npm,340599374,404520017,"@branneman that's true, but some code is transpiled upon release so this has an impact of forcing developers to store all their stuff in git or put the strain on npm servers to pull the code and run all build procedures before publishing. Anyway, there must be a way. 👍 ",2018-07-12T13:53:58Z,704044
527,npm/npm,340599374,404520251,"> Curious thing, all tampered files had the modification date set to Oct 26, 1985. So I guess now we know the birthday of whoever who did this...

fyi @pronebird all files showing Oct 26, 1985 (a Back to the Future reference joke) is actually intented behavior for NPM (see https://github.com/npm/npm/issues/20439#issuecomment-385121133) and not related to this hack.",2018-07-12T13:54:41Z,2393035
528,npm/npm,340599374,404520512,"The actual solution would be https://reproducible-builds.org/

Fixating on one build environment is not a good idea in the grand scheme of things.",2018-07-12T13:55:29Z,7881228
529,npm/npm,340599374,404523546,"@jpdriver Thanks, I didn't know this. I mostly use `yarn`, so 1985 came as a surprise.  👍 (I removed the mention of this everywhere)",2018-07-12T14:04:22Z,704044
530,npm/npm,340599374,404525239,"Has already the attacker got tokens by this attack? If so, will other packages be hacked again?
Do you have a plan to revoke all npm users' token or to stop publishing any packages?",2018-07-12T14:09:23Z,9206
531,npm/npm,340599374,404526848,"It seems like the reasonable thing to do would be:

1. Temporarily halt all package publications
2. Search the entire registry for references to this virus and unpublish any infected packages
3. Globally revoke all authentication tokens 
4. Re-enable package publications",2018-07-12T14:14:09Z,6296006
532,npm/npm,340599374,404530671,"The key is revoking all npmrc tokens globally before the attacker takes action. 

If this isn't done soon enough, the only way to completely prevent the damage will be the manual audit of the almost the npm registry, and that is borderline impossible.

(so I'm really hoping npm revokes everything soon enough, otherwise most people will just have to stop using it)",2018-07-12T14:25:09Z,10137
533,npm/npm,340599374,404550816,"@EdwardDrapkin Agree with all of your points, but I'd go a step further with number 2: in addition to searching the registry, unpublish all packages published since the affected version was published (and even then, it's not clear yet if the eslint-scope infection was a result of a further upstream infection). The virus could have modified itself (or have been manually modified, or use a different version entirely) so as to be unrecognizable from the original. There's nothing to say that the pastebin code from this incident is the same as what would be infected in other packages of authors with their credentials compromised. The credentials are being logged to web counters, which to use the credentials they would have to be monitored from an outside source, so there's nothing stopping that outside source from publishing a different script entirely.

Edit: upon further thought, I realize that my suggestion should only be done if it is determined that a significant amount of packages are affected after halting all package publications and revoking all tokens. It's possible that could effectively resolve this specific incident. However this should at least be a ""shot across the bow"" that a single token being compromised can have disastrous ripple effects across the ecosystem.",2018-07-12T15:24:00Z,1874103
534,npm/npm,340599374,404556093,"Everyone, I am not sure where else to put this and the eslint-scope issue has been locked, but is there anyway to search the npm cache to make sure this version is not cached? 

I am on windows if it helps or doesn't.",2018-07-12T15:39:34Z,2192911
535,npm/npm,340599374,404556294,"> Surprisingly, the tampered code only contained a bootstrap script that downloaded and executed the main script (with eval) from pastebin and the pastebin script had a syntax error in it so that's how it revealed itself.

The script seems to have run successfully on one user's Windows system. The ""problem"" in the script seems to have been that it would only run correctly if the full source code was fetched in the first chunk - the malware author forgot to ensure all chunks had been fetched. I gleaned this from the original eslint-scope issue.

So it is reasonable to expect many users' auth tokens to have been stolen.",2018-07-12T15:40:05Z,374505
536,npm/npm,340599374,404560361,I +1 the global unpublishing of all packages and revoking all tokens until this is resolved. This does NOT look good.,2018-07-12T15:52:36Z,14976516
537,npm/npm,340599374,404562285,"@branneman and others suggesting npm integration into git,

Not every npm package uses git, and certainly not everyone uses github. Marrying the two concepts doesn't make sense to me.

I think the best solution is faster communication and awareness of exploited packages. 

Some brainstorming ideas:

* Maybe integrate the npm ecosystem into something like node security project. Warnings should be shown inside of NPM cli when a vulnerable package is found in the dep tree.
* Easy flagging of infected code to notify maintainers.
* Have stricter publish regimens for foundational (heavily depended on) dependencies. Maybe enforce manual audits with two factor authentication. Realistically dependencies that have thousands and thousands of dependents should not be modified so frequently, or they must go through a manual audit step. 
* Perhaps have a badge awarded to releases that have been manually audited and approved with a 2 factor verification.
... I could go on",2018-07-12T15:58:28Z,284360
538,npm/npm,340599374,404562886,"Just forbid all packages containing a string ""eval"", or make them require manual reviewal. But my advise would be just ban them for good",2018-07-12T16:00:17Z,6149131
539,npm/npm,340599374,404563286,"@gedgaudasnikita how would that help? then they write the code to a file, and `require()` it.. or download a compiled binary instead of using JS for their malware.

btw, why do you have two accounts? @nikita-gedgaudas-ht / @gedgaudasnikita - AFAIK the GitHub ToS only allow one account per human",2018-07-12T16:01:24Z,179599
540,npm/npm,340599374,404563405,"@gedgaudasnikita yea but what about using Function constructor syntax::

```
new Function('console.log(1 + 1)')()
```

I feel like you are addressing the symptom by banning eval not the root problem.",2018-07-12T16:01:46Z,284360
541,npm/npm,340599374,404563902,"@gedgaudasnikita That won't work. Eval is needed for a lot of things, and banning it won't do. We need package signing - which NPM actively rejected, but I feel like this has a use right now judging from the current events right now",2018-07-12T16:03:18Z,14976516
542,npm/npm,340599374,404564500,"@gedgaudasnikita 

> Just forbid all packages containing a string ""eval"", or make them require manual reviewal. But my advise would be just ban them for good

This does not work well with some JS perf optimization patterns. See the monomorphization that improved webpack's speed by ~80% between version 3 and 4.",2018-07-12T16:05:18Z,374505
543,npm/npm,340599374,404565133,"Probably the only acceptable long-term solution is to allow and encourage package signing, with the ability to use a second factor (such as a hardware key) for the signing. Whoever stole the eslint-scope creds could also have stolen signing creds, if they were not using 2 factor auth.

It will take a long time for the majority of package maintainers to reach this level of security awareness, but it's a project that must be started.",2018-07-12T16:07:18Z,374505
544,npm/npm,340599374,404565813,@OliverUv +1 on that but I don't know why NPM rejected this - it was a majorly a good security practice,2018-07-12T16:09:29Z,14976516
545,npm/npm,340599374,404568390,"We've [updated our status page](https://status.npmjs.org/incidents/dn7c1fgrr7ng) about this incident. I'm going to lock this thread because it's likely to draw a lot of traffic and I think actual information will be buried. Please keep an eye on the status page, our social media, [npm.comminuty announcements](https://npm.community/c/announcements), or this thread for any particularly big things.

While you're here, a reminder that [this issue tracker is closing](https://blog.npmjs.org/post/175587538995/announcing-npmcommunity) in favor of [npm.community](https://npm.community), so please direct issues and such to that space. This issue tracker will be closing very soon.",2018-07-12T16:18:06Z,17535
546,npm/npm,340599374,404569202,"(update: apologies, my original post included a link to this issue instead of the actual status page. The link has been updated)",2018-07-12T16:20:47Z,17535
547,npm/npm,327130542,327130542,"#### I'm opening this issue because:

  - [x] npm is crashing.
  - [x] npm is producing an incorrect install.
  - [x]  npm is doing something I don't understand.
  - [x] npm is producing incorrect or undesirable behavior.

all of the above.

#### What's going wrong?
This afternoon all servers in our AWS EU-Frankfurt environment started throwing the error regardless of what package we are trying to install

`
npm ERR! code E418
npm ERR! 418 I'm a teapot: commander@~2.3.0
`

It then locks up and does not exit the process.

#### How can the CLI team reproduce the problem?

`npm install`

There is nothing being written to the npm-debug.log as the process does not do anything

### supporting information:

 - `npm -v` prints:
5.6.0
 - `node -v` prints:
8.9.0
 - `npm config get registry` prints:
http://registry.npmjs.org/

 - Windows, OS X/macOS, or Linux?:
Amazon Linux AMI release 2017.09

 - Network issues:
   - Geographic location where npm was run:
AWS - EU
   - [x ] I use a proxy to connect to the npm registry.
   - [x ] I use a proxy to connect to the web.
   - [x ] I use a proxy when downloading Git repos.
   - [ ] I access the npm registry via a VPN
   - [ ] I don't use a proxy, but have limited or unreliable internet access.
 - Container:
No container.
",2018-05-28T23:07:45Z,5973850
548,npm/npm,327130542,392619452,"I'm getting the same error, started today. Using Azure:

> 2018-05-28T23:54:24.4436370Z npm ERR! code E418
> 2018-05-28T23:54:24.4481896Z npm ERR! 418 I'm a teapot: @angular/cli@^1.6.5

command (using Azure PowerShell 2 extension):
`npm install @angular/cli`

",2018-05-29T00:03:31Z,2467618
549,npm/npm,327130542,392619580,"I'm having the same issue.
```
$ npm install --save-dev express
npm ERR! code E418
npm ERR! 418 I'm a teapot: express@^4.16.3

$ npm -v
5.6.0

$ node -v
v8.11.2

$ npm config get registry
http://registry.npmjs.org/

$ $ cat /etc/os-release
NAME=""Ubuntu""
VERSION=""16.04.4 LTS (Xenial Xerus)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 16.04.4 LTS""
VERSION_ID=""16.04""
HOME_URL=""http://www.ubuntu.com/""
SUPPORT_URL=""http://help.ubuntu.com/""
BUG_REPORT_URL=""http://bugs.launchpad.net/ubuntu/""
VERSION_CODENAME=xenial
UBUNTU_CODENAME=xenial

```

The server is running in Japan and using a proxy to connect to the npm registry and the web.

",2018-05-29T00:05:04Z,18719417
550,npm/npm,327130542,392620073,"Same here, started today.  Error on my local machine (windows) and a VM (linux).",2018-05-29T00:09:03Z,39572711
551,npm/npm,327130542,392620086,"I'm having same issue.

```
citron: [Application] % npm install
npm ERR! code E418
npm ERR! 418 I'm a teapot: ember-test-selectors@^1.0.0
```",2018-05-29T00:09:11Z,18431951
552,npm/npm,327130542,392620192,"Having the same issue on our build server:

`
29-May-2018 10:07:56 | npm ERR! code E418
29-May-2018 10:07:56 | npm ERR! 418 I'm a teapot: source-map@0.5.7
`

",2018-05-29T00:10:18Z,11094060
553,npm/npm,327130542,392620327,"Running into the same issue here, running into problems on both local and build server
npm ERR! 418 I'm a teapot: express@^4.15.3",2018-05-29T00:11:44Z,30345600
554,npm/npm,327130542,392620832,"Same issue here.

I'm behind a corporate proxy too. SSHed into my home computer over my phone, and installs went through just fine.",2018-05-29T00:16:57Z,967145
555,npm/npm,327130542,392620839,"Ditto, 

npm config get registry
https://registry.npmjs.org/

npm -v
5.6.0

node -v
v8.9.4

npm install
npm ERR! code E418

",2018-05-29T00:17:03Z,20423667
556,npm/npm,327130542,392621681,"Yep, same problem here. `npm install <package>` returns 418 I'm a Teapot, and install stalls.
Related: https://github.com/npm/registry/issues/335",2018-05-29T00:26:32Z,4160665
557,npm/npm,327130542,392621819,"I know the Internet loves being cute, but this error is unhelpful.",2018-05-29T00:27:58Z,967145
558,npm/npm,327130542,392623317,"We're having this issue on AWS, behind a corporate proxy. Unfortunately we don't have the chance to go around this proxy.

```
$ npm install
npm ERR! code E418
npm ERR! 418 I'm a teapot

$ npm -v
5.3.0

$ node -v
v8.4.0
```",2018-05-29T00:44:22Z,5118588
559,npm/npm,327130542,392623389,"Corporate proxy - no option to bypass
```
$ npm - v
5.6.0
$node -v
v9.6.1
```",2018-05-29T00:45:24Z,39572711
560,npm/npm,327130542,392623446,"My colleague tried to run `npm install` and he was succeeded. 
His environment:
```
$ npm -v
3.5.2
$ node -v
v4.2.6
$ npm config get registry
https://registry.npmjs.org/
```
log:
```
$ npm install express
/home/username
tqq express@4.16.3
tqq thomash-node-audio-metadata@1.0.1  (git://github.com/osyo-manga/thomash-node-audio-metadata.git#c8af2b43ef97b2753f2ba712e828e0ff131976e8)
mqq xx@0.0.3
```
I also tried that in an older version of npm and node.
```
$ nvm install 4.2.6
Downloading and installing node v4.2.6...
Downloading https://nodejs.org/dist/v4.2.6/node-v4.2.6-linux-x64.tar.xz...
######################################################################## 100.0%
Computing checksum with sha256sum
Checksums matched!
Now using node v4.2.6 (npm v2.14.12)

$ npm -v
2.14.12

$ npm install express
npm WARN package.json rest-test@1.0.0 No description
npm WARN package.json rest-test@1.0.0 No repository field.
npm WARN package.json rest-test@1.0.0 No README data
express@4.16.3 node_modules/express

$ ls node_modules/ | grep express
express
```

Does NOT the issue reproduce in older versions of npm?",2018-05-29T00:45:59Z,18719417
561,npm/npm,327130542,392624104,"Yeah, rolling back to Node 4.26 works. 8 still doesn't. This is dumb.",2018-05-29T00:52:51Z,967145
562,npm/npm,327130542,392624281,"👍  Me too.

For those looking for a workaround, yarn still seems to be healthy.",2018-05-29T00:54:36Z,594211
563,npm/npm,327130542,392626994,"I have started experiencing this issue from just today. I am behind an organisational firewall. 

$ npm install
npm ERR! code E418
npm ERR! 418 I'm a teapot: @react-ag-components/core@latest

Other people in my team have no issues running the same npm code from their workstation. Only me is having the problem.",2018-05-29T01:20:11Z,5455404
564,npm/npm,327130542,392627314,"The body of the 418 is `{""error"":""got unknown host (registry.npmjs.org:443)""}`. Looks like some npm clients are appending the port to the Host header, but only when going through a proxy, and that's confusing the registry:

```
$ wget --header='Host: registry.npmjs.org:443' https://registry.npmjs.org/
--2018-05-29 01:22:08--  https://registry.npmjs.org/
Resolving registry.npmjs.org (registry.npmjs.org)... 104.18.95.96, 104.18.97.96, 104.18.94.96, ...
Connecting to registry.npmjs.org (registry.npmjs.org)|104.18.95.96|:443... connected.
HTTP request sent, awaiting response... 418 I'm a teapot
2018-05-29 01:22:08 ERROR 418: I'm a teapot.
```",2018-05-29T01:23:00Z,806600
565,npm/npm,327130542,392627936,"yes, we are meeting the same issue now.",2018-05-29T01:28:58Z,4564952
566,npm/npm,327130542,392628231,"Me too,

npm ERR! code E418
npm ERR! 418 I'm a teapot: moment@latest

node -v
v8.2.1

npm -v
5.3.0

I'm behind a proxy too.
",2018-05-29T01:31:29Z,19720900
567,npm/npm,327130542,392628425,Same issue here. We have a successful install without a proxy and a 418 error code when installing from inside a container hooked up to the proxy.,2018-05-29T01:33:09Z,14816406
568,npm/npm,327130542,392628846,"I'm gonna lock this for now, because I'm sure it's gonna get plenty of traffic. You really don't need to respond to repeat what every other poster is saying. The registry team has been informed.",2018-05-29T01:36:29Z,17535
569,npm/npm,327130542,392648459,"We've fixed this -- we now accept the port appended to the host. @wgrant's comment was most helpful, thank you!",2018-05-29T04:19:36Z,757502
570,npm/npm,326712539,326712539,"#### I'm opening this issue because:

  - [ ] npm is crashing.
  - [x] npm is producing an incorrect install.
  - [ ] npm is doing something I don't understand.
  - [ ] npm is producing incorrect or undesirable behavior.
  - [ ] Other (_see below for feature requests_):

#### What's going wrong?
using `npm i --no-optional`, but the optionalDependencies still affect the version of other dependencies.

#### How can the CLI team reproduce the problem?

1. `git clone https://github.com/mapleeit/how-to-reproduce-npm-bug.git`
2. `npm i --no-optional`
3. `npm ls | grep request`
```
// it prints 
│ ├─┬ request@2.81.0
│ │ │ │   ├─┬ request@2.87.0
│ │ │ │   ├─┬ request-promise-native@1.0.5
│ │ │ │   │ ├─┬ request-promise-core@1.1.1
    ├── request@2.81.0 deduped
```
__the version of `request` is 2.81.0__

4. `open package.json`
5. delete the `optionalDependencies`
6. `rm -R node_modules && rm package-lock.json`
7. `npm i --no-optional` or `npm i`
8. `npm ls | grep request`
```
// it prints 
│ ├─┬ request@2.87.0
  │ │ │   ├── request@2.87.0 deduped
  │ │ │   ├─┬ request-promise-native@1.0.5
  │ │ │   │ ├─┬ request-promise-core@1.1.1
```
__the version of `request` is 2.87.0__


### supporting information:

 - `npm -v` prints: 6.1.0 
 - `node -v` prints: v8.1.2
 - `npm config get registry` prints: https://registry.npmjs.org/ 
 - Windows, OS X/macOS, or Linux?: OS X
 - Network issues:
   - Geographic location where npm was run:
   - [ ] I use a proxy to connect to the npm registry.
   - [ ] I use a proxy to connect to the web.
   - [ ] I use a proxy when downloading Git repos.
   - [ ] I access the npm registry via a VPN
   - [ ] I don't use a proxy, but have limited or unreliable internet access.
 - Container:
   - [ ] I develop using Vagrant on Windows.
   - [ ] I develop using Vagrant on OS X or Linux.
   - [ ] I develop / deploy using Docker.
   - [ ] I deploy to a PaaS (Triton, Heroku).

<!--
    Thank you for contributing to npm! Please review this checklist
    before submitting your issue.

    - Please check if there's a solution in the troubleshooting wiki:
      https://github.com/npm/npm/blob/latest/TROUBLESHOOTING.md

    - Also ensure that your new issue conforms to npm's contribution guidelines:
      https://github.com/npm/npm/blob/latest/CONTRIBUTING.md

    - Participation in this open source project is subject to the npm Code of Conduct:
      https://www.npmjs.com/policies/conduct

    For feature requests, delete the above and uncomment the section following this one. But first, review the existing feature requests
    and make sure there isn't one that already describes the feature
    you'd like to see added:
      https://github.com/npm/npm/issues?q=is%3Aopen+is%3Aissue+label%3Afeature-request+label%3Aalready-looked-at
-->

<!--

#### What's the feature?

#### What problem is the feature intended to solve?

#### Is the absence of this feature blocking you or your team? If so, how?

#### Is this feature similar to an existing feature in another tool?

#### Is this a feature you're prepared to implement, with support from the npm CLI team?

-->
",2018-05-26T05:44:58Z,4194287
571,npm/npm,326712539,392816540,"Seems to be related to https://github.com/npm/npm/issues/17633, which is getting no love :*(",2018-05-29T15:19:22Z,2255363
572,npm/npm,326712539,403938503,dupe of https://github.com/npm/npm/issues/17633,2018-07-10T19:26:55Z,17535
573,npm/npm,299453048,299453048,"#### I'm opening this issue because:

  - [ ] npm is crashing.
  - [x] npm is producing an incorrect install.
  - [ ] npm is doing something I don't understand.
  - [ ] Other (_see below for feature requests_):

#### What's going wrong?

#19883 causes issues to production environment. Also https://github.com/npm/npm/issues/19883#issuecomment-367698802 describes a way the behavior introduced by 5.7.0 can be exploited to do things that are not intended.

I understand that CVEs are usually assigned before a responsible disclosure, fix is delivered, but as this was publicly disclosed, the clock is ticking and people should be aware of.
",2018-02-22T18:06:29Z,20575
574,npm/npm,299453048,367781159,https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-7408,2018-02-22T18:48:32Z,23705771
575,npm/npm,299453048,367781544,"(this lock is preemptive. We'll have an actual response once we talk about it, but I'm heading off the trolls that are still circling like sharks)",2018-02-22T18:49:43Z,17535
576,npm/npm,299453048,367824728,"I see that someone else filed a CVE, but we wouldn't have filed one ourselves: After internal discussions and consultation with a security team, we decided that a bug that causes unintended behavior, in a prerelease software version especially, does not really warrant a CVE. Regarding the `setuid` thing, we do not consider that a vulnerability because the `setuid` bit is not preserved on a `chown`, therefore not risking creating world-executable setuid executables.

So, with that comment, plus the fact that OP's original request was done _anyway_, I'm gonna close this.",2018-02-22T21:16:24Z,17535
577,npm/npm,232455334,232455334,"#### I'm opening this issue because:

  - [ ] npm is crashing.
  - [ ] npm is producing an incorrect install.
  - [x] npm is doing something I don't understand.
  - [ ] Other (_see below for feature requests_):

#### What's going wrong?
package.json
```
{
""dependencies"":{""some-module"":""~1.2.3""}
}
```
currently available version 1.3.0 & 1.2.4
when perform `npm update some-module`
npm override version in package.json to `^1.3.0`

### supporting information:

 - `npm -v` prints: 5.0.0
 - `node -v` prints: 8.0.0
",2017-05-31T05:12:28Z,783335
578,npm/npm,232455334,305087180,"Hm. So, npm4 doesn't do this either afaik (nor does any older version). Usually, the way to do this is to set `save-prefix='~'` in your local project's `.npmrc` (or in `~/.npmrc`).

Having npm autodetect this and preserve this sounds like a nice thing to have. I think it's reasonable to take a PR for it 👍  I'll double-check with @iarna who might have more context about the implications, though.",2017-05-31T05:28:43Z,17535
579,npm/npm,232455334,305092724,I understand it. But when doing update it should not change range in package.json,2017-05-31T06:05:59Z,783335
580,npm/npm,232455334,305145797,"Had the same issue. (if I understand the original issue correct)

I did 
```diff
-    ""hapi"": ""16.2.x"",
+    ""hapi"": ""16.3.x"",
```
Run `npm update` and it did
```diff
-    ""hapi"": ""16.3.x"",
+    ""hapi"": ""^16.3.0"",
```

npm 4 did not do this and this behaviour is not wanted so it is a bug to me!

npm -v: 5.0.0
node -v: 8.0.0",2017-05-31T10:05:37Z,2361826
581,npm/npm,232455334,305340266,"Fair enough! Thanks for pointing that out, @AdriVanHoudt ",2017-05-31T22:49:18Z,17535
582,npm/npm,232455334,305390062,"np, point me in the right direction and I might even try to PR :P",2017-06-01T05:05:26Z,2361826
583,npm/npm,232455334,305392470,"@AdriVanHoudt ...I should probably take off that tag huh. The more I think about it, the more I realize I've *no idea* how npm4 even did this. @iarna might know but I've no clue :s ",2017-06-01T05:26:17Z,17535
584,npm/npm,232455334,305410475,"I didn't even saw the label haha
I what do you mean by no idea how? npm 4 just did not edit package.json on update (don't know the internals ofc)",2017-06-01T07:20:19Z,2361826
585,npm/npm,232455334,305650972,"oh I thought you meant that `npm up --save pkg` would magically preserve this.

Ok yeah, this is back to feature request.

This is actually something someone might put up a PR for, here's the summary: add some logic to [the bit in `deps.js` that calculates the version spec to use](https://github.com/npm/npm/blob/latest/lib/install/deps.js#L273-L280) to default to the specific prefix used by that version, _if there was one_. Forget about the `save-exact` case, since it's expected that people _must_ use `save-exact` to get npm to save as a non-range and I don't think we're gonna change this (and if we do, it should be a separate thing with a separate discussion).

Give that a whirl, and if the patch works, put up a PR? 👍 ",2017-06-01T23:48:43Z,17535
586,npm/npm,232455334,305711177,"Just to clarify my issue (let me know if you prefer a separate one)

I have a package.json with
```
""hapi"": 16.2.x
```
I run `npm i`
It installed 16.2.0
I update my package.json to 
```
""hapi"": 16.3.x
```
I run `npm update`
it installs 16.3.0 (as it should)
it updates my package.json to 
```
""hapi"": ^16.3.0
```

Why would it edit the package.json? npm 4 does all the sames things except that :O",2017-06-02T07:26:49Z,2361826
587,npm/npm,232455334,306258627,@AdriVanHoudt try `npm update --save` in `npm@4`. That's the difference.,2017-06-05T17:59:08Z,17535
588,npm/npm,232455334,306406494,"@zkat :mindblown: 
ok that makes sense. 🙄 I would still prefer if it kept the .x notation, is that possible?",2017-06-06T07:35:48Z,2361826
589,npm/npm,232455334,306419252,"@AdriVanHoudt as per twitter, if you can figure out a good, clean way to do this and you have well-defined semantics for it, we might take a patch (I haven't checked with @iarna but I defer to her if she'd rather not have this)",2017-06-06T08:33:57Z,17535
590,npm/npm,232455334,306442778,"@zkat I think I will switch to just using the default notation, it is more correct and works better with the lockfile imo",2017-06-06T10:14:30Z,2361826
591,npm/npm,232455334,306490497,"@zkat Why does `npm update` default to `--save` at all?

I don't really see the point, since I don't need to change my semver ranges except for breaking changes (e.g. `^1.0.0` => `^2.0.0`) and shouldn't those be manual anyways? 😕 

I find the commands in `yarn` to make a lot more sense in comparison:
- `yarn upgrade` will upgrade each package to the latest version specified by `package.json`, without modifing `package.json`
- `yarn upgrade package` will upgrade `package` to `latest`, and save the new version with the default range in `package.json`
- `yarn upgrade package@^2.0.0` will upgrade `package` according to the given semver range, and if needed save the new range in `package.json`

### Example of npm breaking my ranges
I have a package `a` that depends on package `b`.

`b` has three published versions: `1.0.0`, `1.0.1` and `1.1.0`.

For some reason, I do not want to use `b@1.1.0`, so I set my dependencies to `""b"": ""~1.0.0""`

I run `npm update`, thinking that I will get `b@1.0.1`, which I do. But npm will change my dependencies to `""b"": ""^1.0.1`.

The next time I run `npm update`, I will get `b@1.1.0`, which is *not* in the original range!

---
It feels like I will have to use `npm update --no-save` all the time, and also instruct everyone on my team to do the same. 😕 Or is there a better solution?",2017-06-06T13:45:00Z,2260305
592,npm/npm,232455334,306572698,"@Maistho If you want to stick with `~`, I recommend you set it in your _project_ config with `echo 'save-prefix=""~""' >> npmrc` (in your project dir). That'll set that to default.

As for everything else? Well, that's the result of setting `--save` as the default across the CLI. This is the behavior as it's always been if you'd set `--save`.

`update` is a command we've been intending to overhaul, and we know it's something where the warts start getting more obvious as soon as you start saving. I wish we'd had time to actually redesign it by the time npm5 came out, but we really just didn't have the bandwidth for it, and it seemed better to just get something out there asap and then do a proper redesign. The same goes for `npm outdated`, which is right now tied up directly with update.

The stuff you're saying is super useful and sounds like a really good direction to take the command in. It's good to hear about the different expectations people have of this so we can make sure to write the thing people want this to be.

If you care enough for it, it'd be great to get a more complete RFC filed that describes the `update` that you wish we had and that outlines your needs from it. If that sounds like too much, just hang tight -- we'll be getting to it soon. 👍 

Thanks y'all for your commentary, and for your understanding!",2017-06-06T18:16:55Z,17535
593,npm/npm,232455334,306811307,"@zkat Thanks for the detailed response :smile: 

Setting the save-prefix will instead change all `^` prefixes into `~`, which will just replace my problem with a different one (since I want to mix caret and tilde ranges).

I'd the more than happy to write an RFC concerning `update`, but I'm not sure how I would go about it. Do you have any example/guide on how it should be structured?",2017-06-07T14:25:45Z,2260305
594,npm/npm,232455334,311071479,"Landed here after noticing npm changing my package.json in ways similar to the above. In my case, `npm update some_module` changed the package declaration from `some_module: ^0.1.6` to `some_module: 0.1.8` (dropping the caret specifier).  As it turns out this was because I'd added `save-prefix=` to `~/.npmrc`.

This behavior feels pretty broken to me.  I added the caret to this particular module as a way of saying, ""take minor updates"".  In removing it, NPM effectively reversed that decision, which is categorically wrong.

I believe the problem here is that the`save-prefix` option is given precedence over what is in `package.json`.  Instead,  `save-prefix` should only apply when installing a new module.  For existing modules, the range specifier should not be altered.

Also, given that `package-lock.json` is now a thing, I can see a case for saying `npm update` shouldn't touch `package.json` at all.  (But I'm still trying to understand how package.json and package-lock.json interact, so not gonna get on a soap box about that.)
",2017-06-26T14:13:53Z,164050
595,npm/npm,232455334,311082387,"touching package.json only makes sense on update if you go from `x: ^0.1.6` to `x: ^0.1.8` since then you say `0.1.8` is the new lowest version. In all other cases it feels weird and counter intuitive that update touches package.json. Since you want it to get your modules up to date that is all.
I think an upgrade command which does this (and preserves the notation in package.json) makes more sense.",2017-06-26T14:50:29Z,2361826
596,npm/npm,232455334,316052784,"Usually my dependency versions are carefully crafted, so I don't really see a use case for `npm update` or `npm update --save` ever, except if you don't care about versions at all.

Is there a shorter alias of `npm update --no-save` or any way I can reset the previous update behaviour?

Btw, the [update docs](https://docs.npmjs.com/cli/update) currently don't mention the npm@5 behavior.",2017-07-18T12:46:24Z,2564094
597,npm/npm,232455334,324749282,"I think that the changed behavior of NPM is really very dangerous. We very carefully monitor which module is being changed and we always use exact versioning for all modules. The fact that now `npm update` is changing `package.json` is totally a breaking change. It is absolutely unclear why this change was done. No docs, no arguments, ... nothing. 😟 ",2017-08-24T20:33:13Z,5693835
598,npm/npm,232455334,336462149,"The use of Tilde is even in the docs. 

https://docs.npmjs.com/cli/update#tilde-dependencies

IMO this is a bug, not a feature-request...
",2017-10-13T14:06:12Z,4494301
599,npm/npm,232455334,336501037,"> it'd be great to get a more complete RFC filed that describes the update that you wish we had and that outlines your needs from it.
(from https://github.com/npm/npm/issues/16813#issuecomment-306572698)

This is the important bit preventing this from moving forward. This is one of those things where a number of good behaviors are mixing in unexpected ways.",2017-10-13T16:25:14Z,17535
600,npm/npm,232455334,373957801,"This is still a relatively major issue, but it seems the trail has gone cold for quite a few months. We want to keep very close tabs on our dependencies, where we mostly use tildes but not for everything. Every time we update, NPM is adding an unnecessary chance for human error should the developer forget to correct the change made to `package.json` before committing the changes. Sometimes it adds carats, sometimes it removes the prefix entirely. Why hasn't this gotten more attention?

`npm -v 5.7.1`",2018-03-17T22:33:34Z,13490339
601,npm/npm,232455334,393327742,"This was opened 1 year ago tomorrow.  How can we get some traction on this?
First, I think the ""feature-request"" label needs to be removed and replaced with a ""bug"" label.  This is not a feature request.  This is npm update behaving in a way that produces unexpected and dangerous results.",2018-05-30T21:36:50Z,351953
602,npm/npm,232455334,393331668,@zkat It looks like you removed the Bug label originally. Can you add it back?,2018-05-30T21:51:45Z,351953
603,npm/npm,232455334,403285720,"This is a serious bug. `npm update` must preserve the semantics of the ranges when it updates `package.json`.
It is ok to update:
* `~1.2.3` -> `~1.2.8` 
* `^1.2.3` -> `^1.4.0`

But it is **not ok** to replace `~` by `^`.",2018-07-08T12:51:18Z,415044
604,npm/npm,232455334,403300432,Or maybe update should have strong/weak options. Strong option would update all dependencies to latest and save them with carets in package.json. Weak option would only update to the latest on the existing ranges (and preserve existing tildes).,2018-07-08T16:46:54Z,415044
605,npm/npm,232455334,403302130,Just discovered `npx npm-check -u` and `npm-merge-driver`. Looks this is not such a major issue because there are better ways to update.,2018-07-08T17:14:43Z,415044
606,npm/npm,232455334,403313186,"> But it is not ok to replace ~ by ^.

Ouch, saw the same in a few of our projects. This is really bad and a regression.",2018-07-08T20:06:28Z,827205
607,npm/npm,232455334,403313221,This is why we will move to yarn and pnpm.,2018-07-08T20:07:10Z,827205
608,npm/npm,232455334,403313472,This is not a regression. This is the behavior npm has always had when it comes to this. The change is that it's more notable due to auto save.,2018-07-08T20:11:09Z,17535
609,npm/npm,232455334,403313905,"I've locked this thread to make it clear that this is no longer the place to have feature request discussions. This is not a bug.

If anyone cares enough to see this behavior changed, please [file a formal RFC](https://github.com/npm/rfcs) with more details around expected behavior. We no longer have the time or bandwidth to discuss more informal, off-the-cuff ideas.

Furthermore, if what you want to do is discuss ideas more openly without a formal proposal, with the understanding that the npm team won't really engage with you about it, you can also post in the [ideas category in npm.community](https://npm.community/c/ideas), which is a great place to brainstorm before filing an RFC.",2018-07-08T20:17:46Z,17535
610,npm/npm,102234932,102234932,"### Expected Behavior

All directories created by `npm install` should have the npm config setting `umask` applied to their file mode/permissions. The `umask` setting is [clearly described as](https://docs.npmjs.com/misc/config#umask) the ""value to use when setting the file creation mode on files and folders.""
### Observed Behavior

During `npm install`, if directories are created by the `tar` module as a side-effect of extracting a file, the umask setting is ignored. For example, when tar extracts `foo/bar/baz.js`, it creates the `foo` and `foo/bar` directories (if they don't exist) before writing `baz.js`. These directories do not respect the umask setting.

Instead, npm's umask setting is ignored and [`process.umask()` is used](https://github.com/npm/fstream/blob/master/lib/writer.js#L15). However, there is a further bug/unexpected behavior where the `tar` module will actually [do the equivalent of `chmod a+x`](https://github.com/npm/fstream/blob/master/lib/writer.js#L352) on directories created as a side-effects of file extraction, meaning the even `process.umask()` isn't strictly observed.
### Details
- OS: Mac OS X 10.9.5
- `npm --version`: `2.13.5`
- `node --version`: `v0.10.40`
- `npm config get umask`: `0077`
- `umask`: `0077`

An example is seen if you install `grunt-lib-phantomjs`. The directory `node_modules/phantomjs/lib` (as an example) should have a mode of `0700`; instead, it has a mode of `0711`.
",2015-08-20T20:54:19Z,567398
611,npm/npm,102234932,135879144,"Since I see this has been tagged as a feature request and support request, I want to be clear that this is definitely a bug in npm.

The umask setting in npm is being completely ignored for a seemingly arbitrary subset of operations during `npm install`. This is almost certainly not the intended behavior. It also explicitly disagrees with the behavior described in the documentation, and is inconsistent even within the observed behavior of npm (sometimes it respects umask, other times it doesn't, for no good reason).

The underlying cause (cited in the issue) is that the tar module isn't aware of npm's `umask` setting, and just does its own thing (using the `process.umask()` value, then overriding the execute bit on it.)

But in summary, this is 99.9% likely to be a real bug, not a new feature or user usage problem. 
",2015-08-28T20:22:45Z,567398
612,npm/npm,102234932,156077224,"This issue still exists in npm version 3.4.0 and is most certainly a bug and not a new feature or user usage problem.

Please see https://github.com/npm/npm/issues/4197 which is correctly tagged as a bug.

#### Details
- OS: Debian GNU/Linux 7.8 (wheezy)
- `npm --version`: `3.4.0`
- `node --version`: `v4.2.2`
- `npm config get umask`: `0022`
- `umask`: `0077`
",2015-11-12T11:15:35Z,1020740
613,npm/npm,102234932,157549963,"This is marked as a feature request in part because the current behavior that npm has is underspecified. The first step to making npm's behavior here clearer is to nail down what the current behavior is, and why. Tagging this with `footgun` gets it onto npm's road map, and the next step for the CLI team is to unearth and document the historical reasoning for how the various pieces of npm (including `node-tar`) handle permissions.
",2015-11-18T00:01:11Z,418097
614,npm/npm,102234932,174083314,"After discussing this as a team, we think the right thing for npm to do is to ignore whatever permissions or UIDs are set in the package tarball, and explicitly squash everything to be written with the current user's user ID and umask in all cases _except_ when npm is being run as root without `--unsafe-perm` being overwritten (you should never have files owned by nobody on your filesystem).

This is a mildly tricky bit of work because it requires good tests, and also because we need to make sure whatever API calls the CLI uses don't cause problems on Windows, but this is something we plan to address within the medium-term. If somebody else wants to treat the first paragraph as a rough spec and start working on a patch, that would be very welcome!
",2016-01-22T23:13:09Z,418097
615,npm/npm,102234932,256336654,"Did this issue get lost (honest question, no sarcasm)?
In my opinion it needs lot more love as the possible security implications could be quite catastrophic on a multi-user system and this issue is reported for over a year now.
The worst case scenario is that such directory results in the possibility to replace its content by a non-root user with evil code that may me executed by another user (including possibly root).

What I observed (even with newest npm 3.10.9) that it **sometimes** creates node_modules directories with permission 777. when doing the `npm install` multiple times the results vary, most of the time it results in 755 but sometimes in 777). This seems to have nothing to do with the source tarballs content (retrieved from registry.npmjs.org) but a more general issue. As mentioned, its not deterministic and the tarballs definitively don't contain any files/directories with such permissions.

This problem was observed while creating packages for a Linux distribution and boiled down to finding this ticket.
",2016-10-26T12:47:59Z,203012
616,npm/npm,102234932,309403145,"We're closing this issue as it has gone thirty days without activity.  In our experience if an issue has gone thirty days without any activity then it's unlikely to be addressed.  In the case of bug reports, often the underlying issue will be addressed but finding related issues is quite difficult and often incomplete.

If this was a bug report and it is still relevant then we encourage you to open it again as a new issue.  If this was a feature request then you should feel free to open it again, or even better open a PR.

For more information about our new issue aging policies and why we've instituted them please see our [blog post](http://blog.npmjs.org/post/161832149430/npm-the-npm-github-issue-tracker-and-you).
",2017-06-19T10:38:24Z,29212966
617,npm/npm,102234932,356125782,"@othiym23 @isaacs @iarna
can you please reopen and revisit this issue, npm 5.6.0 still randomly ends up creating node_modules directories with 777 that contain code.
PS: This npm-robot that auto-closes a security issue because nobody replied is quite damaging
  ",2018-01-08T23:08:31Z,203012
618,npm/npm,102234932,367688924,"Aug 20, 2015

O I am laffin.",2018-02-22T13:59:53Z,10137
619,npm/npm,102234932,367702989,"Issue not fixed in 30 days?
Must be gone!",2018-02-22T14:46:15Z,10676797
620,npm/npm,102234932,367989026,"Not treating security seriously, are we?",2018-02-23T11:50:36Z,797670
621,npm/npm,102234932,368110909,"Yeah, this seems to have gotten swept up -- the bot shouldn't have just closed the issue like that.

There was a step forward and a step back on this over the past year, and we haven't touched the issue since. The `patch-welcome` tag continues to apply, so if you think writing code is a more worthwhile endeavor than snarking on foss issue trackers, we super welcome your contributions!",2018-02-23T19:16:09Z,17535
622,npm/npm,8815842,8815842,"I would like to be able to run `npm install` for a `package.json` file in a subdirectory of a git repo.

This would be useful for projects that use releases systems (such as [nzakas/parserlib](https://github.com/nzakas/parser-lib/blob/master/release/npm/package.json)), and for git repos that contain multiple npm packages .

Can this be added?
",2012-11-29T15:39:54Z,313894
623,npm/npm,8815842,11961652,"`cd subdirectory && npm install` :D
",2013-01-07T17:37:34Z,306324
624,npm/npm,8815842,16284176,"Same problem I have with https://github.com/nmrugg/LZMA-JS - the actual module (package.json) is in a src/ subdirectory, but I don't find a way to reference that subdir from the package.json of a module that depends on it.
Is that possible ? (can't script a cd && npm install in package.json, can I?)
",2013-04-12T09:37:59Z,289678
625,npm/npm,8815842,16325651,"you could try something like `{""scripts"":{""install"":""cd src && npm install""}}` in your package.json.

but that is just not how its meant to be done

> but I don't find a way to reference that subdir from the package.json of a module that depends on it

when your module depends on another module that other module will be installed in `node_modules/<module-name>` and in that directory there will be its `package.json`. thats not only how npm does it - thats how the node-modules work, see http://nodejs.org/api/modules.html#modules_all_together

so i am pretty sure, the way how it works now will not change (its set in stone)
",2013-04-13T01:53:09Z,306324
626,npm/npm,8815842,16335401,"@SLaks There's no plans to do this.  If npm installs a git repo, it assumes that the git repo is the package.  I don't really know how we could specify a sub-path easily, since all parts of the git url are already used for other things.  For example, `git://x.com/path/to/whatever#some-branch`.  Where would you put the subpath in there, such that it can be deterministically differentiated from the path to the repo in all cases?  I don't see any way to do it without resorting to some awful hacks.

How would you express this?
",2013-04-13T15:49:47Z,9287
627,npm/npm,8815842,16372022,"What about a space ?

 `git://x.com/path/to/whatever#some-branch some/sub/dir`
",2013-04-15T08:09:01Z,289678
628,npm/npm,8815842,24532774,"+1 for this use case. For me, it's about including tests from other components (Java components). We use mocha to test these things, and service A wants to import service B's tests but we can't use npm because it doesn't have the concept of branches. Since the tests will be against some branch of service A, AND because the tests are part of the service rather than their own repo, we would benefit from subtree installs in GIT. Maybe another argument like --nodedir?
",2013-09-16T18:29:46Z,240684
629,npm/npm,8815842,41874363,"This would be great. I'm using [highlight.js](https://github.com/isagalaev/highlight.js) which stores its source, including `package.json`, in `src/`.
",2014-05-01T02:38:57Z,1765130
630,npm/npm,8815842,44191817,"This would indeed be great.  As it is I will have to make a whole bunch of dummy projects. I hope that I can simply have each main project as a git submodule and put a link called package.json to the real package.json.  That might break though...
",2014-05-26T14:05:06Z,5982633
631,npm/npm,8815842,47747448,"any news on that? or is there a workaround for that case? 
",2014-07-02T08:02:25Z,3481827
632,npm/npm,8815842,48606850,"+1 Unfortunately not all npm git repos can store their contained npm module in their root's!

git://user@example.com/path/to/resource//path/to/directory#commit-ref
",2014-07-10T13:53:07Z,451073
633,npm/npm,8815842,48607230,"@gaboom: That's actually a breaking change, in the (admittedly unlikely) case of `//` appearing in a git URL path.
",2014-07-10T13:56:00Z,313894
634,npm/npm,8815842,48633928,"Hmm, 
git://user@example.com/path/to/resource/[path/to/directory]#commit-ref
brackets are only legal as IPv6 literal hostnames, needs to be escaped otherwise.
",2014-07-10T17:08:08Z,451073
635,npm/npm,8815842,57716176,"I could definitely use this feature too.

We have a Git repository with several of our product's subsystems, and I want to be able to use npm to install individual micro-services as packages from their subdirectories (services/{package-name}/package.json).
",2014-10-02T21:55:13Z,709274
636,npm/npm,8815842,59743656,"Another +1, I agree URL syntax is ugly but apache/thrift is a good example of a nodejs project embedded in a bigger repo and trying to work with a non-tagged release is made very hard without this feature.
",2014-10-20T12:39:20Z,120915
637,npm/npm,8815842,70303543,"Another +1. Need to import other module that's not in the root and don't want to create a specific project for that module.
",2015-01-16T18:58:55Z,656065
638,npm/npm,8815842,97007071,"An example: https://github.com/pubnub/javascript/tree/develop/node.js - as you see, the actual code for NPM is located in `node.js` directory, not in the root of the repo. So if I need to install develop branch, the only way is to locally do `npm link`.
",2015-04-28T10:28:52Z,1530410
639,npm/npm,8815842,126380401,"+1. would require it in a fork of the breeze node repo https://github.com/JuHwon/breeze.server.node
",2015-07-30T15:58:22Z,9406772
640,npm/npm,8815842,144996288,"+1 for me as well.  The issue I face is that I want to keep code in a private GitHub repository.  If I have one module per repository, I'll end up with tons of private repos which would cost a fortune.
",2015-10-02T11:41:36Z,1498808
641,npm/npm,8815842,145474932,"There are the really big companies (FB, Google) which make the decision to keep software in monolithic repositories. And that's not a decision made out of ignorance, I believe. It would make a lot of sense to support this model of working for others, too.
",2015-10-05T09:29:13Z,4313
642,npm/npm,8815842,145492666,"I agree with @febeling 

I think it would make totally sense.
",2015-10-05T10:44:02Z,1078185
643,npm/npm,8815842,163108116,":+1: for me as well. To give another example, [Babel is a monorepo](https://github.com/babel/babel/blob/master/doc/design/monorepo.md). I think it would be good to accommodate this approach, since the users of a package might not have control over the format of the repo they want to use.
",2015-12-09T04:49:02Z,489866
644,npm/npm,8815842,169389079,"https://en.wikipedia.org/wiki/Bystander_effect
",2016-01-06T17:03:49Z,779738
645,npm/npm,8815842,169392015,"> https://en.wikipedia.org/wiki/Bystander_effect

Lol :-P

Take also in account that maybe the ones that can do it are not interested
on this feature... Me by example, I only started following it because my
company insisted on this feature, but as the only one with knowledges on
Node.js, I think this go against the npm philosophy...
",2016-01-06T17:10:41Z,532414
646,npm/npm,8815842,169451651,"To confirm what @piranna said above, this isn't something that the npm CLI team is interested in adding to the tool. We _are_ interested in continuing to improve the existing Git support (by making it more robust, and by cherry-picking those features of Bower's git support that make sense for npm's users), but we're _not_ interested in adding more complexity and new classes of use cases. I know this makes it awkward to use npm with monorepos, but I'm much more interested in finding a more general solution to the monorepo problem (some useful suggestions are in this thread), but I don't think it makes sense to make the git dependency syntax harder to understand than it already is. As such, I'm closing this feature request.
",2016-01-06T20:28:12Z,418097
647,npm/npm,8815842,169463209,"@othiym23 I guess most of us are subscribed to this issue to figure out how get support for monorepos. So what are those `useful suggestions ... in this thread`?

Is monorepo support (`I'm much more interested in finding a more general solution to the monorepo problem`) tracked somewhere else?
",2016-01-06T21:15:21Z,13697
648,npm/npm,8815842,169465421,"Primarily using package scripts to trigger subinstalls (still requires a `package.json` at root).

> Is monorepo support (`I'm much more interested in finding a more general solution to the monorepo problem`) tracked somewhere else?

It isn't. To be clear, monorepo support is not something that's on the CLI team's roadmap for the next 6-12 months. I'm interested in seeing better support for it within the CLI, but the team isn't going to have the time and attention to do it ourselves. For that reason, specific proposals (perhaps in the form of (prototype) code) are going to be more useful than generic requests for improved monorepo support.
",2016-01-06T21:20:13Z,418097
649,npm/npm,8815842,170845936,"If it's of any interest, this is how the python `pip` installer did it. pypa/pip#1215

``` text
git+https://git.repo/some_repo.git@egg=subdir&subdirectory=subdir_path
```
",2016-01-12T09:09:49Z,189580
650,npm/npm,8815842,170941275,"@LinusU I like it.  It's clean.  Perhaps I should not be surprised, given its heritage.
",2016-01-12T15:06:56Z,5982633
651,npm/npm,8815842,187389306,"@othiym23, sorry to relaunch this thread but I didn’t understand how to use scripts in order to trigger sub installs. Is there a way to target a specific submodule with 
this solution ? 
I thought this was one of the goal of keeping submodules in a same repo but if we have to install all the submodules of the repo each time we install it, I don’t really get what’s the advantage. Did I miss something ?
",2016-02-22T21:24:58Z,579922
652,npm/npm,8815842,187658354,"@akofman 

> how to use scripts in order to trigger sub installs

I use a postinstall script in package.js.  So scripts.postinstall contains:

```
""postinstall"": ""for dir in git_modules/[a-zA-Z0-9]* ; do printf \""=== Installing %s\\n\"" \""$dir\"" ; ( cd \""$dir\"" ; npm install ; ) ; printf \""=== Installation exit code: %s for %s\\n\"" $? \""$dir\"" ; done ""
```

It's a bit of a pain having to write `require('../git_modules/name')`.  One can get around that by having a directory tree like this:

```
package.js
node_modules/<non-git modules here>
src/node_modules/<git modules here>
src/lib/<code here>
```

I do that with a couple of large projects, as then I can just type `require(""a-git-module"")` or `require(""a-non-git-module"")` and it will just work, however it's a lot of scaffolding for a small project and it involves moving things around so I usually don't bother.

That doesn't answer the second half of your question but hopefully it's a start.
",2016-02-23T11:17:31Z,5982633
653,npm/npm,8815842,240672593,"From: http://stackoverflow.com/a/3651867

> A branch name can not:
> - Have a path component that begins with "".""
> - Have a double dot ""..""
> - Have an ASCII control character, ""~"", ""^"", "":"" or SP, anywhere
> - End with a ""/""
> - End with "".lock""
> - Contain a ""\"" (backslash

So `:` or `::` would work as separator:

`git://x.com/path/to/whatever#some-branch::/some/directory`

For the default branch : 

`git://x.com/path/to/whatever#::/some/directory`
",2016-08-18T09:33:53Z,54515
654,npm/npm,8815842,256037505,"+1 for this feature, hope It gets implemented either with '@' (python style) or with '::' delimitators .
",2016-10-25T13:41:05Z,7952216
655,npm/npm,8815842,256039732,"I'm fairly confident this will never get implemented in npm. They'd rather you just publish the module (paid or otherwise). Maybe you would be better off getting it sorted out in yarn...
",2016-10-25T13:49:12Z,240684
656,npm/npm,8815842,256040170,"oh ... ok
thanks @djMax 
",2016-10-25T13:50:49Z,7952216
657,npm/npm,8815842,285859377,"There must be support for this, or at least some suggestion.

A common case I run into is that I run into a bug in some published package. I submit a pull request on that package's GitHub repo that fixes the bug. I can't wait for the maintainer to merge and publish that. So I install from my GitHub fork using a git url. This isn't possible with monorepos.",2017-03-11T10:53:55Z,635591
658,npm/npm,8815842,289311543,"Hi, any progress with this? 
We have a lot of projects and we **very often** need to use master or concrete commit of some _""package""_ in monorepo and there isn't  _""easy/correct""_ way how to do this. It would be a very useful use something like this `npm install --save git://github.com/user/package/path/to/subfolder` (or any better format, but it doesn't matter for now). A lots of companies/projects migrate repositories into one big monorepos and this is realy need. **It is seriously big problem.**",2017-03-26T20:14:41Z,990676
659,npm/npm,8815842,289409967,"> [..], but I don't think it makes sense to make the git dependency syntax harder to understand than it already is.

Everyone seems to assume that this information needs to go into the path. Why is that? Can there not be some object structure, with one attribute containing the URL and other attributes containing metadata about how to handle the repository, such as tag, branch, subdirectory?",2017-03-27T10:05:40Z,730419
660,npm/npm,8815842,291398576,"Maybe we can use [submodules](https://git-scm.com/book/en/v2/Git-Tools-Submodules)?
",2017-04-04T05:39:15Z,16700996
661,npm/npm,8815842,291399497,"Shai, I'm working on an open source monorepo and I am going to have to
solve this. Submodules seems to me like the way to go. If anyone would like
to contact me for a brainstorming session, I'll be up for it.

On Tue, Apr 4, 2017, 08:39 Shai <notifications@github.com> wrote:

> Maybe we can use submodules
> <https://git-scm.com/book/en/v2/Git-Tools-Submodules>?
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/npm/npm/issues/2974#issuecomment-291398576>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AAmyx0bjnQ7FMXxK4AAiLRlJJnDADLXpks5rsdeQgaJpZM4ASJBW>
> .
>
",2017-04-04T05:46:09Z,635591
662,npm/npm,8815842,291400969,"@mightyiam 
it might be good to intergrade it with [lerna](https://github.com/lerna/lerna)
it takes care on the linking between internal packages 
",2017-04-04T05:56:55Z,16700996
663,npm/npm,8815842,305423659,"@zkat @iarna 
is there any chance this could be reconsidered, especially regarding the improved git support in npm5?",2017-06-01T08:18:51Z,9800850
664,npm/npm,8815842,318910849,"Couldn't we just use a query parameter? This doesn't seem like a very hard problem to solve:

```
git://x.com/path/to/whatever#some-branch?path=/folder/path
```",2017-07-30T15:56:20Z,2831444
665,npm/npm,8815842,324931792,How about `git://x.com/path/to/whatever.git/folder/path` since `git://x.com/path/to/whatever.git` works. You can take the subpath after `.git/`.,2017-08-25T14:11:35Z,486224
666,npm/npm,8815842,325069898,"`git://` is essential when you fork something and fix it and then you wait for your PR to be accepted, but with monorepos/lerna that does not work as a path is required :/",2017-08-26T01:27:17Z,21679
667,npm/npm,8815842,330459479,"I forked DefinitelyTyped and I want to add only one type as dependency. Why can't I just say:
`""@types/yandex-maps"": ""git://github.com/sinedsem/DefinitelyTyped.git/types/yandex-maps""`",2017-09-19T07:53:33Z,7715361
668,npm/npm,8815842,333348053,There's a bugfix I need in the babel master branch that's not published yet. There's really no way for me to install https://github.com/babel/babel/tree/master/packages/babel-core as a dependency?,2017-10-01T02:02:00Z,10137
669,npm/npm,8815842,334365735,How to install a Yarn workspace subpackage from a GitHub repository? Usually we were using npm link for that. In NPM 5 some kind of local `file:` dependency style was added. What are best practices now after the whole world collapsed after migration from NPM 4 to NPM 5 and FB promoting their Yarn?..,2017-10-05T05:49:27Z,4148259
670,npm/npm,8815842,334367617,"@othiym23 are you really sure that
```
""scripts"": {
  ""postinstall"": ""mkdir BotBuilder; cd BotBuilder; git init; git remote add -f origin https://github.com/Microsoft/BotBuilder.git; git config core.sparseCheckout true; echo \""Node/core\"" >> .git/info/sparse-checkout; git pull --depth=1 origin master; cd ..; npm i ./BotBuilder/Node/core/""
}
```
is a way to go?..
It's 2017.",2017-10-05T06:03:54Z,4148259
671,npm/npm,8815842,334503635,"@avesus 
Thanks for the clever workaround. It's even less maintainable for me, because I'll have to repeat it for like 8 different modules.",2017-10-05T15:35:23Z,10137
672,npm/npm,8815842,339120992,"I am in the same boat as @errorx666 … there is a bug fix for the package `react-router-redux` which is tucked inside the master branch of `react-router` as a lerna module, and there's no real way to get at it with npm. :( ",2017-10-24T20:29:01Z,627290
673,npm/npm,8815842,339123178,"As for my own repositories, I have broken them apart into proper repositories. And I don't wish to hear the word monorepo again. As for managing multiple repositories with the same linting rules, testing frameworks, workflow tools, etc... There are better ways than a monorepo. Use `npm link`. Package up your common tools and install them as devDependencies of the various packages. Anything but a monorepo. Good luck.",2017-10-24T20:36:49Z,635591
674,npm/npm,8815842,339124478,"@mightyiam not to get too far off topic, but why the intense antipathy to monorepos?",2017-10-24T20:41:35Z,128019
675,npm/npm,8815842,339137347,"> @mightyiam not to get too far off topic, but why the intense antipathy to monorepos?

Simplicity, I swear. One project is idempotent to one repo and viceversa, and if something is common to several other parts, modularize it and convert it an independent, reusable and standalone library. You know, KISS principle, UNIX principle and all this short of things...

By the one, I'm against monorepos, too.",2017-10-24T21:27:57Z,532414
676,npm/npm,8815842,339139324,"@piranna maybe you mean ""isomorphic"", not ""idempotent""? This is a pretty hand-wavy explanation and I'm not sure I buy it. If you want to talk about the KISS principle, from the perspective of versioning a monorepo is the simplest option, and it allows features that require simultaneous changes to several packages to be encapsulated in a single PR.",2017-10-24T21:34:16Z,128019
677,npm/npm,8815842,339157266,"@pelotom simultaneous changes to several packages are bad practice. Consider changing APIs in the same way developers implement database migrations: change first package without breaking old clients, and then publish changes to the dependents. It complicates thinking about code changes and leads to more commits and slower work, but KISS in modules worth it a lot",2017-10-24T22:37:15Z,4148259
678,npm/npm,8815842,339163079,"@avesus 

> simultaneous changes to several packages are bad practice

[citation needed]

> It complicates thinking about code changes and leads to more commits and slower work

Agreed. So where are the concrete benefits that make this price worth paying? Be specific. ",2017-10-24T23:07:33Z,128019
679,npm/npm,8815842,339163410,"> simultaneous changes to several packages are bad practice. 

@avesus  I think companies like Facebook and Google, as well as projects like Babel and istanbul, and many others would disagree with you. If this were the case lerna would not exist. I personally have also found it helpful in certain applications within my team. 

NPM only cares about package.json. When you install a package it pulls it from a centralized repository of hundreds of thousands of packages. I don't think it's that much of a stretch, or unreasonable, to install a package from a single Github repo containing a dozen packages.",2017-10-24T23:08:50Z,486224
680,npm/npm,8815842,339240048,"> @piranna maybe you mean ""isomorphic"", not ""idempotent""?

No, I wanted to mean idempotent. One project equals one repo, and one repo equals one project.",2017-10-25T07:28:08Z,532414
681,npm/npm,8815842,339240461,That's not what idempotent means.,2017-10-25T07:29:38Z,585279
682,npm/npm,8815842,339240812,"![youkeepusingthatword](https://user-images.githubusercontent.com/128019/31986028-cd6cde92-b91b-11e7-8219-ad6d4885116e.gif)
",2017-10-25T07:31:18Z,128019
683,npm/npm,8815842,339241195,Perhaps this monorepo debate could be held somewhere else?,2017-10-25T07:32:59Z,664076
684,npm/npm,8815842,339246666,It looks like npm's inability to support monorepos has made people hate monorepos than hate npm.,2017-10-25T07:56:10Z,5094296
685,npm/npm,8815842,339255138,"I feel much like @piranna.

Also, I'm an avid [Greenkeeper](https://greenkeeper.io/) user and you can forget about using it with monorepos.",2017-10-25T08:30:23Z,635591
686,npm/npm,8815842,339347827,"@piranna 
This debate is basically prescriptivism versus descriptivism.  You're saying that people *shouldn't* be using monorepos.  We're saying that they *are* using monorepos, without regard to whether they *should* be, and therefore we (consumers of npm packages; not necessarily producers) need support for it.",2017-10-25T14:26:10Z,10137
687,npm/npm,8815842,340336309,"So there is no solution ? Are we stuck with making manual installs ?

But isn't this just a change directory to do after the cloning ? Please npm folks !",2017-10-30T03:43:49Z,64956
688,npm/npm,8815842,340344154,"@othiym23 The following are the parts that would need to be modified to make this work, am I right ?

https://github.com/npm/npm-package-arg/blob/master/npa.js#L218
https://github.com/zkat/pacote/blob/latest/lib/fetchers/git.js#L73

If that is the case, I might like to do a PR.",2017-10-30T04:59:13Z,64956
689,npm/npm,8815842,343711152,@othiym23 will you please re-open this issue so we can open bounties for it on bountysource.com?,2017-11-12T03:38:36Z,5356588
690,npm/npm,8815842,351118123,"Hi @zkat, @othiym23

There has been a lot of discussion on this issue and it seems the community has a need for this. A handful of ideas have been proposed from npm users which, IMO, are worth more input from the npm team.

From [using a separator which branch names cannot use](https://github.com/npm/npm/issues/2974#issuecomment-240672593) like `git://github.com/babel/babel.git#master::babel/packages/babel-core/` or [using a query param syntax](https://github.com/clayne11]) like `git://github.com/babel/babel.git#master?path=babel/packages/babel-core/`

It would be nice to get some more input from the npm team on these 😀
",2017-12-12T17:08:27Z,633148
691,npm/npm,8815842,361533511,Please :(,2018-01-30T09:36:54Z,6835266
692,npm/npm,8815842,361732210,"so this issue is now closed after

> There has been a lot of discussion on this issue and it seems the community has a need for this.

...without an actual solution?

seriously?

no wonder npm is considered a cussword in most programming circles.",2018-01-30T20:59:28Z,897864
693,npm/npm,8815842,361744595,"It’s pretty sad.  There seem to be plenty of people who would even write improvements for npm themselves (or we could pay them to do it), but the maintainers will not even respond to us.",2018-01-30T21:44:32Z,5356588
694,npm/npm,8815842,361769220,"i mean now i have to scrub my entire repo and create a separate repo for all the separable packages of my project. if there was even a tiny goddamn HINT in the official ""how to contribute packages"" doc about npm's inability to handle monorepos then i'd have designed my repos accordingly.

it's still going to be a nightmare, a different project for each package in netbeans, and my system so far has about 15 packages... thanks a lot indeed.",2018-01-30T23:24:15Z,897864
695,npm/npm,8815842,361777241,"@subpardaemon @jacksonrayhamilton If I understand the code correctly the main contribution and new tests are to be made on https://github.com/zkat/pacote and https://github.com/npm/npm-package-arg. Then there is not much to do on `npm` itself (maybe add a test case).

This would require to make a PR on both sides: `pacote` does the fetching of the package (in our case, from a git repository) given a *manifest* that is built by `npm-package-arg` from the parsing of a git url.

The difficulty is to convince both parties that this is a feature to have. Apart from @othiym23, we could also lobby @zkat (creator of `pacote`) and @iarna (that also contributed both the files I mentioned in https://github.com/npm/npm/issues/2974#issuecomment-340344154). Again, I'd be glad to contribute that.",2018-01-31T00:03:35Z,64956
696,npm/npm,8815842,364158052,It would be nice if NPM would look at this as a potential win for their project vs a nuisance. ,2018-02-08T16:01:28Z,3149916
697,npm/npm,8815842,364276212,How would npm know about versions of individual packages inside a monorepo? Installing `@latest` might not always be what you want.,2018-02-08T22:51:22Z,406439
698,npm/npm,8815842,364278410,"> How would npm know about versions of individual packages inside a monorepo? Installing @latest might not always be what you want.

Since this is about installing individual monorepo packages within a Git repo, you would target a branch tag or commit.

90% of the time you want to install the individual packages off npm, which is what we do now. 10% of the time there is a bug fix for one of those packages and they haven't done a release yet. I'm experiencing this right now with [babel-plugin-istanbul](https://github.com/istanbuljs/babel-plugin-istanbul) and the arrow function coverage bug w/ Babel 7.

For a non-monorepo package you can simply point directly to the Github repo. This case is similar, but you want to scope it just to a single package within the repo.

For those that make monorepos they don't necessarily need this feature. It's for those that use projects that use monorepos. Telling them to not organize their projects into monorepos doesn't help people who make use of these projects.",2018-02-08T23:00:37Z,486224
699,npm/npm,8815842,364434638,"@othiym23 mentioned back in Jan 2016

> monorepo support is not something that's on the CLI team's roadmap for the next 6-12 months. I'm interested in seeing better support for it within the CLI, but the team isn't going to have the time and attention to do it ourselves. For that reason, specific proposals (perhaps in the form of (prototype) code) are going to be more useful than generic requests for improved monorepo support.

Its been about 25 months now. [People are interested in contributing PRs to get this working](https://github.com/npm/npm/issues/2974#issuecomment-361777241). Some additional guidance from the CLI team would be helpful.

If this issue seems to be making no traction with npm's CLI team, I'd encourage people interested in this feature to look at yarn instead.

There are a few issues on yarns repo about this feature:
* https://github.com/yarnpkg/yarn/issues/4725
* https://github.com/yarnpkg/yarn/issues/1570

Yarn also has an [RFC process](https://github.com/yarnpkg/rfcs/) which may offer a better discussion platform compared to this GitHub issue.",2018-02-09T13:33:35Z,633148
700,npm/npm,8815842,368007849,"This is why i hate JS, UI and all things related to JS community.
First time in my life i need install Vuikit from other branch and from monorepo /packages folder and i see that in 2013 people asked this feature and developers fucking people 5 years.
Haha. Fucking JS. Python forever.",2018-02-23T13:24:01Z,12496447
701,npm/npm,8815842,379122451,"Haha, entertaining. :) 

Still though, this would be useful.",2018-04-06T01:22:53Z,297678
702,PyCQA/pycodestyle,963088392,963088392,"I realise that #703 already got locked as ""too heated"".  But I must ask that this issue be revised carefully and cooly to come to some sort of a solution.  In its current form E722 is more dangerous than it is helpful.

I would like to put it on record that the decision to leave that as-is has tripped me, my colleagues, and many others into writing some serious bugs into our code.  Initially I felt pretty embarrassed for not knowing about this for so long, but the more I've talked to both colleagues and friends about this the more I've realised that developers are often not aware of the bug it introduces:

```pycharm
# The user hit's control-C during do_something() and it left x in a really bad state
# This was supposed to be caught by cleanup()
# So now the user has seriously corrupted data
try:
    do_something(x)
except Exception:  # Really this should have been just except:
    cleanup(x)
    raise
```

PEP 8 does not ban the use of bare `except:`.  That's plainly explicit in PEP 8.  It calls out two specific and common use cases where it is permissable.  I say ""common"" because logging is not an edge case requirement.  And I also say ""common"" because resource cleanup is not an edge case requirement and cannot always be done in a `finally:` block.

-----

Most novices and even many intermediate developers I've spoken to will be to swap `except:` for `except Exception:` where their code was already re-raising the exception with a bare `raise`.  They do this because they think PyCQA's E722 (via Pycharm or similar) has told them to.  When they make this change the warning goes away.  But what have they done? 

- has they fixed a problem? **No** not really.  If their code was capturing the exception and suppressing it (not re-raising) then `except Exception:` is almost always too broad.  Their code is just as likely to misbehave on a badly formatted date or a function call with too few arguments.  It's only fixed one small specific problem of the user hitting ctrl-c.  If their code was already re-raising then there was no bug to start with and this has fixed nothing at all.
  
-  Has it introduced a new bug? **Yes** If they really did intend to catch everything then nudging them away from `except:` onto `except Exception:` simply will not do what they intend.  But worse it will look like it does what they intend long enough to get into production many times over.

The potential bugs PEP 8 / E722 are concerned with are all to do with _suppressing_ exceptions incorrectly.  If PyCQA is incapable of analysing suppression (incapable of seeing the future `raise) this then it's incapable of applying this rule well enough to avoid very many dangerous false positives.

----

In conclusion, I can find nearly no instances where E722 has practically nudged myself or colleagues into writing better code with fewer bugs.  I can find many of instances where it has nudged myself or colleagues into writing faulty code.

I now believe this ""bare except is always bad"" interpretation of PEP 8 is much more harmful than it is ever helpful.",2021-08-06T22:49:24Z,23001043
703,PyCQA/pycodestyle,963088392,894555753,dupe #703,2021-08-06T22:54:26Z,1810591
704,PyCQA/pycodestyle,271165698,271165698,"How I read PEP-8, it doesn't outlaw bare excepts, but merely recommends to catch more specific exceptions when possible. But what if there is no particular exception you want to catch, but when you just want to do some cleanup before propagating any exception?

```python
try:
    self.connection.send(...)
except:
    # We only close the connection on failure, otherwise we keep reusing it.
    self.connection.close()
    raise
```

Using `try...finally` isn't an option here, since we want to reuse the resource on success, and only clean up on failure. I could just explicitly catch `BaseException` instead, but there is no indication in PEP-8 that this is preferable (otherwise why would bare except be supported in the first place).

So how about suppressing E722 if there is a `raise` statement in the `except` block?",2017-11-04T04:28:42Z,1525
705,PyCQA/pycodestyle,271165698,342688144,"Using bare except for resource closing is a common behavior, especially when I implement context manager. The check of `do not use bare except` should be changed to `do not use bare except without raise`.",2017-11-08T02:15:59Z,6367792
706,PyCQA/pycodestyle,271165698,342756658,The error code E722 was implemented in #592 / (#579) and I warned about it but sigmavirus24 wasn't interested and told that I did not read the whole thread.... ,2017-11-08T09:15:08Z,1100188
707,PyCQA/pycodestyle,271165698,342760209,"To be clear, pycodestyle doesn't do look-ahead's so we cannot silence this *if* there is a `raise` in the following block. That's just not how pycodestyle has ever worked and it would require a significant rewrite to enable that kind of behaviour.

----

Quoting the PEP

```
When catching exceptions, mention specific exceptions whenever possible instead of using a bare except: clause.

For example, use:

try:
    import platform_specific_module
except ImportError:
    platform_specific_module = None

A bare except: clause will catch SystemExit and KeyboardInterrupt exceptions, making it harder to interrupt a program with Control-C, and can disguise other problems. If you want to catch all exceptions that signal program errors, use except Exception: (bare except is equivalent to except BaseException:).

A good rule of thumb is to limit use of bare 'except' clauses to two cases:

  1.  If the exception handler will be printing out or logging the traceback; at least the user will be aware that an error has occurred.
  2.  If the code needs to do some cleanup work, but then lets the exception propagate upwards with raise. try...finally can be a better way to handle this case.
```

As a *general* rule, this new check is good. There are specific cases where people will need to do general clean-up work, as described above. Since we cannot check for people re-raising the exception in the except block, it truly is up to the user to determine whether:

1. This check is useful to them at all or whether they feel it should be ignored globally
2. Their particular use of a bare `except` is necessary and it should be ignored in that particular case (e.g., with `# noqa` or `# noqa: E722`).

I would hazard a guess that 90% of pycodestyle's users will find this check worthwhile, useful, and helpful. We can not ever satisfy 100% of our users so I am happy to settle for 90%.",2017-11-08T09:28:58Z,240830
708,PyCQA/pycodestyle,271165698,342886428,"I have two (real world) scenarios:

1. Legacy code, initially written by novices, with inappropriate use of bare except all over the place. E722 is helpful there (somewhat since this usually isn't the only issue in such legacy code, and you end up ignoring most errors there anyway).
2. Code written by me (and similarly experienced Python developers), where bare except is used occasionally, but never without re-raising the exception. For reference, E722 is reported inappropriately three times in [this project](https://github.com/snoack/mypass) of mine, where the exception is re-raised.

If the design of pycodestyle doesn't allow considering the following block, in order to avoid inappropriate errors, my understanding is, that due to the potential of false positives this check should not be enabled by default, or such a check should rather be implemented in pyflakes which considers the AST and therefore is able to ignore bare excepts that re-raise the exception.

I'm not going to clutter my code with `#noqa` comments. IMO the purpose of a linter is to help you writing cleaner code, not to require additional boilerplate for legit practices. This isn't any better than using `except BaseException`.",2017-11-08T17:10:07Z,1525
709,PyCQA/pycodestyle,271165698,343180953,"> I'm not going to clutter my code with #noqa comments. 

I'm not sure if you're intentionally ignoring my suggestion to include it in the `ignore` list for your projects or if you just want to try to argue. Either way, I'm not here to argue with you. Pycodestyle is a tool used by novices and experts alike. Novices will learn from this and so will some experts. Since this is a style tool, there will always be places where some people disagree with the checks and will disable them. That's *normal*. Just because a handful of folks object to a rule doesn't mean we will put it in the `DEFAULT_IGNORE` list even if generally speaking it has value for everyone else.

As for the suggestion that pyflakes entertain a *style* check, you can make that argument to them, but that is typically entirely against their philosophy.",2017-11-09T15:01:30Z,240830
710,PyCQA/pycodestyle,271165698,354739800,"I just hit this as well. In my experience false positives can have a damaging effect. I've seen novices commit naive ""fixes"" for correct code. I believe this issue will cause novices to rewrite what I wrote

```python
try:
    ...
except:
    destfile_tmp.unlink()
    raise
```

to

```python
try:
    ...
except Exception:
    destfile_tmp.unlink()
    raise
```

which will of course leave temporary files around in the case of KeyboardInterrupt/SystemExit.

This check needs to see the raise, or be better worded, to steer people away from naive fixes.",2018-01-02T10:41:01Z,617224
711,PyCQA/pycodestyle,271165698,354802176,"That's kind of a bad example though because you should be using `finally` to clean up resources. It could lead to novices getting a ""wrong"" idea, I agree, but they're already doing it wrong in the first place.",2018-01-02T16:10:14Z,931051
712,PyCQA/pycodestyle,271165698,354813471,"No, as in @snoack's example, this is not an unconditional clean-up. In the happy path the file gets moved into a permanent location.",2018-01-02T16:53:57Z,617224
713,PyCQA/pycodestyle,271165698,369870282,"![image](https://user-images.githubusercontent.com/13496612/36892494-2c2e0a5c-1e05-11e8-94c0-8daecf633955.png)
autopep8 said use `BaseException`.",2018-03-02T09:34:12Z,13496612
714,PyCQA/pycodestyle,271165698,370050577,"I'm with @sigmavirus24 on this one, but I'd take it a little further:  a bare except is inherently unpythonic.  Two important parts of python philosophy (`import this`) are 'Explicit is better than implicit' and 'Special cases aren't special enough to break the rules.'. I point these out because a bare except is a special, highly implicit case. Every other `except` has an 'argument', indicating the scope of the error to be handles.  Even if that's 'everything', it's better to explicitly state that.

'There should be one-- and preferably only one --obvious way to do it.' also applies, since you can accomplish the same thing for the low-low price of 14 characters.  That's simply not enough of a 'savings' to make it worth it.

There's also the fact that things like KeyboardInterrupt aren't caught by `Exception` for a reason.  Those represent *very* exceptional cases - cases which shouldn't occur in normal (e.g. production) operation unless something has gone very wrong or the user is explicitly requesting an immediate shutdown.

In regards to @lordmauve's case: Why do you want to clean up those temporary files in every possible exception case?  `except:` tells me that you don't know what exceptions may be raised.  In those (hopefully rare) cases, your app will certainly crash (unless you have a bare except that silences every possible exception, but that's a whole new level of bad design), and you will probably need to investigate why.  A temporary file is part of the state of the application at the point in time, and therefore will be forensically valuable.

If you know for sure that the temporary file will be forensically worthless, then `except Exception:` or `except BaseException:` makes that clear, and indicates that you're fully aware of the implications.  If you're more worried about old temporary files accumulating in some way, consider having your app do cleanup as part of initialization.  That idea comes from the [Crash-only software](https://lwn.net/Articles/191059/) pattern (It's a lot more reasonable than it sounds, I promise).  Basically, it's a lot easier to assume that your app always terminates unexpectedly and code against that than to handle unexpected termination as a special case (i.e. Turning off the power is guaranteed to be an option, but a shut down command may fail or be unavailable).

As a last resort, there's always `  # noqa`.  If you absolutely must break from best practices, then 8 characters is a very small price to pay.

edit: I r gud riter 😜 ",2018-03-02T20:58:52Z,569403
715,PyCQA/pycodestyle,271165698,370150368,"@hoylemd I appreciate the support.

> Those represent very exceptional cases - cases which shouldn't occur in normal (e.g. production) operation unless something has gone very wrong or the user is explicitly requesting an immediate shutdown.

This sounds like you're assuming all development is on continuously running applications on a remote server. Things like pycodestyle and flake8 are ""production"" applications that should handle `KeyboardInterrupt`. That said, both projects handle it explicitly. I think I understand your point, but it feels like it's imposing a false dichotomy around what is ""production"".

> Why do you want to clean up those temporary files in every possible exception case?

I can imagine a few cases where @lordmauve would want to (and should!) clean up the temporary files. Perhaps those temporary files contain some sensitive information and cleaning them up is the secure thing to do. In reality, neither of us know the details and I think we should be assuming that they know their constraints better than us. Of course having some non-sensitive temporary files lying around can be useful for debugging, but there are certainly valid cases where those shouldn't be left around no matter what exception happens.",2018-03-03T14:15:22Z,240830
716,PyCQA/pycodestyle,271165698,370182512,"@sigmavirus24 

I think you're right re: imposing a false dichotomy - I'm primarily a web developer, where the dichotomy is pretty reliable.

In regards to temporary files containing sensititive information, Wouldn't it be a risk to write them to disk at all?  If the power is shut off at the right moment, the data would still be there on disk, and no exception handling would have a chance to clean it up.  So that makes me think that a temporary file is the wrong tool in the first place.  I'd want to keep that in something volatile (like memory) so that it's sort of fail-safe.

You're more general point is a good one. - We can't know the real requirements on a project we're not a part of.  But if those requirements do require a cleanup in all exception cases, I think we probably agree that it should be done explicitly.",2018-03-03T21:48:40Z,569403
717,PyCQA/pycodestyle,271165698,370185344,">  I'd want to keep that in something volatile (like memory) so that it's sort of fail-safe.

Again, there are constraints where it makes sense. I agree with you in general, but there are always exceptions to the rule.

Also we're getting off topic :smile: ",2018-03-03T22:34:05Z,240830
718,PyCQA/pycodestyle,271165698,370217530,"`except BaseException` doesn't mean the same thing as `except:` in Python 2; in Python 2 you could raise old-style classes as exceptions, which do not inherit from BaseException.

Regarding ""I could never imagine a situation where I'd want to unconditionally clean up in the case of any exception"":

- In the case of temporary files, never leave them around on a production server because they can cause additional impact (eg. filling a disk volume) which can be much more serious than the original crash. Reproduce the issue in dev and diagnose.
- Just came across another example: in the case that my event loop crashes due to programming error, I must kill my subprocesses which are now in an unknown state ([code](https://github.com/lordmauve/chopsticks/blob/master/chopsticks/tunnel.py#L130)).

",2018-03-04T10:17:02Z,617224
719,PyCQA/pycodestyle,271165698,370500899,"@lordmauve 

I'll admit that there are some cases where `except:` is the right call, but they're cases that pycodestyle isn't intended to handle out of the box.  Marking those with `# noqa` might seem a little cluttery, but it's much more helpful to those same junior devs who would be confused by the error.  It explicitly indicates a code smell that is a necessary evil.  Ideally, you'd document why it is necessary as well so that they understand why the exception had to be made. 

So to go back to your initial comment, Pycodestyle isn't designed in a way that it can detect the raise, so that's a non-starter.  As for steering novices away from naive changes, that's the purpose of code review, not linting. A better solution is to prevent them from even knowing about it in the first place though, hence `# noqa` and documentation.",2018-03-05T17:40:18Z,569403
720,PyCQA/pycodestyle,271165698,370515167,"> Pycodestyle isn't designed in a way that it can detect the raise, so that's a non-starter

Or evidence that this check needs to be moved to Pyflakes, which does consider the AST, in order to eliminate this false positive.

> As for steering novices away from naive changes, that's the purpose of code review, not linting. 

In a large organisation that is impossible. Junior people are often two or three steps removed from people who could confidently tell you what good practice looks like. We rely on linters to do this, and both the false positive and false negative rates are important. I'm not saying I'm not worried about the false negative. I'm arguing that this creates an undesirable false positive.

> A better solution is to prevent [novices] from even knowing about it in the first place though [by putting #noqa on that line]

You're assuming I'm writing code now for future novices to read. I'm concerned about the novices now maintaining the code I wrote 5 years ago when this check wasn't a thing.",2018-03-05T18:26:08Z,617224
721,PyCQA/pycodestyle,271165698,370551544,"> Or evidence that this check needs to be moved to Pyflakes, which does consider the AST, in order to eliminate this false positive.

PyFlakes wouldn't accept this for the very reason that it's so controversial. They only accept obvious problems, e.g., unused imports. If this isn't satisfactory here, a different stylistic option for having it would be via a Flake8 AST plugin. Luckily, `flake8-bugbear` already provides this check using the AST but a quick check of the source indicates that it's as strict as this check is. I'd suggest either collaborating with [bugbear](/pycqa/flake8-bugbear) or creating a new plugin that meets the specific needs.

As it stands, this discussion seems to be getting heated. I'm going to lock the thread because it's devolved from constructive conversation to far less productive conversation.",2018-03-05T20:21:08Z,240830
722,tpope/vim-fugitive,549848681,549848681,"In Windows gVim (not `win32unix`), `:Gwrite` doesn't work on files that are in symlinked directories (but `:Gstatus` works). It says that the file is _outside repository_.

In `win32unix` and `nvim` it works instead (it changes the buffer name to the real path).

For gVim (`win32`), I could fix it by changing https://github.com/tpope/vim-fugitive/blob/6bc345f6f16aee0dcc361f2f3bf80e4964f461af/autoload/fugitive.vim#L792 with
```vim
    return FugitiveVimPath(empty(url) ? a:url : has('win32') ? resolve(url) : url)
```

",2020-01-14T22:12:33Z,26169924
723,tpope/vim-fugitive,549848681,574482902,"Where is the symlink, where is it pointing, and where is the repository?",2020-01-15T03:46:50Z,378
724,tpope/vim-fugitive,549848681,574494489,"![Imgur](https://i.imgur.com/2DkGNXk.jpg)

Real path: 

    D:\Gianmaria\Dropbox (Personale)\Linux\vimrc\plugin\buffer_history.vim

and repository in

    D:\Gianmaria\Dropbox (Personale)\Linux\vimrc\plugin",2020-01-15T04:48:51Z,26169924
725,tpope/vim-fugitive,549848681,574509430,You said the symlink was a directory. I ask where the symlink is and you give me a file. I also asked where it's pointing and you did not answer. I can't help you if you won't answer my questions.,2020-01-15T06:00:37Z,378
726,tpope/vim-fugitive,549848681,574898411,"Sorry, it was late and I was sleepy. The symlinked directory is the same as the repo, I have:

```
~\vimfiles                                   (not symlinked, real path)
~\vimfiles\plugin                            (symlink to D:\Gianmaria\Dropbox (Personale)\Linux\vimrc\plugin, that is also a repo)
                                             (repo is also readable here -> ~\vimfiles\plugin\.git)
~\vimfiles\plugin\buffer_history.vim         (:Gwrite here gives problems)

:Gwrite in the real file path (D:\Gianmaria\Dropbox (Personale)\Linux\vimrc\plugin\buffer_history.vim) works.
```

I understand that fighting with Windows paths is very annoying, just saying that fugitive isn't perfect in gVim on Windows, I had other issues as well, I didn't want to bother and I can also understand if you don't consider Windows a top priority. I just had an issue and I reported it, feel free to ingore it if you don't think it's worth wasting time on it.",2020-01-15T23:04:58Z,26169924
727,tpope/vim-fugitive,549848681,574930020,"On initialization, Fugitive checks if the current file is a symlink, and if so, calls `cd .`. On UNIX, and presumably `win32unix` since it's working there for you, this forces the current buffer to have its name resolved.

https://github.com/tpope/vim-fugitive/blob/08601f221a336d13a606b33714e5a1e8457b07f7/plugin/fugitive.vim#L212-L215

If you can find a way to force the buffer name to resolve on win32, we can do that as well. I would start with `exe 'cd' fnameescape(getcwd())`.

Throwing `resolve()` elsewhere is a game of whack-a-mole, plus it adds IO to things that are supposed to be fast. It's not a tenable solution.",2020-01-16T01:01:06Z,378
728,tpope/vim-fugitive,549848681,574931687,"Ok I'll try to do that, thanks.",2020-01-16T01:05:56Z,26169924
729,tpope/vim-fugitive,549848681,574955957,"What I could figure out is that in Windows something work differently with git.
I did some testing and I think that the problem lies in Git for Windows, that works differently, in that it doesn't realizes that the two files belong to the same repository. The command gives _the same error_ also if run from the terminal (cmd.exe).

I updated git, just in case, but the error persists. I think only a `resolve()` call in the `:Gwrite` command for `win32` can fix this.

These are the `cmd` generated in https://github.com/tpope/vim-fugitive/blob/c83355d5c52002f94d08267f1d14ca6d1a2763e9/autoload/fugitive.vim#L429
  
```
WLS Debian (neovim), also here $HOME/.vim/plugin is a symlink to the Dropbox directory,
 :Gwrite works and the file path is changed to its resolved path

['-C', '/mnt/d/Gianmaria/Dropbox (Personale)/Linux/vimrc/plugin',
 '--literal-pathspecs', 'add', '--',
 '/home/gianmaria/.vim/plugin/buffer_history.vim']
```  

```
gVim Windows, also here the starting paths are different, but the command doesn't work

['-C', 'D:/Gianmaria/Dropbox (Personale)/Linux/vimrc/plugin',
 '--literal-pathspecs', 'add', '--',
 'C:/Users/Gianmaria/vimfiles/plugin/buffer_history.vim']
```

I also tested the same commands in the terminal (WSL and cmd.exe)

```
In WSL terminal, this works

$ git -C /mnt/d/Gianmaria/Dropbox\ \(Personale\)/Linux/vimrc/plugin --literal-pathspecs add -- /home/gianmaria/.vim/plugin/buffer_history.vim

In cmd.exe, this doesn't work (same error as with fugitive):

C:\Users\Gianmaria\vimfiles\plugin>git -C ""d:\Gianmaria\Dropbox (Personale)\Linux\vimrc\plugin"" --literal-pathspecs add -- c:\Users\Gianmaria\vimfiles\plugin\buffer_history.vim
```

",2020-01-16T02:45:41Z,26169924
730,tpope/vim-fugitive,549848681,574963987,"This is news to me, but I don't believe it changes anything. Fugitive only passes an absolute path if it can't figure out the appropriate relative path, because older Git versions didn't support absolute paths at all. The core problem remains: Fugitive expects your buffer name to not be a symlink.",2020-01-16T03:22:30Z,378
731,tpope/vim-fugitive,549848681,574964757,"This would fix it:
```
  let file = s:winshell() ? resolve(file) : file
```
just before

https://github.com/tpope/vim-fugitive/blob/c83355d5c52002f94d08267f1d14ca6d1a2763e9/autoload/fugitive.vim#L4605-L4610
",2020-01-16T03:26:01Z,26169924
732,tpope/vim-fugitive,549848681,574965099,"Since the problem is with Git for Windows (imho), I can't think of anything else.

I made a mapping to resolve current file, and it works with it:

    nnoremap \f\ :exe 'file' resolve(@%)<cr>",2020-01-16T03:27:40Z,26169924
733,tpope/vim-fugitive,549848681,574978313,"The problem is not with Git for Windows. You pointed out the real problem in your initial post (emphasis my own).

> In `win32unix` and `nvim` it works instead (**it changes the buffer name to the real path**).

All of Fugitive is built around the assumption that the buffer name is the the real path. At first I ignored this, but then bugs cropped up when people called `:cd` and the buffer name changed from a symlink path to a real path mid flight, so my solution was to force a `:cd .` early, thus preventing symlink paths from ever entering the equation. What you've told me is that this `:cd .` doesn't work. Two solutions spring to mind:

1. Find a way to make it work (where ""make it work"" is ""resolve symlink in the buffer name"").
2. If there is no way to make it work, then the original bug is irrelevant. Stop calling `resolve()` on the Git dir and work tree and everything should be back to matched up.

The following solutions do not spring to mind:

1. Change the semantics of one public function by adding `resolve()` to it, and verify that the one command that is confirmed broken is fixed.

Please stop doubling down on the bad idea that I told you is bad, and try the thing I asked you to try:

> I would start with exe 'cd' fnameescape(getcwd()).

---

Ironically, after I started writing this you edited in a decent idea.

> I made a mapping to resolve current file, and it works with it:
> 
> ```
> nnoremap \f\ :exe 'file' resolve(@%)<cr>
> ```

See also https://github.com/tpope/vim-fugitive/pull/814#issuecomment-446767081 where I attempt to provide a more robust way to do this automatically. This is the sort of thing I would consider doing, but getting to work right requires calling `edit` to reload the file, and I worry that editing the file from `BufReadPost` will cause non-obvious bugs. I would encourage you to try it, at least until the issue is fixed.",2020-01-16T04:35:51Z,378
734,tpope/vim-fugitive,549848681,574985485,"I think I'll use that `Resolve` command of your post and chain it with my `Gwrite` mapping
```
nnoremap \gw :Resolve<cr>:Gwrite<cr>
```
and it should be ok, thanks.

About the issue not being with Git for Windows, I doubt it, because in Linux git commands autoresolve links, in Windows they do not. Because this works in Linux
```
$ git -C /mnt/d/Gianmaria/Dropbox\ \(Personale\)/Linux/vimrc/plugin --literal-pathspecs add -- /home/gianmaria/.vim/plugin/buffer_history.vim
```
and in Windows it does not. But it doesn't matter if I can get it working in one way or another.
",2020-01-16T05:07:30Z,26169924
735,tpope/vim-fugitive,549848681,574992926,"Holy fuck dude, I spent literally an hour explaining the problem in *immense* detail and gave you one simple task to help me come up with a solution, and your response it to repeat the same exact red herring I debunked. I wrote the code, I know how it works. You can't even follow simple directions, I don't know why you think you're qualified to diagnose the problem.

If anyone else has this problem, please open a new issue, I'd love to get it solved.",2020-01-16T05:42:07Z,378
736,restsharp/RestSharp,1100051808,1100051808,"Version 107 got several interfaces removed, which is one of the major breaking changes in that version.

It seems most of the interfaces were okay to remove, except `IRestClient`. My arguments are [described](https://restsharp.dev/v107/#motivation) in the docs.

This issue is a place where we should have a civilised discussion about it. I will not follow up on the interface issue anywhere else, except here.

Some details about the current state of the `RestClient` API signature.
- All the sync methods were removed. Those who need it can make extensions, wrapping up async calls in some [async helper](https://github.com/rebus-org/Rebus/blob/master/Rebus/Bus/Advanced/AsyncHelpers.cs).
- Most of the overloads for making requests are now extensions, as they just call each other, so they would never be part of any interface anyway.
- Most of the options that were previously on `IRestClient` and `IRestRequest` are now in `RestClientOptions`, which is a property bag and, therefore, won't have any interface

So, what's there in stock for `IRestClient`? Basically, it boils down to this signature:

```csharp
namespace RestSharp; 

public interface IRestClient {
    RestClient AddDefaultParameter(Parameter parameter);

    Task<RestResponse> ExecuteAsync(RestRequest request, CancellationToken cancellationToken = default);
}

```

The question now is if it makes sense to introduce an interface with those two functions? Please comment.",2022-01-12T08:52:16Z,2821205
737,restsharp/RestSharp,1100051808,1012383729,"I would think it would, in order to maintain SOLID code.  Also, as several other people pointed out previously, we are not always immune to managers etc. making us code to interfaces, do gratuitous TDD and automated-unit-test writing etc/mocking, and/or use IoC containers.  Bosses like to make software engineers do things ""because I heard about it at a conference,"" or ""the insurance is making us,"" etc.

The interface just supporting the two methods is, in principle, okay -- just make sure that the static extension methods refer to the object through the interface and not the actual class.",2022-01-13T18:09:14Z,8261875
738,restsharp/RestSharp,1100051808,1012444622,"Brian, I understand the argument about managers, although in my 30 years of engineering I never encountered a single manager who was telling me how to write code... I mean, if you write your code against an interface it's one thing. But how some managers can tell a third-party library how to write _their_ code? `HttpClient` doesn't have any interfaces implemented and it seems to be fine, no?

Automated testing and mocking HTTP requests are totally possible with RestSharp in its current shape. I have shown an option in the docs.

I see no issues with using DI containers as well, as `HttpClient` does with its nice extensions. In fact, I plan to add some DI support to make it easier and push people to use RestSharp properly (typed clients, etc).

The SOLID argument is not solid :) What do you mean by that, could you elaborate?

Then again, I am seriously not against adding that small interface. However, I would not be happy to add `IRestRequest` and `IRestResponse` back as those are glorified property bags.",2022-01-13T19:33:13Z,2821205
739,restsharp/RestSharp,1100051808,1015063559,"A typical method in a wrapper service for my HTTP calls does the following
1. Take some inputs to form a request
2. Execute
3. Handle response (maybe retry, return data)

If I can mock `IRestClient`, then I can verify the contents of the `RestRequest` given to it by my method. That lets me avoid splitting 1 into a separate method just for testing that I correctly crafted the web request (which I want because string formatting is tricky).

If I can mock `IRestClient`, then I can setup responses for 200, 404, 401, etc and don't need to split out 3 either.

Each method is already short enough that I _don't want_ to split it up just for test coverage. Example:

```csharp
public async Task<List<Something>> GetData(DateTimeOffset startEpoch, DateTimeOffset endEpoch)
{
    var request = new RestRequest(""/url"", Method.GET);
    await AddBearerToken(request);// other method which might make a request to authenticate or use cached token
    request.AddQueryParameter(""start"", startEpoch.ToUnixTimeSeconds().ToString());
    request.AddQueryParameter(""end"", endEpoch.ToUnixTimeSeconds().ToString());

    var result = await _client.ExecuteAsync<ApiResponseModel>(request);
    if (result.StatusCode != HttpStatusCode.OK)
        throw new Exception($""Failed to fetch data. Status code: \""{(int)result.StatusCode}\"", message: \""{result.StatusDescription}\"""");

    return result.Data.ThingICareAbout;
}
```",2022-01-18T04:33:57Z,1383382
740,restsharp/RestSharp,1100051808,1015173037,"@fjmorel what of that is not possible to do with [MockHttp](https://github.com/richardszalay/mockhttp)? It gives you even better options to ensure the request comes in the correct format and has the correct content type, authorisation headers, etc.",2022-01-18T08:24:12Z,2821205
741,restsharp/RestSharp,1100051808,1015292847,"
> I see no issues with using DI containers as well, as `HttpClient` does with its nice extensions. In fact, I plan to add some DI support to make it easier and push people to use RestSharp properly (typed clients, etc).

Can you tell a little bit more about your plans on DI support? Perhaps a word on DI in the docs could clarify many questions before the've to be asked.",2022-01-18T10:51:37Z,5670510
742,restsharp/RestSharp,1100051808,1015313118,"The DI registration heavily depends if you want to follow the same pattern as Microsoft does, using `IHttpClientFactory`, registering the client as a transient dependency.

For many use cases it's enough to register a typed client as a singleton:

```csharp
services.AddSingleton<IMyApiClient>(new MyApiClient(new RestClient(options)));
```

If I'd to provide some use of `IHttpClientFactory` registration extensions, I'd need to know if people want to configure the `HttpClient` instance used by `RestClient` on the outside. In this case, lots of options that RestSharp uses to configure the message handler won't be applicable.",2022-01-18T11:16:58Z,2821205
743,restsharp/RestSharp,1100051808,1015671496,"> @fjmorel what of that is not possible to do with [MockHttp](https://github.com/richardszalay/mockhttp)? It gives you even better options to ensure the request comes in the correct format and has the correct content type, authorisation headers, etc.

Thanks! I didn't know about that library. I've been using [Moq.Contrib.HttpClient](https://github.com/maxkagamine/Moq.Contrib.HttpClient) in places we used HttpClient rather than RestSharp.

This library using HttpClient is an implementation detail that I don't think library users should have to think about, unless they have specific requirements making the need for a custom HttpClient necessary. Having the interface makes testing easier for all users without having to add extra dependencies.",2022-01-18T17:59:32Z,1383382
744,restsharp/RestSharp,1100051808,1015729443,"The library you mentioned would work as well. As long as you can add a delegating handler, it will be possible to test RestSharp calls. All you need to do is to compose or override the message handler used by the wrapped HttpClient:

```csharp
var client = new RestClient(...) { ConfigureMessageHandler = _ => mockedHandler };
```

Basically, one of the benefits of moving to HttpClient (besides that it's obvious and `HttpWebRequest` is heavy legacy) is the ability to use delegating handlers and build message processing pipelines.

The point about testing real requests is obvious to me. If you look at StackOverflow questions about RestSharp, most of the issues are caused by incorrectly added parameters, content type overrides and other weird things. But, if people would test against a RestShar interface, they would have all their tests passing. But it won't work anyway, because the request is just not correctly formed. It's a great benefit to at least inspect the actual request and ensure that it looks exactly as it should, rather than inspecting the parameters collection on a `RestRequest` instance. The latter gives you zero idea how the actual request would look like.",2022-01-18T18:59:44Z,2821205
745,restsharp/RestSharp,1100051808,1021443552,"This is not necessary if the existing RestClient methods are marked virtual.  This will allow for mocking overrides of the public methods instead of needing a full interface. 

One of the two is necessary as the handler system is not able to simulate invalid http responses.  We switched from Http client to RestSharp because RestSharp had an interface that allowed us to simulate the result of those responses as well as other error conditions not simulatable through the mock handler.   Some of these result in StatusCode 0 or exceptions being thrown which require an interface or virtual method. ",2022-01-25T17:39:10Z,70030176
746,restsharp/RestSharp,1100051808,1021542279,@sspates-starbucks why can't you simulate the error response with a testing handler like `MockHttp`? It provides a behaviour that is much closer to reality. It can return anything you want based on its configuration.,2022-01-25T19:39:19Z,2821205
747,restsharp/RestSharp,1100051808,1021543020,@maor-rosenfeld thanks for your extensive and elaborative feedback for this issue.,2022-01-25T19:40:22Z,2821205
748,restsharp/RestSharp,1100051808,1021563475,"If we keep discussing testing, maybe it's a good idea to post some test samples and check how these tests can be refactored. So far, the discussion goes quite theoretical imo.",2022-01-25T20:07:20Z,2821205
749,restsharp/RestSharp,1100051808,1024188857,"@alexeyzimarev we can argue all day about use cases of using the interface for tests, extension methods, tests on these extension methods and other reasons of why suddenly removing all interfaces from an existing infrastructure package is a bad idea, but at this point it's clear that you're here to tell us how to write code. 

I'm sorry if I choose not to align with your views, but at this point upgrading to the latest version of RestSharp is so painful that there's really no justification to even try. 

On a positive note, I'm completely with you regarding the separation of services and settings objects. I like `RestClientOptions`.",2022-01-28T12:56:15Z,49363375
750,restsharp/RestSharp,1100051808,1024358874,@maor-rosenfeld you definitely know how to motivate OSS maintainers to keep doing their community work for free.,2022-01-28T16:02:11Z,2821205
751,restsharp/RestSharp,1100051808,1025970935,"I've come here from a breaking scenario that I'm not sure I can fix other than backing out the upgrade to 107 due to missing IRestClient.

I updated one of my libraries to use 107. Why not right? I'm trying to be proactive with keeping libs up to date. It all seemed to work OK until some tests starting failing. This is because we use a third-party library and they reference IRestClient and v106 of the library. I could ask them to update but I don't see that they would prioritise that and it would then force all of their customers to update to v107 as well which might be a big deal. If I use an assembly redirect to 107, it won't work because it won't find IRestClient and if I use 106, my code won't work because the method signatures changed.

I'm not sure what I would have done differently but perhaps something as simple a a different nuget package with different namespaces and mark the old one as deprecated. Then we can run them side-by-side until everyone is on 107?

Not sure. Any suggestions?",2022-01-31T16:32:47Z,581018
752,restsharp/RestSharp,1100051808,1026613361,"@lukos I don't think it would work in any scenario, interfaces or not. RestSharp v107 internals are completely different due to migration to `HttpClient` and ditching `HttpWebRequest`. It made it impossible to keep the API backwards compatible, and even with interfaces, the API scope of those interfaces won't ever be compatible. Lots of properties of `RestClient` and `RestRequest` are moved to `RestClientOptions` and there's no way to keep them where those properties were before.",2022-02-01T09:03:09Z,2821205
753,restsharp/RestSharp,1100051808,1027159406,"We are running into cases now where one library requires the interface and another requires the v107 structure, we can't upgrade either one due to the lack of backwards compatibility with IRestClient.   Adding a compatibility wrapper that implements the interfaces might be a way to handle this better rather than implementing the interface on the current client. 
",2022-02-01T18:32:07Z,70030176
754,restsharp/RestSharp,1100051808,1027240464,"@sspates-starbucks Even if there's something to make things build-time compatible, these won't be compatible at runtime anyway. As I mentioned, most of the properties of those (`IRestClient` and `IRestRequest`) interfaces were (by necessity) moved to `RestClientOptions`, and it's something that simply cannot be undone due to the configuration of `HttpMessageHandler`.

I can only suggest opening issues for those libraries that still use RestSharp 106 and helping them with the migration. It's not a lot of work honestly.",2022-02-01T20:08:22Z,2821205
755,restsharp/RestSharp,1100051808,1030191734,"The new class having internal only options generates problems for dependency injection. I set up my DI container to hand out RestClient, but different consumers need different options. There is now now way to set the options after having an instantiated Client. ",2022-02-04T17:20:35Z,11640679
756,restsharp/RestSharp,1100051808,1030572715,"@Terebi42 that's why I advocate wrapping `RestClient` in a particular API client, so you can wire it like this:

```csharp
services.AddSingleton<ITwitterClient>(new TwitterClient(new RestClient(twitterOptions)));
```

But I plan to address it better, just haven't found a good way to do it yet. Either I will make some extensions for `IHttpClientFactory` or make a new factory with named registrations for options.",2022-02-05T08:01:28Z,2821205
757,restsharp/RestSharp,1100051808,1033915256,"> @Terebi42 that's why I advocate wrapping `RestClient` in a particular API client, so you can wire it like this:
> 
> ```cs
> services.AddSingleton<ITwitterClient>(new TwitterClient(new RestClient(twitterOptions)));
> ```
> 
> But I plan to address it better, just haven't found a good way to do it yet. Either I will make some extensions for `IHttpClientFactory` or make a new factory with named registrations for options.

Is there a reason why (some) options can't be set after the constructor? Its a HUGE code change to say go make a new interface and options class for every location that is going to use restsharp, vs just setting the timeout and baseurl after the DI injection is done",2022-02-09T15:57:28Z,11640679
758,restsharp/RestSharp,1100051808,1042736756,"Every man and his dog has an opinion on software design/standards and the debates will rage on forever - they're usually just different mindsets/approaches.  Those mindsets aren't even usually the mindset of the developer using your library, they're probably dictated by the organisation they work for and having a conversation to change those standards is either way above their pay grade or a battle that's not worth fighting.  

The problem here isn't one of who is right or wrong, it's one of practicality.  The removal of the interfaces causes the amount of work some consumers have to do to significantly increase.  Most devs really just need to deliver the work they had promised within the timeframe promised.   I'm a huge fan of MockHttp but in my case, updating over 1000 tests to use it (which were written 5 years ago) is a huge amount of work.  No matter how you plan your pipeline, the sizing of the work item is now completely invalid.  The amount of work required to update your library has become inhibitive to doing so - it will either have to be picked up as a separate work item or it just won't ever get updated.

Irrespective of whether consumers are testing the right thing, using IoC correctly or just plain writing crap code, they no longer have the flexibility to choose like they used to.  I think most devs would agree that the code changes make sense but at the end of the day we all need to deliver functionality to a business and I guarantee that those business users really don't care how well the code is written as long as it reliably does what they need. 

The best development tools out there make our lives easier.  That's why we use them - they abstract us away from the detail/hard work we really don't need to care about and they provide flexibility to customise how we utilise them.  They help us do more, faster.  Removing these interfaces does the opposite of that, irrespective of whether it's ""better"" code or not.

",2022-02-17T09:23:10Z,4030796
759,restsharp/RestSharp,1100051808,1042824033,"What everyone is saying is correct. Of course, we appreciate Alexey for making this library but the changes are very breaking.

It seems like the best way out of it would be to create a separate library with a different name/package name (and importantly, different namespaces) to contain your newer/better way of doing things and which should be used by people going forwards. Any serious bugs found in the old library will probably be fixed but otherwise it won't get any new features.

This way, we can have both packages alongside each other so if we can update our own code for the new library we can do that without forcing all of our dependencies to update all of their code at the same time. If they eventually do, then we can eventually uninstal the old package.

Not sure if you think that sounds fair or not?",2022-02-17T10:58:17Z,581018
760,restsharp/RestSharp,1100051808,1042863329,"The question is why is there such a strong need to migrate to v107 if v106 works fine for most? Migration to `HttpClient` is a big change in itself, the `IRestRequest` (for example) is impossible to shape to the same form as it was before anyway, as well as `IRestClient` interface API surface cannot be recovered.",2022-02-17T11:43:30Z,2821205
761,restsharp/RestSharp,1100051808,1043060686,"Why update?  People want the [stated benefits](https://restsharp.dev/v107/#presumably-solved-issues) that this update (_and future updates_) will bring.  

As a consumer, I shouldn't have to care what implementations are used internally as long as it works.   This is a debate about the impact of the removal of interfaces and the consequences that has for those who use the library.  (The fact that I'm even aware of what components are used internally screams leaky abstraction to me but that's just my opinion and somewhat off topic)

Changing the method signatures is something that's not overly complex to deal with (the compiler will help identify what needs changing and, in our case, this library is abstracted away from our code anyway) however, removing all the interfaces entirely fundamentally changes how to set up and work with the library and how easy/difficult that is given what's gone before. 

The work you've done is great - we're just providing feedback of the real world issues it causing us.  Rightly or wrongly, people consume this library is a myriad of ways and for us, the migration path is so large that it's currently a barrier to entry.  ",2022-02-17T15:18:27Z,4030796
762,restsharp/RestSharp,1100051808,1043079276,"I don't know why, but my message is not getting through. I will try again:
- The API surface of `IRestClient` and `IRestRequest` is impossible to hold intact after the migration
- Removal of interfaces made the maintenance work much easier.",2022-02-17T15:35:23Z,2821205
763,restsharp/RestSharp,1100051808,1043138448,"*** EDITED ***

I really don't want to get bogged down in specifics as I don't think it's helpful

> The question now is if it makes sense to introduce an interface with those two functions?

For us, yes it does.   

There are probably many others who will find that interface useful too for many different reasons.  

Adding them in provides flexibility to choose and reduces the impact of the changes for many people especially given that this library has been around for such a long time. This isn't about what code/approach is better, it's about what's practical and helpful for existing consumers. ",2022-02-17T16:14:19Z,4030796
764,restsharp/RestSharp,1100051808,1043304511,"Thanks very helpful @alexsaare, thank you.

The scope of refactoring, however, would be significant, both for me (all the extensions need to change) and for those who use the previous version of the library.

I am not against it, that's why I opened this issue. I still, though, wait for some sample test code, so I can understand the need better.",2022-02-17T18:56:47Z,2821205
765,restsharp/RestSharp,1100051808,1044107470,"As I mentioned before, I am trying to understand how people use interfaces in tests. Tests aren't PI, nor IP. Please, share your tests.",2022-02-18T08:06:44Z,2821205
766,restsharp/RestSharp,1100051808,1044881153,"To answer some questions @alexeyzimarev posed above : 

1) Why upgrade if we don't need to? Because corporate policy requires us to be on recent versions
2) Though I personally agree with some of your motivations and reasoning regarding testing and interfaces, corporate policy also often doesn't care if a test provides value or not

But my biggest problem is the change to DI because of options. I have an app that connects to 10 different services. Currently I have them all getting an IRestClient via DI, set the options they need in their consumer's constructors, and move on.

You are asking me to create 10 new interfaces, move all that rest configuration logic into the DI configuration, change the signature of every consumer of the rest client etc.  Further, if two services can currently share configuration they can share the injected interface. But if one of them needs a different bit of configuration in the future, now I have to create a new interface for that DI again, and change the signatures again, just to have a different timeout or base url.  


I can inject a vanilla RestClient instead of IRestClient just fine. But not being able to set options on it after I get it, is a deal breaker. ",2022-02-18T17:28:41Z,11640679
767,restsharp/RestSharp,1100051808,1046055925,"@Terebi42 the DI concern should be fixed differently. I will provide a way to do it at some point, similar to how you'd do it with `HttpClient`. The issue with registering a single dependency as `IRestClient` as it won't work anyway if the client options need to be configured. As many options moved from the request to the client itself, it won't work.",2022-02-19T16:35:03Z,2821205
768,restsharp/RestSharp,1100051808,1046056153,"@Terebi42 as per the latest version requirement, I don't think it is even feasible, although I am not aware of your particular case. Say, you have .NET 6 released in November, does the policy mean you have to upgrade all your applications to .NET 6, with all the breaking changes fixed? I never had such experience in any company.",2022-02-19T16:36:25Z,2821205
769,restsharp/RestSharp,1100051808,1067126513,"> @Terebi42 the DI concern should be fixed differently. I will provide a way to do it at some point, similar to how you'd do it with `HttpClient`. The issue with registering a single dependency as `IRestClient` as it won't work anyway if the client options need to be configured. As many options moved from the request to the client itself, it won't work.

Why were the options moved from the request to the client? It was very convenient to be able to set those parameters differently per request. And if they do need to be on the client, why are they now read only? Being able to set them at the time of call would also be very convenient. 

A change like this is literally asking for dozens of new classes to be created to support the new config paradigm and DI.  
If we could even pass in a different options class per request, that would be great.  ",2022-03-14T18:02:54Z,11640679
770,restsharp/RestSharp,1100051808,1067152936,"Because those options are configuring `HttpMessageHandler`, which is wrapped inside `RestClient`. Changing those options would require creating a new `HttpMessageHandler` instance, and it will make `RestClient` not thread-safe.",2022-03-14T18:30:39Z,2821205
771,restsharp/RestSharp,1100051808,1067192425,"> Because those options are configuring `HttpMessageHandler`, which is wrapped inside `RestClient`. Changing those options would require creating a new `HttpMessageHandler` instance, and it will make `RestClient` not thread-safe.

hrm, well, at least I understand.  Perhaps Ill make a restclient factory that I DI, and pass it options to get the old pattern back
",2022-03-14T19:15:08Z,11640679
772,restsharp/RestSharp,1100051808,1067722580,@Terebi42 I opened an issue for that https://github.com/restsharp/RestSharp/issues/1791,2022-03-15T08:57:33Z,2821205
773,restsharp/RestSharp,1100051808,1068070749,"> * All the sync methods were removed. Those who need it can make extensions, wrapping up async calls with `.GetAwaiter().GetResult()`

Can you update this ticket to remove this bit? .GetAwaiter().GetResult() is not the correct solution and will lead to deadlocks. You need to use an async helper like the one in Rebus for this to work correctly in sync code:

https://github.com/rebus-org/Rebus/blob/master/Rebus/Bus/Advanced/AsyncHelpers.cs",2022-03-15T14:45:34Z,535825
774,restsharp/RestSharp,1100051808,1068077334,"BTW, if you did bring back IRestClient, the only function that matters is all variations of ExecuteAsync(). There are good valid reasons to want to mock ExecuteAsync() to avoid an actual call as its high level enough to mock away a significant amount of lower level stuff. But it's also low level enough that I usually don't bother ;). Usually we mock at the level above that, which is our REST API client contract. After all the point of mocking is to be able to stub out lower level stuff as a black box and assume that black box works correctly. That assumption works as then you expect something else to validate that the actual client itself is properly tested (integration test etc).

And there is a good argument to be had that writing tests to ensure the client itself is actually working correctly is much better being done by mocking HttpClient as suggested. Something that was almost impossible to do with earlier versions of RestSharp, but is now much easier to do now it relies on HttpClient internally so you can swap it out for mocking purposes.",2022-03-15T14:51:27Z,535825
775,restsharp/RestSharp,1100051808,1068080413,"> the only function that matters is all variations of ExecuteAsync().

The point here is that there's only one :) All other overloads are extensions. So, if the interface is back, all the extensions need to be on `this IRestClient` instead.

> .GetAwaiter().GetResult() is not the correct solution and will lead to deadlocks

I found this via SO: https://github.com/aspnet/AspNetIdentity/blob/main/src/Microsoft.AspNet.Identity.Core/AsyncHelper.cs",2022-03-15T14:54:17Z,2821205
776,restsharp/RestSharp,1100051808,1068125687,"I believe that works because it uses a separate task, so it's a similar pattern achieved in a different way to how Rebus did it. I am not sure which one is more efficient. We use Rebus in our code and I stole their code to solve the deadlock issues when we had it, and it worked. I have also solved in the past simply by running the code in a separate background thread similar to the AspNet approach and that works also. But I think it's higher overhead than the approach taken by Rebus and other libraries?

I am sure either approach works, but I know that just using GetAwaiter().GetResult() on an async function will cause deadlocks in web apps (or Windows Forms apps). We had tons of hung web requests when I was doing it without the task wrapper. ",2022-03-15T15:30:56Z,535825
777,restsharp/RestSharp,1100051808,1068132434,Updated,2022-03-15T15:37:03Z,2821205
778,restsharp/RestSharp,1100051808,1068132875,"Been a while since I looked and and debugged the Rebus version, but I did step through it with a debugger to understand it back then. I am pretty sure it's much lower overhead as it never triggers a separate background thread, so puts no extra pressure on the thread pool. Rather it swaps out the SynchronizationContext so that when the task returns, it comes back on something other than the original sync context which is where you get the deadlocks. ",2022-03-15T15:37:29Z,535825
779,restsharp/RestSharp,1100051808,1068145814,"I don't think there's an explicit new thread when using the task factory. I think it will use the available IO thread from the pool. Synchronization context, on the other hand, produces some overhead. I believe that doing something like `.ConfigureAwait(false).GetAwaiter().GetResult()` would work, but not in WinForms (for example). I also know that ASP.NET Core behaves differently compared with .NET Framework when it comes to `GetAwaiter().GetResult()`",2022-03-15T15:48:19Z,2821205
780,restsharp/RestSharp,1100051808,1068206934,"Yes, its possible it would work differently in ASP.NET Core. Porting our code to that is a massive undertaking which is still a work in progress :( But we also use RestSharp in WinForms apps (EasyPost and ShipEngine) so the AsyncHelper approach is what has worked for us.

I guess the only way to tell would be to benchmark the two approaches and see which is faster.",2022-03-15T16:41:32Z,535825
781,restsharp/RestSharp,1100051808,1100677637,"This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
",2022-04-16T14:37:41Z,26384082
782,restsharp/RestSharp,1100051808,1100677761,⚠️ This issue has been marked wontfix and will be closed in 3 days,2022-04-16T14:38:00Z,39074581
783,restsharp/RestSharp,1100051808,1108912424,"From the explanation page;

> The best way to test HTTP calls is to make some, using the actual service you call. However, you might still want to check if your API client forms requests in a certain way. 

I have to disagree with this. I don't think calling actual services from unit tests is good practice. At that point they're functional tests. 

next line goes into that;

> As RestSharp uses HttpClient internally, it certainly uses HttpMessageHandler. Features like delegating handlers allow you to intersect the request pipeline, inspect the request, and substitute the response. You can do it yourself, or use a library like [MockHttp](https://github.com/richardszalay/mockhttp)

this requires me to actually know how you're making your calls inside your library that you publish to nuget. The main reason people wants to use external libraries is to not be concerned about the internals of that external code. It also assumes I know to use this MockHttp library (had no idea it existed, I didnt need to since I've used RestSharp to handle all ""HttpClient"" related things so far), with the assumption that, that package will also be maintained properly and will not be making breaking changes.

> Mocking an infrastructure component like RestSharp (or HttpClient) is not the best idea.

This statement is also wrong in my opinion. I think the opposite actually, making us test external library code in our _unit_ tests implicitly makes them not unit tests, because we are always testing more than a small ""unit"" of code, in addition to our code change, now we're testing your code.

> As RestSharp uses HttpClient internally

What happens when you decide to not use HttpClient, but use ""MyGrandmasHttpClient"". Do we then have to refactor all of our code bases with the ""MockMyGrandmasHttpClient"" package?

I am fine with removing every other interface on this package. Except for that single one.

This wouldn't be an issue if we were able to mock classes like Java allows, however removing this 1 single interface with 1 single method causes design issues, as well as wasted hours for millions of developers. 

I would also be skeptical about the suggested ""if you don't like 107, just remain on 106"" By it's nature, RestSharp deals with connectivity, that increases the chances of 0 day vulnerabilities or other security issues. We won't be able remain on 106 forever, and would be forced off of this package. Which would be a shame because we love RestSharp.

I really appreciate your work on maintaining this package, there is a reason it's one of the most popular packages on .NET. I hope you reconsider this decision.",2022-04-25T18:40:40Z,682527
784,restsharp/RestSharp,1100051808,1114185627,"> What happens when you decide to not use HttpClient, but use ""MyGrandmasHttpClient"". Do we then have to refactor all of our code bases with the ""MockMyGrandmasHttpClient"" package?

That's an exaggeration. RestSharp uses elements of .NET under the hood, and for many years it was `WebRequest`. Then, Microsoft decided to scrap it and re-implemented it using `HttpClient`, causing massive problems. Essentially, RestSharp uses the only way available in core .NET to make HTTP calls, and it's `HttpClient` until Microsoft decides to do something else. All other .NET libs for making HTTP calls like Refit or Flur also use `HttpClient`, and most people use `HttpClient` directly. So, when _that_ changes, all the code for .NET will need to change, which is very unlikely.

> This statement is also wrong in my opinion.

The issue here is that by creating an interface mock you will assert if the client is formed according to your idea of how it should be formed. However, it doesn't guarantee that the actual HTTP request will be valid. That's why `HttpClient` has no interface and cannot be mocked. And that's why people normally use a test version of `HttpMessageHandler` instead, as `HttpClient` itself is just forming a message, but the actual _oevr the wire_ implementation is in the handler. See this [SO question](https://stackoverflow.com/questions/36425008/mocking-httpclient-in-unit-tests).

> I have to disagree with this. I don't think calling actual services from unit tests is good practice.

You don't have to call the actual service, you can call a simulation. That's what many RestSharp tests do for making sure that the server can understand the request in the way it should.

Again, I am not against bringing the interface back, but it won't be the original interface. Essentially, there are just a handful of core functions of the client itself (`ExecuteAsync` is the main one), all the others are just extensions.

I actually think that the interface would help for building something like a retryable client with Polly, as it could be done in a wrapper using composition.",2022-05-01T09:41:38Z,2821205
785,restsharp/RestSharp,1100051808,1119342684,"Hello dear community,
I'm using RestSharp since some times (7 years more or less), I like it as it do the job.
For a new project, I used it, by reflex, and surprise ! No more interface 😨
I will not discuss with project owner about, it's a good choice or not, I will just write quickly my opinion.
I will give up RestSharp usage, and stop recommand it to team members.
I'm using library to save time, I'm using library as a tool to make my work easier.
Now I'll have to invest time to introduce ""tricks"" for my unit test ? But why ? 
I choose a tool because it fit to my need, RestSharp doesn't fit anymore.
I'm sure this breaking change is relevant, but not for me.
Most of dev will update lib, keep it in 106 (this proposal is a shame 😅), update their code.
For my part, I choose another tool to save time.",2022-05-06T07:39:41Z,22884539
786,restsharp/RestSharp,1100051808,1119515349,"@RedVinchenzo thank you for your feedback, but I am having a hard time understanding how your comment contributed to the discussion. Keeping the API intact just so people don't need to change their code, hm. I don't remember a single library I used that never changed its API.

Now I see what is going to happen. With .NET you have no choice but to use HttpClient one way or another. Flurl, Refit, or whatever wrapper you use, will either use their own `HttpMessageHandler`, or force you to use the same MockHttp. I am quite puzzled hearing ""I will just use HttpClient because RestSharp has no interfaces"", I wish someone explains that.",2022-05-06T11:24:46Z,2821205
787,restsharp/RestSharp,1100051808,1127816897,"Hi @alexeyzimarev 

If you use your library in your projects then you are familiarized with how can you use it and tested it. But I think many of us use the library in the next way.

For DI just define in Startup.cs
`services.AddTransient<RestSharp.IRestClient, RestSharp.RestClient>();`

Then just need to define this in the constructor and thats it, you can use RestSharp in the class
`public ValidationDataAccess(IRestClient restClient)`

For the unit test I use Moq, so I only neet to define the result of method Execute like this
`var moq = new Mock<IRestClient>();

moq.Setup(m => m.Execute<T>(It.IsAny<RestRequest>()))
    .Returns(new RestResponse<T>()
    {
        ErrorMessage = string.Empty,
        Data = data
    });`

And pass to the constructor like this
`var validationDA = new ValidationDataAccess(moq);`


If I need that RestClient response with an error then I define the result of Execute with an error to test that escenario.

So, for me the problems is the next

- We neet to refactory not just new projects but projects in production only for use the new version
- You say ""just don't update the version"". @Terebi42 say one reason what we neet to update
- You say ""in your unit test you can call a simulation"", but again it's needed time to refactory the projects
- You say ""I don't remember a single library I used that never changed its API"", yes you have right but always that cause troubles and discomfort in developers
- You say this in using of mock ""it doesn't guarantee that the actual HTTP request will be valid"", maybe but that´s why we test local and QA",2022-05-16T15:28:17Z,52088576
788,restsharp/RestSharp,1100051808,1163938377,"This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
",2022-06-23T05:12:08Z,26384082
789,restsharp/RestSharp,1100051808,1163938496,⚠️ This issue has been marked wontfix and will be closed in 3 days,2022-06-23T05:12:18Z,39074581
790,restsharp/RestSharp,1100051808,1198591952,"I am starting a new project making use of RestSharp, so more focused on the recommended way to mock / unit test today. It appears the example in the docs might be incorrect.

```csharp
var client = new RestClient(...) { ConfigureMessageHandler = _ => mockHttp };
```
versus
```csharp
var client = new RestClient(new RestClientOptions { ConfigureMessageHandler = _ => mockHttp });
```

Also the single quotes in the json value isn't valid; MockHttp example has this issue as well but there's no actual deserialization in that example.

From my perspective, I like being able to verify through the request using MockHttp, but would also like `IRestClient` for easier mocking at that level. Having both options is nice, and it doesn't seem onerous for the lib to support it. It sounds like the latest version is a new library from compatibility standpoint, so makes sense that `IRestClient` in this version would be a new interface.
",2022-07-28T20:19:09Z,47162374
791,restsharp/RestSharp,1100051808,1264423791,"This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
",2022-10-01T17:06:29Z,26384082
792,restsharp/RestSharp,1100051808,1264424930,⚠️ This issue has been marked wontfix and will be closed in 3 days,2022-10-01T17:08:19Z,39074581
793,restsharp/RestSharp,1100051808,1273865446,Conclusion: stick with RestSharp 106 and evaluate other options. I cannot maintain confidence in a dependency whose maintainer is so cavalier about breaking published API without even a nod to proper semantic versioning.,2022-10-10T22:16:15Z,227024
794,restsharp/RestSharp,1100051808,1273873772,"> All the deprecated interfaces had only one implementation in RestSharp, so those interfaces were abstracting nothing. It is now unclear what was the purpose for adding those interfaces initially.

It was to allow you to commit to a published API and a decoupled implementation, and to allow users of RestSharp to *easily* write unit tests that respond with canned data without hitting the filesystem or network interface.

Perhaps you have lost sight of the fact that some of us are writing software that calls servers that, while having a published API, are run by a third party.
",2022-10-10T22:29:03Z,227024
795,restsharp/RestSharp,1100051808,1273905628,"Here's the point, when a package exposes an interface such as `IRestClient`, it is a commitment to support that interface as stable going forward. 

By publishing that interface, you must expect people to develop code against same interface going forward, especially when writing mocks and stubs so that their unit tests do not hit the filesystem nor the network. 

To advise devs to test against the live endpoints shows a complete and utter failure to understand the purpose of unit testing, which is to test a given code unit against a purely local mock.
",2022-10-10T23:22:38Z,227024
796,restsharp/RestSharp,1100051808,1275001801,"@richardbuckle sorry, who are you again? good bye.",2022-10-11T16:59:06Z,2821205
797,restsharp/RestSharp,1100051808,1312425546,"This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
",2022-11-12T09:36:50Z,26384082
798,restsharp/RestSharp,1100051808,1312425591,⚠️ This issue has been marked wontfix and will be closed in 3 days,2022-11-12T09:37:06Z,39074581
799,restsharp/RestSharp,1100051808,1319744958,"I reviewed #1952 and have a proposal:

- Rename `ExecuteInternal` to `ExecuteRequestAsync` and make it public
- Rename `InternalResponse` to `HttpResponse` and make it public
- Convert `ExecuteAsync` and `DownloadStreamAsync` to extension methods; both are calling `ExecuteRequestAsync`
- Convert `Execute` and `DownloadStream` sync methods to extension methods
- Add the `IRestClient` interface with only _one_ method: `ExecuteRequestAsync`
- Change all extensions of `RestClient` to `IRestClient`. There will be issues with client members that should be included in the interface, so add those too. `Options` is the property I expect to be there.

This will allow composition. For example, I can add a Polly-based wrapped client to retry particular status codes. Btw, I found that the number of retries in the response `Request` property might not work as intended.

How does that sound?",2022-11-18T09:23:15Z,2821205
800,restsharp/RestSharp,1100051808,1319749381,"Hmm, it doesn't really work well as `Options` become public again... One solution is to make it a record again, with init-only properties, and risk incompatibilities with old SDKs. Not sure what's more important.",2022-11-18T09:27:38Z,2821205
801,restsharp/RestSharp,1100051808,1320401151,"Seems reasonable to me, but why would the options need to be part of the IRestClient interface anyway? The way I set it up in my branch where options is private again, I have some properties to get back important stuff like the baseurl etc. Those could be part of the IRestClient interface if needed. But I don't think exposing options is necessary at all in order for someone to mock up IRestClient is it?",2022-11-18T18:44:56Z,535825
802,restsharp/RestSharp,1100051808,1320452997,"Because lots of extensions need those options. So, it's either to extend the API surface of the interface, which makes little sense or expose a shared options container via the interface.",2022-11-18T19:41:57Z,2821205
803,restsharp/RestSharp,1100051808,1320839687,I guess I took the wrong approach. The internal execute method should remain internal or private. Both `ExecuteAsync` and `DownloadStreamAsync` should be in the interface as it's the basic public API.,2022-11-19T09:02:34Z,2821205
804,restsharp/RestSharp,1100051808,1320899404,"I appreciate some of the feedback on this issue (not all of it). 

However, I see a lot of negativity and unwillingness to understand that:
- the library needs to move on, and breaking changes are inevitable
- keeping the previous API service was hard to maintain, and I am doing most of the work here (thanks to all the contributors, too) for free, which many participants of this discussion, apparently, fail to appreciate
- the number of aggressive and counterproductive comments here is unacceptable

Therefore:
- the interface will be brought back in v109, but its purpose is not to make mocking easier
- I will open another issue for the new interface
- any attack on the maintainers will not be tolerated. those who do it will be blocked for violation of CoC and contribution guidelines
",2022-11-19T14:40:23Z,2821205
805,openzfs/zfs,639068210,639068210,"as I just posted an issue as a person of colour asking to have my thoughts considered, it was closed to discussion and locked as **too heated**.

this is a pretty blatant indicator that you didn't care about race relations at all.

please stop locking all the comment sections for these IMPORTANT ISSUES you supposedly want to further and support.",2020-06-15T18:57:36Z,59658056
806,openzfs/zfs,639068210,644318055,because black comments matter..,2020-06-15T18:57:50Z,59658056
807,openzfs/zfs,639068210,644322665,"As I indicated in https://github.com/openzfs/zfs/issues/10458, I'm open to a discussion on this.  However, we have seen that when comments are open to anyone, the discussion quickly becomes dominated by people who are not involved in ZFS development, and too heated to be productive.  (e.g. reddit, ars technica).

I believe that as the OP you should be able to continue to post to locked issues.  Members of the OpenZFS github organization can also continue to post to locked issues.",2020-06-15T19:07:13Z,799124
808,openzfs/zfs,639068210,644491085,"I determined that even the OP can't post on locked issues (unless they are a member of the organization).  I didn't hear from you privately, so I'm going to unlock the other issue and continue the discussion there.",2020-06-16T02:22:45Z,799124
809,openzfs/zfs,639051823,639051823,"
<!--- Please fill out the following template, which will help other contributors review your Pull Request. -->

<!--- Provide a general summary of your changes in the Title above -->

<!---
Documentation on ZFS Buildbot options can be found at
https://openzfs.github.io/openzfs-docs/Developer%20Resources/Buildbot%20Options.html
-->

### Motivation and Context
<!--- Why is this change required? What problem does it solve? -->
<!--- If it fixes an open issue, please link to the issue here. -->
These terms reinforce the incorrect notion that black is bad and white
is good.

### Description
<!--- Describe your changes in detail -->
Replace this language with more specific terms which are also more clear
and don't rely on metaphor.  Specifically:

* When vdevs are specified on the command line, they are the ""selected""
vdevs.

* Entries in /dev/ which should not be considered as possible disks are
""excluded"" devices.

### How Has This Been Tested?
<!--- Please describe in detail how you tested your changes. -->
<!--- Include details of your testing environment, and the tests you ran to -->
<!--- see how your change affects other areas of the code, etc. -->
<!--- If your change is a performance enhancement, please provide benchmarks here. -->
<!--- Please think about using the draft PR feature if appropriate -->
compiles

### Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Performance enhancement (non-breaking change which improves efficiency)
- [x] Code cleanup (non-breaking change which makes code smaller or more readable)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)
- [ ] Documentation (a change to man pages or other documentation)

### Checklist:
<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->
- [x] My code follows the ZFS on Linux [code style requirements](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md#coding-conventions).
- [ ] I have updated the documentation accordingly.
- [x] I have read the [**contributing** document](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md).
- [ ] I have added [tests](https://github.com/zfsonlinux/zfs/tree/master/tests) to cover my changes.
- [x] I have run the ZFS Test Suite with this change applied.
- [x] All commit messages are properly formatted and contain [`Signed-off-by`](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md#signed-off-by).
",2020-06-15T18:27:25Z,799124
810,openzfs/zfs,638768339,638768339,"Words such as ""wh\*telist"" or ""b\*acklist"" are racist. We cannot have those absolutely disgusting  derogatory words in our codebase. Therefore, we need to change them to race-neutral words.

There are more references to racial supremacy throughout the code. These should be discussed.

Should this macro be renamed to `skip_blankspace`?
https://github.com/openzfs/zfs/blob/master/module/nvpair/nvpair.c#L46

Also, in the Lua module, we mark objects as ""wh\*te"" or ""b\*ack"". I suggest changing the colors to some non-offensive ones. I propose indigo (`#4B0082`) and forest green (`#228B22`).
https://github.com/openzfs/zfs/blob/master/module/lua/lgc.c

### Motivation and Context
Following other open-source projects, we should strive to keep the ZFS code base clean of any vulgar, racist or other unnecessary remarks and to allow for friendly and welcoming environment for people of all kinds.

### How Has This Been Tested?
It compiles - it's just basic refactoring.

### Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Performance enhancement (non-breaking change which improves efficiency)
- [x] Code cleanup (non-breaking change which makes code smaller or more readable)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)
- [ ] Documentation (a change to man pages or other documentation)

### Checklist:
<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->
- [x] My code follows the ZFS on Linux [code style requirements](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md#coding-conventions).
- [ ] I have updated the documentation accordingly.
- [x] I have read the [**contributing** document](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md).
- [ ] I have added [tests](https://github.com/zfsonlinux/zfs/tree/master/tests) to cover my changes.
- [ ] I have run the ZFS Test Suite with this change applied.
- [x] All commit messages are properly formatted and contain [`Signed-off-by`](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md#signed-off-by).
",2020-06-15T11:17:13Z,7243565
811,openzfs/zfs,638768339,644083444,Indigo children are against use of color indigo to color memory.,2020-06-15T11:55:13Z,80306
812,openzfs/zfs,638768339,644152235,I identify as a blockhead and find this PR extremely offensive.,2020-06-15T13:59:10Z,1094821
813,openzfs/zfs,638768339,644167299,"Who put this garbage into your head?  Should we also ban traffic lights because there are red and yellow lights (sorry, I can't even guess anything for green)?",2020-06-15T14:24:58Z,5219480
814,openzfs/zfs,638768339,644260709,"Neither ""whitelist"" nor ""blacklist"" are racist. Please check the history and use of terms before
jumping to conclusions.
",2020-06-15T17:13:38Z,867533
815,openzfs/zfs,638768339,644342588,When this functionality was added more descriptive and clear names could have been used.  I see no reason not to address that now.  PR 10457 makes a very similar change to this with slightly different terms (blocked -> excluded).,2020-06-15T19:49:46Z,148917
816,openzfs/zfs,638643486,638643486,"against stupid political agitation that has no place in opensource world. Whole world use master/slave terminology in almost all technical sciences and there is no reason to change it. As I am not communist, I am doing regular commit and PR and not rewriting the (git) history by force ...

### Motivation and Context
Removes political agitation from the code. Makes code much more readable and clean. 

### Description
Only clean revert of previous weird commit. No additional changes.

### How Has This Been Tested?
Didn't make any tests.

### Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Performance enhancement (non-breaking change which improves efficiency)
- [x] Code cleanup (non-breaking change which makes code smaller or more readable)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)
- [ x] Documentation (a change to man pages or other documentation)
",2020-06-15T08:19:17Z,2601588
817,openzfs/zfs,638643486,644269964,"I disagree that reverting to the previous language makes the code more readable and clear.  In fact, it does the opposite.  The patch adopts the ""dependency"" terminology used by `dmsetup(8)` in order to be as precise as possible when describing the functionality.",2020-06-15T17:32:33Z,148917
818,openzfs/zfs,636440138,636440138,"
<!--- Please fill out the following template, which will help other contributors review your Pull Request. -->

<!--- Provide a general summary of your changes in the Title above -->

<!---
Documentation on ZFS Buildbot options can be found at
https://openzfs.github.io/openzfs-docs/Developer%20Resources/Buildbot%20Options.html
-->

### Motivation and Context
<!--- Why is this change required? What problem does it solve? -->
<!--- If it fixes an open issue, please link to the issue here. -->
The horrible effects of human slavery continue to impact society.  The
casual use of the term ""slave"" in computer software is an unnecessary
reference to a painful human experience.

### Description
<!--- Describe your changes in detail -->
This commit removes all possible references to the term ""slave"".

Implementation notes:

The zpool.d/slaves script is renamed to dm-deps, which uses the same
terminology as `dmsetup deps`.

References to the `/sys/class/block/$dev/slaves` directory remain.  This
directory name is determined by the Linux kernel.  Although
`dmsetup deps` provides the same information, it unfortunately requires
elevated privileges, whereas the `/sys/...` directory is world-readable.

### How Has This Been Tested?
<!--- Please describe in detail how you tested your changes. -->
<!--- Include details of your testing environment, and the tests you ran to -->
<!--- see how your change affects other areas of the code, etc. -->
<!--- If your change is a performance enhancement, please provide benchmarks here. -->
<!--- Please think about using the draft PR feature if appropriate -->

Manual testing of `dm-deps` script and manpage.
Ran the test suite.

### Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Performance enhancement (non-breaking change which improves efficiency)
- [x] Code cleanup (non-breaking change which makes code smaller or more readable)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)
- [ ] Documentation (a change to man pages or other documentation)

### Checklist:
<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->
- [x] My code follows the ZFS on Linux [code style requirements](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md#coding-conventions).
- [x] I have updated the documentation accordingly.
- [x] I have read the [**contributing** document](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md).
- [ ] I have added [tests](https://github.com/zfsonlinux/zfs/tree/master/tests) to cover my changes.
- [x] I have run the ZFS Test Suite with this change applied.
- [x] All commit messages are properly formatted and contain [`Signed-off-by`](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md#signed-off-by).
",2020-06-10T17:55:21Z,799124
819,openzfs/zfs,636440138,642408745,"There is also:
`  CC [M]  /var/lib/buildbot/slaves/zfs/Amazon_2_x86_64__BUILD_/build/zfs/module/icp/algs/aes/aes_impl.o
`

But not very visible.",2020-06-11T05:04:06Z,637168
820,openzfs/zfs,397574283,397574283,"### System information
<!--  add version after ""|"" character -->
Type | Version/Name
 --- | --- 
Distribution Name	| Fedora
Distribution Version	| 30 (rawhide)
Linux Kernel	| 5.0.0-0.rc1.git0.1.fc30.x86_64
Architecture	| x86-64
ZFS Version	| master
SPL Version	| master
<!-- 
Commands to find ZFS/SPL versions:
modinfo zfs | grep -iw version
modinfo spl | grep -iw version 
-->

### Describe the problem you're observing
The 4.2 kernel refractored the FPU code: https://lwn.net/Articles/643235/  It seems the latest kernels removed the old compatibility headers.

We need to rename our #includes `<asm/i387.h> -> <asm/fpu/api.h>` and `<asm/xcr.h> -> <asm/fpu/internal.h>` on newer kernels.

### Describe how to reproduce the problem
`make` on rawhide with ZFS master

### Include any warning/errors/backtraces from the system logs
```
/home/hutter/current_kernel_time64/include/linux/simd_x86.h:98:10: fatal error: asm/i387.h: No such file or directory
 #include <asm/i387.h>
          ^~~~~~~~~~~~
compilation terminated.
```",2019-01-09T21:42:37Z,11469457
821,openzfs/zfs,397574283,452902510,"The underlying issue comes from kernel commit 12209993e98c5fa1855c467f22a24e3d5b8be205 (""x86/fpu: Don't export __kernel_fpu_{begin,end}()""), which unexports __kernel_fpu_begin(), and causes the configure tests not to define HAVE_FPU_API_H.  Changing the header locations will just lead to another failure later because kernel_fpu_begin/end() are exported GPL, and can't be used directly from the zfs module.

See the thread on LKML starting here: https://marc.info/?l=linux-kernel&m=154689892914091

Marc",2019-01-09T23:02:55Z,214290
822,openzfs/zfs,397574283,452918667,@mdionne thanks for bringing this to our attention.  Hopefully they change their minds and `EXPORT_SYMBOL(kernel_fpu_begin)` so we can use that instead.,2019-01-10T00:01:03Z,11469457
823,openzfs/zfs,397574283,453279143,"https://marc.info/?l=linux-kernel&m=154714516832389&w=2

Gotta love the replies. Greg K-H:

> My tolerance for ZFS is pretty non-existant.  Sun explicitly did not
> want their code to work on Linux, so why would we do extra work to get
> their code to work properly?",2019-01-10T22:26:24Z,790061
824,openzfs/zfs,397574283,453348790,It would be nice if Oracle would just fix the license once and for all.,2019-01-11T02:25:20Z,8889
825,openzfs/zfs,397574283,454169579,"I think we need to operate under the assumption that they're not going to export the functions, and just disable the vectorized versions of the checksums if `_kernel_fpu{begin,end}` are not detected.",2019-01-14T21:33:10Z,11469457
826,openzfs/zfs,397574283,496714988,"Looks like the kernel people may have backported their patch to older kernels:


>  For the record, it appears the FPU export was removed in 4.19.38 (released 2019-05-02) via d4ff57d0320bf441ad5a3084b3adbba4da1d79f8 and 4.14.120 (released 2019-05-16) via a725c5201f0807a9f843db525f5f98f6c7a4c25b.

https://github.com/zfsonlinux/zfs/issues/8793#issuecomment-496709454",2019-05-28T22:33:57Z,11469457
827,openzfs/zfs,397487623,397487623,"### System information
<!--  add version after ""|"" character -->
Type | Version/Name
 --- | --- 
Distribution Name	| Fedora
Distribution Version	| 30 (rawhide)
Linux Kernel	| 5.0.0-0.rc1.git0.1.fc30.x86_64
Architecture	| x86-64
ZFS Version	| master
SPL Version	| master
<!-- 
Commands to find ZFS/SPL versions:
modinfo zfs | grep -iw version
modinfo spl | grep -iw version 
-->

### Describe the problem you're observing
`current_kernel_time64()` was removed from recent kernels.   Looks like `ktime_get_coarse_real_ts64()` should be used instead.
https://lkml.org/lkml/2018/7/11/202
https://lkml.org/lkml/2018/12/17/110

### Describe how to reproduce the problem
Build master in rawhide

### Include any warning/errors/backtraces from the system logs
```  CC [M]  /home/hutter/zfs/module/icp/illumos-crypto.o
In file included from /home/hutter/zfs/include/spl/sys/condvar.h:33,
                 from /home/hutter/zfs/include/sys/zfs_context.h:38,
                 from /home/hutter/zfs/include/sys/crypto/common.h:39,
                 from /home/hutter/zfs/module/icp/illumos-crypto.c:35:
/home/hutter/zfs/include/spl/sys/time.h: In function ‘gethrestime’:
/home/hutter/zfs/include/spl/sys/time.h:76:8: error: implicit declaration of function ‘current_kernel_time64’; did you mean ‘core_kernel_text’? [-Werror=implicit-function-declaration]
  *ts = current_kernel_time64();
        ^~~~~~~~~~~~~~~~~~~~~
        core_kernel_text
/home/hutter/zfs/include/spl/sys/time.h:76:6: error: incompatible types when assigning to type ‘inode_timespec_t’ {aka ‘struct timespec64’} from type ‘int’
  *ts = current_kernel_time64();
      ^
/home/hutter/zfs/include/spl/sys/time.h: In function ‘gethrestime_sec’:
/home/hutter/zfs/include/spl/sys/time.h:86:24: error: invalid initializer
  inode_timespec_t ts = current_kernel_time64();
                        ^~~~~~~~~~~~~~~~~~~~~
cc1: all warnings being treated as errors
```",2019-01-09T17:59:22Z,11469457
828,openzfs/zfs,397487623,455895402,Looks related to https://lore.kernel.org/patchwork/patch/562186/,2019-01-20T19:37:55Z,720349
829,openzfs/zfs,397129620,397129620,"<!--- Please fill out the following template, which will help other contributors review your Pull Request. -->

<!--- Provide a general summary of your changes in the Title above -->

<!---
Documentation on ZFS Buildbot options can be found at
https://github.com/zfsonlinux/zfs/wiki/Buildbot-Options
-->

### Motivation and Context
<!--- Why is this change required? What problem does it solve? -->
<!--- If it fixes an open issue, please link to the issue here. -->
Frequently-requested feature to help performance on SSDs and on various other SAN-like storage back-ends which support UNMAP/TRIM.
### Description
<!--- Describe your changes in detail -->
This is a port of Nextenta's TRIM support to the current master branch - post-device removal and post-device initialization.
### How Has This Been Tested?
New ZTS tests.  Also additions to ztest.
<!--- Please describe in detail how you tested your changes. -->
<!--- Include details of your testing environment, and the tests you ran to -->
<!--- see how your change affects other areas of the code, etc. -->
<!--- If your change is a performance enhancement, please provide benchmarks here. -->

### Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [ ] Bug fix (non-breaking change which fixes an issue)
- [x] New feature (non-breaking change which adds functionality)
- [ ] Performance enhancement (non-breaking change which improves efficiency)
- [ ] Code cleanup (non-breaking change which makes code smaller or more readable)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)
- [ ] Documentation (a change to man pages or other documentation)

### Checklist:
<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->
- [x] My code follows the ZFS on Linux [code style requirements](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md#coding-conventions).
- [x] I have updated the documentation accordingly.
- [x] I have read the [**contributing** document](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md).
- [x] I have added [tests](https://github.com/zfsonlinux/zfs/tree/master/tests) to cover my changes.
- [ ] All new and existing tests passed.
- [ ] All commit messages are properly formatted and contain [`Signed-off-by`](https://github.com/zfsonlinux/zfs/blob/master/.github/CONTRIBUTING.md#signed-off-by).
",2019-01-08T22:52:05Z,1432005
830,openzfs/zfs,397129620,452596593,"# [Codecov](https://codecov.io/gh/zfsonlinux/zfs/pull/8255?src=pr&el=h1) Report
> Merging [#8255](https://codecov.io/gh/zfsonlinux/zfs/pull/8255?src=pr&el=desc) into [master](https://codecov.io/gh/zfsonlinux/zfs/commit/0a10863194b0e7c1c64f702f868c10d5dac45ea5?src=pr&el=desc) will **decrease** coverage by `0.04%`.
> The diff coverage is `88.01%`.

[![Impacted file tree graph](https://codecov.io/gh/zfsonlinux/zfs/pull/8255/graphs/tree.svg?width=650&token=NGfxvvG2io&height=150&src=pr)](https://codecov.io/gh/zfsonlinux/zfs/pull/8255?src=pr&el=tree)

```diff
@@            Coverage Diff             @@
##           master    #8255      +/-   ##
==========================================
- Coverage   78.33%   78.29%   -0.05%     
==========================================
  Files         380      382       +2     
  Lines      115719   116678     +959     
==========================================
+ Hits        90653    91350     +697     
- Misses      25066    25328     +262
```

| Flag | Coverage Δ | |
|---|---|---|
| #kernel | `78.93% <86.25%> (+0.04%)` | :arrow_up: |
| #user | `65.8% <87.99%> (-1.21%)` | :arrow_down: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/zfsonlinux/zfs/pull/8255?src=pr&el=continue).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/zfsonlinux/zfs/pull/8255?src=pr&el=footer). Last update [0a10863...3a184b8](https://codecov.io/gh/zfsonlinux/zfs/pull/8255?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).
",2019-01-09T07:19:55Z,22429695
831,openzfs/zfs,397129620,453899635,This has been refreshed with fixes to issues discovered during testing.  There are still some deadlock issues which occur mainly when the `autotrim` property is changed and another pool management is performed (mainly adding and possibly removing vdevs).  The deadlocks are being actively investigated.,2019-01-14T04:25:31Z,1432005
832,openzfs/zfs,397129620,456090719,"Status update: This has just been refreshed with some fixes to the various deadlocks observed while running ztest (which have been left as separate commits).  Reviewers should look at 1eda73773 and the related follow-on commits in particular.  With these changes in place, zloop can now generally survive an overnight run.  I've not yet done any actual testing to make sure this plays well (i.e. actually works as opposed to simply not panicking) with allocation classes or non-trivial cases of device removal (other than those which are performed via ztest, which are plenty).  We (@behlendorf and myself) will begin addressing the review feedback shortly.  Also, some of the new ZTS tests need fixing.

[EDIT]: Note that there were actually 3 (force) pushes in rapid succession because there was an experimental fix attempt left in by accident and also to rebase on current master with recent changes to metaslab.c.  Also, I'll note that as new fixes are added, they're commited to the [dweeezil:ntrim3-next](https://github.com/dweeezil/zfs/tree/ntrim3-next) branch.",2019-01-21T14:23:59Z,1432005
833,openzfs/zfs,397129620,457395610,"Include `atrim` and `mtrim` latency histograms in the man page
```
--- a/man/man8/zpool.8
+++ b/man/man8/zpool.8
@@ -1820,6 +1820,10 @@ Amount of time IO spent in asynchronous priority queues.  Does not include
 disk time.
 .Ar scrub :
 Amount of time IO spent in scrub queue. Does not include disk time.
+.Ar atrim :
+Amount of time IO spent doing a trim operation during an autotrim.
+.Ar mtrim :
+Amount of time IO spent doing a trim operation during a manual trim.
 .It Fl l
 Include average latency statistics:
 .Pp
```",2019-01-24T23:19:57Z,11469457
834,openzfs/zfs,397129620,457400504,You'll want to add the new `trim_start` and `trim_end` events to `man/man5/zfs-events.5`.,2019-01-24T23:41:44Z,11469457
835,openzfs/zfs,397129620,457769901,Is it policy to license all new files under CDDL? May authors supply code under a more permissive license? (Sorry for the OT),2019-01-25T23:27:44Z,146192
836,openzfs/zfs,397129620,457787756,"> the CDDL header is copyrighted text

Not to make a fuss about this, but

- I think Tony's initial question was not about the CDDL header itself but on what's written underneath
- the CDDL header has a clear end marker so any copyright of that header should end there (?!)
- is it even legal to antedate a copyright on new code and attribute it to an organisation where you cannot even be an employe because it no longer exists?

",2019-01-26T01:23:10Z,1756884
837,openzfs/zfs,397129620,464140531,"I'm going to be closing this PR.  @behlendorf has been doing some major re-working of the original TRIM work to, among other things:
  - Harmonize its operation with that of device initialization so it's possible, for example, to TRIM individual vdevs.
  - Add per-vdev trim stats
  - Re-work the underlying discard mechanism to use a new ZIO type rather than the DKIOCFREE ioctl.
  - Utilize the new ""xlat"" vdev method from device initialization where appropriate.
  - Switch from the legacy ioctl interface to the new nvlist-style ioctl interface.
  - Plenty of other restructuring, in particular w.r.t. the management of the trim threads.

Expect a new PR to be posted soon.
",2019-02-15T17:54:24Z,1432005
838,openzfs/zfs,364146679,364146679,"Hello. I am using Gentoo and ZFS-master.
All was fine, until I decided to upgrade ZFS.
In gentoo for upgrade I need to merge 2 packages: zfs and zfs-kmod. 
Zfs was merged OK, but stopped working.

local ~ # zfs list
bad property list: invalid property 'name'

I thought it will work when latest kernel module will be merged. But it fails to compile.

  CC [M]  /var/tmp/portage/sys-fs/zfs-kmod-9999/work/zfs-kmod-9999/module/zfs/zfs_sysfs.o
  CC [M]  /var/tmp/portage/sys-fs/zfs-kmod-9999/work/zfs-kmod-9999/module/zfs/zfs_vfsops.o
/var/tmp/portage/sys-fs/zfs-kmod-9999/work/zfs-kmod-9999/module/zfs/zfs_sysfs.c: In function ‘zfs_kobj_add_attr’:
/var/tmp/portage/sys-fs/zfs-kmod-9999/work/zfs-kmod-9999/module/zfs/zfs_sysfs.c:151:38: error: assignment of member ‘name’ in read-only object
  zkobj->zko_attr_list[attr_num].name = attr_name;
                                      ^
/var/tmp/portage/sys-fs/zfs-kmod-9999/work/zfs-kmod-9999/module/zfs/zfs_sysfs.c:152:38: error: assignment of member ‘mode’ in read-only object
  zkobj->zko_attr_list[attr_num].mode = 0444;


Please help me fixing these issues.
Gentoo-Hardened, zfs-9999, zfs-kmod-9999, Linux-grsecurity-4.9.74, GCC 7.3.0",2018-09-26T18:04:06Z,24352836
839,openzfs/zfs,364146679,424840274,"I can test under gentoo in a little while.  Testing now with gentoo-sources-4.14.72. I am not using 4.9.X on any systems. 

Compiled for me:

>>> Verifying ebuild manifests
>>> Jobs: 0 of 2 complete, 1 running                Load avg: 6.81, 8.56, 6.40
>>> Emerging (1 of 2) sys-fs/zfs-kmod-9999::gentoo
>>> Jobs: 0 of 2 complete, 1 running                Load avg: 6.81, 8.56, 6.40
>>> Installing (1 of 2) sys-fs/zfs-kmod-9999::gentoo
>>> Jobs: 0 of 2 complete, 1 running                Load avg: 3.98, 2.84, 3.82
>>> Jobs: 0 of 2 complete                           Load avg: 3.98, 2.84, 3.82
>>> Jobs: 1 of 2 complete                           Load avg: 3.74, 2.81, 3.80
>>> Jobs: 1 of 2 complete, 1 running                Load avg: 3.74, 2.81, 3.80
>>> Emerging (2 of 2) sys-fs/zfs-9999::gentoo
>>> Jobs: 1 of 2 complete, 1 running                Load avg: 3.74, 2.81, 3.80
>>> Installing (2 of 2) sys-fs/zfs-9999::gentoo
>>> Jobs: 1 of 2 complete, 1 running                Load avg: 3.38, 3.57, 3.88
>>> Jobs: 1 of 2 complete                           Load avg: 3.38, 3.57, 3.88
>>> Jobs: 2 of 2 complete                           Load avg: 3.01, 3.49, 3.85

Although I am using gcc version 6.4.0 (Gentoo 6.4.0-r1 p1.3)

```
vm_gentoo ~ # uname -a
Linux vm_gentoo 4.14.72-gentoo-20180926-1600-vm_gentoo #1 SMP Wed Sep 26 16:09:18 EDT 2018 x86_64 Intel(R) Core(TM) i7 CPU 860 @ 2.80GHz GenuineIntel GNU/Linux
vm_gentoo ~ # eselect kernel list
Available kernel symlink targets:
  [1]   linux-4.14.67-gentoo
  [2]   linux-4.14.70-gentoo
  [3]   linux-4.14.72-gentoo *
vm_gentoo ~ # cat /sys/module/z
zavl/     zcommon/  zfs/      zlua/     znvpair/  zswap/    zunicode/
vm_gentoo ~ # cat /sys/module/zfs/version
0.8.0-rc1_21_gd1261452
vm_gentoo ~ #

```",2018-09-26T19:27:14Z,28434
840,openzfs/zfs,364146679,425100392,"> Hello. I am using Gentoo and ZFS-master.
> All was fine, until I decided to upgrade ZFS.
> In gentoo for upgrade I need to merge 2 packages: zfs and zfs-kmod.
> Zfs was merged OK, but stopped working.
> 
> local ~ # zfs list
> bad property list: invalid property 'name'

Could be a mismatch between kernel and userland ABI:
In case you upgraded from a way older version (=didn't `emerge zfs` for quite some time) you still might have the old userland binaries somewhere in your path, their location changed a while ago.
Similar problem (when you can only locate the new version in your path) could happen when the old kernel module is still loaded but the new version of the userland is used.

The compilation error could be caused in concert with grsecurity, see https://forums.grsecurity.net/viewtopic.php?f=3&t=3890 for a similar case (and possibly a solution).",2018-09-27T13:51:07Z,1021594
841,openzfs/zfs,364146679,425155629,"BTW, I also tried gcc 7.3 on gentoo-sources-4.18.10 (different machine) and the compile worked fine as well.",2018-09-27T16:22:20Z,28434
842,openzfs/zfs,364146679,425857028,"Thanks for replies. About zfs binary stopped working I know, it is because I compiled a new version of zfs and still use old version of zfs-kmod.

I am using this kernel for a long time and compilation of zfs worked well always. My last successful compilation was about 4-6 commits ago to the master. 

I read this link about grsecurity, but didn't found a solution. Also, I don't see any grsecurity errors at dmesg while compiling. 

So now I have new zfs and old zfs-kmod and I am afraid to reboot, I don't know how to compile the same version in gentoo or back-forward for some commits.

I think the problem is related to some new code in last commits, the kernel version or gcc version. 
I am using zfs (master) 1.5+ years at 4.9.24-grsec and about a year on 4.9.74-grsec (minipli), and compilation always was successful. 
Please help me resolve these issues. 
Glibc-2.26-r7, gcc-7.3.0-r5 hardened profile, binutils-2.31.1",2018-10-01T10:15:16Z,24352836
843,openzfs/zfs,364146679,426425810,"@UralZima just fork the repo and create a custom branch if newer commits don't work for you

you can modify zfs-kmod-9999.ebuild e.g. like so:

```
if [ ${PV} == ""9999"" ]; then
	AUTOTOOLS_AUTORECONF=""1""
#	EGIT_REPO_URI=""https://github.com/zfsonlinux/zfs.git""
	EGIT_REPO_URI=""https://github.com/kernelOfTruth/zfs.git""
	inherit git-r3
else
#	SRC_URI=""https://github.com/zfsonlinux/zfs/releases/download/zfs-${PV}/zfs-${PV}.tar.gz""
	SRC_URI=""https://github.com/kernelOfTruth/zfs/releases/download/zfs-${PV}/zfs-${PV}.tar.gz""
	S=""${WORKDIR}/zfs-${PV}""
	KEYWORDS=""~amd64 ~arm ~ppc ~ppc64""
fi

EGIT_BRANCH=""zfs_master_26.05.2018""
```

that way I'm e.g. selecting my repo and via EGIT_BRANCH the specific branch to be used.

This needs to be done **both for sys-fs/zfs and sys-fs/zfs-kmod !**

Hope that helps.",2018-10-02T20:52:03Z,4529709
844,openzfs/zfs,364146679,429906489,"Hello. I tried to compile it on linux-4.9.131-dappersec, the latest grsec-updated kernel, and it also failed.
I have compiled it successfully on latest linux-4.18.14-gentoo. 
As I know, 4.9.x is a longterm, so it is kernel-version-related or grsecurity-related. But as I told you, 4-5 commits back it compiled and worked fine. 

Is it possible to fix master to work with 4.9.x grsecurity kernels? It was working for years before. Thanks.",2018-10-15T15:43:03Z,24352836
845,openzfs/zfs,364146679,436129493,"Hello. Anything on this? Because of some commit (I think it is related to https://github.com/zfsonlinux/zfs/commit/7a23c81342df05ace730bd303b4a73854dba43dd), I can't use dappersec kernel anymore. I also tried with 4.9.132-dappersec, and the error is the same.
It was always compatible with grsecurity patches, please fix it. Thanks so much",2018-11-06T04:44:16Z,24352836
846,openzfs/zfs,364146679,438406957,"Hello. Some update. I decided to go back to the commit it was working. I was looking at history of zfs_sysfs.c file https://github.com/zfsonlinux/zfs/commits/master/module/zfs/zfs_sysfs.c and found that it started to fail since commits from Sep 2. So it is not related to https://github.com/zfsonlinux/zfs/commit/7a23c81342df05ace730bd303b4a73854dba43dd
I compiled it successfully with a commit before that 
EGIT_COMMIT=""bb91178e60df553071ce2e18b0067ef703f7b583""
I hope this information will help fix building on dappersec kernels. Thanks!",2018-11-13T19:35:52Z,24352836
847,openzfs/zfs,364146679,444582010,"The proposed fix is here, http://paste.ubuntu.com/p/prrDP6Kv4z/  @vogelfreiheit can you please open a PR so the CI and test it and we can get it reviewed and integrated.",2018-12-05T17:59:53Z,148917
848,openzfs/zfs,364146679,444605614,"I am ready to test it after PR is open and I am able to modify gentoo's ebuild to include it.
As I said, reverting to commit https://github.com/zfsonlinux/zfs/commit/7a23c81342df05ace730bd303b4a73854dba43dd not resolved the problem, I am able to use it on commit EGIT_COMMIT=""bb91178e60df553071ce2e18b0067ef703f7b583"", but the related file I think is zfs_sysfs.c",2018-12-05T19:10:37Z,24352836
849,openzfs/zfs,364146679,445478342,"Hi, I just saw this. I will let someone know so a PR can be opened, meanwhile I had some other fixes written for a few other spots that need testing.",2018-12-08T18:09:06Z,10137
850,openzfs/zfs,364146679,457913319,"This is long overdue, this project needs a code of conduct.",2019-01-27T12:23:47Z,47079893
851,openzfs/zfs,364146679,457992782,"Eh, after reviewing that other issue where someone is calling for a Code of Conduct (which is a fairly dumb idea), I'm surprised to see that you assumed that person was me (perhaps the intention of the troll all along). Since this begs finally some proper attention, I will describe this issue (feel free to remove the comment later as you obviously are seemingly easily to have your feelings hurt regardless of whether the other person intended so or not...).

I'll just clarify a few things: I've never asked for help in the channel and treated someone ""poorly"". I responded ironically to ptx0's messages, after he spent the past few hours being dismissive or condescending to other people, obviously in some cases users with a very limited skill around ZFS, but that does not merit acting like an imbecile. None of them were insults or hostile, they simply pointed out that ptx0 was assuming things constantly and then replying with an air of arrogance and usually in a condescending tone, despite himself being far from a beacon of excellence as far as technical skill or coding go. Immediately after he proceeded to ban me from the channel, to which I responded in -social pointing out that his attitude and actions making him look puerile and petty regardless of the nature of the ""offense"" by someone else. This knee-jerk reaction came motivated more by petty personal sentiments (and sharing those with some others who, well, fit that profile) than any actually reasonable reasons.

Now, on the project: you are routinely accepting patches quite obviously without really doing your due diligence, ensuring proper QA, etc. Case in point, the one we have here. It's painful to see that you waste more time on petty nothings over actually making your code or merged patches look like something with a modicum of technical quality. The control over whether contributors are just copy-pasting from other spots (*hint* licensing violations get people sued), if the patches actually conflict with major kernel features, or symbol exports, or..... is lacking all over the place.

How about instead of making up half-truths about people offending your feelings, or your buddies' feelings, you go and properly benefit from the resources you so adamantly defend (and ask others to work with): How difficult was it to grab the patch I sent you from the link, test it, test it again, sign it off and merge it? You need to go around deleting comments and engaging in conversation with the other troll, but you are too busy to actually merge an out of band patch while you waste time over petty fights? Well, good luck with that. And you are surprised ZoL is not taken seriously anywhere but by the homelab/non-enterprise crowd. If you want to be a primadonna, at least have the cards in your sleeve to back it up: you can't justify it right now with the standard you are providing your users. And it runs in ring0, so go figure a better way to fend off all those users popping up on IRC asking about destroyed data because you merged a patch from a pull-request that would not stand a moment in the LKML.",2019-01-28T04:02:19Z,10137
852,openzfs/zfs,364146679,457998235,"I am temporarily locking this thread.  If I could please ask everyone to calm down, I'm not about to pick sides in whatever conflict that is currently taking place here.  Brian has asked for someone to submit a PR to fix this issue.  Until that takes place, there is no reason for this issue to remain wide open given its current state.  ",2019-01-28T04:50:12Z,12985665
853,openzfs/zfs,364146679,458315146,"The discussion in this thread (now hidden) has not been productive.  In the future, please speak respectfully to fellow contributors.  @behlendorf and I are working on putting a process in place for handling inappropriate behavior.  In the mean time, please feel free to contact either Brian or I privately if you have concerns about the behavior of ZFSonLinux community members.",2019-01-28T21:49:16Z,799124
854,openzfs/zfs,356609376,356609376,"Hello, as referenced by others like this issue [here](https://github.com/zfsonlinux/zfs/issues/6001), I have a pool that was mistakenly upgraded under Linux that needs to go back to FreeBSD.  Unfortunately userobj_accounting is enabled, which means that FreeBSD won't take the pool due to an unsupported feature.

I saw mention of [this commit](https://github.com/zfsonlinux/zfs/commit/83025286175d1ee1c29b842531070f3250a172ba) enabling feature disable functionality.  I grabbed a Ubuntu 14.04 boot disk and installed ZoL (and an SPL from about that time) from that pull request, but am still getting reference to userobj_accounting being an ""unsupported feature"" when I try to import the pool.

I'm not sure if the pool should be mounted or not?

With the pool exported, trying zhack returns:

```
root@ubuntu:~/zfs# zhack feature disable media org.zfsonlinux:userobj_accounting
zhack: feature 'org.zfsonlinux:userobj_accounting' is active, can not disable
```",2018-09-03T22:45:46Z,13775820
855,openzfs/zfs,356609376,418239329,"Why are you ignoring both rules and requests (https://github.com/zfsonlinux/zfs/issues/6466#issuecomment-320420368)? Please use the issue template if you found a bug, otherwise ask the mailing lists (https://github.com/zfsonlinux/zfs/wiki/Mailing-Lists).

Thank you for your cooperation.",2018-09-04T04:41:30Z,4585738
856,openzfs/zfs,356609376,418241114,"The question I asked was in regard to the lead developer of this project making a patch for himself to use on a catch22 in feature flags, and therefore not addressed to you.  Nor is it addressed to anyone else that is using a current version of this software, since the relevant patch is over three years old (and now deprecated).

> Thank you for your cooperation.

What makes you think I'm agreeing with anything you typed?  Don't thank me, I'm not.

Thank *you*, for nothing.  I'd file a bug report about people with pedophile'ish usernames with such high exposure in a project that Canonical seems to care so much about, but your form is (predictably) excessive.  No thanks.

For those who might find this on Google, don't bother with that patch/commit.  If your pool got userobj_accounting enabled it is now a Linux-only pool.  You'll have to destroy it, since per the (broken) explanation in the manpage, once active it cannot be decactivated.  Sort of like that Doomsday Machine that Slim Pickens was after....
",2018-09-04T04:56:20Z,13775820
857,openzfs/zfs,356609376,418254587,"> The question I asked was in regard to the lead developer of this project making a patch for himself to use on a catch22 in feature flags, and therefore not addressed to you. Nor is it addressed to anyone else that is using a current version of this software, since the relevant patch is over three years old (and now deprecated).

Still, you post on a bug report platform watched by > 400 people that get emailed for each of your posts.

> What makes you think I'm agreeing with anything you typed? Don't thank me, I'm not. [childish personal attacks]

Nobody asked for your opionion. There are rules established by the community, people who largely work on this project for free, an you can either stick to their rules, not contribute (anything) at all, or fork the project and work on your fork under your own rules. :wink:

---
I'll unsubscribe from this issue and won't comment here further.",2018-09-04T06:25:01Z,12913701
858,openzfs/zfs,312114336,312114336,"### System information
<!--  add version after ""|"" character -->
Type                                | Version/Name
  ---                                  |     --- 
Distribution Name       | Scientific Linux
Distribution Version    | 6.8
Linux Kernel                 | 2.6.32-696.23.1.el6.x86_64
Architecture                 | x86_64
ZFS Version                  | 0.7.7
SPL Version                  | 0.7.7

### Describe the problem you're observing
Data loss when copying a directory with large-ish number of files. For example, `cp -r SRC DST` with 10000 files in SRC is likely to result in a couple of ""cp: cannot create regular file `DST/XXX': No space left on device"" error messages, and a few thousand files missing from the listing of the DST directory. (Needless to say, filesystem being full is not the problem.)

The missing files are missing in the sense that they don't appear in the directory listing, but can be accessed using their name (except for the couple of files for which `cp` generated ""No space left on device"" error). For example:

```
# ls -l DST | grep FOO | wc -l
0
# ls -l DST/FOO
-rw-r--r-- 1 root root 5 Apr  6 14:59 DST/FOO
```

The content of DST/FOO are accessible by path (e.g. `cat DST/FOO` works) and is the same as SRC/FOO. If caches are dropped (`echo 3 > /proc/sys/vm/drop_caches`) or the machine is rebooted, opening FOO directly by path fails.

`ls -ld DST` reports N fewer hard links than SRC, where N is the number of files for which `cp` reported ""No space left on device"" error.

Names of missing files are mostly predictable if SRC is small.

Scrub does not find any errors.

I think the problem appeared in 0.7.7, but I am not sure.

### Describe how to reproduce the problem

```
# mkdir SRC
# for i in $(seq 1 10000); do echo $i > SRC/$i ; done
# cp -r SRC DST
cp: cannot create regular file `DST/8442': No space left on device
cp: cannot create regular file `DST/2629': No space left on device
# ls -l
total 3107
drwxr-xr-x 2 root root 10000 Apr  6 15:28 DST
drwxr-xr-x 2 root root 10002 Apr  6 15:27 SRC
# find DST -type f | wc -l 
8186
# ls -l DST | grep 8445 | wc -l
0
# ls -l DST/8445
-rw-r--r-- 1 root root 5 Apr  6 15:28 DST/8445
# cat DST/8445
8445
# echo 3 > /proc/sys/vm/drop_caches
# cat DST/8445
cat: DST/8445: No such file or directory
```

### Include any warning/errors/backtraces from the system logs
```
# zpool status
  pool: tank
 state: ONLINE
  scan: scrub repaired 0B in 87h47m with 0 errors on Sat Mar 31 07:09:27 2018
config:

        NAME                        STATE     READ WRITE CKSUM
        tank                        ONLINE       0     0     0
          raidz1-0                  ONLINE       0     0     0
            wwn-0x5000c50085ac4c0f  ONLINE       0     0     0
            wwn-0x5000c50085acda77  ONLINE       0     0     0
            wwn-0x5000c500858db3d7  ONLINE       0     0     0
            wwn-0x5000c50085ac9887  ONLINE       0     0     0
            wwn-0x5000c50085aca6df  ONLINE       0     0     0
          raidz1-1                  ONLINE       0     0     0
            wwn-0x5000c500858db743  ONLINE       0     0     0
            wwn-0x5000c500858db347  ONLINE       0     0     0
            wwn-0x5000c500858db4a7  ONLINE       0     0     0
            wwn-0x5000c500858dbb0f  ONLINE       0     0     0
            wwn-0x5000c50085acaa97  ONLINE       0     0     0
          raidz1-2                  ONLINE       0     0     0
            wwn-0x5000c50085accb4b  ONLINE       0     0     0
            wwn-0x5000c50085acab9f  ONLINE       0     0     0
            wwn-0x5000c50085ace783  ONLINE       0     0     0
            wwn-0x5000c500858db67b  ONLINE       0     0     0
            wwn-0x5000c50085acb983  ONLINE       0     0     0
          raidz1-3                  ONLINE       0     0     0
            wwn-0x5000c50085ac4fd7  ONLINE       0     0     0
            wwn-0x5000c50085acb24b  ONLINE       0     0     0
            wwn-0x5000c50085ace13b  ONLINE       0     0     0
            wwn-0x5000c500858db43f  ONLINE       0     0     0
            wwn-0x5000c500858db61b  ONLINE       0     0     0
          raidz1-4                  ONLINE       0     0     0
            wwn-0x5000c500858dbbb7  ONLINE       0     0     0
            wwn-0x5000c50085acce7f  ONLINE       0     0     0
            wwn-0x5000c50085acd693  ONLINE       0     0     0
            wwn-0x5000c50085ac3d87  ONLINE       0     0     0
            wwn-0x5000c50085acc89b  ONLINE       0     0     0
          raidz1-5                  ONLINE       0     0     0
            wwn-0x5000c500858db28b  ONLINE       0     0     0
            wwn-0x5000c500858db68f  ONLINE       0     0     0
            wwn-0x5000c500858dbadf  ONLINE       0     0     0
            wwn-0x5000c500858db623  ONLINE       0     0     0
            wwn-0x5000c500858db48b  ONLINE       0     0     0
          raidz1-6                  ONLINE       0     0     0
            wwn-0x5000c500858db6ef  ONLINE       0     0     0
            wwn-0x5000c500858db39b  ONLINE       0     0     0
            wwn-0x5000c500858db47f  ONLINE       0     0     0
            wwn-0x5000c500858dbb23  ONLINE       0     0     0
            wwn-0x5000c500858db803  ONLINE       0     0     0
        logs
          zfs-slog                  ONLINE       0     0     0
        spares
          wwn-0x5000c500858db463    AVAIL   

errors: No known data errors
```
```
# zpool list
NAME   SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
tank   254T   159T  94.3T         -    27%    62%  1.00x  ONLINE  -
```
```
# zfs list -t all
NAME           USED  AVAIL  REFER  MOUNTPOINT
tank           127T  69.0T  11.5T  /mnt/tank
tank/jade      661G  69.0T   661G  /mnt/tank/jade
tank/simprod   115T  14.8T   115T  /mnt/tank/simprod
```

```
# zfs get all tank
NAME  PROPERTY              VALUE                  SOURCE
tank  type                  filesystem             -
tank  creation              Sat Jan 20 12:11 2018  -
tank  used                  127T                   -
tank  available             68.9T                  -
tank  referenced            11.6T                  -
tank  compressratio         1.00x                  -
tank  mounted               yes                    -
tank  quota                 none                   default
tank  reservation           none                   default
tank  recordsize            128K                   default
tank  mountpoint            /mnt/tank              local
tank  sharenfs              off                    default
tank  checksum              on                     default
tank  compression           off                    default
tank  atime                 off                    local
tank  devices               on                     default
tank  exec                  on                     default
tank  setuid                on                     default
tank  readonly              off                    default
tank  zoned                 off                    default
tank  snapdir               hidden                 default
tank  aclinherit            restricted             default
tank  createtxg             1                      -
tank  canmount              on                     default
tank  xattr                 sa                     local
tank  copies                1                      default
tank  version               5                      -
tank  utf8only              off                    -
tank  normalization         none                   -
tank  casesensitivity       sensitive              -
tank  vscan                 off                    default
tank  nbmand                off                    default
tank  sharesmb              off                    default
tank  refquota              none                   default
tank  refreservation        none                   default
tank  guid                  2271746520743372128    -
tank  primarycache          all                    default
tank  secondarycache        all                    default
tank  usedbysnapshots       0B                     -
tank  usedbydataset         11.6T                  -
tank  usedbychildren        116T                   -
tank  usedbyrefreservation  0B                     -
tank  logbias               latency                default
tank  dedup                 off                    default
tank  mlslabel              none                   default
tank  sync                  standard               default
tank  dnodesize             legacy                 default
tank  refcompressratio      1.00x                  -
tank  written               11.6T                  -
tank  logicalused           128T                   -
tank  logicalreferenced     11.6T                  -
tank  volmode               default                default
tank  filesystem_limit      none                   default
tank  snapshot_limit        none                   default
tank  filesystem_count      none                   default
tank  snapshot_count        none                   default
tank  snapdev               hidden                 default
tank  acltype               off                    default
tank  context               none                   default
tank  fscontext             none                   default
tank  defcontext            none                   default
tank  rootcontext           none                   default
tank  relatime              off                    default
tank  redundant_metadata    all                    default
tank  overlay               off                    default
```

```
# zpool get all tank   
NAME  PROPERTY                       VALUE                          SOURCE
tank  size                           254T                           -
tank  capacity                       62%                            -
tank  altroot                        -                              default
tank  health                         ONLINE                         -
tank  guid                           7056741522691970971            -
tank  version                        -                              default
tank  bootfs                         -                              default
tank  delegation                     on                             default
tank  autoreplace                    on                             local
tank  cachefile                      -                              default
tank  failmode                       wait                           default
tank  listsnapshots                  off                            default
tank  autoexpand                     off                            default
tank  dedupditto                     0                              default
tank  dedupratio                     1.00x                          -
tank  free                           94.2T                          -
tank  allocated                      160T                           -
tank  readonly                       off                            -
tank  ashift                         0                              default
tank  comment                        -                              default
tank  expandsize                     -                              -
tank  freeing                        0                              -
tank  fragmentation                  27%                            -
tank  leaked                         0                              -
tank  multihost                      off                            default
tank  feature@async_destroy          enabled                        local
tank  feature@empty_bpobj            active                         local
tank  feature@lz4_compress           active                         local
tank  feature@multi_vdev_crash_dump  enabled                        local
tank  feature@spacemap_histogram     active                         local
tank  feature@enabled_txg            active                         local
tank  feature@hole_birth             active                         local
tank  feature@extensible_dataset     active                         local
tank  feature@embedded_data          active                         local
tank  feature@bookmarks              enabled                        local
tank  feature@filesystem_limits      enabled                        local
tank  feature@large_blocks           enabled                        local
tank  feature@large_dnode            enabled                        local
tank  feature@sha512                 enabled                        local
tank  feature@skein                  enabled                        local
tank  feature@edonr                  enabled                        local
tank  feature@userobj_accounting     active                         local
```",2018-04-06T20:53:19Z,22751545
859,openzfs/zfs,312114336,379404598,"I can **confirm** the same behavior on a minimal CentOS 7.4 installation (running inside VirtualBox) and latest ZFS 0.7.7. Please note that when copying somewhat bigger files (ie: kernel source) it does not happen, so it seems something as a race condition...

```
; the only changed property was xattr=sa
[root@localhost ~]# zpool list
NAME   SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
tank  7.94G  25.3M  7.91G         -     0%     0%  1.00x  ONLINE  -
[root@localhost ~]# zfs list
NAME        USED  AVAIL  REFER  MOUNTPOINT
tank       24.5M  7.67G  3.36M  /tank
tank/test  21.0M  7.67G  21.0M  /tank/test

; creating the source dir on a XFS filesystem
[root@localhost ~]# cd /root/
[root@localhost ~]# mkdir test
[root@localhost ~]# cd test
[root@localhost ~]# for i in $(seq 1 10000); do echo $i > SRC/$i ; done

; copying from XFS to ZFS: no problem at all
[root@localhost ~]# cd /tank/test
[root@localhost test]# cp -r /root/test/SRC/ DST1
[root@localhost test]# cp -r /root/test/SRC/ DST2
[root@localhost test]# cp -r /root/test/SRC/ DST3
[root@localhost test]# find DST1/ | wc -l
10001
[root@localhost test]# find DST2/ | wc -l
10001
[root@localhost test]# find DST3/ | wc -l
10001

; copying from ZFS dataset itself: big troubles!
[root@localhost test]# rm -rf SRC DST1 DST2 DST3
[root@localhost test]# cp -r /root/test/SRC .
[root@localhost test]# cp -r SRC DST1
cp: cannot create regular file ‘DST1/8809’: No space left on device
[root@localhost test]# cp -r SRC DST2
[root@localhost test]# cp -r SRC DST3
cp: cannot create regular file ‘DST3/6507’: No space left on device
[root@localhost test]# find DST1/ | wc -l
10000
[root@localhost test]# find DST2/ | wc -l
10001
[root@localhost test]# find DST3/ | wc -l
8189

; disabling cache: nothing changes (we continue to ""lose"" files)
[root@localhost test]# zfs set primarycache=none tank
[root@localhost test]# zfs set primarycache=none tank/test
[root@localhost test]# echo 3 > /proc/sys/vm/drop_caches
[root@localhost test]# rm -rf SRC; mkdir SRC; for i in $(seq 1 10000); do echo $i > SRC/$i ; done; find SRC | wc -l
10001
[root@localhost test]# rm -rf SRC; mkdir SRC; for i in $(seq 1 10000); do echo $i > SRC/$i ; done; find SRC | wc -l
10001
[root@localhost test]# rm -rf SRC; mkdir SRC; for i in $(seq 1 10000); do echo $i > SRC/$i ; done; find SRC | wc -l
10001

```



The problem does **NOT** appear on ZoL 0.7.6:

```
; creating the dataset and copying the SRC dir
[root@localhost ~]# zfs create tank/test
[root@localhost ~]# zfs set xattr=sa tank
[root@localhost ~]# zfs set xattr=sa tank/test
[root@localhost ~]# cp -r /root/test/SRC/ /tank/test/
[root@localhost ~]# cd /tank/test/
[root@localhost test]# find SRC/ | wc -l
10001

; more copies
[root@localhost test]# cp -r SRC/ DST
[root@localhost test]# cp -r SRC/ DST1
[root@localhost test]# cp -r SRC/ DST2
[root@localhost test]# cp -r SRC/ DST3
[root@localhost test]# cp -r SRC/ DST4
[root@localhost test]# cp -r SRC/ DST5
[root@localhost test]# find DST | wc -l
10001
[root@localhost test]# find DST1 | wc -l
10001
[root@localhost test]# find DST2 | wc -l
10001
[root@localhost test]# find DST3 | wc -l
10001
[root@localhost test]# find DST4 | wc -l
10001
[root@localhost test]# find DST5 | wc -l
10001
```

Maybe it can help. Here you find the output of `zdb -dddddddd tank/test 192784` (a ""good"" DST directory):
```
Dataset tank/test [ZPL], ID 74, cr_txg 13, 26.5M, 190021 objects, rootbp DVA[0]=<0:5289e00:200> DVA[1]=<0:65289e00:200> [L0 DMU objset] fletcher4 lz4 LE contiguous unique double size=800L/200P birth=123L/123P fill=190021 cksum=d622b78d2:50c053a50d0:fca8cd4455d7:2216d160ee7f7d

    Object  lvl   iblk   dblk  dsize  dnsize  lsize   %full  type
    192784    2   128K    16K   909K     512  1.02M  100.00  ZFS directory (K=inherit) (Z=inherit)
                                               272   bonus  System attributes
        dnode flags: USED_BYTES USERUSED_ACCOUNTED USEROBJUSED_ACCOUNTED
        dnode maxblkid: 64
        path    /DST16
        uid     0
        gid     0
        atime   Sat Apr  7 01:11:29 2018
        mtime   Sat Apr  7 01:11:31 2018
        ctime   Sat Apr  7 01:11:31 2018
        crtime  Sat Apr  7 01:11:29 2018
        gen     97
        mode    40755
        size    10002
        parent  34
        links   2
        pflags  40800000144
        SA xattrs: 96 bytes, 1 entries

                security.selinux = unconfined_u:object_r:unlabeled_t:s0\000
        Fat ZAP stats:
                Pointer table:
                        1024 elements
                        zt_blk: 0
                        zt_numblks: 0
                        zt_shift: 10
                        zt_blks_copied: 0
                        zt_nextblk: 0
                ZAP entries: 10000
                Leaf blocks: 64
                Total blocks: 65
                zap_block_type: 0x8000000000000001
                zap_magic: 0x2f52ab2ab
                zap_salt: 0x13c18a19
                Leafs with 2^n pointers:
                          4:     64 ****************************************
                Blocks with n*5 entries:
                          9:     64 ****************************************
                Blocks n/10 full:
                          6:      4 ****
                          7:     43 ****************************************
                          8:     16 ***************
                          9:      1 *
                Entries with n chunks:
                          3:  10000 ****************************************
                Buckets with n entries:
                          0:  24119 ****************************************
                          1:   7414 *************
                          2:   1126 **
                          3:    102 *
                          4:      7 *
```

... and `zdb -dddddddd tank/test 202785` (a ""bad"" DST directory):
```
Dataset tank/test [ZPL], ID 74, cr_txg 13, 26.5M, 190021 objects, rootbp DVA[0]=<0:5289e00:200> DVA[1]=<0:65289e00:200> [L0 DMU objset] fletcher4 lz4 LE contiguous unique double size=800L/200P birth=123L/123P fill=190021 cksum=d622b78d2:50c053a50d0:fca8cd4455d7:2216d160ee7f7d

    Object  lvl   iblk   dblk  dsize  dnsize  lsize   %full  type
    202785    2   128K    16K   766K     512   896K  100.00  ZFS directory (K=inherit) (Z=inherit)
                                               272   bonus  System attributes
        dnode flags: USED_BYTES USERUSED_ACCOUNTED USEROBJUSED_ACCOUNTED
        dnode maxblkid: 55
        path    /DST17
        uid     0
        gid     0
        atime   Sat Apr  7 01:12:49 2018
        mtime   Sat Apr  7 01:11:33 2018
        ctime   Sat Apr  7 01:11:33 2018
        crtime  Sat Apr  7 01:11:32 2018
        gen     98
        mode    40755
        size    10001
        parent  34
        links   2
        pflags  40800000144
        SA xattrs: 96 bytes, 1 entries

                security.selinux = unconfined_u:object_r:unlabeled_t:s0\000
        Fat ZAP stats:
                Pointer table:
                        1024 elements
                        zt_blk: 0
                        zt_numblks: 0
                        zt_shift: 10
                        zt_blks_copied: 0
                        zt_nextblk: 0
                ZAP entries: 8259
                Leaf blocks: 55
                Total blocks: 56
                zap_block_type: 0x8000000000000001
                zap_magic: 0x2f52ab2ab
                zap_salt: 0x1bf8e8a3
                Leafs with 2^n pointers:
                          4:     50 ****************************************
                          5:      3 ***
                          6:      2 **
                Blocks with n*5 entries:
                          9:     55 ****************************************
                Blocks n/10 full:
                          5:      6 ******
                          6:      7 *******
                          7:     32 ********************************
                          8:      6 ******
                          9:      4 ****
                Entries with n chunks:
                          3:   8259 ****************************************
                Buckets with n entries:
                          0:  20964 ****************************************
                          1:   6217 ************
                          2:    904 **
                          3:     66 *
                          4:      9 *

```",2018-04-06T22:49:51Z,13137622
860,openzfs/zfs,312114336,379411966,We are also seeing similar behavior since the install of 0.7.7,2018-04-06T23:40:26Z,625982
861,openzfs/zfs,312114336,379434208,"I have a hand-built ZoL 0.7.7 on a stock Ubuntu 16.04 server (currently with Ubuntu kernel version '4.4.0-109-generic') and I can't reproduce this problem on it, following the reproduction here and some variants (eg using 'seq -w' to make all of the filenames the same size). The pool I'm testing against has a single mirrored vdev.",2018-04-07T05:40:12Z,865382
862,openzfs/zfs,312114336,379454983,"One more data point, with the hope that it helps narrow down the issue.

I cannot reproduce the issue on the few machines I have here, neither with 10k files, nor with 100k or even 1M. They all have very similar configuraition. They use a single 2-drive mirrored vdev. The drives are Samsung SSD 950 PRO 512GB (NVMe, quite fast).

```
$ uname -a
Linux pat 4.9.90-gentoo #1 SMP PREEMPT Tue Mar 27 00:19:59 CEST 2018 x86_64 Intel(R) Xeon(R) CPU E3-1505M v5 @ 2.80GHz GenuineIntel GNU/Linux

$ qlist -I -v zfs-kmod
sys-fs/zfs-kmod-0.7.7

$ qlist -I -v spl
sys-kernel/spl-0.7.7

$ zpool status
  pool: pat:pool
 state: ONLINE
  scan: scrub repaired 0B in 0h1m with 0 errors on Sat Apr  7 03:35:12 2018
config:

        NAME                                                 STATE     READ WRITE CKSUM
        pat:pool                                             ONLINE       0     0     0
          mirror-0                                           ONLINE       0     0     0
            nvme0n1p4                                        ONLINE       0     0     0
            nvme1n1p4                                        ONLINE       0     0     0
        spares
          ata-Samsung_SSD_850_EVO_1TB_S2RFNXAH118721D-part8  AVAIL   

errors: No known data errors

$ zpool list
NAME       SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
pat:pool   408G   110G   298G         -    18%    26%  1.00x  ONLINE  -

$ zpool get all pat:pool
NAME      PROPERTY                       VALUE                          SOURCE
pat:pool  size                           408G                           -
pat:pool  capacity                       26%                            -
pat:pool  altroot                        -                              default
pat:pool  health                         ONLINE                         -
pat:pool  guid                           16472389984482033769           -
pat:pool  version                        -                              default
pat:pool  bootfs                         -                              default
pat:pool  delegation                     on                             default
pat:pool  autoreplace                    on                             local
pat:pool  cachefile                      -                              default
pat:pool  failmode                       wait                           default
pat:pool  listsnapshots                  off                            default
pat:pool  autoexpand                     off                            default
pat:pool  dedupditto                     0                              default
pat:pool  dedupratio                     1.00x                          -
pat:pool  free                           298G                           -
pat:pool  allocated                      110G                           -
pat:pool  readonly                       off                            -
pat:pool  ashift                         12                             local
pat:pool  comment                        -                              default
pat:pool  expandsize                     -                              -
pat:pool  freeing                        0                              -
pat:pool  fragmentation                  18%                            -
pat:pool  leaked                         0                              -
pat:pool  multihost                      off                            default
pat:pool  feature@async_destroy          enabled                        local
pat:pool  feature@empty_bpobj            active                         local
pat:pool  feature@lz4_compress           active                         local
pat:pool  feature@multi_vdev_crash_dump  enabled                        local
pat:pool  feature@spacemap_histogram     active                         local
pat:pool  feature@enabled_txg            active                         local
pat:pool  feature@hole_birth             active                         local
pat:pool  feature@extensible_dataset     active                         local
pat:pool  feature@embedded_data          active                         local
pat:pool  feature@bookmarks              enabled                        local
pat:pool  feature@filesystem_limits      enabled                        local
pat:pool  feature@large_blocks           enabled                        local
pat:pool  feature@large_dnode            enabled                        local
pat:pool  feature@sha512                 enabled                        local
pat:pool  feature@skein                  enabled                        local
pat:pool  feature@edonr                  enabled                        local
pat:pool  feature@userobj_accounting     active                         local

$ zfs list
NAME                                          USED  AVAIL  REFER  MOUNTPOINT
(...)
pat:pool/home/joe/tmp                        27.9G   285G  27.9G  /home/joe/tmp
(...)

$ zfs get all pat:pool/home/joe/tmp
NAME                   PROPERTY               VALUE                  SOURCE
pat:pool/home/joe/tmp  type                   filesystem             -
pat:pool/home/joe/tmp  creation               Sat Mar 12 17:32 2016  -
pat:pool/home/joe/tmp  used                   27.9G                  -
pat:pool/home/joe/tmp  available              285G                   -
pat:pool/home/joe/tmp  referenced             27.9G                  -
pat:pool/home/joe/tmp  compressratio          1.16x                  -
pat:pool/home/joe/tmp  mounted                yes                    -
pat:pool/home/joe/tmp  quota                  none                   default
pat:pool/home/joe/tmp  reservation            none                   default
pat:pool/home/joe/tmp  recordsize             128K                   default
pat:pool/home/joe/tmp  mountpoint             /home/joe/tmp          inherited from pat:pool/home
pat:pool/home/joe/tmp  sharenfs               off                    default
pat:pool/home/joe/tmp  checksum               on                     default
pat:pool/home/joe/tmp  compression            lz4                    inherited from pat:pool
pat:pool/home/joe/tmp  atime                  off                    inherited from pat:pool
pat:pool/home/joe/tmp  devices                on                     default
pat:pool/home/joe/tmp  exec                   on                     default
pat:pool/home/joe/tmp  setuid                 on                     default
pat:pool/home/joe/tmp  readonly               off                    default
pat:pool/home/joe/tmp  zoned                  off                    default
pat:pool/home/joe/tmp  snapdir                hidden                 default
pat:pool/home/joe/tmp  aclinherit             restricted             default
pat:pool/home/joe/tmp  createtxg              507                    -
pat:pool/home/joe/tmp  canmount               on                     default
pat:pool/home/joe/tmp  xattr                  sa                     inherited from pat:pool
pat:pool/home/joe/tmp  copies                 1                      default
pat:pool/home/joe/tmp  version                5                      -
pat:pool/home/joe/tmp  utf8only               off                    -
pat:pool/home/joe/tmp  normalization          none                   -
pat:pool/home/joe/tmp  casesensitivity        sensitive              -
pat:pool/home/joe/tmp  vscan                  off                    default
pat:pool/home/joe/tmp  nbmand                 off                    default
pat:pool/home/joe/tmp  sharesmb               off                    default
pat:pool/home/joe/tmp  refquota               none                   default
pat:pool/home/joe/tmp  refreservation         none                   default
pat:pool/home/joe/tmp  guid                   10274125767907263189   -
pat:pool/home/joe/tmp  primarycache           all                    default
pat:pool/home/joe/tmp  secondarycache         all                    default
pat:pool/home/joe/tmp  usedbysnapshots        0B                     -
pat:pool/home/joe/tmp  usedbydataset          27.9G                  -
pat:pool/home/joe/tmp  usedbychildren         0B                     -
pat:pool/home/joe/tmp  usedbyrefreservation   0B                     -
pat:pool/home/joe/tmp  logbias                latency                default
pat:pool/home/joe/tmp  dedup                  off                    default
pat:pool/home/joe/tmp  mlslabel               none                   default
pat:pool/home/joe/tmp  sync                   standard               default
pat:pool/home/joe/tmp  dnodesize              legacy                 default
pat:pool/home/joe/tmp  refcompressratio       1.16x                  -
pat:pool/home/joe/tmp  written                27.9G                  -
pat:pool/home/joe/tmp  logicalused            31.6G                  -
pat:pool/home/joe/tmp  logicalreferenced      31.6G                  -
pat:pool/home/joe/tmp  volmode                default                default
pat:pool/home/joe/tmp  filesystem_limit       none                   default
pat:pool/home/joe/tmp  snapshot_limit         none                   default
pat:pool/home/joe/tmp  filesystem_count       none                   default
pat:pool/home/joe/tmp  snapshot_count         none                   default
pat:pool/home/joe/tmp  snapdev                hidden                 default
pat:pool/home/joe/tmp  acltype                posixacl               inherited from pat:pool
pat:pool/home/joe/tmp  context                none                   default
pat:pool/home/joe/tmp  fscontext              none                   default
pat:pool/home/joe/tmp  defcontext             none                   default
pat:pool/home/joe/tmp  rootcontext            none                   default
pat:pool/home/joe/tmp  relatime               off                    default
pat:pool/home/joe/tmp  redundant_metadata     all                    default
pat:pool/home/joe/tmp  overlay                off                    default
pat:pool/home/joe/tmp  net.c-space:snapshots  keep=1M                inherited from pat:pool/home/joe
pat:pool/home/joe/tmp  net.c-space:root       0                      inherited from pat:pool
```",2018-04-07T09:09:59Z,245024
863,openzfs/zfs,312114336,379458689,"I get a worse situation on latest Centos 7 with kmod:

`[root@zirconia test]# mkdir SRC
[root@zirconia test]# for i in $(seq 1 10000); do echo $i > SRC/$i ; done
[root@zirconia test]# cp -r SRC DST
cp: cannot create regular file ‘DST/5269’: No space left on device
cp: cannot create regular file ‘DST/9923’: No space left on device
[root@zirconia test]# cat DST/5269
cat: DST/5269: No such file or directory
[root@zirconia test]# cat DST/9923
cat: DST/9923: No such file or directory
[root@zirconia test]# cat DST/9924
9924
[root@zirconia test]# cat DST/9923
cat: DST/9923: No such file or directory
[root@zirconia test]# ls -l DST/9923
ls: cannot access DST/9923: No such file or directory

[root@zirconia test]# zpool status
  pool: storage
 state: ONLINE
  scan: none requested
config:

	NAME                                            STATE     READ WRITE CKSUM
	storage                                         ONLINE       0     0     0
	  raidz1-0                                      ONLINE       0     0     0
	    ata-Hitachi_HDS723020BLA642_MN1220F30KPM0D  ONLINE       0     0     0
	    ata-Hitachi_HDS723020BLA642_MN1220F30NJDDD  ONLINE       0     0     0
	    ata-Hitachi_HDS723020BLA642_MN1220F30NJAHD  ONLINE       0     0     0
	  raidz1-1                                      ONLINE       0     0     0
	    ata-Hitachi_HDS723020BLA642_MN1220F30NGXDD  ONLINE       0     0     0
	    ata-Hitachi_HDS723020BLA642_MN1220F30NJ91D  ONLINE       0     0     0
	    ata-Hitachi_HDS723020BLA642_MN1220F30LN7GD  ONLINE       0     0     0
	  raidz1-2                                      ONLINE       0     0     0
	    ata-Hitachi_HDS723020BLA642_MN1220F30NJM5D  ONLINE       0     0     0
	    ata-HGST_HUS724020ALA640_PN2134P5GAY9PX     ONLINE       0     0     0
	    ata-Hitachi_HDS723020BLA642_MN1220F30NJD5D  ONLINE       0     0     0
	  raidz1-3                                      ONLINE       0     0     0
	    ata-Hitachi_HDS723020BLA642_MN1220F30NJD8D  ONLINE       0     0     0
	    ata-Hitachi_HDS723020BLA642_MN1220F30NJHVD  ONLINE       0     0     0
	    ata-Hitachi_HDS723020BLA642_MN1220F30K5PMD  ONLINE       0     0     0
	  raidz1-4                                      ONLINE       0     0     0
	    ata-Hitachi_HDS723020BLA642_MN1220F30NLZLD  ONLINE       0     0     0
	    ata-Hitachi_HDS723020BLA642_MN1220F30MVW4D  ONLINE       0     0     0
	    ata-HGST_HUS724020ALA640_PN2134P5GBBL9X     ONLINE       0     0     0
	logs
	  mirror-5                                      ONLINE       0     0     0
	    nvme0n1p1                                   ONLINE       0     0     0
	    nvme1n1p1                                   ONLINE       0     0     0
	cache
	  nvme0n1p2                                     ONLINE       0     0     0
	  nvme1n1p2                                     ONLINE       0     0     0`

I
",2018-04-07T10:11:11Z,1042393
864,openzfs/zfs,312114336,379460489,"@rblank Did you use empty files? Please try the following:
- cd into your ZFS dataset
- execute `mkdir SRC; for i in $(seq 1 10000); do echo -n > SRC/$i; done; find SRC | wc -l`
- now issue `for i in $(seq 1 10); do cp -r SRC DST$i; find DST$i | wc -l; done`

Thanks.",2018-04-07T10:45:24Z,13137622
865,openzfs/zfs,312114336,379486565,"I used the exact commands from the OP (which create non-empty files), only changing 10000 to 100000 and 1000000. But for completeness, I tried yours as well.

```
$ mkdir SRC; for i in $(seq 1 10000); do echo -n > SRC/$i; done; find SRC | wc -l
10001
$ for i in $(seq 1 10); do cp -r SRC DST$i; find DST$i | wc -l; done
10001
10001
10001
10001
10001
10001
10001
10001
10001
10001
```

The few data points above weakly hint at raidz, since no one was able to reproduce on mirrors so far.",2018-04-07T17:39:39Z,245024
866,openzfs/zfs,312114336,379490740,"On one of my pools this works fine, on another it exhibits the problems.  Both datasets belong to the same pool.

bash-4.2$ mkdir SRC
bash-4.2$ for i in $(seq 1 10000); do echo $i > SRC/$i ; done
bash-4.2$ cp -r SRC DST
cp: cannot create regular file ‘DST/222’: No space left on device
cp: cannot create regular file ‘DST/6950’: No space left on device

On beast/engineering the above commands run without issue.  On beast/dataio they fail.

```
bash-4.2$ zfs get all beast/engineering
NAME               PROPERTY               VALUE                  SOURCE
beast/engineering  type                   filesystem             -
beast/engineering  creation               Sun Nov  5 17:53 2017  -
beast/engineering  used                   1.85T                  -
beast/engineering  available              12.0T                  -
beast/engineering  referenced             1.85T                  -
beast/engineering  compressratio          1.04x                  -
beast/engineering  mounted                yes                    -
beast/engineering  quota                  none                   default
beast/engineering  reservation            none                   default
beast/engineering  recordsize             1M                     inherited from beast
beast/engineering  mountpoint             /beast/engineering     default
beast/engineering  sharenfs               on                     inherited from beast
beast/engineering  checksum               on                     default
beast/engineering  compression            lz4                    inherited from beast
beast/engineering  atime                  off                    inherited from beast
beast/engineering  devices                on                     default
beast/engineering  exec                   on                     default
beast/engineering  setuid                 on                     default
beast/engineering  readonly               off                    default
beast/engineering  zoned                  off                    default
beast/engineering  snapdir                hidden                 default
beast/engineering  aclinherit             restricted             default
beast/engineering  createtxg              20615173               -
beast/engineering  canmount               on                     default
beast/engineering  xattr                  sa                     inherited from beast
beast/engineering  copies                 1                      default
beast/engineering  version                5                      -
beast/engineering  utf8only               off                    -
beast/engineering  normalization          none                   -
beast/engineering  casesensitivity        sensitive              -
beast/engineering  vscan                  off                    default
beast/engineering  nbmand                 off                    default
beast/engineering  sharesmb               off                    inherited from beast
beast/engineering  refquota               none                   default
beast/engineering  refreservation         none                   default
beast/engineering  guid                   18311947624891459017   -
beast/engineering  primarycache           metadata               local
beast/engineering  secondarycache         all                    default
beast/engineering  usedbysnapshots        151M                   -
beast/engineering  usedbydataset          1.85T                  -
beast/engineering  usedbychildren         0B                     -
beast/engineering  usedbyrefreservation   0B                     -
beast/engineering  logbias                latency                default
beast/engineering  dedup                  off                    default
beast/engineering  mlslabel               none                   default
beast/engineering  sync                   disabled               inherited from beast
beast/engineering  dnodesize              auto                   inherited from beast
beast/engineering  refcompressratio       1.04x                  -
beast/engineering  written                0                      -
beast/engineering  logicalused            1.92T                  -
beast/engineering  logicalreferenced      1.92T                  -
beast/engineering  volmode                default                default
beast/engineering  filesystem_limit       none                   default
beast/engineering  snapshot_limit         none                   default
beast/engineering  filesystem_count       none                   default
beast/engineering  snapshot_count         none                   default
beast/engineering  snapdev                hidden                 default
beast/engineering  acltype                posixacl               inherited from beast
beast/engineering  context                none                   default
beast/engineering  fscontext              none                   default
beast/engineering  defcontext             none                   default
beast/engineering  rootcontext            none                   default
beast/engineering  relatime               off                    default
beast/engineering  redundant_metadata     all                    default
beast/engineering  overlay                off                    default
beast/engineering  com.sun:auto-snapshot  true                   inherited from beast
```

```
bash-4.2$ zfs get all beast/dataio
NAME          PROPERTY               VALUE                  SOURCE
beast/dataio  type                   filesystem             -
beast/dataio  creation               Fri Oct 13 11:13 2017  -
beast/dataio  used                   45.0T                  -
beast/dataio  available              12.0T                  -
beast/dataio  referenced             45.0T                  -
beast/dataio  compressratio          1.09x                  -
beast/dataio  mounted                yes                    -
beast/dataio  quota                  none                   default
beast/dataio  reservation            none                   default
beast/dataio  recordsize             1M                     inherited from beast
beast/dataio  mountpoint             /beast/dataio          default
beast/dataio  sharenfs               on                     inherited from beast
beast/dataio  checksum               on                     default
beast/dataio  compression            lz4                    inherited from beast
beast/dataio  atime                  off                    inherited from beast
beast/dataio  devices                on                     default
beast/dataio  exec                   on                     default
beast/dataio  setuid                 on                     default
beast/dataio  readonly               off                    default
beast/dataio  zoned                  off                    default
beast/dataio  snapdir                hidden                 default
beast/dataio  aclinherit             restricted             default
beast/dataio  createtxg              19156147               -
beast/dataio  canmount               on                     default
beast/dataio  xattr                  sa                     inherited from beast
beast/dataio  copies                 1                      default
beast/dataio  version                5                      -
beast/dataio  utf8only               off                    -
beast/dataio  normalization          none                   -
beast/dataio  casesensitivity        sensitive              -
beast/dataio  vscan                  off                    default
beast/dataio  nbmand                 off                    default
beast/dataio  sharesmb               off                    inherited from beast
beast/dataio  refquota               none                   default
beast/dataio  refreservation         none                   default
beast/dataio  guid                   7216940837685529084    -
beast/dataio  primarycache           all                    default
beast/dataio  secondarycache         all                    default
beast/dataio  usedbysnapshots        0B                     -
beast/dataio  usedbydataset          45.0T                  -
beast/dataio  usedbychildren         0B                     -
beast/dataio  usedbyrefreservation   0B                     -
beast/dataio  logbias                latency                default
beast/dataio  dedup                  off                    default
beast/dataio  mlslabel               none                   default
beast/dataio  sync                   disabled               inherited from beast
beast/dataio  dnodesize              auto                   inherited from beast
beast/dataio  refcompressratio       1.09x                  -
beast/dataio  written                45.0T                  -
beast/dataio  logicalused            49.3T                  -
beast/dataio  logicalreferenced      49.3T                  -
beast/dataio  volmode                default                default
beast/dataio  filesystem_limit       none                   default
beast/dataio  snapshot_limit         none                   default
beast/dataio  filesystem_count       none                   default
beast/dataio  snapshot_count         none                   default
beast/dataio  snapdev                hidden                 default
beast/dataio  acltype                posixacl               inherited from beast
beast/dataio  context                none                   default
beast/dataio  fscontext              none                   default
beast/dataio  defcontext             none                   default
beast/dataio  rootcontext            none                   default
beast/dataio  relatime               off                    default
beast/dataio  redundant_metadata     all                    default
beast/dataio  overlay                off                    default
beast/dataio  com.sun:auto-snapshot  false                  local
```",2018-04-07T18:48:02Z,625982
867,openzfs/zfs,312114336,379491047,I think the issue is related to primarycache=all.  If I set a pool to have primarycache=metadata there are no errors.,2018-04-07T18:52:59Z,625982
868,openzfs/zfs,312114336,379492172,"@rblank I replicated the issue with a simple, single-vdev pool. I'll try and report back with mirror, anyway.

@alatteri What pool/vdev layout do you use? Can you show `zpool status` on both machines? I tried with primarycache=none and it failed, albeit with much lower frequency (ie: it failed after the 5th copy). I'll try with primarycache=metadata.",2018-04-07T19:10:31Z,13137622
869,openzfs/zfs,312114336,379492263,"Same machine, different datasets on the same pool.

```
beast: /nfs/beast/home/alan % zpool status
  pool: beast
 state: ONLINE
  scan: scrub canceled on Fri Mar  2 16:47:01 2018
config:

	NAME                                   STATE     READ WRITE CKSUM
	beast                                  ONLINE       0     0     0
	  raidz2-0                             ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NAHN5M1X  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NAHN5NPX  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NAHNP9BX  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NAHN6M4Y  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NAHNPBLX  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NAHKY7PX  ONLINE       0     0     0
	  raidz2-1                             ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NCG1G8SL  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NCG1BVVL  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NCG13K0L  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NCG1GA9L  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NCG1G9YL  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NCG6D9ZS  ONLINE       0     0     0
	  raidz2-2                             ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NCG68U3S  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NCG2WW7S  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NAHMHVGY  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NAHKRYUX  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NAHKXMKX  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NCG5ZYKS  ONLINE       0     0     0
	  raidz2-3                             ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NCGSM01S  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NCGSY9HS  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NCGTHJUS  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NCGTKV1S  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NCGTMN4S  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NCGTGTLS  ONLINE       0     0     0
	  raidz2-4                             ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NCGTKUWS  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NCGTG3YS  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NCGTLYZS  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NCGSZ2GS  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NCGSV93S  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE610_NCGT04NS  ONLINE       0     0     0
	  raidz2-5                             ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1HHZGSB  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1GTE6HD  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1GU06VD  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1GS5KNF  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_NCHA3DZS  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_NCHAE5JS  ONLINE       0     0     0
	  raidz2-6                             ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1HJ21DB  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_NCH9WUXS  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_NCHAXNTS  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_NCHA0DLS  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1HJG72B  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1HHX19B  ONLINE       0     0     0
	cache
	  nvme0n1                              ONLINE       0     0     0

errors: No known data errors

  pool: pimplepaste
 state: ONLINE
  scan: scrub repaired 0B in 2h38m with 0 errors on Mon Mar 19 00:17:45 2018
config:

	NAME                                   STATE     READ WRITE CKSUM
	pimplepaste                            ONLINE       0     0     0
	  raidz2-0                             ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1JVHTBD  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1JVHVSD  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1JVHT1D  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1HUYA5D  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1JVDPMD  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1HZAZDD  ONLINE       0     0     0
	  raidz2-1                             ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1JVATKD  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1HZB0ND  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1HY6LYD  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1JT32KD  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1JVAGVD  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1HZBL5D  ONLINE       0     0     0
	  raidz2-2                             ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1HWZ1AD  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1HZAYJD  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1HZ8YMD  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1JVDN8D  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1HZAKPD  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1HWZ2ZD  ONLINE       0     0     0
	  raidz2-3                             ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1HZAX7D  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1JVHD8D  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1JVG6ND  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1HW7VBD  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1HZBHMD  ONLINE       0     0     0
	    ata-HGST_HDN726060ALE614_K1JVB2SD  ONLINE       0     0     0

errors: No known data errors
```
",2018-04-07T19:12:04Z,625982
870,openzfs/zfs,312114336,379492879,"@vbrik what's the HW config of this system - how much RAM, what model of x86_64 CPU?",2018-04-07T19:21:10Z,214141
871,openzfs/zfs,312114336,379495948,"I can confirm this bug on a mirrored zpool. It is a production system so I didn't do much testing before downgrading to 0.7.6:

```
pool: ssdzfs-array
state: ONLINE
status: Some supported features are not enabled on the pool. The pool can
	still be used, but some features are unavailable. [it is at the 0.6.5.11 features level]
action: Enable all features using 'zpool upgrade'. Once this is done,
	the pool may no longer be accessible by software that does not support
	the features. See zpool-features(5) for details.
  scan: scrub repaired 0B in 0h16m with 0 errors on Sun Apr  1 01:46:59 2018
config:

	NAME                                     STATE     READ WRITE CKSUM
	ssdzfs-array                             ONLINE       0     0     0
	  mirror-0                               ONLINE       0     0     0
	    ata-XXXX-enc  ONLINE       0     0     0
	    ata-YYYY-enc  ONLINE       0     0     0
	  mirror-1                               ONLINE       0     0     0
	    ata-ZZZZ-enc  ONLINE       0     0     0
	    ata-QQQQ-enc  ONLINE       0     0     0

errors: No known data errors
$zfs create ssdzfs-array/tmp
$(run test as previously described; fails about 1/2 the time)
$uname -a
Linux MASKED 3.10.0-693.21.1.el7.x86_64 #1 SMP Wed Mar 7 19:03:37 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
```

I have attempted to reproduce the bug on 0.7.6 without success. Here is an except of one of the processor feature levels:
```
processor	: 3
vendor_id	: GenuineIntel
cpu family	: 6
model		: 26
model name	: Intel(R) Core(TM) i7 CPU         920  @ 2.67GHz
stepping	: 5
microcode	: 0x19
cpu MHz		: 1600.000
cache size	: 8192 KB
physical id	: 0
siblings	: 4
core id		: 3
cpu cores	: 4
apicid		: 6
initial apicid	: 6
fpu		: yes
fpu_exception	: yes
cpuid level	: 11
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida
bogomips	: 5333.51
clflush size	: 64
cache_alignment	: 64
address sizes	: 36 bits physical, 48 bits virtual
power management:
[    1.121288] microcode: CPU3 sig=0x106a5, pf=0x2, revision=0x19
```
",2018-04-07T20:10:12Z,27139960
872,openzfs/zfs,312114336,379501782,"I still get it with primarycache=metadata, on the first attempt to cp:
`[root@zirconia ~]# zfs set primarycache=metadata storage/rhev
[root@zirconia ~]# cd /storage/rhev/
[root@zirconia rhev]# ls
export  test
[root@zirconia rhev]# cd test/
[root@zirconia test]# rm -rf DST
[root@zirconia test]# rm -rf SRC/*
[root@zirconia test]# for i in $(seq 1 10000); do echo $i > SRC/$i ; done
[root@zirconia test]# cp -r SRC DST
cp: cannot create regular file ‘DST/5269’: No space left on device
cp: cannot create regular file ‘DST/3759’: No space left on device
`",2018-04-07T21:51:36Z,1042393
873,openzfs/zfs,312114336,379506938,For those that have upgraded to the 0.7.7 branch - is it advisable to downgrade back to 0.7.6 until this regression is resolved?,2018-04-07T23:39:25Z,4956234
874,openzfs/zfs,312114336,379509313,What is the procedure to downgrade ZFS on CentOS 7.4?,2018-04-08T00:34:10Z,625982
875,openzfs/zfs,312114336,379514720,"For reverts, I usually do:
```
$ yum history  (identify transaction that installed 0.7.7 over 0.7.6; yum history info XXX can be used to confirm)
$ yum history undo XXX (where XXX is the transaction number identified in the previous step)
```

Note that with dkms installs, after reverts, I usually find I need to:
```
$ dkms remove zfs/0.7.6 -k `uname -r`
$ dkms remove spl/0.7.6 -k `uname -r`
$ dkms install spl/0.7.6 -k `uname -r` --force
$ dkms install zfs/0.7.6 -k `uname -r` --force
```
To make sure all modules are actually happy and loadable on reboot.",2018-04-08T02:39:26Z,27139960
876,openzfs/zfs,312114336,379531805,Is this seen with rsync instead of cp?,2018-04-08T08:36:33Z,4440028
877,openzfs/zfs,312114336,379547362,"I'm not able to reproduce this, and I have several machines (Debian unstable; 0.7.7, Linux 4.15). Can people also include `uname -srvmo`? Maybe the kernel version is playing a role?

```Linux 4.15.0-2-amd64 #1 SMP Debian 4.15.11-1 (2018-03-20) x86_64 GNU/Linux```
",2018-04-08T12:38:28Z,6502699
878,openzfs/zfs,312114336,379550690,"Ok, I've done some more tests.
System is CentOS 7.4 x86-64 with latest available kernel:

- single vdev pool: reproduced
- mirrored pool: reproduced
- kmod and dkms: reproduced
- compiled from source [1]: reproduced
- compression lz4 and off: reproduced
- primary cache all, metadata and none: reproduced

On a Ubuntu Server 16.04 LTS with compiled 0.7.7 spl+zfs (so not using the repository version), I can **not** reproduce the error. As a side note, compiling on Ubuntu does not give any warning.

So, the problem seems confined in CentOS/RHEL territory. To me, it seems a timing/racing problem (possibly related to the ARC): anything which increases copy time lowers the error probability/frequency. Some example of action which lower the fail rate:

- `cp -a` (it copies file attributes)
- disabling cache
- copy from SRC on another filesystem (eg: root XFS). Note: this seems to *completely* avoid the problem.

[1] compilation give the following warning:
```
/usr/src/zfs-0.7.7/module/zcommon/zfs_fletcher_avx512.o: warning: objtool: fletcher_4_avx512f_byteswap()+0x4e: can't find jump dest instruction at .text+0x171
/usr/src/zfs-0.7.7/module/zfs/vdev_raidz_math_avx512f.o: warning: objtool: mul_x2_2()+0x24: can't find jump dest instruction at .text+0x39
/usr/src/zfs-0.7.7/module/zfs/vdev_raidz_math_avx512bw.o: warning: objtool: raidz_zero_abd_cb()+0x33: can't find jump dest instruction at .text+0x3d
```",2018-04-08T13:28:57Z,13137622
879,openzfs/zfs,312114336,379551270,"~~@shodanshok I'm sorry, I'm having a lot of trouble tracking this piece of information down. What Linux kernel version is Centos 7.4 on?~~ I assume this is with `kernel-3.10.0-693.21.1.el7.x86_64`.

Is anyone experiencing this issue with ""recent"" mainline kernels (like 4.x)?",2018-04-08T13:37:26Z,6502699
880,openzfs/zfs,312114336,379552009,"Greetings,
I have mirrors with the same problem.
Scientific Linux 7.4 (fully updated)
zfs-0.7.7 from zfsonlinux.org repos
```
$ uname -srvmo
Linux 3.10.0-693.21.1.el7.x86_64 #1 SMP Wed Mar 7 13:12:24 CST 2018 x86_64 GNU/Linux
```

The output of my yum install:
```
Running transaction
  Installing : kernel-devel-3.10.0-693.21.1.el7.x86_64                                                                         1/10 
  Installing : kernel-headers-3.10.0-693.21.1.el7.x86_64                                                                       2/10 
  Installing : glibc-headers-2.17-196.el7_4.2.x86_64                                                                           3/10 
  Installing : glibc-devel-2.17-196.el7_4.2.x86_64                                                                             4/10 
  Installing : gcc-4.8.5-16.el7_4.2.x86_64                                                                                     5/10 
  Installing : dkms-2.4.0-1.20170926git959bd74.el7.noarch                                                                      6/10 
  Installing : spl-dkms-0.7.7-1.el7_4.noarch                                                                                   7/10 
Loading new spl-0.7.7 DKMS files...
Building for 3.10.0-693.21.1.el7.x86_64
Building initial module for 3.10.0-693.21.1.el7.x86_64
Done.

spl:
Running module version sanity check.
 - Original module
   - No original module exists within this kernel
 - Installation
   - Installing to /lib/modules/3.10.0-693.21.1.el7.x86_64/extra/spl/spl/

splat.ko:
Running module version sanity check.
 - Original module
   - No original module exists within this kernel
 - Installation
   - Installing to /lib/modules/3.10.0-693.21.1.el7.x86_64/extra/splat/splat/
Adding any weak-modules

depmod....

DKMS: install completed.
  Installing : zfs-dkms-0.7.7-1.el7_4.noarch                                                                                   8/10 
Loading new zfs-0.7.7 DKMS files...
Building for 3.10.0-693.21.1.el7.x86_64
Building initial module for 3.10.0-693.21.1.el7.x86_64
Done.

zavl:
Running module version sanity check.
 - Original module
   - No original module exists within this kernel
 - Installation
   - Installing to /lib/modules/3.10.0-693.21.1.el7.x86_64/extra/avl/avl/

znvpair.ko:
Running module version sanity check.
 - Original module
   - No original module exists within this kernel
 - Installation
   - Installing to /lib/modules/3.10.0-693.21.1.el7.x86_64/extra/nvpair/znvpair/

zunicode.ko:
Running module version sanity check.
 - Original module
   - No original module exists within this kernel
 - Installation
   - Installing to /lib/modules/3.10.0-693.21.1.el7.x86_64/extra/unicode/zunicode/

zcommon.ko:
Running module version sanity check.
 - Original module
   - No original module exists within this kernel
 - Installation
   - Installing to /lib/modules/3.10.0-693.21.1.el7.x86_64/extra/zcommon/zcommon/

zfs.ko:
Running module version sanity check.
 - Original module
   - No original module exists within this kernel
 - Installation
   - Installing to /lib/modules/3.10.0-693.21.1.el7.x86_64/extra/zfs/zfs/

zpios.ko:
Running module version sanity check.
 - Original module
   - No original module exists within this kernel
 - Installation
   - Installing to /lib/modules/3.10.0-693.21.1.el7.x86_64/extra/zpios/zpios/

icp.ko:
Running module version sanity check.
 - Original module
   - No original module exists within this kernel
 - Installation
   - Installing to /lib/modules/3.10.0-693.21.1.el7.x86_64/extra/icp/icp/
Adding any weak-modules

depmod....

DKMS: install completed.
  Installing : spl-0.7.7-1.el7_4.x86_64                                                                                        9/10 
  Installing : zfs-0.7.7-1.el7_4.x86_64                                                                                       10/10 
  Verifying  : dkms-2.4.0-1.20170926git959bd74.el7.noarch                                                                      1/10 
  Verifying  : zfs-dkms-0.7.7-1.el7_4.noarch                                                                                   2/10 
  Verifying  : zfs-0.7.7-1.el7_4.x86_64                                                                                        3/10 
  Verifying  : spl-0.7.7-1.el7_4.x86_64                                                                                        4/10 
  Verifying  : kernel-devel-3.10.0-693.21.1.el7.x86_64                                                                         5/10 
  Verifying  : glibc-devel-2.17-196.el7_4.2.x86_64                                                                             6/10 
  Verifying  : kernel-headers-3.10.0-693.21.1.el7.x86_64                                                                       7/10 
  Verifying  : gcc-4.8.5-16.el7_4.2.x86_64                                                                                     8/10 
  Verifying  : spl-dkms-0.7.7-1.el7_4.noarch                                                                                   9/10 
  Verifying  : glibc-headers-2.17-196.el7_4.2.x86_64                                                                          10/10 

Installed:
  zfs.x86_64 0:0.7.7-1.el7_4                                                                                                        

Dependency Installed:
  dkms.noarch 0:2.4.0-1.20170926git959bd74.el7                      gcc.x86_64 0:4.8.5-16.el7_4.2                                   
  glibc-devel.x86_64 0:2.17-196.el7_4.2                             glibc-headers.x86_64 0:2.17-196.el7_4.2                         
  kernel-devel.x86_64 0:3.10.0-693.21.1.el7                         kernel-headers.x86_64 0:3.10.0-693.21.1.el7                     
  spl.x86_64 0:0.7.7-1.el7_4                                        spl-dkms.noarch 0:0.7.7-1.el7_4                                 
  zfs-dkms.noarch 0:0.7.7-1.el7_4                                  

Complete!
```

I am using rsnapshot to do backups. It is when it runs the equivalent to below that issues come up.
```
$ /usr/bin/cp -al /bkpfs/Rsnapshot/hourly.0 /bkpfs/Rsnapshot/hourly.1
/usr/bin/cp: cannot create hard link ‘/bkpfs/Rsnapshot/hourly.1/System/home/user/filename’ to ‘/bkpfs/Rsnapshot/hourly.0/System/home/user/filename’: No space left on device
```

There's plenty of space
```
$ df -h /bkpfs/
Filesystem      Size  Used Avail Use% Mounted on
bkpfs           5.0T  4.2T  776G  85% /bkpfs
$ df -i /bkpfs/
Filesystem         Inodes   IUsed      IFree IUse% Mounted on
bkpfs          1631487194 5614992 1625872202    1% /bkpfs
```

```
zpool iostat -v bkpfs
                                                  capacity     operations     bandwidth 
pool                                            alloc   free   read  write   read  write
----------------------------------------------  -----  -----  -----  -----  -----  -----
bkpfs                                           4.52T   950G      9      5  25.4K   117K
  mirror                                        1.84T   912G      4      3  22.0K  94.7K
    ata-Hitachi_HUA723030ALA640                     -      -      2      1  11.2K  47.4K
    ata-Hitachi_HUA723030ALA640                     -      -      2      1  10.8K  47.4K
  mirror                                        2.68T  37.3G      4      2  3.46K  22.2K
    ata-Hitachi_HUA723030ALA640                     -      -      2      1  1.71K  11.1K
    ata-Hitachi_HUA723030ALA640                     -      -      2      1  1.75K  11.1K
cache                                               -      -      -      -      -      -
  ata-INTEL_SSDSC2BW120H6                       442M   111G     17      0  9.48K  10.0K
----------------------------------------------  -----  -----  -----  -----  -----  -----
```

```
zpool status
  pool: bkpfs
 state: ONLINE
  scan: scrub repaired 0B in 11h17m with 0 errors on Sun Apr  1 05:34:09 2018
config:

	NAME                                            STATE     READ WRITE CKSUM
	bkpfs                                           ONLINE       0     0     0
	  mirror-0                                      ONLINE       0     0     0
	    ata-Hitachi_HUA723030ALA640                 ONLINE       0     0     0
	    ata-Hitachi_HUA723030ALA640                 ONLINE       0     0     0
	  mirror-1                                      ONLINE       0     0     0
	    ata-Hitachi_HUA723030ALA640                 ONLINE       0     0     0
	    ata-Hitachi_HUA723030ALA640                 ONLINE       0     0     0
	cache
	  ata-INTEL_SSDSC2BW120H6                      ONLINE       0     0     0

errors: No known data errors
```
```
zfs get all bkpfs
NAME   PROPERTY              VALUE                  SOURCE
bkpfs  type                  filesystem             -
bkpfs  creation              Fri Dec 22 10:34 2017  -
bkpfs  used                  4.52T                  -
bkpfs  available             776G                   -
bkpfs  referenced            4.19T                  -
bkpfs  compressratio         1.00x                  -
bkpfs  mounted               yes                    -
bkpfs  quota                 none                   default
bkpfs  reservation           none                   default
bkpfs  recordsize            128K                   default
bkpfs  mountpoint            /bkpfs                 default
bkpfs  sharenfs              off                    default
bkpfs  checksum              on                     default
bkpfs  compression           off                    default
bkpfs  atime                 on                     default
bkpfs  devices               on                     default
bkpfs  exec                  on                     default
bkpfs  setuid                on                     default
bkpfs  readonly              off                    default
bkpfs  zoned                 off                    default
bkpfs  snapdir               hidden                 default
bkpfs  aclinherit            restricted             default
bkpfs  createtxg             1                      -
bkpfs  canmount              on                     default
bkpfs  xattr                 on                     default
bkpfs  copies                1                      default
bkpfs  version               5                      -
bkpfs  utf8only              off                    -
bkpfs  normalization         none                   -
bkpfs  casesensitivity       sensitive              -
bkpfs  vscan                 off                    default
bkpfs  nbmand                off                    default
bkpfs  sharesmb              off                    default
bkpfs  refquota              none                   default
bkpfs  refreservation        none                   default
bkpfs  guid                  8662648373298485368    -
bkpfs  primarycache          all                    default
bkpfs  secondarycache        all                    default
bkpfs  usedbysnapshots       334G                   -
bkpfs  usedbydataset         4.19T                  -
bkpfs  usedbychildren        234M                   -
bkpfs  usedbyrefreservation  0B                     -
bkpfs  logbias               latency                default
bkpfs  dedup                 off                    default
bkpfs  mlslabel              none                   default
bkpfs  sync                  standard               default
bkpfs  dnodesize             legacy                 default
bkpfs  refcompressratio      1.00x                  -
bkpfs  written               1.38T                  -
bkpfs  logicalused           4.51T                  -
bkpfs  logicalreferenced     4.18T                  -
bkpfs  volmode               default                default
bkpfs  filesystem_limit      none                   default
bkpfs  snapshot_limit        none                   default
bkpfs  filesystem_count      none                   default
bkpfs  snapshot_count        none                   default
bkpfs  snapdev               hidden                 default
bkpfs  acltype               off                    default
bkpfs  context               none                   default
bkpfs  fscontext             none                   default
bkpfs  defcontext            none                   default
bkpfs  rootcontext           none                   default
bkpfs  relatime              off                    default
bkpfs  redundant_metadata    all                    default
bkpfs  overlay               off                    default
```

For those that want to know my hardware, the system is a AMD X2 255 processor with 8GB of memory (so far more than enough for my home backup system).

I can revert today, or I can help test if someone needs me to try something. Just let me know.

Thanks!",2018-04-08T13:48:25Z,1934264
881,openzfs/zfs,312114336,379552574,Can someone who can repro this try bisecting the changes between 0.7.6 and 0.7.7 so we can see which commit breaks people?,2018-04-08T13:56:24Z,214141
882,openzfs/zfs,312114336,379552932,"Most likely https://github.com/zfsonlinux/zfs/commit/cc63068e95ee725cce03b1b7ce50179825a6cda5, seems to be a race condition in the mzap->fzap upgrade phase.",2018-04-08T14:01:32Z,4585738
883,openzfs/zfs,312114336,379553111,"@loli10K this, uh, seems horrendous enough that unless someone volunteers a fix for the race Real Fast, a revert and cutting a point release for this alone seems like it would be merited, to me at least.",2018-04-08T14:04:11Z,214141
884,openzfs/zfs,312114336,379553197,"@rincebrain I can try later today. I'm meeting some friends for lunch and will be gone for a few hours but I'm happy to help how I can when I get back.
[Edit] To try to bisect the changes that is. :-)",2018-04-08T14:05:22Z,1934264
885,openzfs/zfs,312114336,379553376,"@cstackpole if you do, it's probably worth trying with and without the commit @loli10K pointed to, rather than letting the bisect naturally find it.",2018-04-08T14:07:59Z,214141
886,openzfs/zfs,312114336,379553960,From what we have seen so far it certainly seems to only affect older (by which I mean lower-versioned) kernels. I have not been able to reproduce the issue on Linux 4.15 (Fedora).,2018-04-08T14:17:23Z,173088
887,openzfs/zfs,312114336,379558073,"@aerusso 
```
[root@localhost test]# uname -a
Linux localhost.localdomain 3.10.0-693.21.1.el7.x86_64 #1 SMP Wed Mar 7 19:03:37 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

```

@loli10K any clue on why it affect 3.x kernels only, while 4.x seems immune?",2018-04-08T15:18:02Z,13137622
888,openzfs/zfs,312114336,379559299," BTW, I bisected it, and couldn't repro it on CentOS 7 with 3.10.0-693.21.1 on eb9c453 but could on cc63068, so that does appear to be the cause.",2018-04-08T15:35:13Z,214141
889,openzfs/zfs,312114336,379559840,"I haven't done any testing yet, but I very much appreciate the speed at which you've found the commit, rincebrain! Since seeing this issue raised, I've been quite nervous, and I don't yet know if I'm affected.",2018-04-08T15:43:04Z,4440028
890,openzfs/zfs,312114336,379565498,"% uname -srvmo
Linux 3.10.0-693.21.1.el7.x86_64 #1 SMP Wed Mar 7 19:03:37 UTC 2018 x86_64 GNU/Linux",2018-04-08T17:04:41Z,625982
891,openzfs/zfs,312114336,379573246,"Since this seems to be FRAME_POINTER-specific (unless anyone's got a counter-example), I would guess this is #5041 2.0: Elec boogalootric",2018-04-08T18:54:10Z,214141
892,openzfs/zfs,312114336,379573877,"Thanks @rincebrain for confirming!
Since this is just my personal-at-home system, I don't mind leaving it in its reproducible state if anyone wants me to test something later in the week.",2018-04-08T19:02:04Z,1934264
893,openzfs/zfs,312114336,379601501,"@kpande Yes, I've been following this one but haven't looked into it at all.  Has this for sure been narrowed down to cc63068e95ee725cce03b1b7ce50179825a6cda5?  This is clearly something that has to get fixed right away.",2018-04-09T01:25:50Z,1432005
894,openzfs/zfs,312114336,379602540,"@dweeezil I couldn't readily repro it on CentOS 7 x86_64 on the commit before cc63068, and could easily repro it on cc63068, same SPL both times.",2018-04-09T01:35:37Z,214141
895,openzfs/zfs,312114336,379602972,"cc63068 sets a limit on the number of times `zap_add` will try to expand (split) a zap leaf blocks for a directory when adding a new entry would overflow an existing leaf.

The limit (2) is sufficient for handling a colliding name when `casesensitivity=sensitive` is set, but appears to bails out too early (with `ENOSPC`) when the zap for the directory grows past a certain size (possibly also due to leaf hash collisions). When `zap_add` fails, it rolls back the transaction so the znode for the new file is removed.

So far, this is undesirable but doesn't result in data loss per se, since the system just refuses to create new files with ""No space left on device"".

My hypothesis is that a subsequent `zap_add`s is successful as the directory's zap has already grown (as long as one to two additional leaf splits is sufficient to fit the new entry), but the subsequent zap expansions are being discarded, due to a side effect of the previous rollback (possibly closing the transaction there). The vfs page cache still reflects the new files but they're not present in ARC (or committed to disk), hence flushing the page cache makes them go away. It's not clear if the znodes for the files are leaked as a result (unlinked from the directory but still present) or if they're also being discarded.",2018-04-09T01:39:56Z,207920
896,openzfs/zfs,312114336,379621112,"I have masked 0.7.7 in Gentoo based on this issue.

https://bugs.gentoo.org/652828

I have cleared my schedule for tomorrow so that I have time to spend on this. I'd say more, but this blind sided me and it is too late at night for me to start looking into it now.",2018-04-09T03:42:19Z,1386373
897,openzfs/zfs,312114336,379623951,"Ok, so the expand retry limit 2 is not enough. In fact, there shouldn't be a limit at all until we hit the limit of ZAP itself.

The reason you can create a ZAP with a lot of file but cannot copy is because, when you create file, you create file randomly in terms of hash value. However, if you copy files from one directory to another directory, you create file sequentially in terms of hash value. That means if the source directory expanded its leaves 6 times, you need to expand the destination leaves 6 times in one go.

One thing to note is that we do use different salt for different directory, so theoretically, a strong enough salt should prevent this from happening. This shows that the current salt is not strong enough.

To remove the expand limit, try removing this if block.
https://github.com/zfsonlinux/zfs/blob/cc63068e95ee725cce03b1b7ce50179825a6cda5/module/zfs/zap.c#L861

The file missing afterward is a strange issue. I'll have to investigate to see what happened. I don't think there's any transaction rollback in the error path.",2018-04-09T04:02:17Z,119672
898,openzfs/zfs,312114336,379624619,"Getting rid of the limit doesn't panic the box when running the `casenorm` ZTS group and seems to prevent this issue:

```diff
@@ -855,15 +855,6 @@ retry:
        if (err == 0) {
                zap_increment_num_entries(zap, 1, tx);
        } else if (err == EAGAIN) {
-               /*
-                * If the last two expansions did not help, there is no point
-                * trying to expand again
-                */
-               if (expand_retries > MAX_EXPAND_RETRIES && prev_l == l) {
-                       err = SET_ERROR(ENOSPC);
-                       goto out;
-               }
-
                err = zap_expand_leaf(zn, l, tag, tx, &l);
                zap = zn->zn_zap;       /* zap_expand_leaf() may change zap */
                if (err == 0) {
```

```
[root@centos ~]# lsb_release -a
LSB Version:	:core-4.1-amd64:core-4.1-noarch
Distributor ID:	CentOS
Description:	CentOS Linux release 7.4.1708 (Core) 
Release:	7.4.1708
Codename:	Core
[root@centos ~]# uname -r
3.10.0-693.21.1.el7.x86_64
[root@centos ~]# cat /sys/module/zfs/version 
0.7.7-1
[root@centos ~]# while :; do
>    zpool destroy testpool
>    zpool create testpool -f -O xattr=dir -O atime=off -O mountpoint=none -O recordsize=1M /dev/vdb
>    zfs create testpool/src -o mountpoint=/mnt
>    zfs create testpool/dst -o mountpoint=/mnt/DST
>    mkdir /mnt/SRC; for i in $(seq 1 10000); do echo -n > /mnt/SRC/$i; done;
>    printf ""$(find /mnt/SRC -type f | wc -l) -> ""
>    cp -r /mnt/SRC /mnt/DST
>    echo ""$(find /mnt/DST -type f | wc -l)""
> done
10000 -> 10000
10000 -> 10000
10000 -> 10000
10000 -> 10000
10000 -> 10000
10000 -> 10000
10000 -> 10000
10000 -> 10000
10000 -> 10000
10000 -> 10000
10000 -> 10000
10000 -> 10000
10000 -> 10000
10000 -> 10000
10000 -> 10000
^C
[root@centos ~]#
...
[root@centos ~]# sudo -u nobody -s /usr/share/zfs/zfs-tests.sh -d /var/tmp -T casenorm
Test: /usr/share/zfs/zfs-tests/tests/functional/casenorm/setup (run as root) [00:00] [PASS]
Test: /usr/share/zfs/zfs-tests/tests/functional/casenorm/case_all_values (run as root) [00:00] [PASS]
Test: /usr/share/zfs/zfs-tests/tests/functional/casenorm/norm_all_values (run as root) [00:01] [PASS]
Test: /usr/share/zfs/zfs-tests/tests/functional/casenorm/mixed_create_failure (run as root) [00:10] [PASS]
Test: /usr/share/zfs/zfs-tests/tests/functional/casenorm/cleanup (run as root) [00:00] [PASS]

Results Summary
PASS	   5

Running Time:	00:00:12
Percent passed:	100.0%
Log directory:	/var/tmp/test_results/20180401T016189

[root@centos ~]# 
```

Now testing kernel 3.10.x on Debian 8 with the same Kconfig from previous CentOS7 box ... EDIT: Debian stays strong and does not seem to be affected running 3.10.108.",2018-04-09T04:07:05Z,4585738
899,openzfs/zfs,312114336,379628367,"I can confirm the ENOSPC (No space left on device) is coming from `fzap_add_cd` when we hit the retry limit, running the reproducer under the following stap script:

```c
probe
module(""zfs"").function(""zap_leaf_split"").call,
module(""zfs"").function(""fzap_add_cd"").call,
module(""zfs"").function(""mzap_upgrade"").call,
module(""zfs"").function(""zap_entry_create"").call,
module(""zfs"").function(""zap_expand_leaf"").call
{
   printf("" %s -> %s\n"", symname(caller_addr()), ppfunc());
}
probe
module(""zfs"").function(""zap_leaf_split"").return,
module(""zfs"").function(""fzap_add_cd"").return,
module(""zfs"").function(""mzap_upgrade"").return,
module(""zfs"").function(""zap_entry_create"").return,
module(""zfs"").function(""zap_expand_leaf"").return
{
   printf("" %s <- %s %s\n"", symname(caller_addr()), ppfunc(), $$return$);
}
probe
module(""zfs"").statement(""fzap_add_cd@module/zfs/zap.c:867"")
{
   printf("" * %s <- %s expand_retries=%s\n"", symname(caller_addr()), ppfunc(), $expand_retries$$);
}
````

relevant output
```
 fzap_add_cd -> zap_entry_create
 0xffffffff816b9459 <- zap_entry_create return=11
 * 0xffffffff816b9459 <- fzap_add_cd expand_retries=0
 fzap_add_cd -> zap_expand_leaf
 zap_expand_leaf -> zap_leaf_split
 0xffffffff816b9459 <- zap_leaf_split 
 0xffffffff816b9459 <- zap_expand_leaf return=0
 fzap_add_cd -> zap_entry_create
 0xffffffff816b9459 <- zap_entry_create return=11
 * 0xffffffff816b9459 <- fzap_add_cd expand_retries=1
 fzap_add_cd -> zap_expand_leaf
 zap_expand_leaf -> zap_leaf_split
 0xffffffff816b9459 <- zap_leaf_split 
 0xffffffff816b9459 <- zap_expand_leaf return=0
 fzap_add_cd -> zap_entry_create
 0xffffffff816b9459 <- zap_entry_create return=11
 * 0xffffffff816b9459 <- fzap_add_cd expand_retries=2
 fzap_add_cd -> zap_expand_leaf
 zap_expand_leaf -> zap_leaf_split
 0xffffffff816b9459 <- zap_leaf_split 
 0xffffffff816b9459 <- zap_expand_leaf return=0
 fzap_add_cd -> zap_entry_create
 0xffffffff816b9459 <- zap_entry_create return=11
 * 0xffffffff816b9459 <- fzap_add_cd expand_retries=3
 zap_add_impl <- fzap_add_cd return=28 (ENOSPC)
```",2018-04-09T04:34:29Z,4585738
900,openzfs/zfs,312114336,379641436,"Well, i could not reproduce this running CentOS7 kernel on Debian8 but using its `cp`:

On CentOS7, testing also with `cp` from Debian8:
```
[root@centos ~]# while :; do
>    zpool destroy testpool
>    zpool create testpool -f -O xattr=dir -O atime=off -O mountpoint=none -O recordsize=1M /dev/vdb
>    zfs create testpool/src -o mountpoint=/mnt
>    zfs create testpool/dst -o mountpoint=/mnt/DST
>    mkdir /mnt/SRC; for i in $(seq 1 10000); do echo -n > /mnt/SRC/$i; done;
>    ./debian-cp -r /mnt/SRC /mnt/DST-debian
>    cp -r /mnt/SRC /mnt/DST-centos
> done
cp: cannot create regular file ‘/mnt/DST-centos/4143’: No space left on device
cp: cannot create regular file ‘/mnt/DST-centos/1970’: No space left on device
cp: cannot create regular file ‘/mnt/DST-centos/5654’: No space left on device
cp: cannot create regular file ‘/mnt/DST-centos/5945’: No space left on device
cp: cannot create regular file ‘/mnt/DST-centos/2740’: No space left on device
cp: cannot create regular file ‘/mnt/DST-centos/3659’: No space left on device
cp: cannot create regular file ‘/mnt/DST-centos/2070’: No space left on device
cp: cannot create regular file ‘/mnt/DST-centos/5183’: No space left on device
cp: cannot create regular file ‘/mnt/DST-centos/7715’: No space left on device
cp: cannot create regular file ‘/mnt/DST-centos/8593’: No space left on device
cp: cannot create regular file ‘/mnt/DST-centos/9654’: No space left on device
cp: cannot create regular file ‘/mnt/DST-centos/1064’: No space left on device
cp: cannot create regular file ‘/mnt/DST-centos/2862’: No space left on device
cp: cannot create regular file ‘/mnt/DST-centos/6636’: No space left on device
cp: cannot create regular file ‘/mnt/DST-centos/865’: No space left on device
cp: cannot create regular file ‘/mnt/DST-centos/6090’: No space left on device
cp: cannot create regular file ‘/mnt/DST-centos/6066’: No space left on device
cp: cannot create regular file ‘/mnt/DST-centos/9233’: No space left on device
^C
[root@centos ~]# lsb_release -a
LSB Version:	:core-4.1-amd64:core-4.1-noarch
Distributor ID:	CentOS
Description:	CentOS Linux release 7.4.1708 (Core) 
Release:	7.4.1708
Codename:	Core
[root@centos ~]# rpm -qa coreutils
coreutils-8.22-18.el7.x86_64
[root@centos ~]# 
```

On Debian8, with `cp` from CentOS7:
```
root@linux:~# while :; do
>    zpool destroy testpool
>    zpool create testpool -f -O xattr=dir -O atime=off -O mountpoint=none -O recordsize=1M /dev/vdb
>    zfs create testpool/src -o mountpoint=/mnt
>    zfs create testpool/dst -o mountpoint=/mnt/DST
>    mkdir /mnt/SRC; for i in $(seq 1 10000); do echo -n > /mnt/SRC/$i; done;
>    cp -r /mnt/SRC /mnt/DST-debian
>    ./centos-cp -r /mnt/SRC /mnt/DST-centos
> done
./centos-cp: cannot create regular file ‘/mnt/DST-centos/5423’: No space left on device
./centos-cp: cannot create regular file ‘/mnt/DST-centos/8558’: No space left on device
./centos-cp: cannot create regular file ‘/mnt/DST-centos/4338’: No space left on device
./centos-cp: cannot create regular file ‘/mnt/DST-centos/3524’: No space left on device
./centos-cp: cannot create regular file ‘/mnt/DST-centos/4601’: No space left on device
./centos-cp: cannot create regular file ‘/mnt/DST-centos/9311’: No space left on device
./centos-cp: cannot create regular file ‘/mnt/DST-centos/7348’: No space left on device
./centos-cp: cannot create regular file ‘/mnt/DST-centos/3211’: No space left on device
./centos-cp: cannot create regular file ‘/mnt/DST-centos/8768’: No space left on device
./centos-cp: cannot create regular file ‘/mnt/DST-centos/6951’: No space left on device
./centos-cp: cannot create regular file ‘/mnt/DST-centos/4538’: No space left on device
./centos-cp: cannot create regular file ‘/mnt/DST-centos/7596’: No space left on device
./centos-cp: cannot create regular file ‘/mnt/DST-centos/7539’: No space left on device
^C
root@linux:~# lsb_release -a
No LSB modules are available.
Distributor ID:	Debian
Description:	Debian GNU/Linux 8.0 (jessie)
Release:	8.0
Codename:	jessie
root@linux:~# dpkg -l coreutils
Desired=Unknown/Install/Remove/Purge/Hold
| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend
|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)
||/ Name                                           Version                      Architecture                 Description
+++-==============================================-============================-============================-==================================================================================================
ii  coreutils                                      8.23-4                       amd64                        GNU core utilities
root@linux:~# 
```

We may need to find a better reproducer than ""cp"" for the regression test proposed in #7411.
",2018-04-09T05:56:55Z,4585738
901,openzfs/zfs,312114336,379761845,"This update is still being offered when RHEL-based systems do a ""yum update""; given the serious nature of this bug, should the update not be pulled, leaving 0.7.6 as the latest available version?

Today is a day when I'm EXTREMELY glad I have ZFS/SPL updates blocked and do them manually during designated downtime windows or otherwise more convenient times.",2018-04-09T13:59:15Z,15248607
902,openzfs/zfs,312114336,379821060,"@flynnjFIU @behlendorf is the maintainer for RHEL-based systems and he just got into the office. He likely does not even know about this yet. I'll give him a call to let him know so that he can take the update out of the RPM repository. Thanks for pointing it out.

@perfinion pointed out to me in IRC that it could be that this is mainly reproducible only on RHEL-based systems because they use `xattr=sa` to speed up SELinux's handling of filesystem labels. `xattr=sa` might be related. I had a late start on this today, so I am not certain either way at this point, but I think that he made a good point that the interaction with `xattr=sa` should be considered.",2018-04-09T16:56:46Z,1386373
903,openzfs/zfs,312114336,379824224,@ryao same problem occurs with `xatrr=on`.,2018-04-09T17:07:31Z,22751545
904,openzfs/zfs,312114336,379824235,@flynnjFIU I spoke to Brian. He just learned about this in something like the past hour. The tentative plan is to pull 0.7.7 from the RPM repository and push out 0.7.8 with a revert of cc63068e95ee725cce03b1b7ce50179825a6cda5. He is going to have a chat with @tonyhutter before he finalizes the plan to deal with this.,2018-04-09T17:07:34Z,1386373
905,openzfs/zfs,312114336,379824494,@vbrik Thanks for that information. That helps narrow things down. :),2018-04-09T17:08:30Z,1386373
906,openzfs/zfs,312114336,379826067,@ryao Is there any risk that data created with 0.7.7 on CentOS will be corrupted/disappear with the fix in 0.7.8??,2018-04-09T17:13:37Z,625982
907,openzfs/zfs,312114336,379827550,"@alatteri My tentative understanding is that If ENOSPC did not occur, the data should be fine. I suggest downgrading to 0.7.6 for the time being though.",2018-04-09T17:18:57Z,1386373
908,openzfs/zfs,312114336,379828303,"Would people who can/cannot reproduce this issue post this information about the systems tested?

1. Reproducibility (yes or no)
2. ZoL version
3. Distribution name and version
4. Kernel Version
5. Coreutils Version
6. SELinux status (enforcing, permissive, off/unused)",2018-04-09T17:21:41Z,1386373
909,openzfs/zfs,312114336,379831142,"For those who need them, here are links to the RPM packages for coreutils on CentOS 6 and CentOS 7:

https://centos.pkgs.org/6/centos-x86_64/coreutils-8.4-46.el6.x86_64.rpm.html
https://centos.pkgs.org/7/centos-x86_64/coreutils-8.22-18.el7.x86_64.rpm.html

They contain the `cp` used on CentOS. Instructions on how to extract them are here:

https://www.cyberciti.biz/tips/how-to-extract-an-rpm-package-without-installing-it.html",2018-04-09T17:30:43Z,1386373
910,openzfs/zfs,312114336,379831296,"Compiler: gcc version 6.4.0 (Gentoo Hardened 6.4.0-r1 p1.3)
uname -a: Linux baraddur 4.16.0-gentoo #1 SMP PREEMPT Wed Apr 4 12:18:23 +08 2018 x86_64 AMD Ryzen Threadripper 1950X 16-Core Processor AuthenticAMD GNU/Linux
distro: gentoo hardened selinux
ZFS kmod from HEAD: Loaded module v0.7.0-403_g1724eb62
SELinux enforcing and permissive both hit it

gentoo cp 8.28-r1 binary: cant repro even with 100k files
debian 8 8.26 binary: also cant repro
centos7 8.22 binary: hits it instantly
",2018-04-09T17:31:13Z,363227
911,openzfs/zfs,312114336,379832245,"Reproducibility: yes
ZoL version: zfs-0.7.7-1.el6.x86_64
Distribution name and version: Scientific Linux 6.8
Kernel Version: 2.6.32-696.23.1.el6.x86_64
Coreutils Version: coreutils-8.4-46.el6.x86_64
SELinux status: off",2018-04-09T17:34:24Z,22751545
912,openzfs/zfs,312114336,379833634,"Reproducibility: no

Distribution name and version: Arch Linux
ZoL version:
```
local/spl-linux-git 2018.04.04.r1070.581bc01.4.15.15.1-1 (archzfs-linux-git)
local/spl-utils-common-git 2018.04.04.r1070.581bc01-1 (archzfs-linux-git)
local/zfs-linux-git 2018.04.04.r3402.533ea0415.4.15.15.1-1 (archzfs-linux-git)
local/zfs-utils-common-git 2018.04.04.r3402.533ea0415-1 (archzfs-linux-git)
```
This is ZFS build built from commit 533ea0415 .

Kernel Version: `Linux kiste 4.15.15-1-ARCH #1 SMP PREEMPT Sat Mar 31 23:59:25 UTC 2018 x86_64 GNU/Linux`
Coreutils Version: `local/coreutils 8.29-1`
SELinux status (enforcing, permissive, off/unused): off

Unable to test CentOS 7 `cp` due to dependency on SELinux libraries (Arch doesn't support SELinux).",2018-04-09T17:39:14Z,19325781
913,openzfs/zfs,312114336,379833847,"@tuxoko Nice analysis!
>The reason you can create a ZAP with a lot of file but cannot copy is because, when you create file, you create file randomly in terms of hash value. However, if you copy files from one directory to another directory, you create file sequentially in terms of hash value. That means if the source directory expanded its leaves 6 times, you need to expand the destination leaves 6 times in one go.

>One thing to note is that we do use different salt for different directory, so theoretically, a strong enough salt should prevent this from happening. This shows that the current salt is not strong enough.

The salt is pretty weak (see `mzap_create_impl()`); I'm not sure why we didn't just use `random_get_pseudo_bytes()`.  I wonder if they are actually getting the same exact hash, or if there's some weakness in the way that the salt is used in `zap_hash()`?  zdb can dump the salt to see if they are the same.",2018-04-09T17:39:57Z,799124
914,openzfs/zfs,312114336,379835266,"1. Reproducable = yes
2. zfs.x86_64   0.7.7-1.el7_4     @zfs-kmod
3. CentOS 7.4
4. Linux 3.10.0-693.21.1.el7.x86_64 x86_64 GNU/Linux
5. coreutils.x86_64  8.22-18.el7
6. SELINUX=disabled.  SELINUXTYPE=targeted",2018-04-09T17:44:36Z,625982
915,openzfs/zfs,312114336,379839977,We're working to get an 0.7.8 release out with https://github.com/zfsonlinux/zfs/commit/cc63068e95ee725cce03b1b7ce50179825a6cda5 reverted ASAP.,2018-04-09T17:59:52Z,11469457
916,openzfs/zfs,312114336,379841097,"Before anyone starts `bindiff`ing binaries: CentOS cp `open(O_CREAT)` is randomized, Debian is not: random file order = random hash values = more likely to `zap_expand_leaf()`/`zap_leaf_split()` i guess ...
```
[root@centos ~]# grep DST /tmp/debian.txt | head -n 100
execve(""./debian-cp"", [""./debian-cp"", ""-r"", ""/mnt/SRC"", ""/mnt/DST-debian""], [/* 18 vars */]) = 0
stat(""/mnt/DST-debian"", 0x7ffc25990cb0) = -1 ENOENT (No such file or directory)
lstat(""/mnt/DST-debian"", 0x7ffc25990a40) = -1 ENOENT (No such file or directory)
mkdir(""/mnt/DST-debian"", 0755)          = 0
lstat(""/mnt/DST-debian"", {st_mode=S_IFDIR|0755, st_size=2, ...}) = 0
open(""/mnt/DST-debian/3357"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3358"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3359"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3360"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3361"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3362"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3363"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3364"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3365"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3366"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3367"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3368"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3369"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3370"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3371"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3372"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3373"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3374"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3375"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3376"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3377"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3378"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3379"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3380"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3381"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3382"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3383"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3384"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3385"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3386"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3387"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3388"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3389"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3390"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3391"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3392"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3393"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3394"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3395"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/1"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/2"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/3"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/4"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/5"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/6"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/7"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/8"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/9"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/10"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/11"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/12"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/13"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/14"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/15"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/16"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/17"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/18"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/19"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/20"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/21"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/22"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/23"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/24"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/25"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/26"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/27"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/28"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/29"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/30"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/31"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/32"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/33"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/34"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/35"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/36"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/37"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/38"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/39"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/40"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/41"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/42"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/43"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/44"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/45"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/46"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/47"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/48"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/49"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/50"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/51"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/52"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/53"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/54"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/55"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-debian/56"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
[root@centos ~]# grep DST /tmp/centos.txt | head -n 100
execve(""/bin/cp"", [""cp"", ""-r"", ""/mnt/SRC"", ""/mnt/DST-centos""], [/* 18 vars */]) = 0
stat(""/mnt/DST-centos"", 0x7ffc6299e1d0) = -1 ENOENT (No such file or directory)
lstat(""/mnt/DST-centos"", 0x7ffc6299df30) = -1 ENOENT (No such file or directory)
mkdir(""/mnt/DST-centos"", 0755)          = 0
lstat(""/mnt/DST-centos"", {st_mode=S_IFDIR|0755, st_size=2, ...}) = 0
open(""/mnt/DST-centos/6667"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/4153"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/8772"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/2455"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/8691"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/6784"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/2422"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/8705"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/2878"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/4124"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/6610"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/2558"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/2896"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/2902"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/2975"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/8608"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/4029"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/6689"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/9017"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/5636"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/688"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/1590"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/7102"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/9183"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/1404"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/7096"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/3330"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/3347"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/1473"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/7175"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/5641"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/1829"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/9060"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/611"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/1509"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/1953"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/785"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/7078"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/1924"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/666"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/2065"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/4939"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/4563"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/6257"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/8342"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/8335"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/6220"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/2186"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/4514"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/2012"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/4480"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/2168"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/4834"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/4843"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/8238"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/4419"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/7968"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/3700"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/5392"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/1034"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/9427"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/7532"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/3694"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/5206"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/5271"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/7545"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/9450"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/1043"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/3777"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/3799"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/7865"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/221"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/9970"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/1139"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/256"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/9907"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/7812"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/7448"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/9893"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/7986"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/4944"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/2018"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/4933"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/4569"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/8348"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/4849"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/2115"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/4587"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/8232"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/6327"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/2081"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/4413"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/4464"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/6350"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
open(""/mnt/DST-centos/8245"", O_WRONLY|O_CREAT|O_EXCL, 0644) = 4
[root@centos ~]# 
```",2018-04-09T18:03:33Z,4585738
917,openzfs/zfs,312114336,379841623,"Would someone with a CentOS-family system please install `gdb` and `coreutils-debuginfo`. Then run `gdb -ex 'info sources' $(which cp)` and post the output for me? It will save me some trouble of getting my hands on a system so that I can try to figure out what is different between CentOS's `cp` and Gentoo's `cp`.

I ran this on Gentoo's cp, which is coreutils 8.28 to get the files that were used to build `cp` and after some commandline-foo, I have tentatively identified these as the patches relevant to `cp` on CentOS:

```
./coreutils-selinux.patch:diff -urNp coreutils-8.21-orig/src/copy.c coreutils-8.21/src/copy.c
./coreutils-selinux.patch:diff -urNp coreutils-8.21-orig/src/cp.c coreutils-8.21/src/cp.c
./coreutils-8.22-selinux-optionsseparate.patch:diff -urNp coreutils-8.22-orig/src/cp.c coreutils-8.22/src/cp.c
./coreutils-8.22-mv-hardlinksrace.patch:diff -urNp coreutils-8.22-orig/src/copy.c coreutils-8.22/src/copy.c
./coreutils-8.22-cp-sparsecorrupt.patch:diff --git a/src/copy.c b/src/copy.c
./coreutils-8.22-cp-selinux.patch:diff --git a/src/selinux.c b/src/selinux.c
```

The files that are touched are included.

Unfortunately, the files used between coreutils versions could have changed, so I need to rerun that analysis on the output from a system using CentOS 6 or CentOS 7 to get a true list. I plan to review / test on Gentoo these patches to see if I can track down the issue from the user space side. Enough people are scrutinizing the kernel side that I'll delay tackling that until after I figured out what makes CentOS' `cp` special.",2018-04-09T18:05:15Z,1386373
918,openzfs/zfs,312114336,379842270,I could set up a CentOS 7.4 VM but that could take an hour. Let me know if I should go on or if someone else has a system ready for testing.,2018-04-09T18:07:17Z,19325781
919,openzfs/zfs,312114336,379843055,"On 2018-04-09 14:05, Richard Yao wrote:
> Would someone with a CentOS-family system please install gdb and
> coreutils-debuginfo. Then run gdb -ex 'info sources' $(which cp) and
> post the output for me? It will save me some trouble of getting my
> hands on a system so that I can try to figure out what is different
> between CentOS's cp and Gentoo's cp.
> 
> I ran this on Gentoo's cp, which is coreutils 8.28 to get the files
> that were used to build cp and after some commandline-foo, I have
> tentatively identified these as the patches relevant to cp on CentOS:
> 
> ./coreutils-selinux.patch
> ./coreutils-8.22-selinux-optionsseparate.patch
> ./coreutils-8.22-non-defaulttests.patch
> ./coreutils-8.22-mv-hardlinksrace.patch
> ./coreutils-8.22-failingtests.patch
> ./coreutils-8.22-cp-sparsecorrupt.patch
> ./coreutils-8.22-cp-selinux.patch
> 
> Unfortunately, the files used between coreutils versions could have
> changed, so I need to rerun that analysis on the output from a system
> using CentOS 6 or CentOS 7 to get a true list. I plan to review / test
> on Gentoo these patches to see if I can track down the issue from the
> user space side. Enough people are scrutinizing the kernel side that
> I'll delay tackling that until after I figured out what makes CentOS'
> cp special.

CentOS 7.4:

[root@nas ~]# gdb --ex 'info sources' /usr/bin/cp
GNU gdb (GDB) Red Hat Enterprise Linux 7.6.1-100.el7_4.1
Copyright (C) 2013 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later 
<http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type ""show 
copying""
and ""show warranty"" for details.
This GDB was configured as ""x86_64-redhat-linux-gnu"".
For bug reporting instructions, please see:
<http://www.gnu.org/software/gdb/bugs/>...
Reading symbols from /usr/bin/cp...Reading symbols from 
/usr/bin/cp...(no debugging symbols found)...done.
(no debugging symbols found)...done.
No symbol table is loaded.  Use the ""file"" command.
Missing separate debuginfos, use: debuginfo-install 
coreutils-8.22-18.el7.x86_64

",2018-04-09T18:09:50Z,2582198
920,openzfs/zfs,312114336,379843380,"@dswartz You are missing the debuginfo. Do `debuginfo-install coreutils-8.22-18.el7.x86_64` and try again. Output should look something like this:

https://paste.pound-python.org/raw/xNxJ6p2mHLj3LZVsW4Qr/",2018-04-09T18:10:48Z,1386373
921,openzfs/zfs,312114336,379843455,"
Disregard my last: wrong package...
",2018-04-09T18:11:03Z,2582198
922,openzfs/zfs,312114336,379844883,"Source files for which symbols have been read in:



Source files for which symbols will be read in on demand:

/usr/src/debug/coreutils-8.22/src/cp.c, /usr/include/sys/stat.h, 
/usr/include/bits/string3.h, /usr/include/bits/stdio2.h,
/usr/src/debug/coreutils-8.22/src/system.h, 
/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/stddef.h, 
/usr/include/bits/types.h,
/usr/include/stdio.h, /usr/include/libio.h, /usr/include/sys/types.h, 
/usr/include/time.h, /usr/include/getopt.h,
/usr/include/selinux/selinux.h, /usr/include/bits/stat.h, 
/usr/src/debug/coreutils-8.22/lib/argmatch.h,
/usr/src/debug/coreutils-8.22/lib/hash.h, 
/usr/src/debug/coreutils-8.22/lib/backupfile.h,
/usr/src/debug/coreutils-8.22/src/copy.h, 
/usr/src/debug/coreutils-8.22/lib/stat-time.h,
/usr/src/debug/coreutils-8.22/src/version.h, 
/usr/src/debug/coreutils-8.22/lib/exitfail.h,
/usr/src/debug/coreutils-8.22/lib/progname.h, 
/usr/src/debug/coreutils-8.22/<built-in>,
/usr/src/debug/coreutils-8.22/lib/xalloc.h, 
/usr/src/debug/coreutils-8.22/lib/quote.h, /usr/include/libintl.h,
/usr/include/stdlib.h, /usr/src/debug/coreutils-8.22/lib/error.h, 
/usr/include/string.h, /usr/include/bits/errno.h,
/usr/src/debug/coreutils-8.22/lib/dirname.h, 
/usr/src/debug/coreutils-8.22/lib/utimens.h, /usr/include/unistd.h,
/usr/src/debug/coreutils-8.22/lib/acl.h, /usr/include/locale.h, 
/usr/src/debug/coreutils-8.22/lib/filenamecat.h,
/usr/src/debug/coreutils-8.22/lib/propername.h, 
/usr/src/debug/coreutils-8.22/lib/version-etc.h,
/usr/src/debug/coreutils-8.22/src/cp-hash.h, 
/usr/src/debug/coreutils-8.22/src/copy.c, /usr/include/bits/unistd.h,
/usr/include/bits/stdio.h, 
/usr/src/debug/coreutils-8.22/src/ioblksize.h, 
/usr/src/debug/coreutils-8.22/src/extent-scan.h,
/usr/lib/gcc/x86_64-redhat-linux/4.8.5/include/stdarg.h, 
/usr/include/stdint.h, /usr/src/debug/coreutils-8.22/lib/fadvise.h,
/usr/src/debug/coreutils-8.22/lib/utimecmp.h, 
/usr/include/attr/error_context.h, 
/usr/src/debug/coreutils-8.22/src/selinux.h,
/usr/src/debug/coreutils-8.22/lib/write-any-file.h, 
/usr/src/debug/coreutils-8.22/lib/full-write.h, 
/usr/include/attr/libattr.h,
/usr/src/debug/coreutils-8.22/lib/verror.h, 
/usr/src/debug/coreutils-8.22/lib/unistd.h,
/usr/src/debug/coreutils-8.22/lib/filemode.h, 
/usr/src/debug/coreutils-8.22/lib/same.h, 
/usr/src/debug/coreutils-8.22/lib/yesno.h,
/usr/src/debug/coreutils-8.22/lib/file-set.h, 
/usr/src/debug/coreutils-8.22/lib/areadlink.h,
/usr/src/debug/coreutils-8.22/lib/savedir.h, 
/usr/src/debug/coreutils-8.22/lib/fcntl-safer.h,
/usr/src/debug/coreutils-8.22/lib/buffer-lcm.h, 
/usr/include/sys/ioctl.h, /usr/include/assert.h,
/usr/src/debug/coreutils-8.22/src/cp-hash.c, 
/usr/src/debug/coreutils-8.22/src/extent-scan.c,
/usr/src/debug/coreutils-8.22/src/fiemap.h, 
/usr/src/debug/coreutils-8.22/src/selinux.c, /usr/include/bits/fcntl2.h,
/usr/include/selinux/context.h, 
/usr/src/debug/coreutils-8.22/lib/canonicalize.h, 
/usr/src/debug/coreutils-8.22/lib/i-ring.h,
/usr/src/debug/coreutils-8.22/lib/fts_.h, /usr/include/dirent.h, 
/usr/src/debug/coreutils-8.22/lib/xfts.h,
/usr/src/debug/coreutils-8.22/src/version.c, 
/usr/src/debug/coreutils-8.22/lib/copy-acl.c,
/usr/src/debug/coreutils-8.22/lib/set-acl.c, 
/usr/src/debug/coreutils-8.22/lib/areadlink-with-size.c,
/usr/src/debug/coreutils-8.22/lib/argmatch.c, 
/usr/src/debug/coreutils-8.22/lib/quotearg.h,
/usr/src/debug/coreutils-8.22/lib/backupfile.c, 
/usr/include/bits/dirent.h, 
/usr/src/debug/coreutils-8.22/lib/dirent-safer.h,
/usr/include/bits/confname.h, 
/usr/src/debug/coreutils-8.22/lib/buffer-lcm.c, 
/usr/src/debug/coreutils-8.22/lib/canonicalize.c,
/usr/include/bits/string2.h, 
/usr/src/debug/coreutils-8.22/lib/xgetcwd.h, 
/usr/src/debug/coreutils-8.22/lib/closein.c,
/usr/src/debug/coreutils-8.22/lib/freadahead.h, 
/usr/src/debug/coreutils-8.22/lib/close-stream.h,
/usr/src/debug/coreutils-8.22/lib/stdio.h, 
/usr/src/debug/coreutils-8.22/lib/closeout.h,
/usr/src/debug/coreutils-8.22/lib/closeout.c, 
/usr/src/debug/coreutils-8.22/lib/opendir-safer.c,
/usr/src/debug/coreutils-8.22/lib/unistd-safer.h, 
/usr/src/debug/coreutils-8.22/lib/dirname.c,
/usr/src/debug/coreutils-8.22/lib/dirname-lgpl.c, 
/usr/src/debug/coreutils-8.22/lib/basename-lgpl.c,
/usr/src/debug/coreutils-8.22/lib/stripslash.c, 
/usr/src/debug/coreutils-8.22/lib/exitfail.c,
/usr/src/debug/coreutils-8.22/lib/fadvise.c, /usr/include/fcntl.h, 
/usr/src/debug/coreutils-8.22/lib/open-safer.c,
/usr/src/debug/coreutils-8.22/lib/file-set.c, 
/usr/src/debug/coreutils-8.22/lib/hash-triple.h,
/usr/src/debug/coreutils-8.22/lib/filemode.c, 
/usr/src/debug/coreutils-8.22/lib/filenamecat.c,
/usr/src/debug/coreutils-8.22/lib/filenamecat-lgpl.c, 
/usr/src/debug/coreutils-8.22/lib/full-write.c,
/usr/src/debug/coreutils-8.22/lib/safe-write.h, 
/usr/src/debug/coreutils-8.22/lib/hash.c,
/usr/src/debug/coreutils-8.22/lib/bitrotate.h, 
/usr/src/debug/coreutils-8.22/lib/hash-triple.c,
/usr/src/debug/coreutils-8.22/lib/hash-pjw.h, 
/usr/src/debug/coreutils-8.22/lib/progname.c, /usr/include/errno.h,
/usr/src/debug/coreutils-8.22/lib/propername.c, 
/usr/src/debug/coreutils-8.22/lib/mbuiter.h,
/usr/src/debug/coreutils-8.22/lib/mbchar.h, /usr/include/wchar.h, 
/usr/src/debug/coreutils-8.22/lib/strnlen1.h,
/usr/include/wctype.h, /usr/include/ctype.h, 
/usr/src/debug/coreutils-8.22/lib/string.h, 
/usr/src/debug/coreutils-8.22/lib/trim.h,
/usr/src/debug/coreutils-8.22/lib/xstriconv.h, 
/usr/src/debug/coreutils-8.22/lib/localcharset.h,
/usr/src/debug/coreutils-8.22/lib/c-strcase.h, 
/usr/src/debug/coreutils-8.22/lib/qcopy-acl.c, /usr/include/sys/acl.h,
/usr/src/debug/coreutils-8.22/lib/acl-internal.h, 
/usr/src/debug/coreutils-8.22/lib/qset-acl.c, /usr/include/acl/libacl.h,
/usr/src/debug/coreutils-8.22/lib/quotearg.c, 
/usr/src/debug/coreutils-8.22/lib/c-strcaseeq.h,
/usr/src/debug/coreutils-8.22/lib/safe-read.c, 
/usr/src/debug/coreutils-8.22/lib/same.c,
/usr/src/debug/coreutils-8.22/lib/savedir.c, 
/usr/src/debug/coreutils-8.22/lib/strnlen1.c,
/usr/src/debug/coreutils-8.22/lib/trim.c, 
/usr/src/debug/coreutils-8.22/lib/mbiter.h,
/usr/src/debug/coreutils-8.22/lib/dup-safer.c, 
/usr/src/debug/coreutils-8.22/lib/fcntl.h,
/usr/src/debug/coreutils-8.22/lib/fd-safer.c, 
/usr/src/debug/coreutils-8.22/lib/utimecmp.c,
/usr/src/debug/coreutils-8.22/lib/utimens.c, /usr/include/bits/time.h, 
/usr/src/debug/coreutils-8.22/lib/timespec.h,
/usr/src/debug/coreutils-8.22/lib/sys/stat.h, /usr/include/sys/time.h, 
/usr/src/debug/coreutils-8.22/lib/verror.c,
/usr/src/debug/coreutils-8.22/lib/xvasprintf.h, 
/usr/src/debug/coreutils-8.22/lib/version-etc.c,
/usr/src/debug/coreutils-8.22/lib/version-etc-fsf.c, 
/usr/src/debug/coreutils-8.22/lib/write-any-file.c,
/usr/src/debug/coreutils-8.22/lib/xmalloc.c, 
/usr/src/debug/coreutils-8.22/lib/xalloc-die.c,
/usr/src/debug/coreutils-8.22/lib/xfts.c, 
/usr/src/debug/coreutils-8.22/lib/xgetcwd.c,
/usr/src/debug/coreutils-8.22/lib/xstriconv.c, /usr/include/iconv.h, 
/usr/src/debug/coreutils-8.22/lib/striconv.h,
/usr/src/debug/coreutils-8.22/lib/xvasprintf.c, 
/usr/src/debug/coreutils-8.22/lib/xsize.h,
/usr/src/debug/coreutils-8.22/lib/yesno.c, 
/usr/src/debug/coreutils-8.22/lib/fcntl.c, 
/usr/src/debug/coreutils-8.22/lib/fflush.c,
/usr/include/stdio_ext.h, 
/usr/src/debug/coreutils-8.22/lib/freadahead.c, 
/usr/src/debug/coreutils-8.22/lib/fseeko.c,
/usr/src/debug/coreutils-8.22/lib/fts-cycle.c, 
/usr/src/debug/coreutils-8.22/lib/fts.c,
/usr/src/debug/coreutils-8.22/lib/cycle-check.h, 
/usr/src/debug/coreutils-8.22/lib/dev-ino.h, /usr/include/bits/statfs.h,
/usr/src/debug/coreutils-8.22/lib/cloexec.h, /usr/include/sys/statfs.h, 
/usr/src/debug/coreutils-8.22/lib/getfilecon.c,
/usr/src/debug/coreutils-8.22/lib/linkat.c, 
/usr/src/debug/coreutils-8.22/lib/at-func.c,
/usr/src/debug/coreutils-8.22/lib/utimensat.c, 
/usr/src/debug/coreutils-8.22/lib/save-cwd.h,
/usr/src/debug/coreutils-8.22/lib/openat-priv.h, 
/usr/src/debug/coreutils-8.22/lib/openat.h,
/usr/src/debug/coreutils-8.22/lib/vasprintf.c, 
/usr/src/debug/coreutils-8.22/lib/vasnprintf.h,
/usr/src/debug/coreutils-8.22/lib/areadlinkat.c, 
/usr/src/debug/coreutils-8.22/lib/careadlinkat.h,
/usr/src/debug/coreutils-8.22/lib/c-strcasecmp.c, 
/usr/src/debug/coreutils-8.22/lib/careadlinkat.c,
/usr/src/debug/coreutils-8.22/lib/allocator.h, 
/usr/src/debug/coreutils-8.22/lib/cloexec.c,
/usr/src/debug/coreutils-8.22/lib/close-stream.c, 
/usr/src/debug/coreutils-8.22/lib/cycle-check.c,
/usr/src/debug/coreutils-8.22/lib/gettime.c, 
/usr/src/debug/coreutils-8.22/lib/hash-pjw.c,
/usr/src/debug/coreutils-8.22/lib/i-ring.c, 
/usr/src/debug/coreutils-8.22/lib/localcharset.c, 
/usr/include/nl_types.h,
/usr/include/langinfo.h, /usr/src/debug/coreutils-8.22/lib/mbchar.c, 
/usr/src/debug/coreutils-8.22/lib/str-kmp.h,
/usr/src/debug/coreutils-8.22/lib/mbsstr.c, 
/usr/src/debug/coreutils-8.22/lib/malloca.h,
/usr/src/debug/coreutils-8.22/lib/openat-die.c, 
/usr/src/debug/coreutils-8.22/lib/openat-safer.c,
/usr/src/debug/coreutils-8.22/lib/acl-errno-valid.c, 
/usr/src/debug/coreutils-8.22/lib/file-has-acl.c,
/usr/src/debug/coreutils-8.22/lib/save-cwd.c, 
/usr/src/debug/coreutils-8.22/lib/chdir-long.h,
/usr/src/debug/coreutils-8.22/lib/striconv.c, 
/usr/src/debug/coreutils-8.22/lib/chdir-long.c,
/usr/src/debug/coreutils-8.22/lib/fclose.c, 
/usr/src/debug/coreutils-8.22/lib/openat-proc.c,
/usr/src/debug/coreutils-8.22/lib/vasnprintf.c, 
/usr/src/debug/coreutils-8.22/lib/printf-args.h,
/usr/src/debug/coreutils-8.22/lib/printf-parse.h, 
/usr/src/debug/coreutils-8.22/lib/fpucw.h,
/usr/src/debug/coreutils-8.22/lib/isnanl-nolibm.h, 
/usr/src/debug/coreutils-8.22/lib/allocator.c,
/usr/src/debug/coreutils-8.22/lib/malloca.c, 
/usr/src/debug/coreutils-8.22/lib/mbslen.c,
/usr/src/debug/coreutils-8.22/lib/isnan.c, 
/usr/src/debug/coreutils-8.22/lib/printf-args.c,
/usr/src/debug/coreutils-8.22/lib/printf-parse.c

",2018-04-09T18:16:00Z,2582198
923,openzfs/zfs,312114336,379845437,"@dswartz Would you edit your post to use a pastebin? Also, why is there nothing under `Source files for which symbols have been read in:`? Did you edit the output?",2018-04-09T18:17:46Z,1386373
924,openzfs/zfs,312114336,379845995,"On 2018-04-09 14:17, Richard Yao wrote:
> @dswartz [1] Would you edit your post to use a pastebin?

Sure.


",2018-04-09T18:19:31Z,2582198
925,openzfs/zfs,312114336,379847631,"The patches that apply to cp as far as what gdb claims its source files are (with patches editing only test cases that do not apply to the cp binary removed) is the same as I got after processing the gdb output from Gentoo's cp, which is:

```
./coreutils-selinux.patch:diff -urNp coreutils-8.21-orig/src/copy.c coreutils-8.21/src/copy.c
./coreutils-selinux.patch:diff -urNp coreutils-8.21-orig/src/cp.c coreutils-8.21/src/cp.c
./coreutils-8.22-selinux-optionsseparate.patch:diff -urNp coreutils-8.22-orig/src/cp.c coreutils-8.22/src/cp.c
./coreutils-8.22-mv-hardlinksrace.patch:diff -urNp coreutils-8.22-orig/src/copy.c coreutils-8.22/src/copy.c
./coreutils-8.22-cp-sparsecorrupt.patch:diff --git a/src/copy.c b/src/copy.c
./coreutils-8.22-cp-selinux.patch:diff --git a/src/selinux.c b/src/selinux.c
```

The changes in `./coreutils-8.22-mv-hardlinksrace.patch` look questionable to me, but I don't see a smoking gun. Testing it on Gentoo after applying these patches should allow us to figure out which one is making it reproducible on CentOS.",2018-04-09T18:24:55Z,1386373
926,openzfs/zfs,312114336,379847890,"On 2018-04-09 14:17, Richard Yao wrote:
> @dswartz [1] Would you edit your post to use a pastebin?

https://pastebin.com/raw/TNCNJRau
",2018-04-09T18:25:44Z,2582198
927,openzfs/zfs,312114336,379852431,"Reproducibility: no  
Distribution name and version: Fedora 27  
Kernel Version: 4.15.10-300.fc27.x86_64
Coreutils Version: 8.27-20.fc27  
SELinux status: off

EDIT: This machine's cp is copying in alphanumeric order (verified using strace).",2018-04-09T18:40:37Z,173088
928,openzfs/zfs,312114336,379854735,"Not reproducible using archzfs repo of Arch Linux (thanks, @demizer).

```
■ mkdir SRC
■ for i in $(seq 1 10000); do echo $i > SRC/$i ; done
■ cp -r SRC DST
■ uname -srvmo
Linux 4.15.15-1-ARCH #1 SMP PREEMPT Sat Mar 31 23:59:25 UTC 2018 x86_64 GNU/Linux
■ LC_ALL=C pacman -Qi coreutils spl-linux spl-utils-common zfs-linux zfs-utils-common | grep '^Version '
Version         : 8.29-1
Version         : 0.7.7.4.15.15.1-1
Version         : 0.7.7-1
Version         : 0.7.7.4.15.15.1-1
Version         : 0.7.7-1
■ zpool get all | sed '2,$s/^..../tank/g'
NAME  PROPERTY                       VALUE                          SOURCE
tank  size                           43.5T                          -
tank  capacity                       81%                            -
tank  altroot                        -                              default
tank  health                         ONLINE                         -
tank  guid                           xxxxxxxxxxxxxxxxxxx            -
tank  version                        -                              default
tank  bootfs                         -                              default
tank  delegation                     on                             default
tank  autoreplace                    off                            default
tank  cachefile                      -                              default
tank  failmode                       wait                           default
tank  listsnapshots                  off                            default
tank  autoexpand                     off                            default
tank  dedupditto                     0                              default
tank  dedupratio                     1.00x                          -
tank  free                           8.00T                          -
tank  allocated                      35.5T                          -
tank  readonly                       off                            -
tank  ashift                         12                             local
tank  comment                        -                              default
tank  expandsize                     -                              -
tank  freeing                        0                              -
tank  fragmentation                  34%                            -
tank  leaked                         0                              -
tank  multihost                      off                            default
tank  feature@async_destroy          enabled                        local
tank  feature@empty_bpobj            active                         local
tank  feature@lz4_compress           active                         local
tank  feature@multi_vdev_crash_dump  disabled                       local
tank  feature@spacemap_histogram     active                         local
tank  feature@enabled_txg            active                         local
tank  feature@hole_birth             active                         local
tank  feature@extensible_dataset     enabled                        local
tank  feature@embedded_data          active                         local
tank  feature@bookmarks              enabled                        local
tank  feature@filesystem_limits      enabled                        local
tank  feature@large_blocks           enabled                        local
tank  feature@large_dnode            disabled                       local
tank  feature@sha512                 disabled                       local
tank  feature@skein                  disabled                       local
tank  feature@edonr                  disabled                       local
tank  feature@userobj_accounting     disabled                       local
```",2018-04-09T18:48:00Z,10365452
929,openzfs/zfs,312114336,379858112,The 0.7.7 release has been removed from the CentOS and Fedora RPM repositories.  ,2018-04-09T18:59:23Z,148917
930,openzfs/zfs,312114336,379859322,@rincebrain confirmed that this is reproducible using `touch` to create file in the right order (to inflate the zap with hash collisions). I'll post a minimal testcase.,2018-04-09T19:03:17Z,207920
931,openzfs/zfs,312114336,379860680,@trisk  you might want to look at the testcase in https://github.com/zfsonlinux/zfs/pull/7411 first,2018-04-09T19:07:59Z,11469457
932,openzfs/zfs,312114336,379863672,"These changed the sorting:

http://git.savannah.gnu.org/cgit/coreutils.git/commit/src?id=069723c5eef34736b35b21af1d76ddfbd594a259 or  
http://git.savannah.gnu.org/cgit/coreutils.git/commit/src?id=7bbfd1735574be80971b597b34f779fe6938a595",2018-04-09T19:19:14Z,173088
933,openzfs/zfs,312114336,379864218,@Ringdingcoder Nice find. That could explain things nicely if some tests with/without that confirm it is the difference.,2018-04-09T19:21:19Z,1386373
934,openzfs/zfs,312114336,379865895,@Ringdingcoder I just reproduced this in an old Gentoo VM that uses coreutils 8.21. It is affected. No redhat patches are in place there. I'll try reproducing with the patches that you linked and see what happens. I expect one of them to make the issue disappear.,2018-04-09T19:27:03Z,1386373
935,openzfs/zfs,312114336,379866211,"You'll need this:
```--- a/src/copy.c
+++ b/src/copy.c
@@ -717,7 +717,7 @@ copy_dir (char const *src_name_in, char const *dst_name_in, bool new_dst,
   struct cp_options non_command_line_options = *x;
   bool ok = true;
 
-  name_space = savedir (src_name_in, SAVEDIR_SORT_FASTREAD);
+  name_space = savedir (src_name_in, SAVEDIR_SORT_NONE);
   if (name_space == NULL)
     {
       /* This diagnostic is a bit vague because savedir can fail in
```",2018-04-09T19:27:45Z,173088
936,openzfs/zfs,312114336,379866767,Shell script to reproduce (cp not needed): https://gist.github.com/trisk/9966159914d9d5cd5772e44885112d30,2018-04-09T19:29:07Z,207920
937,openzfs/zfs,312114336,379866988,"No, actually you're going the other way around. Then you would need patch gnulib. Easier going back to unsorted from a recent version.",2018-04-09T19:29:48Z,173088
938,openzfs/zfs,312114336,379868269,@Ringdingcoder I just realized that when I found that lib/savedir.c didn't exist in the source files that I have. I picked an old VM that just happened to have 8.21. I'll update it to 8.23 and then revert to verify things.,2018-04-09T19:34:14Z,1386373
939,openzfs/zfs,312114336,379868980,"I can reproduce it immediately by going with SAVEDIR_SORT_NONE. This explains why only old distros experience this. IIRC, tar is unsorted, so a `""tar cf - SRC | tar -C DST -xf -""` should be able to trigger this everywhere (untested).
Obviously the hard-coded sequence from trisk's script also does it.",2018-04-09T19:36:44Z,173088
940,openzfs/zfs,312114336,379870309,"With @trisk 's script I can immediately reproduce this on 64-bit Ubuntu 16.04 (kernel 4.4.0-109-generic) with ZFS 0.7.7. It fails as expected:

````
touch: cannot touch 'DST/9259': No space left on device
````
",2018-04-09T19:41:43Z,865382
941,openzfs/zfs,312114336,379872156,"@ryao Yes, the actual code change is this: http://git.savannah.gnu.org/cgit/gnulib.git/commit/?id=be7d73709d2b3bceb987f1be00a049bb7021bf87",2018-04-09T19:48:12Z,173088
942,openzfs/zfs,312114336,379874743,"@Ringdingcoder I think we have satisfactorily explained what is different between various distributions that only the RHEL ones are affected. I am going to switch to understanding what is going wrong inside the kernel.

I observed a link count corruption issue that persisted between umounts when I reproduced this, but I have had trouble reliably reproducing the problem so that I have a reproducer for the link count issue.",2018-04-09T19:57:18Z,1386373
943,openzfs/zfs,312114336,379875898,"I have now reproduced the bug on Arch Linux, using a corrected version of @trisk's script (unexpected token error, line 6).  I am unable to reproduce the bug consistently:

```
■ export LC_ALL=C
■ rm -r DST
■ ./zap-collision-test.sh 
■ rm -r DST
■ ./zap-collision-test.sh 
touch: cannot touch 'DST/9259': No space left on device
■ rm -r DST
■ ./zap-collision-test.sh 
touch: cannot touch 'DST/9259': No space left on device
■ rm -r DST
■ ./zap-collision-test.sh 
■ rm -r DST
■ ./zap-collision-test.sh 
touch: cannot touch 'DST/9259': No space left on device
■ rm -r DST
■ ./zap-collision-test.sh 
■ 
```",2018-04-09T20:01:15Z,10365452
944,openzfs/zfs,312114336,379877139,"@NoSuck Can confirm on Arch, too, using @trisk's script. This invalidates my previous comment.
```
local/spl-linux-git 2018.04.04.r1070.581bc01.4.15.15.1-1 (archzfs-linux-git)
local/spl-utils-common-git 2018.04.04.r1070.581bc01-1 (archzfs-linux-git)
local/zfs-linux-git 2018.04.04.r3402.533ea0415.4.15.15.1-1 (archzfs-linux-git)
local/zfs-utils-common-git 2018.04.04.r3402.533ea0415-1 (archzfs-linux-git)
```",2018-04-09T20:05:52Z,19325781
945,openzfs/zfs,312114336,379877454,"Using @trisk's script (well, a slightly corrected version) I can reproduce this on an almost current version of the git tip, g1724eb62d, on Fedora 27 on a simple mirrored vdev in a VM. It doesn't happen on every run, but it happens reasonably frequently (at least half the time, I think).

(This git version is the most recent version I've built for my own use. I can test with the very latest git tip, but I don't see anything there that would change this, if the identified cause is right. I'd be happy to test updates in the VM.)",2018-04-09T20:07:01Z,865382
946,openzfs/zfs,312114336,379878349,"It might be significant that we hit the zap expansion limit at 2048 files (unclear if this reflects a property of the coreutils sorting, or the zap hash function, though).",2018-04-09T20:10:19Z,207920
947,openzfs/zfs,312114336,379883140,"The original reproducer creates orphaned files when it triggers, while @trisk's reproducer does not. After running the original reproducer, I observed a failure on 1 file, 8186 files in the directory according to `ls -l DST | wc` and a directory size of 10001. Unlinking all of the files while trying to stat them to see if any were accessible failed, despite a directory size of 1816. Here is zdb output from a testpool that I used to reproduce the issue:

https://bpaste.net/show/d9f2f0de6c61

I forget how many times that I ran the reproducer on this (likely twice), but the orphaned files are clearly visible. Here is a compressed image of the pool:

https://dev.gentoo.org/~ryao/7401-pool-orphaned-files.xz

It has sha256 5bf54d804f0cd6cd155cc781efeefdabaa6e0ddddc500695eb24061d802474ac.  The pool itself is just a 1GB sparse file. The compressed version is 1938032 bytes (~2MB) in size. Others can use zdb on it and poke around to observe the orphaned files.",2018-04-09T20:26:49Z,1386373
948,openzfs/zfs,312114336,379884429,"I am stepping out for a bit due to an appointment that I cannot preempt, but I just want to point out that those who lost files might still have them around as orphans. We'll need to examine a pool where this happened with files storing actual data to confirm that the data is there. If it is, the data could be recoverable.",2018-04-09T20:31:24Z,1386373
949,openzfs/zfs,312114336,379891384,"Thank you everyone for your help with this unfortunate regression. As [described above](https://github.com/zfsonlinux/zfs/issues/7401#issuecomment-379623951) by @tuxoko the root cause of this issue is understood and a complete fix is currently being worked on.  In the meanwhile commit cc63068e95ee725cce03b1b7ce50179825a6cda5 which introduced this issue will be shortly reverted from the master branch, release branch, and v0.7.8 will be tagged.  We'll open a new PR with the full fix for review and feedback when it's ready. ",2018-04-09T20:56:28Z,148917
950,openzfs/zfs,312114336,379895691,"@behlendorf There are still some loose ends. In particular, how are we going to deal with those affected by this? There could be orphan files in their datasets.

At present, we could tell people to backup changes between what they have now and the snapshot before the issue happened, rollback and then restore, provided that they have snapshots at all. If not, the solution at the moment would be to make a new dataset, copy the files over to it and then destroy the old one.

Neither is as clean a solution as doing something like `zfs lost+found -r tank` and having the orphaned files put into lost+found directories. It gets messier when we consider that orphaned files could be in recently made snapshots.

This being hard to reproduce on non-RHEL family systems had been a loose end, but it was just tied. The change being in the bundled gnulib between coreutils 8.22 and 8.23 switched the order in how things had been copied from sequential order to a pseudo-random one.

Finally, we had something like a dozen people around the world drop everything to work on this. Not all of us are yet on the same page yet and it will take some time to sync our understandings. That way we can all review the final fix.",2018-04-09T21:12:30Z,1386373
951,openzfs/zfs,312114336,379897674,"I should add that we also need a way to check for the presence of orphans. I have confirmed that zdb can show it, but I have not yet determined what zdb would show in all cases (mainly, non-zero files) to allow reliable detection.",2018-04-09T21:20:29Z,1386373
952,openzfs/zfs,312114336,379898356,Our analysis so far has not determined how the additional files whose `zap_add` completes after a prior zap expansion failure on the directory end up orphaned.,2018-04-09T21:23:01Z,207920
953,openzfs/zfs,312114336,379899335,Our analysis is not finished. I am reopening this pending the completion of our analysis.,2018-04-09T21:27:09Z,1386373
954,openzfs/zfs,312114336,379903582,"Right I didn't mean to suggest this issue should be closed, and reverting the change was all that was needed.  There's still clearly careful investigation to be done, which we can now focus on.

@ryao when possible rolling back to a snapshot would be the cleanest way to recover these files.  However, since that won't always be an option let's investigate implementing a generic orhpan recovery mechanism.  Adding this functionality initially to `zdb` would allow us to check existing datasets, and would be nice additional test coverage for `ztest` to leverage.  We could potentially follow this up with support for a `.zfs/lost+found` directory.",2018-04-09T21:44:07Z,148917
955,openzfs/zfs,312114336,379969486,"Given the improved understanding of the cause of this regression, can anything be said about the behaviour of rsync? If it reports no errors, are the data fine?

What about mv? And what if mv is from one dataset to another, on the same pool?",2018-04-10T04:22:56Z,4440028
956,openzfs/zfs,312114336,379973603,"@darrenfreeman The mailing list or IRC chatroom would probably be a better place to ask, but
- rsync should be fine, since _I think_ it should bail out on e.g. rsync -a src/ dst/ once it gets ENOSPC once, and not try any additional files
- mv across datasets on a pool is just like mv across other filesystems, cp then rm, so I would guess that might be subject to the same caveats about version peculiarities as cp above, but I haven't tested that.

Also, one final caveat:
- knowledge, particularly about how much vulnerability exists for files that get lost in the metaphorical shuffle after getting back ENOSPC, is incomplete, so it's safest to revert versions (or bump once 0.7.8 is cut) if at all possible, and everything above is based on incomplete information.",2018-04-10T04:55:10Z,214141
957,openzfs/zfs,312114336,379986385,"rsync always sorts files, so it *should* be fine. And as long as you don't receive errors, you *should* be fine.
Since data is not silently lost, this is not the worst-case catastrophic bug, just a major annoyance. The most inconvenient issue about it are the orphaned files, but fortunately they are tied to their respective datasets, not to the entire pool, and can get rid of by rolling back or re-creating individual datasets.",2018-04-10T06:17:08Z,173088
958,openzfs/zfs,312114336,379990688,"Reproducibility: yes
ZoL version: git, recent commit, 10adee27ced279c381816e1321226fce5834340c
Distribution: Ubuntu 17.10
Kernel Version: 4.13.0-38-generic
Coreutils Version: 8.26-3ubuntu4
SELinux status: not installed AFAICT

Reproduced using: `./zap-collision-test.sh` .

Furthermore, this didn't look good:

```
rm -Rf DST
Segmentation fault (core dumped)
```

The pool was freshly created as,

```
zfs create rpool/test -o recordsize=4k
touch -s 1G /rpool/test/file
zpool create test /rpool/test/file -o ashift=12
```

I am trying to install the debug symbols for `rm`, however I am now also getting segfaults when not even touching this zpool. (apt-key is segfaulting when trying to trust the debug repo.) So I fear I better push the comment button now and reboot :/

Update: can't reproduce the segfault on `rm -Rf DST`, after rebooting and installing debug symbols.",2018-04-10T06:38:50Z,4440028
959,openzfs/zfs,312114336,380015910,"Thanks for the solutions and quick efforts to fix.
Are there any methods to check a complete filesystem if there any affected files? I do have backups - anyone give me a oneliner to list them?",2018-04-10T08:21:11Z,24358068
960,openzfs/zfs,312114336,380035412,"Given this bug has now been listed on The Register (https://www.theregister.co.uk/2018/04/10/zfs_on_linux_data_loss_fixed/), it might be wise have an FAQ article on the wiki page (with a link in this ticket). The FAQ article should clearly state which versions of ZoL are affected and which distros/kernel versions (similar to the birthhole bug). This would hopefully limit any panic concerns about the reliability of ZoL as a storage layer.",2018-04-10T09:29:37Z,9853568
961,openzfs/zfs,312114336,380045208,"> Given this bug has now been listed on The Register (https://www.theregister.co.uk/2018/04/10/zfs_on_linux_data_loss_fixed/) 

From that article (emphasis mine): 
""So even though **three reviewers signed off on the cruddy commit**, the speedy response **may** mean it’s possible to consider this a triumph **of sorts** for open source."" 

Ouch.

I agree with @markdesouza that there should be a FAQ article for that so we ZFS apologizers can point anyone who questions us about that to it. I would also like to suggest that the ZFS signing-off procedure be reviewed to avoid (or at least make it *way* more improbable) for such a ""cruddy commit""  to make it into a ZFS **stable** release, and that notice of this review also be added to that same FAQ article. ",2018-04-10T10:03:02Z,5199766
962,openzfs/zfs,312114336,380065566,"In #7411, the `random_creation` test looks like it may be a more robust reproducer (especially for future bugs) because it naturally relies on the ordering of the ZAP hashes. Also, if there are other reproducers, it might be a good idea to centralize discussion of them in that PR so they can be easily included.",2018-04-10T11:27:21Z,6502699
963,openzfs/zfs,312114336,380123393,"Answering my earlier question. Debian 9.3 as above.

`rsync` doesn't hit the bug, it creates files in lexical order. (I.e. file `999` is followed by `9990`.) In a very small number of tests, I didn't find a combination of switches that would fail.

So anyone who prefers `rsync`, should have a pretty good chance of having missed the bug.

Something similar to `mv /pool/dataset1/SRC /pool/dataset2/` also didn't fail. (Move between datasets within the same pool.) Although, on the same box, `cp` doesn't fail either, so that doesn't prove much.",2018-04-10T14:35:49Z,4440028
964,openzfs/zfs,312114336,380151942,"FYI - you probably all saw it already, but we released [zfs-0.7.8](https://github.com/zfsonlinux/zfs/releases/tag/zfs-0.7.8 ) with the reverted patch last night.",2018-04-10T15:53:01Z,11469457
965,openzfs/zfs,312114336,380205514,"@ort163 We do not have a one liner yet. People are continuing to analyze the issue and we will have a proper fix in the near future. That will include a way to detect+correct the wrong directory sizes, list snapshots affected and place the orphaned files in some kind of lost+found directory. I am leaning toward extending scrub to do it.",2018-04-10T18:40:40Z,1386373
966,openzfs/zfs,312114336,380212322,"@markdesouza I have spent a fair amount of time explaining things to end users on Hacker News, Reddit and Phoronix. I do not think that our understanding is sufficient to post a final FAQ yet, but we could post an interim FAQ. 

I think the interim FAQ entry should advise users to upgrade ASAP to avoid having to possibly deal with orphaned files if nothing has happened yet, or more orphaned files if something has already happened; and not to change how they do things after upgrading unless they deem it necessary until we finish our analysis, make a proper fix, and issue proper instructions on how to repair the damage in the release notes. I do not think there is any harm to pools if datasets have incorrect directory sizes and orphaned files while people wait for us to release a proper fix with instructions on how to completely address the issue, so telling them to wait after upgrading should be fine. The orphan files should stay around and persist through send/recv unless snapshot rollback is done or the dataset is destroyed.

Until that is up, you could point users to my hacker news post:

https://news.ycombinator.com/item?id=16797932

In specific, we need to nail down whether existing files’ directory entries could be lost, what if any other side effects happen when this is triggered on new file creation, what course of events leads to directory entries disappearing after ENOSPC, how system administrators could detect it and how system administrators will repair it. Then we should be able to make a proper FAQ entry.

Edit: The first 3 questions are answered satisfactorily in #7421.",2018-04-10T19:02:36Z,1386373
967,openzfs/zfs,298311894,298311894,"This simplifies the logic a bit and gets rid of the main loop that looks
for where the key is located.

Signed-off-by: Matthew Thode <mthode@mthode.org>


Related: https://github.com/zfsonlinux/zfs/pull/7189

### Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [x] Performance enhancement (non-breaking change which improves efficiency)
- [x] Code cleanup (non-breaking change which makes code smaller or more readable)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)
- [ ] Documentation (a change to man pages or other documentation)

### Checklist:
<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->
- [x] My code follows the ZFS on Linux code style requirements.
- [x] I have updated the documentation accordingly.
- [x] I have read the **CONTRIBUTING** document.
- [ ] I have added tests to cover my changes.
- [x] All new and existing tests passed.
- [x] All commit messages are properly formatted and contain `Signed-off-by`.
- [ ] Change has been approved by a ZFS on Linux member.
",2018-02-19T15:18:41Z,1869923
968,openzfs/zfs,298311894,366723337,@tcaputi @kpande @bunder2015 your review here would be nice :D,2018-02-19T15:19:05Z,1869923
969,openzfs/zfs,298311894,366774624,"I'm still not an expert in bash or dracut, but the approach looks correct to me.",2018-02-19T18:41:17Z,1920053
970,openzfs/zfs,298311894,366776125,"That would be changing one `if` out for another.  I'm not sure what would be gained or fixed doing so either (the logic here is straight forward, which I like).",2018-02-19T18:48:40Z,1869923
971,openzfs/zfs,298311894,366798615,"# [Codecov](https://codecov.io/gh/zfsonlinux/zfs/pull/7194?src=pr&el=h1) Report
> Merging [#7194](https://codecov.io/gh/zfsonlinux/zfs/pull/7194?src=pr&el=desc) into [master](https://codecov.io/gh/zfsonlinux/zfs/commit/e921f6508b212c61fcedd0eeb2f9cf9da1abc4d1?src=pr&el=desc) will **decrease** coverage by `0.23%`.
> The diff coverage is `n/a`.

[![Impacted file tree graph](https://codecov.io/gh/zfsonlinux/zfs/pull/7194/graphs/tree.svg?width=650&height=150&src=pr&token=NGfxvvG2io)](https://codecov.io/gh/zfsonlinux/zfs/pull/7194?src=pr&el=tree)

```diff
@@            Coverage Diff             @@
##           master    #7194      +/-   ##
==========================================
- Coverage   76.39%   76.16%   -0.24%     
==========================================
  Files         327      327              
  Lines      103768   103768              
==========================================
- Hits        79278    79037     -241     
- Misses      24490    24731     +241
```

| Flag | Coverage Δ | |
|---|---|---|
| #kernel | `76.16% <ø> (+0.22%)` | :arrow_up: |
| #user | `65.43% <ø> (-0.46%)` | :arrow_down: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/zfsonlinux/zfs/pull/7194?src=pr&el=continue).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/zfsonlinux/zfs/pull/7194?src=pr&el=footer). Last update [e921f65...e5619df](https://codecov.io/gh/zfsonlinux/zfs/pull/7194?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).
",2018-02-19T20:44:46Z,22429695
972,openzfs/zfs,298311894,366845661,"@tcaputi: lol, your choice of input to the open source effectively guarantees you're going to be a bootloader and init system expert for the rest of your days.
@kpande: that sounds reasonable, but we need to be careful with all of this. Root context execution of user supplied inputs  seems like it would have pitfalls...",2018-02-20T01:56:50Z,1331084
973,openzfs/zfs,298311894,366846467,Complexity being roughly analogous to attack surface. This should be a KISS exercise to the degree possible.,2018-02-20T02:02:59Z,1331084
974,openzfs/zfs,298311894,366850553,I have not tested the current iteration of the patch,2018-02-20T02:34:16Z,1869923
975,openzfs/zfs,298311894,367075155,"so... uh, do I need to resubmit this now, since I can't reopen?",2018-02-20T18:36:48Z,1869923
976,openzfs/zfs,298311894,367120872,"@prometheanfire unfortunately, Github won't let me reopening this issue either.  But please open a new PR with your updated version.   I'd like to think we can continue discussing the relative merrits of the two proposed solutions and then move forward with one of them.",2018-02-20T21:12:01Z,148917
977,openzfs/zfs,298311894,367129849,"@behlendorf I'm actually fine with @kpande's solution, I think ours now differ by newlines only.  I just wanted to call out the behaviour.",2018-02-20T21:41:50Z,1869923
978,openzfs/zfs,298311894,367139373,I'm glad to hear it was simply a misunderstanding and there was no harm done.  ,2018-02-20T22:14:49Z,148917
979,openzfs/zfs,298311894,367141638,"is not misunderstanding, he randomly ban ppl from IRC, verbally abuse issue reportes, delete comments... i not know why he can moderate the project, he is very toxic in this community, this is not first time, like prometheanfire say is behaviour.",2018-02-20T22:23:25Z,30300957
980,openzfs/zfs,298311894,367142083,This is the first time I've had any problem at least.,2018-02-20T22:25:06Z,1869923
981,openzfs/zfs,298311894,367147796,"no better place becaus you ban me from IRC. respond to criticism is lies, everyone who do not think like yourself you consider stupid",2018-02-20T22:46:35Z,30300957
982,openzfs/zfs,40463575,40463575,"```
SPL: Loaded module v0.6.3-1
ZFS: Loaded module v0.6.3-1, ZFS pool version 5000, ZFS filesystem version 5
```

doing nothing but some basic rsync's with moderate sizes 4-10G result always in having approx 1MB/sec throughtput (very slow) on a up-date 16G RAM HP server - CentOS 6 with OpenVZ and selfcompiled modules for ZFS - dmesg:

```
INFO: task rsync:6517 blocked for more than 120 seconds.
      Tainted: P           ---------------    2.6.32-042stab092.3 #1
""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
rsync         D ffff8804029bafb0     0  6517   6516    0 0x00000080
 ffff88001e55da18 0000000000000086 0000000000000003 00000003d2dad320
 000000000001ec80 0000000000000001 ffff88001e55dab8 0000000000000082
 ffffc900105a3428 ffff8803fe837d60 ffff8804029bb578 000000000001ec80
Call Trace:
 [<ffffffff810a1dfe>] ? prepare_to_wait_exclusive+0x4e/0x80
 [<ffffffffa019fb35>] cv_wait_common+0x105/0x1c0 [spl]
 [<ffffffff810a1bb0>] ? autoremove_wake_function+0x0/0x40
 [<ffffffffa019fc45>] __cv_wait+0x15/0x20 [spl]
 [<ffffffffa02ae1fb>] txg_wait_open+0x8b/0x110 [zfs]
 [<ffffffffa027194e>] dmu_tx_wait+0x29e/0x2b0 [zfs]
 [<ffffffff81530bfe>] ? mutex_lock+0x1e/0x50
 [<ffffffffa0271a41>] dmu_tx_assign+0x91/0x490 [zfs]
 [<ffffffffa027fab7>] ? dsl_dataset_block_freeable+0x27/0x60 [zfs]
 [<ffffffffa02e8d3e>] zfs_write+0x43e/0xcf0 [zfs]
 [<ffffffff8100bc4e>] ? apic_timer_interrupt+0xe/0x20
 [<ffffffff811c5e5c>] ? core_sys_select+0x1ec/0x2d0
 [<ffffffffa02fd354>] zpl_write_common+0x54/0xd0 [zfs]
 [<ffffffffa02fd438>] zpl_write+0x68/0xa0 [zfs]
 [<ffffffff811ac798>] vfs_write+0xb8/0x1a0
 [<ffffffff811ad091>] sys_write+0x51/0x90
 [<ffffffff810f4dee>] ? __audit_syscall_exit+0x25e/0x290
 [<ffffffff8100b102>] system_call_fastpath+0x16/0x1b

INFO: task txg_sync:876 blocked for more than 120 seconds.
      Tainted: P           ---------------    2.6.32-042stab092.3 #1
""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
txg_sync      D ffff880405d28640     0   876      2    0 0x00000000
 ffff8803fe839b70 0000000000000046 0000000000000001 ffff880405101430
 0000000000000000 0000000000000000 ffff8803fe839af0 ffffffff81060032
 ffff8803fe839b40 ffffffff81054939 ffff880405d28c08 000000000001ec80
Call Trace:
 [<ffffffff81060032>] ? default_wake_function+0x12/0x20
 [<ffffffff81054939>] ? __wake_up_common+0x59/0x90
 [<ffffffff8152f833>] io_schedule+0x73/0xc0
 [<ffffffffa019fadc>] cv_wait_common+0xac/0x1c0 [spl]
 [<ffffffffa02f53e0>] ? zio_execute+0x0/0x140 [zfs]
 [<ffffffff810a1bb0>] ? autoremove_wake_function+0x0/0x40
 [<ffffffffa019fc08>] __cv_wait_io+0x18/0x20 [spl]
 [<ffffffffa02f561b>] zio_wait+0xfb/0x1b0 [zfs]
 [<ffffffffa02867e3>] dsl_pool_sync+0xb3/0x440 [zfs]
 [<ffffffffa029a67b>] spa_sync+0x40b/0xae0 [zfs]
 [<ffffffffa02aebb4>] txg_sync_thread+0x384/0x5e0 [zfs]
 [<ffffffff8105b309>] ? set_user_nice+0xc9/0x130
 [<ffffffffa02ae830>] ? txg_sync_thread+0x0/0x5e0 [zfs]
 [<ffffffffa01978e8>] thread_generic_wrapper+0x68/0x80 [spl]
 [<ffffffffa0197880>] ? thread_generic_wrapper+0x0/0x80 [spl]
 [<ffffffff810a1596>] kthread+0x96/0xa0
 [<ffffffff8100c34a>] child_rip+0xa/0x20
 [<ffffffff810a1500>] ? kthread+0x0/0xa0
 [<ffffffff8100c340>] ? child_rip+0x0/0x20
```
",2014-08-18T07:56:21Z,3446978
983,openzfs/zfs,40463575,52543847,"I hit something similar but maybe different last night. I believe it occurred while KVM was copying disk blocks from another server to this one. This ended up causing actual corruption on at least one of the zvols (as seen by the VM). 

```
[381360.908047] INFO: task zvol/0:331 blocked for more than 120 seconds.
[381360.908147]       Tainted: PF          O 3.13.0-32-generic #57-Ubuntu
[381360.908240] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[381360.908352] zvol/0          D ffff88062fc54440     0   331      2 0x00000000
[381360.908357]  ffff88060796dbe0 0000000000000002 ffff880609aa17f0 ffff88060796dfd8
[381360.908360]  0000000000014440 0000000000014440 ffff880609aa17f0 ffff880609a5eb30
[381360.908362]  ffff880609a5e9f8 ffff880609a5eb38 0000000000000000 0000000000000002
[381360.908365] Call Trace:
[381360.908374]  [<ffffffff817200d9>] schedule+0x29/0x70
[381360.908396]  [<ffffffffa00c0485>] cv_wait_common+0x105/0x1a0 [spl]
[381360.908402]  [<ffffffff810aaf00>] ? prepare_to_wait_event+0x100/0x100
[381360.908408]  [<ffffffffa00c0535>] __cv_wait+0x15/0x20 [spl]
[381360.908459]  [<ffffffffa0209abb>] txg_wait_open+0x8b/0x110 [zfs]
[381360.908476]  [<ffffffffa01cd83b>] dmu_tx_wait+0x29b/0x2a0 [zfs]
[381360.908492]  [<ffffffffa01cd8cc>] dmu_tx_assign+0x8c/0x460 [zfs]
[381360.908520]  [<ffffffffa025a8c7>] zvol_write+0xa7/0x480 [zfs]
[381360.908527]  [<ffffffffa00bab27>] taskq_thread+0x237/0x4b0 [spl]
[381360.908530]  [<ffffffff81097508>] ? finish_task_switch+0x128/0x170
[381360.908534]  [<ffffffff8109a800>] ? wake_up_state+0x20/0x20
[381360.908539]  [<ffffffffa00ba8f0>] ? taskq_cancel_id+0x1f0/0x1f0 [spl]
[381360.908543]  [<ffffffff8108b3d2>] kthread+0xd2/0xf0
[381360.908545]  [<ffffffff8108b300>] ? kthread_create_on_node+0x1d0/0x1d0
[381360.908548]  [<ffffffff8172c5bc>] ret_from_fork+0x7c/0xb0
[381360.908550]  [<ffffffff8108b300>] ? kthread_create_on_node+0x1d0/0x1d0
```

This was repeated for `zvol/0` through `zvol/9`. There are 31 zvols on this system.

```
[    4.995804] SPL: Loaded module v0.6.3-1~precise
[    5.130074] ZFS: Loaded module v0.6.3-2~precise, ZFS pool version 5000, ZFS filesystem version 5
```

I tried migrating a VM again today and all hell broke loose but I did not get these errors. The system load was in the 100's on an 8 core system. Major I/O wait time. I killed the migration but ended up having at least 3 corrupt zvols anyways. 
",2014-08-18T19:41:18Z,624233
984,openzfs/zfs,40463575,52595665,"i do not have any corruption or problem - the system is stable and running - also the rsync tasks are done 100% perfect - but very slow 1M/second write performance - and while running the whole system is not responding fast - but no errors and no corruption - these ""blocked for more than 120 seconds"" dmesg's come in pairs for rsync and txg_sync once a day
",2014-08-19T06:37:46Z,3446978
985,openzfs/zfs,40463575,53931260,"not similar but related message:

Aug 29 05:37:06 morpheus kernel: [46185.239554] ata6.00: configured for UDMA/133
Aug 29 05:37:06 morpheus kernel: [46185.239562] ata6: EH complete
Aug 29 05:53:40 morpheus kernel: [47179.890587] INFO: task txg_sync:1462 blocked for more than 180 seconds.
Aug 29 05:53:40 morpheus kernel: [47179.890589]       Tainted: P           O  3.16.0_ck1-smtnice6_BFQ_integra_intel #1
Aug 29 05:53:40 morpheus kernel: [47179.890590] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
Aug 29 05:53:40 morpheus kernel: [47179.890591] txg_sync        D 0000000000000006     0  1462      2 0x00000000
Aug 29 05:53:40 morpheus kernel: [47179.890594]  ffff8805fbda9e40 0000000000000046 ffff88020931a9b0 ffff88065b610000
Aug 29 05:53:40 morpheus kernel: [47179.890596]  0000000000000066 ffff8807fb2a0f20 000000000000b020 0000000000013100
Aug 29 05:53:40 morpheus kernel: [47179.890597]  ffff88065b6103a0 00002992b81d47dd ffff880617373fd8 ffff88065b610000
Aug 29 05:53:40 morpheus kernel: [47179.890599] Call Trace:
Aug 29 05:53:40 morpheus kernel: [47179.890605]  [<ffffffff82a3eaa8>] ? io_schedule+0x88/0xd0
Aug 29 05:53:40 morpheus kernel: [47179.890613]  [<ffffffffc028b536>] ? __cv_timedwait+0x96/0x110 [spl]
Aug 29 05:53:40 morpheus kernel: [47179.890616]  [<ffffffff820fa7b0>] ? finish_wait+0x90/0x90
Aug 29 05:53:40 morpheus kernel: [47179.890623]  [<ffffffffc03c38cb>] ? zio_wait+0xeb/0x1a0 [zfs]
Aug 29 05:53:40 morpheus kernel: [47179.890631]  [<ffffffffc03553da>] ? dsl_pool_sync+0xaa/0x450 [zfs]
Aug 29 05:53:40 morpheus kernel: [47179.890639]  [<ffffffffc036d4c3>] ? spa_sync+0x483/0xb20 [zfs]
Aug 29 05:53:40 morpheus kernel: [47179.890642]  [<ffffffff820f7c3d>] ? default_wake_function+0xd/0x20
Aug 29 05:53:40 morpheus kernel: [47179.890644]  [<ffffffff821168bd>] ? ktime_get_ts+0x3d/0xe0
Aug 29 05:53:40 morpheus kernel: [47179.890652]  [<ffffffffc037ce8a>] ? txg_sync_start+0x6ea/0x900 [zfs]
Aug 29 05:53:40 morpheus kernel: [47179.890655]  [<ffffffff820013ca>] ? __switch_to+0x2a/0x560
Aug 29 05:53:40 morpheus kernel: [47179.890662]  [<ffffffffc037cb60>] ? txg_sync_start+0x3c0/0x900 [zfs]
Aug 29 05:53:40 morpheus kernel: [47179.890664]  [<ffffffffc0286f75>] ? spl_kmem_fini+0xa5/0xc0 [spl]
Aug 29 05:53:40 morpheus kernel: [47179.890667]  [<ffffffffc0286f00>] ? spl_kmem_fini+0x30/0xc0 [spl]
Aug 29 05:53:40 morpheus kernel: [47179.890669]  [<ffffffff820ea5dc>] ? kthread+0xbc/0xe0
Aug 29 05:53:40 morpheus kernel: [47179.890670]  [<ffffffff82a40000>] ? __ww_mutex_lock_slowpath+0x8c/0x2cc
Aug 29 05:53:40 morpheus kernel: [47179.890672]  [<ffffffff820ea520>] ? flush_kthread_worker+0x80/0x80
Aug 29 05:53:40 morpheus kernel: [47179.890674]  [<ffffffff82a4242c>] ? ret_from_fork+0x7c/0xb0
Aug 29 05:53:40 morpheus kernel: [47179.890675]  [<ffffffff820ea520>] ? flush_kthread_worker+0x80/0x80
Aug 29 06:28:01 morpheus kernel: [49241.775276] usb 1-1.1: USB disconnect, device number 3

this seems to occur from time to time with a rather slow USB3.0 powered 4TB hdd (Touro Desk 3.0, HGST5K4000) in an external case during rsync & transferring of large files (several GiB)
",2014-08-29T21:20:51Z,4529709
986,openzfs/zfs,40463575,55506136,"clarification for above:

I had this happen with the above mentioned drive - of which I still don't know what causes it (but I suspect it could be related either to the chipset in the external harddrive enclosure where the drive sits in or powersaving features of the XHCI driver & hardware, which I already had issues with in the past)

another drive showed this behavior (a seagate ST3000DM001) which likely underwent a headcrash and did reallocate several sectors (<10). It had been placed in an external enclosure by fantec [db-f8u3e with an incompatible chipset against smartctl] that had shown in the past to have a life & mind of its own: it would occasionally turn off during transfers and causing trouble with other filesystems, on ZFS, however, the files so far seemed fine. The day before yesterday I placed the drive in another external enclosure and it worked well during backups (only transferring several hundreds of MiB of data per backup job incrementally  via rsync) until I decided to run a scrub and check everything: after several hours the drive again screamed and made hearable noises of a head-crash and/or sector re-allocation (had those in the past) and access wasn't possible anymore to the drive

that's where the above posted message occured again
so at least in the second case (ST3000DM001) ZFS seemingly showed indirectly that something was wrong with the hardware/harddrive

so when encountering this message - make sure to double- or triple-check that it's not a hardware-issue instead a ""software""- (ZFS-related) problem
",2014-09-13T20:43:35Z,4529709
987,openzfs/zfs,40463575,55510374,"@wankdanker I think that your issue is separate. It might have been caused by the zvol processing occuring inside an interrupt context. Pull request #2484 might resolve it.
",2014-09-13T23:29:30Z,1386373
988,openzfs/zfs,40463575,55510478,"@freakout42 Would you tell us more about your pool configuration? Also, do you have data deduplication enabled on this pool?
",2014-09-13T23:33:48Z,1386373
989,openzfs/zfs,40463575,55700820,"```
[root@blood ~]# zpool status
  pool: tank
 state: ONLINE
  scan: scrub repaired 0 in 0h53m with 0 errors on Tue Sep  9 12:45:13 2014
config:

    NAME        STATE     READ WRITE CKSUM
    tank        ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
        sda4    ONLINE       0     0     0
        sdb4    ONLINE       0     0     0

errors: No known data errors

[root@blood ~]# zpool get all
NAME  PROPERTY               VALUE                  SOURCE
tank  size                   824G                   -
tank  capacity               34%                    -
tank  altroot                -                      default
tank  health                 ONLINE                 -
tank  guid                   3198719639486948540    default
tank  version                -                      default
tank  bootfs                 -                      default
tank  delegation             on                     default
tank  autoreplace            off                    default
tank  cachefile              -                      default
tank  failmode               wait                   default
tank  listsnapshots          off                    default
tank  autoexpand             off                    default
tank  dedupditto             0                      default
tank  dedupratio             1.00x                  -
tank  free                   538G                   -
tank  allocated              286G                   -
tank  readonly               off                    -
tank  ashift                 0                      default
tank  comment                -                      default
tank  expandsize             0                      -
tank  freeing                0                      default
tank  feature@async_destroy  enabled                local
tank  feature@empty_bpobj    active                 local
tank  feature@lz4_compress   enabled                local
```
",2014-09-16T06:29:41Z,3446978
990,openzfs/zfs,40463575,56999855,"Had similar failures, which occurred during heavy rsync pulls from remote machine. 

I was able to make it happen very quickly by starting up the remote pull.  Did this three times in a row and caused the fault every time. The symptom was that any userland zfs/zpool commands hang, but the machine was still responsive to other commands. 

I set the parameter spl_kmem_cache_slab_limit=0 (it had been spl_kmem_cache_slab_limit=16384), and the problem seems to be gone, or not easliy triggered.

Part of the process which triggers this includes  snapshot renaming, **but no zvols** are involved in this process, although the pool has some.

The pool is a raidz1 pool, and there are no hardware issues on the server.

```
Sep 23 16:17:51 zephyr kernel: [   27.497382] SPL: Loaded module v0.6.3-1~precise
Sep 23 16:17:51 zephyr kernel: [   27.515752] ZFS: Loaded module v0.6.3-2~precise, ZFS pool version 5000, ZFS filesystem version 5

Sep 26 00:11:23 zephyr kernel: [200583.071397] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
Sep 26 00:11:23 zephyr kernel: [200583.093284] txg_sync        D ffffffff8180cbe0     0  4178      2 0x00000000
Sep 26 00:11:23 zephyr kernel: [200583.093291]  ffff8807d9fd1aa0 0000000000000046 00000000000000ff ffffffffa026c280
Sep 26 00:11:23 zephyr kernel: [200583.093297]  ffff8807d9fd1fd8 ffff8807d9fd1fd8 ffff8807d9fd1fd8 00000000000139c0
Sep 26 00:11:23 zephyr kernel: [200583.093303]  ffff8807f3d6ae20 ffff8807e0189710 00000000ffffffff ffffffffa026c280
Sep 26 00:11:23 zephyr kernel: [200583.093309] Call Trace:
Sep 26 00:11:23 zephyr kernel: [200583.093333]  [<ffffffff8169f099>] schedule+0x29/0x70
Sep 26 00:11:23 zephyr kernel: [200583.093338]  [<ffffffff8169f35e>] schedule_preempt_disabled+0xe/0x10
Sep 26 00:11:23 zephyr kernel: [200583.093343]  [<ffffffff8169df77>] __mutex_lock_slowpath+0xd7/0x150
Sep 26 00:11:23 zephyr kernel: [200583.093350]  [<ffffffff8169db8a>] mutex_lock+0x2a/0x50
Sep 26 00:11:23 zephyr kernel: [200583.093395]  [<ffffffffa0230f19>] zvol_rename_minors+0x79/0x180 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093432]  [<ffffffffa01ad349>] dsl_dataset_rename_snapshot_sync_impl+0x189/0x2c0 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093460]  [<ffffffffa01ad52f>] dsl_dataset_rename_snapshot_sync+0xaf/0x190 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093491]  [<ffffffffa01bde52>] dsl_sync_task_sync+0xf2/0x100 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093521]  [<ffffffffa01b63b3>] dsl_pool_sync+0x2f3/0x420 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093554]  [<ffffffffa01cc4f4>] spa_sync+0x414/0xb20 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093590]  [<ffffffffa01db021>] ? spa_txg_history_set+0x21/0xd0 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093625]  [<ffffffffa01de335>] txg_sync_thread+0x385/0x5d0 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093659]  [<ffffffffa01ddfb0>] ? txg_init+0x260/0x260 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.093672]  [<ffffffffa00d3fc8>] thread_generic_wrapper+0x78/0x90 [spl]
Sep 26 00:11:23 zephyr kernel: [200583.093682]  [<ffffffffa00d3f50>] ? __thread_create+0x300/0x300 [spl]
Sep 26 00:11:23 zephyr kernel: [200583.093688]  [<ffffffff81077ee3>] kthread+0x93/0xa0
Sep 26 00:11:23 zephyr kernel: [200583.093694]  [<ffffffff816a9b24>] kernel_thread_helper+0x4/0x10
Sep 26 00:11:23 zephyr kernel: [200583.093700]  [<ffffffff81077e50>] ? flush_kthread_worker+0xb0/0xb0
Sep 26 00:11:23 zephyr kernel: [200583.093704]  [<ffffffff816a9b20>] ? gs_change+0x13/0x13
Sep 26 00:11:23 zephyr kernel: [200583.093737] INFO: task zfs:22581 blocked for more than 120 seconds.
Sep 26 00:11:23 zephyr kernel: [200583.104867] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
Sep 26 00:11:23 zephyr kernel: [200583.126712] zfs             D ffffffff8180cbe0     0 22581  22253 0x00000000
Sep 26 00:11:23 zephyr kernel: [200583.126717]  ffff880182f2f958 0000000000000082 ffff8807eee9c530 ffff8807deda2550
Sep 26 00:11:23 zephyr kernel: [200583.126722]  ffff880182f2ffd8 ffff880182f2ffd8 ffff880182f2ffd8 00000000000139c0
Sep 26 00:11:23 zephyr kernel: [200583.126728]  ffff8807f3f04530 ffff8807eee9c530 0000000000000292 ffff8807deda2550
Sep 26 00:11:23 zephyr kernel: [200583.126733] Call Trace:
Sep 26 00:11:23 zephyr kernel: [200583.126740]  [<ffffffff8169f099>] schedule+0x29/0x70
Sep 26 00:11:23 zephyr kernel: [200583.126753]  [<ffffffffa00dbd8d>] cv_wait_common+0xfd/0x1b0 [spl]
Sep 26 00:11:23 zephyr kernel: [200583.126768]  [<ffffffff810787c0>] ? add_wait_queue+0x60/0x60
Sep 26 00:11:23 zephyr kernel: [200583.126779]  [<ffffffffa00dbe95>] __cv_wait+0x15/0x20 [spl]
Sep 26 00:11:23 zephyr kernel: [200583.126813]  [<ffffffffa01c4aec>] rrw_enter_read+0x3c/0x130 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126849]  [<ffffffffa01c4c70>] rrw_enter+0x20/0x30 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126878]  [<ffffffffa01b51cd>] dsl_pool_config_enter+0x1d/0x20 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126915]  [<ffffffffa01b709a>] dsl_pool_hold+0x4a/0x60 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126927]  [<ffffffffa019957b>] dmu_objset_hold+0x2b/0xb0 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126929]  [<ffffffff816a85d6>] ? ftrace_call+0x5/0x2b
Sep 26 00:11:23 zephyr kernel: [200583.126942]  [<ffffffffa01b8a6f>] dsl_prop_get+0x3f/0x90 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126944]  [<ffffffff816a85d6>] ? ftrace_call+0x5/0x2b
Sep 26 00:11:23 zephyr kernel: [200583.126956]  [<ffffffffa01b8ade>] dsl_prop_get_integer+0x1e/0x20 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126970]  [<ffffffffa022ef6f>] __zvol_create_minor+0xbf/0x630 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126985]  [<ffffffffa0230bc7>] zvol_create_minor+0x27/0x40 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.126998]  [<ffffffffa0230bee>] zvol_create_minors_cb+0xe/0x20 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127010]  [<ffffffffa0197e6e>] dmu_objset_find_impl+0x37e/0x3f0 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127023]  [<ffffffffa0230be0>] ? zvol_create_minor+0x40/0x40 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127034]  [<ffffffffa0197cae>] dmu_objset_find_impl+0x1be/0x3f0 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127048]  [<ffffffffa0230be0>] ? zvol_create_minor+0x40/0x40 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127061]  [<ffffffffa0230be0>] ? zvol_create_minor+0x40/0x40 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127072]  [<ffffffffa0197f32>] dmu_objset_find+0x52/0x80 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127074]  [<ffffffff816a85d6>] ? ftrace_call+0x5/0x2b
Sep 26 00:11:23 zephyr kernel: [200583.127088]  [<ffffffffa0230d53>] zvol_create_minors+0x33/0x40 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127102]  [<ffffffffa0202a69>] zfs_ioc_snapshot+0x259/0x2a0 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127116]  [<ffffffffa0206530>] zfsdev_ioctl+0x180/0x500 [zfs]
Sep 26 00:11:23 zephyr kernel: [200583.127118]  [<ffffffff816a85d6>] ? ftrace_call+0x5/0x2b
Sep 26 00:11:23 zephyr kernel: [200583.127131]  [<ffffffff8119ae9a>] do_vfs_ioctl+0x8a/0x340
Sep 26 00:11:23 zephyr kernel: [200583.127135]  [<ffffffff8119b1e1>] sys_ioctl+0x91/0xa0
Sep 26 00:11:23 zephyr kernel: [200583.127141]  [<ffffffff816a8829>] system_call_fastpath+0x16/0x1b
```
",2014-09-26T18:14:41Z,2551893
991,openzfs/zfs,40463575,57042179,"@ColdCanuck Your comments regarding `spl_kmem_cache_slab_limit` piqued my interest in this issue which until now, I've not had time to follow.  In fact, a quick skim of this whole issue makes me wonder how related each of the problem reports are and whether there's too much issue creep.

Back to the point at hand:  I'm posting this followup because there have been a disturbing number of seemingly otherwise unrelated problems sporadically seemingly caused by using the Linux slab.  Although I've not been able to spend the time on it I've wanted to, I've been rather knee deep investigating the series of issues related to Posix ACLs and SA xattrs and have seen at least one report (#2701) and, more interestingly #2725 which makes me think there may be a tie-in to our use of the Linux slab for <= 16KiB objects.  I don't have any other brilliant observations offer at the moment other than to raise concern there may be problems realted to using the Linux slab and to ask @behlendorf, @ryao et al. what your thoughts are on this (particularly given the last few comments in #2725).
",2014-09-27T04:36:29Z,1432005
992,openzfs/zfs,40463575,57053655,"just posting what comes to mind:

could scheduling a regular cronjob which compacts memory via
echo 1 > /proc/sys/vm/compact_memory

change things (provided slab issues and timeouts are related to memory fragmentation)
",2014-09-27T14:06:38Z,4529709
993,openzfs/zfs,40463575,70959389,"I'm systematically having this issue when trying to RSync when using the latest ZFS from Arch: zfs-git 0.6.3_r170_gd958324f_3.18.2_2-1

```
[46181.967521] perf interrupt took too long (2506 > 2495), lowering kernel.perf_event_max_sample_rate to 50100
[79468.027144] INFO: task txg_sync:583 blocked for more than 120 seconds.
[79468.027287]       Tainted: P           O   3.18.2-2-ARCH #1
[79468.027363] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[79468.027468] txg_sync        D 0000000000000000     0   583      2 0x00000000
[79468.027476]  ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[79468.027483]  ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[79468.027489]  ffff8801591955b8 ffffffff00000000 ffff880159195570 00000000025e92cf
[79468.027494] Call Trace:
[79468.027509]  [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[79468.027518]  [<ffffffff81550b59>] schedule+0x29/0x70
[79468.027524]  [<ffffffff81550e38>] io_schedule+0x98/0x100
[79468.027547]  [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[79468.027554]  [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[79468.027565]  [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[79468.027588]  [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[79468.027616]  [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[79468.027625]  [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[79468.027656]  [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[79468.027663]  [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[79468.027694]  [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[79468.027724]  [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[79468.027734]  [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[79468.027742]  [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[79468.027749]  [<ffffffff81090e0a>] kthread+0xea/0x100
[79468.027755]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[79468.027761]  [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[79468.027767]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[82589.120097] INFO: task txg_sync:583 blocked for more than 120 seconds.
[82589.120252]       Tainted: P           O   3.18.2-2-ARCH #1
[82589.120347] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[82589.120479] txg_sync        D 0000000000000001     0   583      2 0x00000000
[82589.120489]  ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[82589.120497]  ffff88021ce03fd8 0000000000013640 ffff8802240ceeb0 ffff88021cd5eeb0
[82589.120505]  ffff88022363f7b0 ffff88022363f798 0000000000000000 0000000000000003
[82589.120512] Call Trace:
[82589.120529]  [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[82589.120540]  [<ffffffff810b2715>] ? __wake_up_common+0x55/0x90
[82589.120549]  [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[82589.120558]  [<ffffffff81550b59>] schedule+0x29/0x70
[82589.120564]  [<ffffffff81550e38>] io_schedule+0x98/0x100
[82589.120591]  [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[82589.120599]  [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[82589.120613]  [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[82589.120641]  [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[82589.120676]  [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[82589.120689]  [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[82589.120727]  [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[82589.120735]  [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[82589.120774]  [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[82589.120811]  [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[82589.120823]  [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[82589.120834]  [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[82589.120842]  [<ffffffff81090e0a>] kthread+0xea/0x100
[82589.120849]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[82589.120858]  [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[82589.120864]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89311.468460] INFO: task txg_sync:583 blocked for more than 120 seconds.
[89311.468644]       Tainted: P           O   3.18.2-2-ARCH #1
[89311.468741] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[89311.468872] txg_sync        D 0000000000000000     0   583      2 0x00000000
[89311.468882]  ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[89311.468890]  ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[89311.468897]  ffff88001afc4498 fffffffe00000000 ffff88001afc4490 00000000025e92cf
[89311.468905] Call Trace:
[89311.468922]  [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[89311.468933]  [<ffffffff81550b59>] schedule+0x29/0x70
[89311.468940]  [<ffffffff81550e38>] io_schedule+0x98/0x100
[89311.468968]  [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[89311.468976]  [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[89311.468989]  [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[89311.469017]  [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[89311.469052]  [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[89311.469065]  [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[89311.469103]  [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[89311.469112]  [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[89311.469150]  [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[89311.469187]  [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[89311.469199]  [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[89311.469211]  [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[89311.469218]  [<ffffffff81090e0a>] kthread+0xea/0x100
[89311.469226]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89311.469234]  [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[89311.469241]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
```

```
hinoki% free -h
              total        used        free      shared  buff/cache   available
Mem:           7.8G        4.3G        533M        1.1M        3.0G        702M
Swap:          4.0G          0B        4.0G
```
",2015-01-22T02:00:11Z,30030
994,openzfs/zfs,40463575,70960389,"Still trying to run rsync:

```
[89431.510429] INFO: task txg_sync:583 blocked for more than 120 seconds.
[89431.510571]       Tainted: P           O   3.18.2-2-ARCH #1
[89431.510647] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[89431.510753] txg_sync        D 0000000000000000     0   583      2 0x00000000
[89431.510762]  ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[89431.510769]  ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[89431.510774]  ffff88001afc4498 fffffffe00000000 ffff88001afc4490 00000000025e92cf
[89431.510780] Call Trace:
[89431.510796]  [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[89431.510805]  [<ffffffff81550b59>] schedule+0x29/0x70
[89431.510811]  [<ffffffff81550e38>] io_schedule+0x98/0x100
[89431.510836]  [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[89431.510842]  [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[89431.510853]  [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[89431.510877]  [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[89431.510906]  [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[89431.510915]  [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[89431.510946]  [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[89431.510953]  [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[89431.510984]  [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[89431.511014]  [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[89431.511024]  [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[89431.511033]  [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[89431.511039]  [<ffffffff81090e0a>] kthread+0xea/0x100
[89431.511045]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89431.511052]  [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[89431.511058]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89551.552404] INFO: task txg_sync:583 blocked for more than 120 seconds.
[89551.552565]       Tainted: P           O   3.18.2-2-ARCH #1
[89551.552660] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[89551.552792] txg_sync        D 0000000000000000     0   583      2 0x00000000
[89551.552803]  ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[89551.552812]  ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[89551.552819]  ffff88001afc4498 fffffffe00000000 ffff88001afc4490 00000000025e92cf
[89551.552826] Call Trace:
[89551.552844]  [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[89551.552854]  [<ffffffff81550b59>] schedule+0x29/0x70
[89551.552862]  [<ffffffff81550e38>] io_schedule+0x98/0x100
[89551.552890]  [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[89551.552898]  [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[89551.552911]  [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[89551.552940]  [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[89551.552975]  [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[89551.552987]  [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[89551.553025]  [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[89551.553034]  [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[89551.553073]  [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[89551.553110]  [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[89551.553122]  [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[89551.553133]  [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[89551.553140]  [<ffffffff81090e0a>] kthread+0xea/0x100
[89551.553148]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89551.553156]  [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[89551.553163]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89671.594371] INFO: task txg_sync:583 blocked for more than 120 seconds.
[89671.594507]       Tainted: P           O   3.18.2-2-ARCH #1
[89671.594605] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[89671.594711] txg_sync        D 0000000000000000     0   583      2 0x00000000
[89671.594720]  ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[89671.594727]  ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[89671.594733]  ffff88001afc4498 fffffffe00000000 ffff88001afc4490 00000000025e92cf
[89671.594738] Call Trace:
[89671.594753]  [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[89671.594762]  [<ffffffff81550b59>] schedule+0x29/0x70
[89671.594768]  [<ffffffff81550e38>] io_schedule+0x98/0x100
[89671.594792]  [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[89671.594798]  [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[89671.594809]  [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[89671.594832]  [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[89671.594860]  [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[89671.594870]  [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[89671.594901]  [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[89671.594908]  [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[89671.594939]  [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[89671.594969]  [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[89671.594979]  [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[89671.594988]  [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[89671.594994]  [<ffffffff81090e0a>] kthread+0xea/0x100
[89671.595000]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89671.595007]  [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[89671.595013]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89791.636364] INFO: task txg_sync:583 blocked for more than 120 seconds.
[89791.636507]       Tainted: P           O   3.18.2-2-ARCH #1
[89791.636583] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[89791.636688] txg_sync        D 0000000000000000     0   583      2 0x00000000
[89791.636697]  ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[89791.636703]  ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[89791.636709]  ffff88001afc4498 fffffffe00000000 ffff88001afc4490 00000000025e92cf
[89791.636715] Call Trace:
[89791.636729]  [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[89791.636738]  [<ffffffff81550b59>] schedule+0x29/0x70
[89791.636744]  [<ffffffff81550e38>] io_schedule+0x98/0x100
[89791.636767]  [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[89791.636774]  [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[89791.636785]  [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[89791.636808]  [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[89791.636836]  [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[89791.636852]  [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[89791.636883]  [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[89791.636890]  [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[89791.636921]  [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[89791.636951]  [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[89791.636960]  [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[89791.636969]  [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[89791.636975]  [<ffffffff81090e0a>] kthread+0xea/0x100
[89791.636981]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[89791.636989]  [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[89791.636994]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[90105.367103] systemd[1]: systemd-journald.service stop-sigabrt timed out. Terminating.
[90105.490377] systemd[1]: Listening on Journal Audit Socket.
[90105.490436] systemd[1]: Starting Journal Service...
[90151.762415] INFO: task kswapd0:31 blocked for more than 120 seconds.
[90151.762569]       Tainted: P           O   3.18.2-2-ARCH #1
[90151.762663] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[90151.762795] kswapd0         D 0000000000000000     0    31      2 0x00000000
[90151.762805]  ffff8802235a3908 0000000000000046 ffff880224225a90 0000000000013640
[90151.762814]  ffff8802235a3fd8 0000000000013640 ffffffff81818540 ffff880224225a90
[90151.762821]  ffff88022fc93640 ffff88021cd5e4a0 ffff88021cd5ec02 0000000000000000
[90151.762828] Call Trace:
[90151.762845]  [<ffffffff8109e6e7>] ? try_to_wake_up+0x1e7/0x380
[90151.762856]  [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[90151.762865]  [<ffffffff81550b59>] schedule+0x29/0x70
[90151.762893]  [<ffffffffa01cbb8d>] __cv_broadcast+0x12d/0x160 [spl]
[90151.762902]  [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[90151.762915]  [<ffffffffa01cbbd5>] __cv_wait+0x15/0x20 [spl]
[90151.762956]  [<ffffffffa02b6d03>] txg_wait_open+0x73/0xb0 [zfs]
[90151.762984]  [<ffffffffa027514a>] dmu_tx_wait+0x33a/0x350 [zfs]
[90151.763011]  [<ffffffffa02751f1>] dmu_tx_assign+0x91/0x4b0 [zfs]
[90151.763040]  [<ffffffffa02f2a73>] zfs_inactive+0x163/0x200 [zfs]
[90151.763049]  [<ffffffff810b2e70>] ? autoremove_wake_function+0x40/0x40
[90151.763075]  [<ffffffffa030af18>] zpl_vap_init+0x838/0xa10 [zfs]
[90151.763083]  [<ffffffff811eab68>] evict+0xb8/0x1b0
[90151.763090]  [<ffffffff811eaca1>] dispose_list+0x41/0x50
[90151.763097]  [<ffffffff811ebce6>] prune_icache_sb+0x56/0x80
[90151.763106]  [<ffffffff811d2b25>] super_cache_scan+0x115/0x180
[90151.763115]  [<ffffffff81169e89>] shrink_slab_node+0x129/0x2f0
[90151.763123]  [<ffffffff8116abcb>] shrink_slab+0x8b/0x160
[90151.763131]  [<ffffffff8116e0e9>] kswapd_shrink_zone+0x129/0x1d0
[90151.763138]  [<ffffffff8116eb7a>] kswapd+0x54a/0x8f0
[90151.763147]  [<ffffffff8116e630>] ? mem_cgroup_shrink_node_zone+0x1c0/0x1c0
[90151.763155]  [<ffffffff81090e0a>] kthread+0xea/0x100
[90151.763162]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[90151.763171]  [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[90151.763178]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[90151.763190] INFO: task systemd-journal:138 blocked for more than 120 seconds.
[90151.763340]       Tainted: P           O   3.18.2-2-ARCH #1
[90151.763433] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[90151.763563] systemd-journal D 0000000000000000     0   138      1 0x00000004
[90151.763571]  ffff88022310b128 0000000000000082 ffff880222c25080 0000000000013640
[90151.763577]  ffff88022310bfd8 0000000000013640 ffff88021cd5e4a0 ffff880222c25080
[90151.763583]  ffff88022fc93640 ffff88021cd5e4a0 ffff88021cd5ec02 0000000000000000
[90151.763590] Call Trace:
[90151.763599]  [<ffffffff8109e6e7>] ? try_to_wake_up+0x1e7/0x380
[90151.763607]  [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[90151.763614]  [<ffffffff81550b59>] schedule+0x29/0x70
[90151.763627]  [<ffffffffa01cbb8d>] __cv_broadcast+0x12d/0x160 [spl]
[90151.763635]  [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[90151.763647]  [<ffffffffa01cbbd5>] __cv_wait+0x15/0x20 [spl]
[90151.763683]  [<ffffffffa02b6d03>] txg_wait_open+0x73/0xb0 [zfs]
[90151.763710]  [<ffffffffa027514a>] dmu_tx_wait+0x33a/0x350 [zfs]
[90151.763737]  [<ffffffffa02751f1>] dmu_tx_assign+0x91/0x4b0 [zfs]
[90151.763765]  [<ffffffffa02f2a73>] zfs_inactive+0x163/0x200 [zfs]
[90151.763774]  [<ffffffff810b2e70>] ? autoremove_wake_function+0x40/0x40
[90151.763799]  [<ffffffffa030af18>] zpl_vap_init+0x838/0xa10 [zfs]
[90151.763805]  [<ffffffff811eab68>] evict+0xb8/0x1b0
[90151.763812]  [<ffffffff811eaca1>] dispose_list+0x41/0x50
[90151.763819]  [<ffffffff811ebce6>] prune_icache_sb+0x56/0x80
[90151.763827]  [<ffffffff811d2b25>] super_cache_scan+0x115/0x180
[90151.763834]  [<ffffffff81169e89>] shrink_slab_node+0x129/0x2f0
[90151.763842]  [<ffffffff811c1523>] ? mem_cgroup_iter+0x2f3/0x4d0
[90151.763850]  [<ffffffff8116abcb>] shrink_slab+0x8b/0x160
[90151.763858]  [<ffffffff8116da45>] do_try_to_free_pages+0x365/0x4e0
[90151.763866]  [<ffffffff8116dc71>] try_to_free_pages+0xb1/0x1a0
[90151.763873]  [<ffffffff81160ca7>] __alloc_pages_nodemask+0x697/0xb50
[90151.763884]  [<ffffffff811a6e9c>] alloc_pages_current+0x9c/0x120
[90151.763891]  [<ffffffff811afba5>] new_slab+0x305/0x370
[90151.763899]  [<ffffffff811b2175>] __slab_alloc.isra.51+0x545/0x650
[90151.763907]  [<ffffffff81445af9>] ? __alloc_skb+0x89/0x210
[90151.763914]  [<ffffffff814380f4>] ? raw_pci_write+0x24/0x50
[90151.763923]  [<ffffffff812e8be6>] ? pci_bus_write_config_word+0x66/0x80
[90151.763949]  [<ffffffffa004ed7f>] ? ata_bmdma_start+0x2f/0x40 [libata]
[90151.763960]  [<ffffffffa01121ad>] ? atiixp_bmdma_start+0x9d/0xe0 [pata_atiixp]
[90151.763969]  [<ffffffff811b5445>] __kmalloc_node_track_caller+0xa5/0x240
[90151.763976]  [<ffffffff81445af9>] ? __alloc_skb+0x89/0x210
[90151.763984]  [<ffffffff81445a11>] __kmalloc_reserve.isra.38+0x31/0x90
[90151.763990]  [<ffffffff81445acb>] ? __alloc_skb+0x5b/0x210
[90151.763997]  [<ffffffff81445af9>] __alloc_skb+0x89/0x210
[90151.764004]  [<ffffffff81445de4>] alloc_skb_with_frags+0x64/0x1e0
[90151.764011]  [<ffffffff8143efb9>] sock_alloc_send_pskb+0x219/0x290
[90151.764020]  [<ffffffff810136fb>] ? __switch_to+0x1fb/0x600
[90151.764029]  [<ffffffff814f886d>] unix_dgram_sendmsg+0x18d/0x690
[90151.764037]  [<ffffffff8143ac39>] sock_sendmsg+0x79/0xb0
[90151.764045]  [<ffffffff810986da>] ? finish_task_switch+0x4a/0xf0
[90151.764051]  [<ffffffff815504c8>] ? __schedule+0x3e8/0xa50
[90151.764059]  [<ffffffff8143c76c>] ? move_addr_to_kernel+0x2c/0x50
[90151.764066]  [<ffffffff8144a3c7>] ? verify_iovec+0x47/0xd0
[90151.764074]  [<ffffffff8143b928>] ___sys_sendmsg+0x408/0x420
[90151.764083]  [<ffffffff812149d0>] ? ep_read_events_proc+0xe0/0xe0
[90151.764089]  [<ffffffff81440030>] ? sk_prot_alloc.isra.33+0x30/0x130
[90151.764097]  [<ffffffff811b291a>] ? kmem_cache_alloc+0x16a/0x170
[90151.764104]  [<ffffffff811d100c>] ? get_empty_filp+0x5c/0x1c0
[90151.764112]  [<ffffffff8126e496>] ? security_file_alloc+0x16/0x20
[90151.764118]  [<ffffffff811d1084>] ? get_empty_filp+0xd4/0x1c0
[90151.764126]  [<ffffffff811d118f>] ? alloc_file+0x1f/0xb0
[90151.764134]  [<ffffffff8143d651>] __sys_sendmsg+0x51/0x90
[90151.764142]  [<ffffffff8143d6a2>] SyS_sendmsg+0x12/0x20
[90151.764149]  [<ffffffff81554ca9>] system_call_fastpath+0x12/0x17
[90151.764186] INFO: task txg_sync:583 blocked for more than 120 seconds.
[90151.764325]       Tainted: P           O   3.18.2-2-ARCH #1
[90151.764418] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[90151.764547] txg_sync        D 0000000000000000     0   583      2 0x00000000
[90151.764554]  ffff88021ce03b18 0000000000000046 ffff88021cd5eeb0 0000000000013640
[90151.764560]  ffff88021ce03fd8 0000000000013640 ffff88021f601420 ffff88021cd5eeb0
[90151.764566]  ffff88018de384b8 ffffffff00000000 ffff88018de38490 00000000025e92cf
[90151.764572] Call Trace:
[90151.764581]  [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[90151.764588]  [<ffffffff81550b59>] schedule+0x29/0x70
[90151.764594]  [<ffffffff81550e38>] io_schedule+0x98/0x100
[90151.764607]  [<ffffffffa01cbb45>] __cv_broadcast+0xe5/0x160 [spl]
[90151.764615]  [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[90151.764627]  [<ffffffffa01cbc18>] __cv_wait_io+0x18/0x180 [spl]
[90151.764651]  [<ffffffffa030367b>] zio_wait+0x11b/0x200 [zfs]
[90151.764686]  [<ffffffffa028ac71>] dsl_pool_sync+0xc1/0x480 [zfs]
[90151.764698]  [<ffffffffa01c60af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[90151.764735]  [<ffffffffa02a5b60>] spa_sync+0x480/0xbd0 [zfs]
[90151.764744]  [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[90151.764781]  [<ffffffffa02b730c>] txg_delay+0x4ec/0xa60 [zfs]
[90151.764818]  [<ffffffffa02b6f90>] ? txg_delay+0x170/0xa60 [zfs]
[90151.764830]  [<ffffffffa01c707a>] __thread_exit+0x9a/0xb0 [spl]
[90151.764841]  [<ffffffffa01c7000>] ? __thread_exit+0x20/0xb0 [spl]
[90151.764847]  [<ffffffff81090e0a>] kthread+0xea/0x100
[90151.764855]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[90151.764862]  [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[90151.764869]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[90195.648689] systemd[1]: systemd-journald.service stop-sigterm timed out. Killing.
[90195.649942] systemd[1]: Starting Journal Service...
```
",2015-01-22T02:12:12Z,30030
995,openzfs/zfs,40463575,71075154,"This might be caused by #2523, can you verify you have the fix to the SPL applied: zfsonlinux/spl@a3c1eb77721a0d511b4fe7111bb2314686570c4b
",2015-01-22T18:48:14Z,148917
996,openzfs/zfs,40463575,71172250,"More of the same

```
[ 1320.575152] INFO: task txg_sync:583 blocked for more than 120 seconds.
[ 1320.575263]       Tainted: P           O   3.18.2-2-ARCH #1
[ 1320.575314] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1320.575384] txg_sync        D 0000000000000000     0   583      2 0x00000000
[ 1320.575390]  ffff88021ba43b18 0000000000000046 ffff8800cbfc4670 0000000000013640
[ 1320.575395]  ffff88021ba43fd8 0000000000013640 ffff88021dee2840 ffff8800cbfc4670
[ 1320.575399]  ffff880131dcc138 ffffffff00000000 ffff880131dcc130 00000000b3f1aac7
[ 1320.575402] Call Trace:
[ 1320.575414]  [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[ 1320.575420]  [<ffffffff81550b59>] schedule+0x29/0x70
[ 1320.575424]  [<ffffffff81550e38>] io_schedule+0x98/0x100
[ 1320.575442]  [<ffffffffa01cfb45>] __cv_broadcast+0xe5/0x160 [spl]
[ 1320.575447]  [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1320.575453]  [<ffffffffa01cfc18>] __cv_wait_io+0x18/0x180 [spl]
[ 1320.575470]  [<ffffffffa030767b>] zio_wait+0x11b/0x200 [zfs]
[ 1320.575490]  [<ffffffffa028ec71>] dsl_pool_sync+0xc1/0x480 [zfs]
[ 1320.575496]  [<ffffffffa01ca0af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[ 1320.575517]  [<ffffffffa02a9b60>] spa_sync+0x480/0xbd0 [zfs]
[ 1320.575522]  [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[ 1320.575543]  [<ffffffffa02bb30c>] txg_delay+0x4ec/0xa60 [zfs]
[ 1320.575563]  [<ffffffffa02baf90>] ? txg_delay+0x170/0xa60 [zfs]
[ 1320.575569]  [<ffffffffa01cb07a>] __thread_exit+0x9a/0xb0 [spl]
[ 1320.575575]  [<ffffffffa01cb000>] ? __thread_exit+0x20/0xb0 [spl]
[ 1320.575579]  [<ffffffff81090e0a>] kthread+0xea/0x100
[ 1320.575583]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1320.575588]  [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[ 1320.575592]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1560.661841] INFO: task txg_sync:583 blocked for more than 120 seconds.
[ 1560.661995]       Tainted: P           O   3.18.2-2-ARCH #1
[ 1560.662090] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1560.662221] txg_sync        D 0000000000000000     0   583      2 0x00000000
[ 1560.662232]  ffff88021ba43b18 0000000000000046 ffff8800cbfc4670 0000000000013640
[ 1560.662240]  ffff88021ba43fd8 0000000000013640 ffffffff81818540 ffff8800cbfc4670
[ 1560.662247]  ffff880223335eb0 ffff880223335e98 0000000000000000 0000000000000003
[ 1560.662255] Call Trace:
[ 1560.662272]  [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[ 1560.662283]  [<ffffffff810b2715>] ? __wake_up_common+0x55/0x90
[ 1560.662292]  [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[ 1560.662301]  [<ffffffff81550b59>] schedule+0x29/0x70
[ 1560.662307]  [<ffffffff81550e38>] io_schedule+0x98/0x100
[ 1560.662335]  [<ffffffffa01cfb45>] __cv_broadcast+0xe5/0x160 [spl]
[ 1560.662343]  [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1560.662355]  [<ffffffffa01cfc18>] __cv_wait_io+0x18/0x180 [spl]
[ 1560.662383]  [<ffffffffa030767b>] zio_wait+0x11b/0x200 [zfs]
[ 1560.662418]  [<ffffffffa028ec71>] dsl_pool_sync+0xc1/0x480 [zfs]
[ 1560.662430]  [<ffffffffa01ca0af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[ 1560.662468]  [<ffffffffa02a9b60>] spa_sync+0x480/0xbd0 [zfs]
[ 1560.662477]  [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[ 1560.662516]  [<ffffffffa02bb30c>] txg_delay+0x4ec/0xa60 [zfs]
[ 1560.662552]  [<ffffffffa02baf90>] ? txg_delay+0x170/0xa60 [zfs]
[ 1560.662564]  [<ffffffffa01cb07a>] __thread_exit+0x9a/0xb0 [spl]
[ 1560.662575]  [<ffffffffa01cb000>] ? __thread_exit+0x20/0xb0 [spl]
[ 1560.662583]  [<ffffffff81090e0a>] kthread+0xea/0x100
[ 1560.662590]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1560.662598]  [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[ 1560.662605]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1560.662617] INFO: task java:1218 blocked for more than 120 seconds.
[ 1560.662753]       Tainted: P           O   3.18.2-2-ARCH #1
[ 1560.662846] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1560.662975] java            D 0000000000000001     0  1218    655 0x00000000
[ 1560.662983]  ffff8800c17f7ac8 0000000000000086 ffff8800c33e1420 0000000000013640
[ 1560.662990]  ffff8800c17f7fd8 0000000000013640 ffff8802240ceeb0 ffff8800c33e1420
[ 1560.662997]  0000008000000000 ffff88002041b7a0 0000000000000000 ffff88002041b778
[ 1560.663003] Call Trace:
[ 1560.663027]  [<ffffffffa0260398>] ? dbuf_rele_and_unlock+0x2c8/0x4d0 [zfs]
[ 1560.663049]  [<ffffffffa0261eca>] ? dbuf_read+0x8da/0xf20 [zfs]
[ 1560.663061]  [<ffffffffa01c8901>] ? kmem_asprintf+0x51/0x80 [spl]
[ 1560.663068]  [<ffffffff81550b59>] schedule+0x29/0x70
[ 1560.663080]  [<ffffffffa01cfb8d>] __cv_broadcast+0x12d/0x160 [spl]
[ 1560.663088]  [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1560.663099]  [<ffffffffa01cfbd5>] __cv_wait+0x15/0x20 [spl]
[ 1560.663126]  [<ffffffffa0278eab>] dmu_tx_wait+0x9b/0x350 [zfs]
[ 1560.663153]  [<ffffffffa02791f1>] dmu_tx_assign+0x91/0x4b0 [zfs]
[ 1560.663161]  [<ffffffff810e1006>] ? getrawmonotonic+0x36/0xd0
[ 1560.663186]  [<ffffffffa0268e1c>] dmu_free_long_range+0x1ac/0x280 [zfs]
[ 1560.663217]  [<ffffffffa02daf1c>] zfs_rmnode+0x6c/0x340 [zfs]
[ 1560.663244]  [<ffffffffa02fe141>] zfs_zinactive+0xc1/0x1d0 [zfs]
[ 1560.663273]  [<ffffffffa02f6974>] zfs_inactive+0x64/0x200 [zfs]
[ 1560.663281]  [<ffffffff810b2e70>] ? autoremove_wake_function+0x40/0x40
[ 1560.663307]  [<ffffffffa030ef18>] zpl_vap_init+0x838/0xa10 [zfs]
[ 1560.663315]  [<ffffffff811eab68>] evict+0xb8/0x1b0
[ 1560.663322]  [<ffffffff811eb405>] iput+0xf5/0x1a0
[ 1560.663330]  [<ffffffff811df7f2>] do_unlinkat+0x1e2/0x350
[ 1560.663337]  [<ffffffff811d4839>] ? SyS_newstat+0x39/0x60
[ 1560.663345]  [<ffffffff811e02c6>] SyS_unlink+0x16/0x20
[ 1560.663353]  [<ffffffff81554ca9>] system_call_fastpath+0x12/0x17
[ 1560.663371] INFO: task sshd:1381 blocked for more than 120 seconds.
[ 1560.663508]       Tainted: P           O   3.18.2-2-ARCH #1
[ 1560.663601] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1560.663791] sshd            D 0000000000000001     0  1381    764 0x00000004
[ 1560.663798]  ffff880113ad7b38 0000000000000086 ffff8800a7356eb0 0000000000013640
[ 1560.663804]  ffff880113ad7fd8 0000000000013640 ffff8802240ceeb0 ffff8800a7356eb0
[ 1560.663810]  ffff8800c9ccabb8 ffff8800c9ccabb8 0000000000000280 0000000000000001
[ 1560.663816] Call Trace:
[ 1560.663825]  [<ffffffff81550b59>] schedule+0x29/0x70
[ 1560.663839]  [<ffffffffa01cfb8d>] __cv_broadcast+0x12d/0x160 [spl]
[ 1560.663847]  [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1560.663858]  [<ffffffffa01cfbd5>] __cv_wait+0x15/0x20 [spl]
[ 1560.663885]  [<ffffffffa0278eab>] dmu_tx_wait+0x9b/0x350 [zfs]
[ 1560.663912]  [<ffffffffa02791f1>] dmu_tx_assign+0x91/0x4b0 [zfs]
[ 1560.663940]  [<ffffffffa02f66d7>] zfs_dirty_inode+0xf7/0x330 [zfs]
[ 1560.663951]  [<ffffffffa01c8956>] ? kmem_free_debug+0x16/0x20 [spl]
[ 1560.663962]  [<ffffffffa01d0b10>] ? crfree+0x170/0x180 [spl]
[ 1560.663972]  [<ffffffffa01d123d>] ? tsd_exit+0x19d/0x1b0 [spl]
[ 1560.663998]  [<ffffffffa02fe288>] ? zfs_tstamp_update_setup+0x38/0x1c0 [zfs]
[ 1560.664026]  [<ffffffffa02f0cfe>] ? zfs_read+0x39e/0x460 [zfs]
[ 1560.664049]  [<ffffffffa030ef2e>] zpl_vap_init+0x84e/0xa10 [zfs]
[ 1560.664056]  [<ffffffff811fb0f8>] __mark_inode_dirty+0x38/0x2d0
[ 1560.664082]  [<ffffffffa02fb3cd>] zfs_mark_inode_dirty+0x4d/0x60 [zfs]
[ 1560.664106]  [<ffffffffa030d626>] zpl_putpage+0x576/0xd50 [zfs]
[ 1560.664114]  [<ffffffff811d0d3c>] __fput+0x9c/0x200
[ 1560.664122]  [<ffffffff811d0eee>] ____fput+0xe/0x10
[ 1560.664128]  [<ffffffff8108f33f>] task_work_run+0x9f/0xe0
[ 1560.664137]  [<ffffffff81014e75>] do_notify_resume+0x95/0xa0
[ 1560.664145]  [<ffffffff81554f20>] int_signal+0x12/0x17
[ 1680.702704] INFO: task txg_sync:583 blocked for more than 120 seconds.
[ 1680.702864]       Tainted: P           O   3.18.2-2-ARCH #1
[ 1680.702958] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1680.703089] txg_sync        D 0000000000000000     0   583      2 0x00000000
[ 1680.703100]  ffff88021ba43b18 0000000000000046 ffff8800cbfc4670 0000000000013640
[ 1680.703108]  ffff88021ba43fd8 0000000000013640 ffffffff81818540 ffff8800cbfc4670
[ 1680.703116]  ffff880223335eb0 ffff880223335e98 0000000000000000 0000000000000003
[ 1680.703123] Call Trace:
[ 1680.703141]  [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[ 1680.703152]  [<ffffffff810b2715>] ? __wake_up_common+0x55/0x90
[ 1680.703161]  [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[ 1680.703169]  [<ffffffff81550b59>] schedule+0x29/0x70
[ 1680.703176]  [<ffffffff81550e38>] io_schedule+0x98/0x100
[ 1680.703203]  [<ffffffffa01cfb45>] __cv_broadcast+0xe5/0x160 [spl]
[ 1680.703211]  [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1680.703224]  [<ffffffffa01cfc18>] __cv_wait_io+0x18/0x180 [spl]
[ 1680.703252]  [<ffffffffa030767b>] zio_wait+0x11b/0x200 [zfs]
[ 1680.703287]  [<ffffffffa028ec71>] dsl_pool_sync+0xc1/0x480 [zfs]
[ 1680.703299]  [<ffffffffa01ca0af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[ 1680.703338]  [<ffffffffa02a9b60>] spa_sync+0x480/0xbd0 [zfs]
[ 1680.703346]  [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[ 1680.703385]  [<ffffffffa02bb30c>] txg_delay+0x4ec/0xa60 [zfs]
[ 1680.703422]  [<ffffffffa02baf90>] ? txg_delay+0x170/0xa60 [zfs]
[ 1680.703433]  [<ffffffffa01cb07a>] __thread_exit+0x9a/0xb0 [spl]
[ 1680.703444]  [<ffffffffa01cb000>] ? __thread_exit+0x20/0xb0 [spl]
[ 1680.703452]  [<ffffffff81090e0a>] kthread+0xea/0x100
[ 1680.703459]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1680.703467]  [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[ 1680.703474]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1800.742712] INFO: task txg_sync:583 blocked for more than 120 seconds.
[ 1800.742869]       Tainted: P           O   3.18.2-2-ARCH #1
[ 1800.742964] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1800.743095] txg_sync        D 0000000000000000     0   583      2 0x00000000
[ 1800.743106]  ffff88021ba43b18 0000000000000046 ffff8800cbfc4670 0000000000013640
[ 1800.743114]  ffff88021ba43fd8 0000000000013640 ffffffff81818540 ffff8800cbfc4670
[ 1800.743121]  ffff880223335eb0 ffff880223335e98 0000000000000000 0000000000000003
[ 1800.743129] Call Trace:
[ 1800.743146]  [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[ 1800.743157]  [<ffffffff810b2715>] ? __wake_up_common+0x55/0x90
[ 1800.743165]  [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[ 1800.743174]  [<ffffffff81550b59>] schedule+0x29/0x70
[ 1800.743180]  [<ffffffff81550e38>] io_schedule+0x98/0x100
[ 1800.743206]  [<ffffffffa01cfb45>] __cv_broadcast+0xe5/0x160 [spl]
[ 1800.743214]  [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1800.743227]  [<ffffffffa01cfc18>] __cv_wait_io+0x18/0x180 [spl]
[ 1800.743256]  [<ffffffffa030767b>] zio_wait+0x11b/0x200 [zfs]
[ 1800.743290]  [<ffffffffa028ec71>] dsl_pool_sync+0xc1/0x480 [zfs]
[ 1800.743302]  [<ffffffffa01ca0af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[ 1800.743341]  [<ffffffffa02a9b60>] spa_sync+0x480/0xbd0 [zfs]
[ 1800.743349]  [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[ 1800.743388]  [<ffffffffa02bb30c>] txg_delay+0x4ec/0xa60 [zfs]
[ 1800.743425]  [<ffffffffa02baf90>] ? txg_delay+0x170/0xa60 [zfs]
[ 1800.743437]  [<ffffffffa01cb07a>] __thread_exit+0x9a/0xb0 [spl]
[ 1800.743448]  [<ffffffffa01cb000>] ? __thread_exit+0x20/0xb0 [spl]
[ 1800.743455]  [<ffffffff81090e0a>] kthread+0xea/0x100
[ 1800.743462]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1800.743471]  [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[ 1800.743477]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1920.782195] INFO: task txg_sync:583 blocked for more than 120 seconds.
[ 1920.782354]       Tainted: P           O   3.18.2-2-ARCH #1
[ 1920.782449] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1920.782580] txg_sync        D 0000000000000000     0   583      2 0x00000000
[ 1920.782590]  ffff88021ba43b18 0000000000000046 ffff8800cbfc4670 0000000000013640
[ 1920.782599]  ffff88021ba43fd8 0000000000013640 ffffffff81818540 ffff8800cbfc4670
[ 1920.782606]  ffff880223335eb0 ffff880223335e98 0000000000000000 0000000000000003
[ 1920.782614] Call Trace:
[ 1920.782631]  [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[ 1920.782641]  [<ffffffff810b2715>] ? __wake_up_common+0x55/0x90
[ 1920.782650]  [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[ 1920.782658]  [<ffffffff81550b59>] schedule+0x29/0x70
[ 1920.782665]  [<ffffffff81550e38>] io_schedule+0x98/0x100
[ 1920.782693]  [<ffffffffa01cfb45>] __cv_broadcast+0xe5/0x160 [spl]
[ 1920.782701]  [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1920.782714]  [<ffffffffa01cfc18>] __cv_wait_io+0x18/0x180 [spl]
[ 1920.782742]  [<ffffffffa030767b>] zio_wait+0x11b/0x200 [zfs]
[ 1920.782777]  [<ffffffffa028ec71>] dsl_pool_sync+0xc1/0x480 [zfs]
[ 1920.782789]  [<ffffffffa01ca0af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[ 1920.782827]  [<ffffffffa02a9b60>] spa_sync+0x480/0xbd0 [zfs]
[ 1920.782836]  [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[ 1920.782874]  [<ffffffffa02bb30c>] txg_delay+0x4ec/0xa60 [zfs]
[ 1920.782911]  [<ffffffffa02baf90>] ? txg_delay+0x170/0xa60 [zfs]
[ 1920.782923]  [<ffffffffa01cb07a>] __thread_exit+0x9a/0xb0 [spl]
[ 1920.782934]  [<ffffffffa01cb000>] ? __thread_exit+0x20/0xb0 [spl]
[ 1920.782942]  [<ffffffff81090e0a>] kthread+0xea/0x100
[ 1920.782949]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1920.782957]  [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[ 1920.782964]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 1920.782976] INFO: task java:1218 blocked for more than 120 seconds.
[ 1920.783136]       Tainted: P           O   3.18.2-2-ARCH #1
[ 1920.783236] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1920.783366] java            D 0000000000000000     0  1218    655 0x00000000
[ 1920.783373]  ffff8800c17f7ab8 0000000000000086 ffff8800c33e1420 0000000000013640
[ 1920.783381]  ffff8800c17f7fd8 0000000000013640 ffffffff81818540 ffff8800c33e1420
[ 1920.783387]  ffff8800c17f7a08 ffffffffa025a872 0000000000000000 ffff8800235be4a0
[ 1920.783393] Call Trace:
[ 1920.783416]  [<ffffffffa025a872>] ? arc_buf_eviction_needed+0x82/0xc0 [zfs]
[ 1920.783439]  [<ffffffffa0260398>] ? dbuf_rele_and_unlock+0x2c8/0x4d0 [zfs]
[ 1920.783476]  [<ffffffffa02b634d>] ? bp_get_dsize+0xad/0xf0 [zfs]
[ 1920.783503]  [<ffffffffa02770e4>] ? dmu_tx_callback_register+0x324/0xab0 [zfs]
[ 1920.783512]  [<ffffffff81550b59>] schedule+0x29/0x70
[ 1920.783524]  [<ffffffffa01cfb8d>] __cv_broadcast+0x12d/0x160 [spl]
[ 1920.783532]  [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1920.783544]  [<ffffffffa01cfbd5>] __cv_wait+0x15/0x20 [spl]
[ 1920.783570]  [<ffffffffa0278eab>] dmu_tx_wait+0x9b/0x350 [zfs]
[ 1920.783597]  [<ffffffffa02791f1>] dmu_tx_assign+0x91/0x4b0 [zfs]
[ 1920.783627]  [<ffffffffa02810e5>] ? dsl_dataset_block_freeable+0x45/0x1d0 [zfs]
[ 1920.783653]  [<ffffffffa0276fb9>] ? dmu_tx_callback_register+0x1f9/0xab0 [zfs]
[ 1920.783681]  [<ffffffffa02f74a0>] zfs_write+0x3c0/0xbf0 [zfs]
[ 1920.783688]  [<ffffffff810a9dee>] ? enqueue_entity+0x24e/0xaa0
[ 1920.783696]  [<ffffffff8109a1f0>] ? resched_curr+0xd0/0xe0
[ 1920.783705]  [<ffffffff810ec497>] ? wake_futex+0x67/0x90
[ 1920.783711]  [<ffffffff810ef856>] ? do_futex+0x8f6/0xae0
[ 1920.783736]  [<ffffffffa030d2cb>] zpl_putpage+0x21b/0xd50 [zfs]
[ 1920.783744]  [<ffffffff811cf1d7>] vfs_write+0xb7/0x200
[ 1920.783752]  [<ffffffff811cfd29>] SyS_write+0x59/0xd0
[ 1920.783760]  [<ffffffff81554ca9>] system_call_fastpath+0x12/0x17
[ 1920.783778] INFO: task imap:1389 blocked for more than 120 seconds.
[ 1920.783916]       Tainted: P           O   3.18.2-2-ARCH #1
[ 1920.784009] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 1920.784138] imap            D 0000000000000000     0  1389    770 0x00000000
[ 1920.784145]  ffff88000f97fb38 0000000000000082 ffff880222d3bc60 0000000000013640
[ 1920.784151]  ffff88000f97ffd8 0000000000013640 ffff88009d290a10 ffff880222d3bc60
[ 1920.784161]  ffff8800c9ccabb8 ffff8800c9ccabb8 0000000000000fd8 0000000000000001
[ 1920.784168] Call Trace:
[ 1920.784176]  [<ffffffff81550b59>] schedule+0x29/0x70
[ 1920.784190]  [<ffffffffa01cfb8d>] __cv_broadcast+0x12d/0x160 [spl]
[ 1920.784197]  [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 1920.784209]  [<ffffffffa01cfbd5>] __cv_wait+0x15/0x20 [spl]
[ 1920.784236]  [<ffffffffa0278eab>] dmu_tx_wait+0x9b/0x350 [zfs]
[ 1920.784263]  [<ffffffffa02791f1>] dmu_tx_assign+0x91/0x4b0 [zfs]
[ 1920.784292]  [<ffffffffa02f66d7>] zfs_dirty_inode+0xf7/0x330 [zfs]
[ 1920.784303]  [<ffffffffa01d0b10>] ? crfree+0x170/0x180 [spl]
[ 1920.784314]  [<ffffffffa01d123d>] ? tsd_exit+0x19d/0x1b0 [spl]
[ 1920.784338]  [<ffffffffa030ef2e>] zpl_vap_init+0x84e/0xa10 [zfs]
[ 1920.784345]  [<ffffffff811fb0f8>] __mark_inode_dirty+0x38/0x2d0
[ 1920.784372]  [<ffffffffa02fb3cd>] zfs_mark_inode_dirty+0x4d/0x60 [zfs]
[ 1920.784395]  [<ffffffffa030d626>] zpl_putpage+0x576/0xd50 [zfs]
[ 1920.784403]  [<ffffffff811d0d3c>] __fput+0x9c/0x200
[ 1920.784411]  [<ffffffff811d0eee>] ____fput+0xe/0x10
[ 1920.784417]  [<ffffffff8108f33f>] task_work_run+0x9f/0xe0
[ 1920.784426]  [<ffffffff81014e75>] do_notify_resume+0x95/0xa0
[ 1920.784434]  [<ffffffff81554f20>] int_signal+0x12/0x17
[ 2280.901736] INFO: task txg_sync:583 blocked for more than 120 seconds.
[ 2280.901897]       Tainted: P           O   3.18.2-2-ARCH #1
[ 2280.901992] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[ 2280.902124] txg_sync        D 0000000000000000     0   583      2 0x00000000
[ 2280.902134]  ffff88021ba43b18 0000000000000046 ffff8800cbfc4670 0000000000013640
[ 2280.902143]  ffff88021ba43fd8 0000000000013640 ffffffff81818540 ffff8800cbfc4670
[ 2280.902150]  ffff880223335eb0 ffff880223335e98 0000000000000000 0000000000000003
[ 2280.902157] Call Trace:
[ 2280.902175]  [<ffffffff8109e8e2>] ? default_wake_function+0x12/0x20
[ 2280.902186]  [<ffffffff810b2715>] ? __wake_up_common+0x55/0x90
[ 2280.902194]  [<ffffffff810b2978>] ? __wake_up+0x48/0x60
[ 2280.902203]  [<ffffffff81550b59>] schedule+0x29/0x70
[ 2280.902210]  [<ffffffff81550e38>] io_schedule+0x98/0x100
[ 2280.902237]  [<ffffffffa01cfb45>] __cv_broadcast+0xe5/0x160 [spl]
[ 2280.902245]  [<ffffffff810b2e30>] ? __wake_up_sync+0x20/0x20
[ 2280.902258]  [<ffffffffa01cfc18>] __cv_wait_io+0x18/0x180 [spl]
[ 2280.902286]  [<ffffffffa030767b>] zio_wait+0x11b/0x200 [zfs]
[ 2280.902321]  [<ffffffffa028ec71>] dsl_pool_sync+0xc1/0x480 [zfs]
[ 2280.902333]  [<ffffffffa01ca0af>] ? spl_kmem_cache_free+0x10f/0x4a0 [spl]
[ 2280.902371]  [<ffffffffa02a9b60>] spa_sync+0x480/0xbd0 [zfs]
[ 2280.902380]  [<ffffffff810b2e46>] ? autoremove_wake_function+0x16/0x40
[ 2280.902419]  [<ffffffffa02bb30c>] txg_delay+0x4ec/0xa60 [zfs]
[ 2280.902455]  [<ffffffffa02baf90>] ? txg_delay+0x170/0xa60 [zfs]
[ 2280.902467]  [<ffffffffa01cb07a>] __thread_exit+0x9a/0xb0 [spl]
[ 2280.902478]  [<ffffffffa01cb000>] ? __thread_exit+0x20/0xb0 [spl]
[ 2280.902486]  [<ffffffff81090e0a>] kthread+0xea/0x100
[ 2280.902493]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[ 2280.902502]  [<ffffffff81554bfc>] ret_from_fork+0x7c/0xb0
[ 2280.902508]  [<ffffffff81090d20>] ? kthread_create_on_node+0x1c0/0x1c0
[38275.738573] perf interrupt took too long (2506 > 2495), lowering kernel.perf_event_max_sample_rate to 50100
```

Running spl versions from arch:

```
hinoki% pacman -Q | grep spl
spl-git 0.6.3_r54_g03a7835_3.18.2_2-1
spl-utils-git 0.6.3_r54_g03a7835_3.18.2_2-1
```
",2015-01-23T10:07:44Z,30030
997,openzfs/zfs,40463575,71172358,"Free memory:

```
hinoki% free -h
              total        used        free      shared  buff/cache   available
Mem:           7.8G        3.6G        691M        1.0M        3.6G        872M
Swap:          4.0G          0B        4.0G
```
",2015-01-23T10:08:32Z,30030
998,openzfs/zfs,40463575,71190880,"I'm getting this every couple of days now since upgrading to the latest Debian build (which according to the linked bug has the fix in it).  Oops messages more or less the same as those already posted.. The zvols lock up and load average climbs into the hundreds.  I've never had an issue prior to this. 
",2015-01-23T13:10:24Z,263573
999,openzfs/zfs,40463575,77185226,"I'm seeing similar symptoms after running a `zfs rollback` on a SMB shared dataset.  After seeing load quickly rise I've stopped the SMB service and will give the rollback command some time to see if it recovers.

From dmesg, on Ubuntu Server 14.04.2 LTS, ZoL 0.6.3-5 from the Ubuntu PPA:

```
[3764100.795021] smbd            D ffff88010b433480     0 34303  60686 0x00000000
[3764100.795025]  ffff8800ae491d68 0000000000000082 ffff8800cf951800 ffff8800ae491fd8
[3764100.795029]  0000000000013480 0000000000013480 ffff8800cf951800 ffff8800e48dc3d0
[3764100.795032]  ffff8800e48dc3a0 ffff8800e48dc3d8 ffff8800e48dc3c8 0000000000000000
[3764100.795037] Call Trace:
[3764100.795052]  [<ffffffff817251a9>] schedule+0x29/0x70
[3764100.795067]  [<ffffffffa006b7b5>] cv_wait_common+0x125/0x1c0 [spl]
[3764100.795073]  [<ffffffff810ab0b0>] ? prepare_to_wait_event+0x100/0x100
[3764100.795091]  [<ffffffffa006b865>] __cv_wait+0x15/0x20 [spl]
[3764100.795130]  [<ffffffffa017019b>] rrw_enter_read+0x3b/0x150 [zfs]
[3764100.795181]  [<ffffffffa01bf65d>] zfs_getattr_fast+0x3d/0x180 [zfs]
[3764100.795230]  [<ffffffffa01d81fd>] zpl_getattr+0x2d/0x50 [zfs]
[3764100.795234]  [<ffffffff811c2829>] vfs_getattr_nosec+0x29/0x40
[3764100.795237]  [<ffffffff811c28fd>] vfs_getattr+0x2d/0x40
[3764100.795240]  [<ffffffff811c29d2>] vfs_fstatat+0x62/0xa0
[3764100.795244]  [<ffffffff811c2e5f>] SYSC_newstat+0x1f/0x40
[3764100.795248]  [<ffffffff811cdc99>] ? putname+0x29/0x40
[3764100.795252]  [<ffffffff811bcfe8>] ? do_sys_open+0x1b8/0x280
[3764100.795256]  [<ffffffff811c30ae>] SyS_newstat+0xe/0x10
[3764100.795260]  [<ffffffff8173186d>] system_call_fastpath+0x1a/0x1f
[3764100.795263] INFO: task smbd:34313 blocked for more than 120 seconds.
```
",2015-03-04T16:01:01Z,274960
1000,openzfs/zfs,40463575,100021832,"I can trigger the same error by using rsync from two different pools. The rsync process is hanging and can not be killed.

If I won't stop the rsync from the cli, the <my target pool> will endup into a fault status. The fault status is gone after a reboot.

```
May 07 22:43:27 <my host> kernel: INFO: task txg_sync:1408 blocked for more than 120 seconds.
May 07 22:43:27 <my host> kernel:       Tainted: P           O    4.0.1-1-ARCH #1
May 07 22:43:27 <my host> kernel: ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
May 07 22:43:27 <my host> kernel: txg_sync        D ffff880028b63a28     0  1408      2 0x00000000
May 07 22:43:27 <my host> kernel:  ffff880028b63a28 ffff88018c5f3cc0 ffff880070a16540 0000000000000000
May 07 22:43:27 <my host> kernel:  ffff880028b63fd8 ffff88021fc93e00 7fffffffffffffff ffff8801b1ccae08
May 07 22:43:27 <my host> kernel:  0000000000000001 ffff880028b63a48 ffffffff8156fa87 ffff88008e7bdcb0
May 07 22:43:27 <my host> kernel: Call Trace:
May 07 22:43:27 <my host> kernel:  [<ffffffff8156fa87>] schedule+0x37/0x90
May 07 22:43:27 <my host> kernel:  [<ffffffff8157246c>] schedule_timeout+0x1bc/0x250
May 07 22:43:27 <my host> kernel:  [<ffffffff8101f599>] ? read_tsc+0x9/0x10
May 07 22:43:27 <my host> kernel:  [<ffffffff810e6757>] ? ktime_get+0x37/0xb0
May 07 22:43:27 <my host> kernel:  [<ffffffff8156ef9a>] io_schedule_timeout+0xaa/0x130
May 07 22:43:27 <my host> kernel:  [<ffffffffa034daa0>] ? zio_taskq_member.isra.6+0x80/0x80 [zfs]
May 07 22:43:27 <my host> kernel:  [<ffffffffa0192248>] cv_wait_common+0xb8/0x140 [spl]
May 07 22:43:27 <my host> kernel:  [<ffffffff810b6b20>] ? wake_atomic_t_function+0x60/0x60
May 07 22:43:27 <my host> kernel:  [<ffffffffa0192328>] __cv_wait_io+0x18/0x20 [spl]
May 07 22:43:27 <my host> kernel:  [<ffffffffa034f943>] zio_wait+0x123/0x210 [zfs]
May 07 22:43:27 <my host> kernel:  [<ffffffffa02d6be1>] dsl_pool_sync+0xc1/0x480 [zfs]
May 07 22:43:27 <my host> kernel:  [<ffffffffa02f1f80>] spa_sync+0x480/0xbf0 [zfs]
May 07 22:43:27 <my host> kernel:  [<ffffffff810b6b36>] ? autoremove_wake_function+0x16/0x40
May 07 22:43:27 <my host> kernel:  [<ffffffffa0303e06>] txg_sync_thread+0x386/0x630 [zfs]
May 07 22:43:27 <my host> kernel:  [<ffffffff8156fd62>] ? preempt_schedule_common+0x22/0x40
May 07 22:43:27 <my host> kernel:  [<ffffffffa0303a80>] ? txg_quiesce_thread+0x3a0/0x3a0 [zfs]
May 07 22:43:27 <my host> kernel:  [<ffffffffa018d561>] thread_generic_wrapper+0x71/0x80 [spl]
May 07 22:43:27 <my host> kernel:  [<ffffffffa018d4f0>] ? __thread_exit+0x20/0x20 [spl]
May 07 22:43:27 <my host> kernel:  [<ffffffff81093338>] kthread+0xd8/0xf0
May 07 22:43:27 <my host> kernel:  [<ffffffff81093260>] ? kthread_worker_fn+0x170/0x170
May 07 22:43:27 <my host> kernel:  [<ffffffff81573718>] ret_from_fork+0x58/0x90
May 07 22:43:27 <my host> kernel:  [<ffffffff81093260>] ? kthread_worker_fn+0x170/0x170
May 07 22:43:27 <my host> kernel: INFO: task rsync:10064 blocked for more than 120 seconds.
May 07 22:43:27 <my host> kernel:       Tainted: P           O    4.0.1-1-ARCH #1
May 07 22:43:27 <my host> kernel: ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
May 07 22:43:27 <my host> kernel: rsync           D ffff8801a4633a58     0 10064      1 0x00000004
May 07 22:43:27 <my host> kernel:  ffff8801a4633a58 ffff880216186f60 ffff880070a56540 ffff8801a4633a68
May 07 22:43:27 <my host> kernel:  ffff8801a4633fd8 ffff8800da2a4a20 ffff8800da2a4ae0 ffff8800da2a4a48
May 07 22:43:27 <my host> kernel:  0000000000000000 ffff8801a4633a78 ffffffff8156fa87 ffff8800da2a4a20
May 07 22:43:27 <my host> kernel: Call Trace:
May 07 22:43:27 <my host> kernel:  [<ffffffff8156fa87>] schedule+0x37/0x90
May 07 22:43:27 <my host> kernel:  [<ffffffffa019229d>] cv_wait_common+0x10d/0x140 [spl]
May 07 22:43:27 <my host> kernel:  [<ffffffff810b6b20>] ? wake_atomic_t_function+0x60/0x60
May 07 22:43:27 <my host> kernel:  [<ffffffffa01922e5>] __cv_wait+0x15/0x20 [spl]
May 07 22:43:27 <my host> kernel:  [<ffffffffa030327b>] txg_wait_synced+0x8b/0xd0 [zfs]
May 07 22:43:27 <my host> kernel:  [<ffffffffa02c078c>] dmu_tx_wait+0x25c/0x3a0 [zfs]
May 07 22:43:27 <my host> kernel:  [<ffffffffa02c096e>] dmu_tx_assign+0x9e/0x520 [zfs]
May 07 22:43:27 <my host> kernel:  [<ffffffffa02c8e10>] ? dsl_dataset_block_freeable+0x20/0x70 [zfs]
May 07 22:43:27 <my host> kernel:  [<ffffffffa02be639>] ? dmu_tx_count_dnode+0x59/0xb0 [zfs]
May 07 22:43:27 <my host> kernel:  [<ffffffffa033f5de>] zfs_write+0x3ce/0xc50 [zfs]
May 07 22:43:27 <my host> kernel:  [<ffffffff81515821>] ? unix_stream_recvmsg+0x701/0x7e0
May 07 22:43:27 <my host> kernel:  [<ffffffffa03558cd>] zpl_write+0xbd/0x130 [zfs]
May 07 22:43:27 <my host> kernel:  [<ffffffff811d56b3>] vfs_write+0xb3/0x200
May 07 22:43:27 <my host> kernel:  [<ffffffff811d624c>] ? vfs_read+0x11c/0x140
May 07 22:43:27 <my host> kernel:  [<ffffffff811d6399>] SyS_write+0x59/0xd0
May 07 22:43:27 <my host> kernel:  [<ffffffff815737c9>] system_call_fastpath+0x12/0x17
May 07 22:44:27 <my host> kernel: WARNING: Pool '<my target pool>' has encountered an uncorrectable I/O failure and has been suspended.
May 07 22:44:27 <my host> zed[12566]: eid=2302 class=data pool=<my target pool>
May 07 22:44:27 <my host> zed[12569]: eid=2303 class=io_failure pool=<my target pool>
```

```
# zpool status -v
  pool: <my target pool>
 state: ONLINE
  scan: scrub repaired 0 in 9h15m with 0 errors on Thu May  7 09:01:37 2015
config:

        NAME                    STATE     READ WRITE CKSUM
        <my target pool>          ONLINE       0     0     0
          <my target pool>-crypt  ONLINE       0     0     0

errors: No known data errors

  pool: <my source pool>
 state: ONLINE
  scan: scrub repaired 0 in 6h3m with 0 errors on Thu May  7 05:49:33 2015
config:

        NAME                       STATE     READ WRITE CKSUM
        <my source pool>               ONLINE       0     0     0
          mirror-0                 ONLINE       0     0     0
            crypt-<my source pool>-00  ONLINE       0     0     0
            crypt-<my source pool>-01  ONLINE       0     0     0

errors: No known data errors
```

```
#modinfo zfs | head
filename:       /lib/modules/4.0.1-1-ARCH/extra/zfs/zfs.ko.gz
version:        0.6.4.1-1
license:        CDDL
author:         OpenZFS on Linux
description:    ZFS
srcversion:     8324F6AEA2A06B2B6F0A0F5
depends:        spl,znvpair,zunicode,zcommon,zavl
vermagic:       4.0.1-1-ARCH SMP preempt mod_unload modversions 
```

```
#modinfo spl | head
filename:       /lib/modules/4.0.1-1-ARCH/extra/spl/spl.ko.gz
version:        0.6.4.1-1
license:        GPL
author:         OpenZFS on Linux
description:    Solaris Porting Layer
srcversion:     8907748310B8940C9D0DCD2
depends:        
vermagic:       4.0.1-1-ARCH SMP preempt mod_unload modversions 
```

```
#uname -a
Linux <my host> 4.0.1-1-ARCH #1 SMP PREEMPT Wed Apr 29 12:00:26 CEST 2015 x86_64 GNU/Linux
```

Fingers crossed I've provided good information. I'm running an arch linux with demz repo.
",2015-05-07T21:17:51Z,2287220
1001,openzfs/zfs,40463575,107098137,"I got the same issue with high-loaded mongodb on ZFS.

```
free -m
             total       used       free     shared    buffers     cached
Mem:          7928       7422        506         76        192       1024
-/+ buffers/cache:       6205       1723
Swap:            0          0          0
```

```
# modinfo spl|head
filename:       /lib/modules/4.0.0-1-amd64/updates/dkms/spl.ko
version:        0.6.4-2-62e2eb
license:        GPL
author:         OpenZFS on Linux
description:    Solaris Porting Layer
srcversion:     3F1EAF06925B312A0B3F767
depends:        
vermagic:       4.0.0-1-amd64 SMP mod_unload modversions 
parm:           spl_hostid:The system hostid. (ulong)
parm:           spl_hostid_path:The system hostid file (/etc/hostid) (charp)
```

```
# modinfo zfs|head
filename:       /lib/modules/4.0.0-1-amd64/updates/dkms/zfs.ko
version:        0.6.4-16-544f71
license:        CDDL
author:         OpenZFS on Linux
description:    ZFS
srcversion:     29BA21B62706579B75D5974
depends:        spl,znvpair,zunicode,zcommon,zavl
vermagic:       4.0.0-1-amd64 SMP mod_unload modversions 
parm:           zvol_inhibit_dev:Do not create zvol device nodes (uint)
parm:           zvol_major:Major number for zvol device (uint)
```

```
# uname -a
Linux dan-desktop 4.0.0-1-amd64 #1 SMP Debian 4.0.2-1 (2015-05-11) x86_64 GNU/Linux
```

```
# lsb_release -a
No LSB modules are available.
Distributor ID: Debian
Description:    Debian GNU/Linux unstable (sid)
Release:        unstable
Codename:       sid
```
",2015-05-30T23:05:25Z,505407
1002,openzfs/zfs,40463575,173840814,"Same for me on Debian Jessie with Linux 3.16.0-4-amd64 and zfs 0.6.5.2-2.
Happened on a postgresql server, no load at that time.

Jan 22 07:04:42 db04 kernel: [5056080.684110] INFO: task txg_sync:378 blocked for more than 120 seconds.
Jan 22 07:04:43 db04 kernel: [5056080.684128]       Tainted: P           O  3.16.0-4-amd64 #1
Jan 22 07:04:43 db04 kernel: [5056080.684134] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
Jan 22 07:04:43 db04 kernel: [5056080.684142] txg_sync        D ffff8800f96bc7e8     0   378      2 0x00000000
Jan 22 07:04:43 db04 kernel: [5056080.684153]  ffff8800f96bc390 0000000000000246 0000000000012f00 ffff8800f707ffd8
Jan 22 07:04:43 db04 kernel: [5056080.684165]  0000000000012f00 ffff8800f96bc390 ffff8800ff3137b0 ffff880048db7570
Jan 22 07:04:43 db04 kernel: [5056080.684176]  ffff880048db75b0 0000000000000001 ffff8800f8eb9000 0000000000000000
Jan 22 07:04:43 db04 kernel: [5056080.684187] Call Trace:
Jan 22 07:04:43 db04 kernel: [5056080.684201]  [<ffffffff8150e159>] ? io_schedule+0x99/0x120
Jan 22 07:04:43 db04 kernel: [5056080.684220]  [<ffffffffa00f8760>] ? cv_wait_common+0x90/0x100 [spl]
Jan 22 07:04:43 db04 kernel: [5056080.684232]  [<ffffffff810a7a40>] ? prepare_to_wait_event+0xf0/0xf0
Jan 22 07:04:43 db04 kernel: [5056080.684264]  [<ffffffffa02b679b>] ? zio_wait+0x10b/0x1e0 [zfs]
Jan 22 07:04:43 db04 kernel: [5056080.684287]  [<ffffffffa02447ca>] ? dsl_pool_sync+0xaa/0x460 [zfs]
Jan 22 07:04:43 db04 kernel: [5056080.684313]  [<ffffffffa025e6b6>] ? spa_sync+0x366/0xb30 [zfs]
Jan 22 07:04:43 db04 kernel: [5056080.684339]  [<ffffffffa0270131>] ? txg_sync_thread+0x3d1/0x680 [zfs]
Jan 22 07:04:43 db04 kernel: [5056080.684363]  [<ffffffffa026fd60>] ? txg_quiesce_thread+0x3e0/0x3e0 [zfs]
Jan 22 07:04:43 db04 kernel: [5056080.684374]  [<ffffffffa00f3deb>] ? thread_generic_wrapper+0x6b/0x80 [spl]
Jan 22 07:04:43 db04 kernel: [5056080.684388]  [<ffffffffa00f3d80>] ? __thread_exit+0x20/0x20 [spl]
Jan 22 07:04:43 db04 kernel: [5056080.684398]  [<ffffffff81087f7d>] ? kthread+0xbd/0xe0
Jan 22 07:04:43 db04 kernel: [5056080.684406]  [<ffffffff81087ec0>] ? kthread_create_on_node+0x180/0x180
Jan 22 07:04:43 db04 kernel: [5056080.684417]  [<ffffffff81511618>] ? ret_from_fork+0x58/0x90
Jan 22 07:04:43 db04 kernel: [5056080.684428]  [<ffffffff81087ec0>] ? kthread_create_on_node+0x180/0x180
",2016-01-22T08:19:48Z,7279540
1003,openzfs/zfs,40463575,173924068,"@andreas-p please update to 0.6.5.4\* if available

alternatively: you can build your own latest zfsonlinux packages:

http://zfsonlinux.org/generic-deb.html
",2016-01-22T13:40:57Z,4529709
1004,openzfs/zfs,40463575,173938093,"Unfortunately, debian8 is DKMS still on 6.5.2, no update in the last three months. Any clue when this gets resolved?
",2016-01-22T14:42:17Z,7279540
1005,openzfs/zfs,40463575,173946827,"@andreas-p sorry, no idea,

but there's always the option of building the packages on your own - which is some effort but you'll know that you can trust those instead of having to rely on third-party repositories, etc.
",2016-01-22T15:15:27Z,4529709
1006,openzfs/zfs,40463575,192286325,"Got the very same problem with 0.6.5.4 on a different machine with Debian8, zfs built from source. The stack trace shows exactly the same positions as the trace from Jan 18.
Virtual machine: 6GB RAM, 1.5GB swap, 4 AMD CPU cores.

12:54 Starting an rsync from a 2.5TB xfs to 4TB zfs partition, memory rising from 2GB to 5GB within 5 minutes.
13:09 CPU load/1m steps up from 1 to 10, CPU utilization around 50%, mostly system
13:11 txg_sync hung_timeout first appearance, CPU load/1m steps to 16, committed memory drops to 0, CPU utilization 85% system.
13:15 CPU utilization 95% system, load 16, need reboot.
",2016-03-04T13:35:52Z,7279540
1007,openzfs/zfs,40463575,195726832,"same problem here with Debian 8, ZoL 6.5.2 from the official package repository.
",2016-03-12T12:00:49Z,776801
1008,openzfs/zfs,40463575,200819779,"Exactly the same block with 0.6.5.5:
Mar 24 11:13:06 db04 kernel: [4746960.372343] [<ffffffff815107c9>] ? io_schedule+0x99/0x120
Mar 24 11:13:06 db04 kernel: [4746960.372358] [<ffffffffa0198572>] ? cv_wait_common+0x92/0x110 [spl]
Mar 24 11:13:06 db04 kernel: [4746960.372370] [<ffffffff810a7ba0>] ? prepare_to_wait_event+0xf0/0xf0
Mar 24 11:13:06 db04 kernel: [4746960.372400] [<ffffffffa05410ab>] ? zio_wait+0x10b/0x1e0 [zfs]
Mar 24 11:13:06 db04 kernel: [4746960.372423] [<ffffffffa04ce75a>] ? dsl_pool_sync+0xaa/0x460 [zfs]
Mar 24 11:13:06 db04 kernel: [4746960.372447] [<ffffffffa04e8706>] ? spa_sync+0x366/0xb30 [zfs]
Mar 24 11:13:06 db04 kernel: [4746960.372472] [<ffffffffa04fa181>] ? txg_sync_thread+0x3d1/0x680 [zfs]
Mar 24 11:13:06 db04 kernel: [4746960.372496] [<ffffffffa04f9db0>] ? txg_quiesce_thread+0x3e0/0x3e0 [zfs]
Mar 24 11:13:06 db04 kernel: [4746960.372507] [<ffffffffa0193cab>] ? thread_generic_wrapper+0x6b/0x80 [spl]
Mar 24 11:13:06 db04 kernel: [4746960.372518] [<ffffffffa0193c40>] ? __thread_exit+0x20/0x20 [spl]
Mar 24 11:13:06 db04 kernel: [4746960.372529] [<ffffffff8108805d>] ? kthread+0xbd/0xe0
Mar 24 11:13:06 db04 kernel: [4746960.372547] [<ffffffff81087fa0>] ? kthread_create_on_node+0x180/0x180
Mar 24 11:13:06 db04 kernel: [4746960.372558] [<ffffffff81513c58>] ? ret_from_fork+0x58/0x90
Mar 24 11:13:06 db04 kernel: [4746960.372567] [<ffffffff81087fa0>] ? kthread_create_on_node+0x180/0x180

I got two consecutive ""txg_sync blocked for more than 120 seconds"", then it went back to normal.
",2016-03-24T12:53:52Z,7279540
1009,openzfs/zfs,40463575,226332520,"I'm suffering from the similar problem. `rsync` reads/writes are extremely slow ~3M. After struggling like this ~24h (it's a multi-million file dataset) machine gets bricked. 

Please advice.

---

```
# uname -a
Linux ip-172-30-0-118 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt25-2 (2016-04-08) x86_64 GNU/Linux
```

```
# lsb_release -a
No LSB modules are available.
Distributor ID: Debian
Description:    Debian GNU/Linux 8.5 (jessie)
Release:        8.5
Codename:       jessie
```

```
# dpkg -l '*zfs*' | grep ii
ii  libzfs2linux   0.6.5.7-1    amd64        OpenZFS filesystem library for Linux
ii  zfs-dkms       0.6.5.7-1    all          OpenZFS filesystem kernel modules for Linux
ii  zfs-zed        0.6.5.7-1    amd64        OpenZFS Event Daemon
ii  zfsutils-linux 0.6.5.7-1    amd64        command-line tools to manage OpenZFS filesystems
```

```
# dpkg -l '*spl*' | grep ii
ii  spl            0.6.5.7-1    amd64        Solaris Porting Layer user-space utilities for Linux
ii  spl-dkms       0.6.5.7-1    all          Solaris Porting Layer kernel modules for Linux
```

```
# zpool status
  pool: zfs-backup
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        zfs-backup  ONLINE       0     0     0
          xvdf      ONLINE       0     0     0
        logs
          xvdg      ONLINE       0     0     0

errors: No known data errors
```

```
# zpool iostat -v
               capacity     operations    bandwidth
pool        alloc   free   read  write   read  write
----------  -----  -----  -----  -----  -----  -----
zfs-backup   564G   380G    538      0  1.48M  2.42K
  xvdf       564G   380G    538      0  1.48M  2.11K
logs            -      -      -      -      -      -
  xvdg          0  1.98G      0      0    471    312
----------  -----  -----  -----  -----  -----  -----
```

```
# zpool get all
NAME        PROPERTY                    VALUE                       SOURCE
zfs-backup  size                        944G                        -
zfs-backup  capacity                    59%                         -
zfs-backup  altroot                     -                           default
zfs-backup  health                      ONLINE                      -
zfs-backup  guid                        2876612074418704500         default
zfs-backup  version                     -                           default
zfs-backup  bootfs                      -                           default
zfs-backup  delegation                  on                          default
zfs-backup  autoreplace                 off                         default
zfs-backup  cachefile                   -                           default
zfs-backup  failmode                    wait                        default
zfs-backup  listsnapshots               off                         default
zfs-backup  autoexpand                  off                         default
zfs-backup  dedupditto                  0                           default
zfs-backup  dedupratio                  1.00x                       -
zfs-backup  free                        380G                        -
zfs-backup  allocated                   564G                        -
zfs-backup  readonly                    off                         -
zfs-backup  ashift                      0                           default
zfs-backup  comment                     -                           default
zfs-backup  expandsize                  -                           -
zfs-backup  freeing                     0                           default
zfs-backup  fragmentation               44%                         -
zfs-backup  leaked                      0                           default
zfs-backup  feature@async_destroy       enabled                     local
zfs-backup  feature@empty_bpobj         active                      local
zfs-backup  feature@lz4_compress        active                      local
zfs-backup  feature@spacemap_histogram  active                      local
zfs-backup  feature@enabled_txg         active                      local
zfs-backup  feature@hole_birth          active                      local
zfs-backup  feature@extensible_dataset  enabled                     local
zfs-backup  feature@embedded_data       active                      local
zfs-backup  feature@bookmarks           enabled                     local
zfs-backup  feature@filesystem_limits   enabled                     local
zfs-backup  feature@large_blocks        enabled                     local
```

```
Jun 15 16:55:40 ip-172-30-0-118 kernel: [113640.618167] txg_sync        D ffff880079c259c8     0  1361      2 0x00000000
Jun 15 16:55:41 ip-172-30-0-118 kernel: [113640.621686]  ffff880079c25570 0000000000000046 0000000000012f00 ffff88007aaf3fd8
Jun 15 16:55:43 ip-172-30-0-118 kernel: [113640.625572]  0000000000012f00 ffff880079c25570 ffff88007fc137b0 ffff8800070cf050
Jun 15 16:55:43 ip-172-30-0-118 kernel: [113640.629691]  ffff8800070cf090 0000000000000001 ffff8800790a1000 0000000000000000
Jun 15 16:55:44 ip-172-30-0-118 kernel: [113640.633524] Call Trace:
Jun 15 16:55:45 ip-172-30-0-118 kernel: [113640.634740]  [<ffffffff815114a9>] ? io_schedule+0x99/0x120
Jun 15 16:55:47 ip-172-30-0-118 kernel: [113640.637492]  [<ffffffffa0152572>] ? cv_wait_common+0x92/0x110 [spl]
Jun 15 16:55:48 ip-172-30-0-118 kernel: [113640.640409]  [<ffffffff810a7e60>] ? prepare_to_wait_event+0xf0/0xf0
Jun 15 16:55:49 ip-172-30-0-118 kernel: [113640.643375]  [<ffffffffa029d12b>] ? zio_wait+0x10b/0x1e0 [zfs]
Jun 15 16:55:49 ip-172-30-0-118 kernel: [113640.646118]  [<ffffffffa022a77a>] ? dsl_pool_sync+0xaa/0x460 [zfs]
Jun 15 16:55:50 ip-172-30-0-118 kernel: [113640.649130]  [<ffffffffa0244766>] ? spa_sync+0x366/0xb30 [zfs]
Jun 15 16:55:51 ip-172-30-0-118 kernel: [113640.651910]  [<ffffffffa0256231>] ? txg_sync_thread+0x3d1/0x680 [zfs]
Jun 15 16:55:53 ip-172-30-0-118 kernel: [113640.655111]  [<ffffffffa0255e60>] ? txg_quiesce_thread+0x3e0/0x3e0 [zfs]
Jun 15 16:55:53 ip-172-30-0-118 kernel: [113640.658303]  [<ffffffffa014dcab>] ? thread_generic_wrapper+0x6b/0x80 [spl]
Jun 15 16:55:54 ip-172-30-0-118 kernel: [113640.661609]  [<ffffffffa014dc40>] ? __thread_exit+0x20/0x20 [spl]
Jun 15 16:55:55 ip-172-30-0-118 kernel: [113640.664528]  [<ffffffff8108809d>] ? kthread+0xbd/0xe0
Jun 15 16:55:56 ip-172-30-0-118 kernel: [113640.667024]  [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 16:55:58 ip-172-30-0-118 kernel: [113640.670190]  [<ffffffff81514958>] ? ret_from_fork+0x58/0x90
Jun 15 16:55:58 ip-172-30-0-118 kernel: [113640.672778]  [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 16:57:39 ip-172-30-0-118 kernel: [113760.681975] txg_sync        D ffff880079c259c8     0  1361      2 0x00000000
Jun 15 16:57:40 ip-172-30-0-118 kernel: [113760.685521]  ffff880079c25570 0000000000000046 0000000000012f00 ffff88007aaf3fd8
Jun 15 16:57:41 ip-172-30-0-118 kernel: [113760.689295]  0000000000012f00 ffff880079c25570 ffff88007fc137b0 ffff8800070cf050
Jun 15 16:57:42 ip-172-30-0-118 kernel: [113760.693211]  ffff8800070cf090 0000000000000001 ffff8800790a1000 0000000000000000
Jun 15 16:57:43 ip-172-30-0-118 kernel: [113760.697110] Call Trace:
Jun 15 16:57:44 ip-172-30-0-118 kernel: [113760.698425]  [<ffffffff815114a9>] ? io_schedule+0x99/0x120
Jun 15 16:57:45 ip-172-30-0-118 kernel: [113760.701106]  [<ffffffffa0152572>] ? cv_wait_common+0x92/0x110 [spl]
Jun 15 16:57:47 ip-172-30-0-118 kernel: [113760.704131]  [<ffffffff810a7e60>] ? prepare_to_wait_event+0xf0/0xf0
Jun 15 16:57:48 ip-172-30-0-118 kernel: [113760.707157]  [<ffffffffa029d12b>] ? zio_wait+0x10b/0x1e0 [zfs]
Jun 15 16:57:49 ip-172-30-0-118 kernel: [113760.709944]  [<ffffffffa022a77a>] ? dsl_pool_sync+0xaa/0x460 [zfs]
Jun 15 16:57:51 ip-172-30-0-118 kernel: [113760.713084]  [<ffffffffa0244766>] ? spa_sync+0x366/0xb30 [zfs]
Jun 15 16:57:52 ip-172-30-0-118 kernel: [113760.715920]  [<ffffffffa0256231>] ? txg_sync_thread+0x3d1/0x680 [zfs]
Jun 15 16:57:54 ip-172-30-0-118 kernel: [113760.719014]  [<ffffffffa0255e60>] ? txg_quiesce_thread+0x3e0/0x3e0 [zfs]
Jun 15 16:57:55 ip-172-30-0-118 kernel: [113760.722283]  [<ffffffffa014dcab>] ? thread_generic_wrapper+0x6b/0x80 [spl]
Jun 15 16:57:57 ip-172-30-0-118 kernel: [113760.725571]  [<ffffffffa014dc40>] ? __thread_exit+0x20/0x20 [spl]
Jun 15 16:57:57 ip-172-30-0-118 kernel: [113760.728485]  [<ffffffff8108809d>] ? kthread+0xbd/0xe0
Jun 15 16:57:58 ip-172-30-0-118 kernel: [113760.730974]  [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 16:58:00 ip-172-30-0-118 kernel: [113760.734102]  [<ffffffff81514958>] ? ret_from_fork+0x58/0x90
Jun 15 16:58:02 ip-172-30-0-118 kernel: [113760.736819]  [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 16:59:38 ip-172-30-0-118 kernel: [113880.747394] txg_sync        D ffff880079c259c8     0  1361      2 0x00000000
Jun 15 16:59:39 ip-172-30-0-118 kernel: [113880.750796]  ffff880079c25570 0000000000000046 0000000000012f00 ffff88007aaf3fd8
Jun 15 16:59:39 ip-172-30-0-118 kernel: [113880.754658]  0000000000012f00 ffff880079c25570 ffff88007fc137b0 ffff8800070cf050
Jun 15 16:59:41 ip-172-30-0-118 kernel: [113880.758412]  ffff8800070cf090 0000000000000001 ffff8800790a1000 0000000000000000
Jun 15 16:59:42 ip-172-30-0-118 kernel: [113880.762234] Call Trace:
Jun 15 16:59:43 ip-172-30-0-118 kernel: [113880.763454]  [<ffffffff815114a9>] ? io_schedule+0x99/0x120
Jun 15 16:59:44 ip-172-30-0-118 kernel: [113880.766117]  [<ffffffffa0152572>] ? cv_wait_common+0x92/0x110 [spl]
Jun 15 16:59:45 ip-172-30-0-118 kernel: [113880.769024]  [<ffffffff810a7e60>] ? prepare_to_wait_event+0xf0/0xf0
Jun 15 16:59:45 ip-172-30-0-118 kernel: [113880.772106]  [<ffffffffa029d12b>] ? zio_wait+0x10b/0x1e0 [zfs]
Jun 15 16:59:46 ip-172-30-0-118 kernel: [113880.775310]  [<ffffffffa022a77a>] ? dsl_pool_sync+0xaa/0x460 [zfs]
Jun 15 16:59:48 ip-172-30-0-118 kernel: [113880.778331]  [<ffffffffa0244766>] ? spa_sync+0x366/0xb30 [zfs]
Jun 15 16:59:50 ip-172-30-0-118 kernel: [113880.781198]  [<ffffffffa0256231>] ? txg_sync_thread+0x3d1/0x680 [zfs]
Jun 15 16:59:51 ip-172-30-0-118 kernel: [113880.784324]  [<ffffffffa0255e60>] ? txg_quiesce_thread+0x3e0/0x3e0 [zfs]
Jun 15 16:59:52 ip-172-30-0-118 kernel: [113880.787500]  [<ffffffffa014dcab>] ? thread_generic_wrapper+0x6b/0x80 [spl]
Jun 15 16:59:54 ip-172-30-0-118 kernel: [113880.790757]  [<ffffffffa014dc40>] ? __thread_exit+0x20/0x20 [spl]
Jun 15 16:59:55 ip-172-30-0-118 kernel: [113880.793707]  [<ffffffff8108809d>] ? kthread+0xbd/0xe0
Jun 15 16:59:56 ip-172-30-0-118 kernel: [113880.796081]  [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 16:59:57 ip-172-30-0-118 kernel: [113880.799104]  [<ffffffff81514958>] ? ret_from_fork+0x58/0x90
Jun 15 16:59:58 ip-172-30-0-118 kernel: [113880.801724]  [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 17:01:41 ip-172-30-0-118 kernel: [114000.817906] txg_sync        D ffff880079c259c8     0  1361      2 0x00000000
Jun 15 17:01:42 ip-172-30-0-118 kernel: [114000.822256]  ffff880079c25570 0000000000000046 0000000000012f00 ffff88007aaf3fd8
Jun 15 17:01:43 ip-172-30-0-118 kernel: [114000.827952]  0000000000012f00 ffff880079c25570 ffff88007fc137b0 ffff8800070cf050
Jun 15 17:01:45 ip-172-30-0-118 kernel: [114000.833092]  ffff8800070cf090 0000000000000001 ffff8800790a1000 0000000000000000
Jun 15 17:01:45 ip-172-30-0-118 kernel: [114000.837533] Call Trace:
Jun 15 17:01:47 ip-172-30-0-118 kernel: [114000.838994]  [<ffffffff815114a9>] ? io_schedule+0x99/0x120
Jun 15 17:01:48 ip-172-30-0-118 kernel: [114000.842069]  [<ffffffffa0152572>] ? cv_wait_common+0x92/0x110 [spl]
Jun 15 17:01:49 ip-172-30-0-118 kernel: [114000.845833]  [<ffffffff810a7e60>] ? prepare_to_wait_event+0xf0/0xf0
Jun 15 17:01:50 ip-172-30-0-118 kernel: [114000.849362]  [<ffffffffa029d12b>] ? zio_wait+0x10b/0x1e0 [zfs]
Jun 15 17:01:51 ip-172-30-0-118 kernel: [114000.852630]  [<ffffffffa022a77a>] ? dsl_pool_sync+0xaa/0x460 [zfs]
Jun 15 17:01:52 ip-172-30-0-118 kernel: [114000.856037]  [<ffffffffa0244766>] ? spa_sync+0x366/0xb30 [zfs]
Jun 15 17:01:53 ip-172-30-0-118 kernel: [114000.859185]  [<ffffffffa0256231>] ? txg_sync_thread+0x3d1/0x680 [zfs]
Jun 15 17:01:54 ip-172-30-0-118 kernel: [114000.862633]  [<ffffffffa0255e60>] ? txg_quiesce_thread+0x3e0/0x3e0 [zfs]
Jun 15 17:01:55 ip-172-30-0-118 kernel: [114000.866301]  [<ffffffffa014dcab>] ? thread_generic_wrapper+0x6b/0x80 [spl]
Jun 15 17:01:56 ip-172-30-0-118 kernel: [114000.870041]  [<ffffffffa014dc40>] ? __thread_exit+0x20/0x20 [spl]
Jun 15 17:01:57 ip-172-30-0-118 kernel: [114000.873530]  [<ffffffff8108809d>] ? kthread+0xbd/0xe0
Jun 15 17:01:57 ip-172-30-0-118 kernel: [114000.876425]  [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
Jun 15 17:01:59 ip-172-30-0-118 kernel: [114000.879915]  [<ffffffff81514958>] ? ret_from_fork+0x58/0x90
Jun 15 17:02:00 ip-172-30-0-118 kernel: [114000.882644]  [<ffffffff81087fe0>] ? kthread_create_on_node+0x180/0x180
```
",2016-06-15T21:57:41Z,12384868
1010,openzfs/zfs,40463575,226363428,"@narunask could you please post output of 

```
cat /proc/spl/kstat/zfs/arcstats
```

or meanwhile access to the box is denied (""bricked"") ?

Also please post some hardware and specific configuration data (RAM, processor, mainboard, harddrive type, lvm/cryptsetup/etc. etc.)

thanks
",2016-06-16T01:11:31Z,4529709
1011,openzfs/zfs,40463575,226393994,"Server is on the `EC2`, currently 1 core, 2GB RAM (`t2.small`), HDD - the magnetic one, I believe it is documented [here](https://aws.amazon.com/ebs/previous-generation/).

HDD that I'm copying from is LVM based, consisting of 3 PVs. HDD are not encrypted.

Also FYI, currently `rsync` is calculating deltas and should start moving data again very soon, if that adds something to the stats.

```
# cat /proc/spl/kstat/zfs/arcstats
6 1 0x01 91 4368 35041278654 31678952268315
name                            type data
hits                            4    42347862
misses                          4    10285010
demand_data_hits                4    0
demand_data_misses              4    68
demand_metadata_hits            4    34887103
demand_metadata_misses          4    6345542
prefetch_data_hits              4    0
prefetch_data_misses            4    0
prefetch_metadata_hits          4    7460759
prefetch_metadata_misses        4    3939400
mru_hits                        4    20830447
mru_ghost_hits                  4    2991105
mfu_hits                        4    14056656
mfu_ghost_hits                  4    1782606
deleted                         4    2167955
mutex_miss                      4    90369
evict_skip                      4    1781974161
evict_not_enough                4    15390660
evict_l2_cached                 4    0
evict_l2_eligible               4    35359622656
evict_l2_ineligible             4    55754657792
evict_l2_skip                   4    0
hash_elements                   4    5452
hash_elements_max               4    48255
hash_collisions                 4    57676
hash_chains                     4    69
hash_chain_max                  4    4
p                               4    7007439
c                               4    34933248
c_min                           4    33554432
c_max                           4    1053282304
size                            4    105425184
hdr_size                        4    1969640
data_size                       4    0
metadata_size                   4    30842880
other_size                      4    72612664
anon_size                       4    753664
anon_evictable_data             4    0
anon_evictable_metadata         4    0
mru_size                        4    27006976
mru_evictable_data              4    0
mru_evictable_metadata          4    3653632
mru_ghost_size                  4    6850560
mru_ghost_evictable_data        4    0
mru_ghost_evictable_metadata    4    6850560
mfu_size                        4    3082240
mfu_evictable_data              4    0
mfu_evictable_metadata          4    32768
mfu_ghost_size                  4    25972224
mfu_ghost_evictable_data        4    0
mfu_ghost_evictable_metadata    4    25972224
l2_hits                         4    0
l2_misses                       4    0
l2_feeds                        4    0
l2_rw_clash                     4    0
l2_read_bytes                   4    0
l2_write_bytes                  4    0
l2_writes_sent                  4    0
l2_writes_done                  4    0
l2_writes_error                 4    0
l2_writes_lock_retry            4    0
l2_evict_lock_retry             4    0
l2_evict_reading                4    0
l2_evict_l1cached               4    0
l2_free_on_write                4    0
l2_abort_lowmem                 4    0
l2_cksum_bad                    4    0
l2_io_error                     4    0
l2_size                         4    0
l2_asize                        4    0
l2_hdr_size                     4    0
l2_compress_successes           4    0
l2_compress_zeros               4    0
l2_compress_failures            4    0
memory_throttle_count           4    0
duplicate_buffers               4    0
duplicate_buffers_size          4    0
duplicate_reads                 4    0
memory_direct_count             4    947
memory_indirect_count           4    259028
arc_no_grow                     4    0
arc_tempreserve                 4    0
arc_loaned_bytes                4    0
arc_prune                       4    3960
arc_meta_used                   4    105425184
arc_meta_limit                  4    789961728
arc_meta_max                    4    816526072
arc_meta_min                    4    16777216
arc_need_free                   4    0
arc_sys_free                    4    32911360

```

```
# cat /proc/cpuinfo 
processor       : 0
vendor_id       : GenuineIntel
cpu family      : 6
model           : 63
model name      : Intel(R) Xeon(R) CPU E5-2676 v3 @ 2.40GHz
stepping        : 2
microcode       : 0x25
cpu MHz         : 2394.530
cache size      : 30720 KB
physical id     : 0
siblings        : 1
core id         : 0
cpu cores       : 1
apicid          : 0
initial apicid  : 0
fpu             : yes
fpu_exception   : yes
cpuid level     : 13
wp              : yes
flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx rdtscp lm constant_tsc rep_good nopl xtopology eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm xsaveopt fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips        : 4789.06
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:
```

```
# dmidecode --type memory
# dmidecode 3.0
Scanning /dev/mem for entry point.
SMBIOS 2.4 present.

Handle 0x1000, DMI type 16, 15 bytes
Physical Memory Array
        Location: Other
        Use: System Memory
        Error Correction Type: Multi-bit ECC
        Maximum Capacity: 2 GB
        Error Information Handle: Not Provided
        Number Of Devices: 1

Handle 0x1100, DMI type 17, 21 bytes
Memory Device
        Array Handle: 0x1000
        Error Information Handle: 0x0000
        Total Width: 64 bits
        Data Width: 64 bits
        Size: 2048 MB
        Form Factor: DIMM
        Set: None
        Locator: DIMM 0
        Bank Locator: Not Specified
        Type: RAM
        Type Detail: None
```

Thanks
",2016-06-16T05:49:45Z,12384868
1012,openzfs/zfs,40463575,232594842,"I haven't seen the problem for quite a while now (seemed to have gone since 0.6.5.6), but this morning I had the very same hung task with 0.6.5.7 (single occurrance) come up again. Sigh...
",2016-07-14T08:14:01Z,7279540
1013,openzfs/zfs,40463575,232600005,"I can confirm that I have not seen this issue happen for ages on the same hardware. I have not had it reoccur, probably within the last 3-6 months.
",2016-07-14T08:27:59Z,30030
1014,openzfs/zfs,40463575,272592987,"Still happening with heavy RSync backups.

```
[88234.807775] INFO: task rsync:18699 blocked for more than 120 seconds.
[88234.807929]       Tainted: P           O    4.8.13-1-ARCH #1
[88234.808026] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[88234.808159] rsync           D ffff880124bcb9d0     0 18699  18698 0x00000000
[88234.808171]  ffff880124bcb9d0 00ffffff81003016 ffff88021cd00d00 ffff880127fa6800
[88234.808179]  ffff88022fc17ff0 ffff880124bcc000 ffff88021d8eaa10 ffff88021d8eab58
[88234.808185]  ffff88021d8eaa38 0000000000000000 ffff880124bcb9e8 ffffffff815f40ec
[88234.808190] Call Trace:
[88234.808203]  [<ffffffff815f40ec>] schedule+0x3c/0x90
[88234.808217]  [<ffffffffa0171e1f>] cv_wait_common+0x10f/0x130 [spl]
[88234.808225]  [<ffffffff810c0450>] ? wake_atomic_t_function+0x60/0x60
[88234.808235]  [<ffffffffa0171e55>] __cv_wait+0x15/0x20 [spl]
[88234.808294]  [<ffffffffa03095f8>] txg_wait_open+0xa8/0xe0 [zfs]
[88234.808338]  [<ffffffffa02c73db>] dmu_tx_wait+0x32b/0x340 [zfs]
[88234.808380]  [<ffffffffa02c747b>] dmu_tx_assign+0x8b/0x490 [zfs]
[88234.808422]  [<ffffffffa0342d09>] zfs_write+0x3f9/0xc80 [zfs]
[88234.808461]  [<ffffffffa034f567>] ? zio_destroy+0xb7/0xc0 [zfs]
[88234.808500]  [<ffffffffa0352c78>] ? zio_wait+0x138/0x1d0 [zfs]
[88234.808507]  [<ffffffffa016ab9a>] ? spl_kmem_free+0x2a/0x40 [spl]
[88234.808551]  [<ffffffffa0337510>] ? zfs_range_unlock+0x1a0/0x2c0 [zfs]
[88234.808590]  [<ffffffffa035826c>] zpl_write_common_iovec+0x8c/0xe0 [zfs]
[88234.808627]  [<ffffffffa0358497>] zpl_write+0x87/0xc0 [zfs]
[88234.808635]  [<ffffffff81208797>] __vfs_write+0x37/0x140
[88234.808642]  [<ffffffff810c7be7>] ? percpu_down_read+0x17/0x50
[88234.808648]  [<ffffffff81209566>] vfs_write+0xb6/0x1a0
[88234.808652]  [<ffffffff8120a9e5>] SyS_write+0x55/0xc0
[88234.808658]  [<ffffffff815f8032>] entry_SYSCALL_64_fastpath+0x1a/0xa4
[88234.808664] INFO: task imap:19376 blocked for more than 120 seconds.
[88234.808803]       Tainted: P           O    4.8.13-1-ARCH #1
[88234.808898] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[88234.809029] imap            D ffff8801b875fad8     0 19376   1488 0x00000000
[88234.809036]  ffff8801b875fad8 00ff88021d8eab00 ffffffff81a0d500 ffff8801a9f95b00
[88234.809042]  ffff8801b875fab8 ffff8801b8760000 ffff88021d8eaa10 ffff88021d8eab58
[88234.809048]  ffff88021d8eaa38 0000000000000000 ffff8801b875faf0 ffffffff815f40ec
[88234.809053] Call Trace:
[88234.809059]  [<ffffffff815f40ec>] schedule+0x3c/0x90
[88234.809067]  [<ffffffffa0171e1f>] cv_wait_common+0x10f/0x130 [spl]
[88234.809073]  [<ffffffff810c0450>] ? wake_atomic_t_function+0x60/0x60
[88234.809081]  [<ffffffffa0171e55>] __cv_wait+0x15/0x20 [spl]
[88234.809131]  [<ffffffffa03095f8>] txg_wait_open+0xa8/0xe0 [zfs]
[88234.809175]  [<ffffffffa02c73db>] dmu_tx_wait+0x32b/0x340 [zfs]
[88234.809217]  [<ffffffffa02c747b>] dmu_tx_assign+0x8b/0x490 [zfs]
[88234.809259]  [<ffffffffa0342299>] zfs_dirty_inode+0xe9/0x300 [zfs]
[88234.809265]  [<ffffffff8122a3b4>] ? mntput+0x24/0x40
[88234.809272]  [<ffffffff81189adb>] ? release_pages+0x2cb/0x380
[88234.809278]  [<ffffffff811c763e>] ? free_pages_and_swap_cache+0x8e/0xa0
[88234.809315]  [<ffffffffa035a2cc>] zpl_dirty_inode+0x2c/0x40 [zfs]
[88234.809322]  [<ffffffff81237785>] __mark_inode_dirty+0x45/0x400
[88234.809362]  [<ffffffffa0346fe8>] zfs_mark_inode_dirty+0x48/0x50 [zfs]
[88234.809399]  [<ffffffffa0358826>] zpl_release+0x46/0x90 [zfs]
[88234.809404]  [<ffffffff8120b42f>] __fput+0x9f/0x1e0
[88234.809408]  [<ffffffff8120b5ae>] ____fput+0xe/0x10
[88234.809414]  [<ffffffff8109a0d0>] task_work_run+0x80/0xa0
[88234.809420]  [<ffffffff8100366a>] exit_to_usermode_loop+0xba/0xc0
[88234.809425]  [<ffffffff81003b2e>] syscall_return_slowpath+0x4e/0x60
[88234.809430]  [<ffffffff815f80ba>] entry_SYSCALL_64_fastpath+0xa2/0xa4   
```

Most of the time performance is fine.",2017-01-14T02:00:24Z,30030
1015,openzfs/zfs,40463575,273150765,"So, I replaced the drive which might have been causing issues, with a new Samsung 840 PRO SSD, partitioned 50GB OS, 4GB Swap and the rest available as L2ARC.

I've set the arc max size to 4GB on a system with only 8GB memory.

Tonight, the same issue occurred, rsync and then everything ground to a halt. So, it seems unlikely to be a disk issue. Nominal read speeds are 100MB/s+ so things are humming along quite nicely.",2017-01-17T13:12:08Z,30030
1016,openzfs/zfs,40463575,273153577,I can reproduce this issue fairly easily so just let me know what information you'd like me to collect and I'll try to do it.,2017-01-17T13:23:22Z,30030
1017,openzfs/zfs,40463575,273613345,"```
[78889.098553] INFO: task txg_sync:1372 blocked for more than 120 seconds.
[78889.098667]       Tainted: P           O    4.8.13-1-ARCH #1
[78889.098719] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[78889.098790] txg_sync        D ffff88021c893ab8     0  1372      2 0x00000000
[78889.098798]  ffff88021c893ab8 00ffffff810a5d2f ffff8802212b5b00 ffff88021d3ece00
[78889.098802]  0000000000000046 ffff88021c894000 ffff88022fc17f80 7fffffffffffffff
[78889.098806]  ffff880105123b60 0000000000000001 ffff88021c893ad0 ffffffff815f40ec
[78889.098809] Call Trace:
[78889.098818]  [<ffffffff815f40ec>] schedule+0x3c/0x90
[78889.098821]  [<ffffffff815f6fa3>] schedule_timeout+0x243/0x3d0
[78889.098825]  [<ffffffff810a6c12>] ? default_wake_function+0x12/0x20
[78889.098828]  [<ffffffff810bfd9d>] ? __wake_up_common+0x4d/0x80
[78889.098832]  [<ffffffff810f2c51>] ? ktime_get+0x41/0xb0
[78889.098835]  [<ffffffff815f38c4>] io_schedule_timeout+0xa4/0x110
[78889.098843]  [<ffffffffa0203dc1>] cv_wait_common+0xb1/0x130 [spl]
[78889.098846]  [<ffffffff810c0450>] ? wake_atomic_t_function+0x60/0x60
[78889.098851]  [<ffffffffa0203e98>] __cv_wait_io+0x18/0x20 [spl]
[78889.098886]  [<ffffffffa0343c3d>] zio_wait+0xfd/0x1d0 [zfs]
[78889.098912]  [<ffffffffa02cf258>] dsl_pool_sync+0xb8/0x480 [zfs]
[78889.098939]  [<ffffffffa02ea38f>] spa_sync+0x37f/0xb30 [zfs]
[78889.098942]  [<ffffffff810a6c12>] ? default_wake_function+0x12/0x20
[78889.098969]  [<ffffffffa02fac1a>] txg_sync_thread+0x3ba/0x620 [zfs]
[78889.098997]  [<ffffffffa02fa860>] ? txg_delay+0x160/0x160 [zfs]
[78889.099002]  [<ffffffffa01fef61>] thread_generic_wrapper+0x71/0x80 [spl]
[78889.099006]  [<ffffffffa01feef0>] ? __thread_exit+0x20/0x20 [spl]
[78889.099010]  [<ffffffff8109be38>] kthread+0xd8/0xf0
[78889.099013]  [<ffffffff8102c782>] ? __switch_to+0x2d2/0x630
[78889.099016]  [<ffffffff815f823f>] ret_from_fork+0x1f/0x40
[78889.099019]  [<ffffffff8109bd60>] ? kthread_worker_fn+0x170/0x170
[79380.619090] INFO: task txg_sync:1372 blocked for more than 120 seconds.
[79380.619245]       Tainted: P           O    4.8.13-1-ARCH #1
[79380.619342] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[79380.619475] txg_sync        D ffff88021c893ab8     0  1372      2 0x00000000
[79380.619487]  ffff88021c893ab8 00ffffff810a5d2f ffffffff81a0d500 ffff88021d3ece00
[79380.619494]  0000000000000046 ffff88021c894000 ffff88022fc17f80 7fffffffffffffff
[79380.619500]  ffff8801e9c0c850 0000000000000001 ffff88021c893ad0 ffffffff815f40ec
[79380.619506] Call Trace:
[79380.619519]  [<ffffffff815f40ec>] schedule+0x3c/0x90
[79380.619525]  [<ffffffff815f6fa3>] schedule_timeout+0x243/0x3d0
[79380.619531]  [<ffffffff810a6c12>] ? default_wake_function+0x12/0x20
[79380.619537]  [<ffffffff810bfd9d>] ? __wake_up_common+0x4d/0x80
[79380.619543]  [<ffffffff810f2c51>] ? ktime_get+0x41/0xb0
[79380.619547]  [<ffffffff815f38c4>] io_schedule_timeout+0xa4/0x110
[79380.619560]  [<ffffffffa0203dc1>] cv_wait_common+0xb1/0x130 [spl]
[79380.619565]  [<ffffffff810c0450>] ? wake_atomic_t_function+0x60/0x60
[79380.619574]  [<ffffffffa0203e98>] __cv_wait_io+0x18/0x20 [spl]
[79380.619623]  [<ffffffffa0343c3d>] zio_wait+0xfd/0x1d0 [zfs]
[79380.619672]  [<ffffffffa02cf258>] dsl_pool_sync+0xb8/0x480 [zfs]
[79380.619724]  [<ffffffffa02ea38f>] spa_sync+0x37f/0xb30 [zfs]
[79380.619729]  [<ffffffff810a6c12>] ? default_wake_function+0x12/0x20
[79380.619780]  [<ffffffffa02fac1a>] txg_sync_thread+0x3ba/0x620 [zfs]
[79380.619830]  [<ffffffffa02fa860>] ? txg_delay+0x160/0x160 [zfs]
[79380.619839]  [<ffffffffa01fef61>] thread_generic_wrapper+0x71/0x80 [spl]
[79380.619847]  [<ffffffffa01feef0>] ? __thread_exit+0x20/0x20 [spl]
[79380.619854]  [<ffffffff8109be38>] kthread+0xd8/0xf0
[79380.619860]  [<ffffffff8102c782>] ? __switch_to+0x2d2/0x630
[79380.619866]  [<ffffffff815f823f>] ret_from_fork+0x1f/0x40
[79380.619872]  [<ffffffff8109bd60>] ? kthread_worker_fn+0x170/0x170
[81223.826929] INFO: task txg_sync:1372 blocked for more than 120 seconds.
[81223.827091]       Tainted: P           O    4.8.13-1-ARCH #1
[81223.827226] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[81223.827381] txg_sync        D ffff88021c893ab8     0  1372      2 0x00000000
[81223.827393]  ffff88021c893ab8 00ffffff810a5d2f ffff8802157c3400 ffff88021d3ece00
[81223.827400]  0000000000000046 ffff88021c894000 ffff88022fc17f80 7fffffffffffffff
[81223.827407]  ffff88015f37d0f0 0000000000000001 ffff88021c893ad0 ffffffff815f40ec
[81223.827413] Call Trace:
[81223.827426]  [<ffffffff815f40ec>] schedule+0x3c/0x90
[81223.827432]  [<ffffffff815f6fa3>] schedule_timeout+0x243/0x3d0
[81223.827439]  [<ffffffff810a6c12>] ? default_wake_function+0x12/0x20
[81223.827444]  [<ffffffff810bfd9d>] ? __wake_up_common+0x4d/0x80
[81223.827451]  [<ffffffff810f2c51>] ? ktime_get+0x41/0xb0
[81223.827455]  [<ffffffff815f38c4>] io_schedule_timeout+0xa4/0x110
[81223.827468]  [<ffffffffa0203dc1>] cv_wait_common+0xb1/0x130 [spl]
[81223.827473]  [<ffffffff810c0450>] ? wake_atomic_t_function+0x60/0x60
[81223.827482]  [<ffffffffa0203e98>] __cv_wait_io+0x18/0x20 [spl]
[81223.827531]  [<ffffffffa0343c3d>] zio_wait+0xfd/0x1d0 [zfs]
[81223.827580]  [<ffffffffa02cf258>] dsl_pool_sync+0xb8/0x480 [zfs]
[81223.827632]  [<ffffffffa02ea38f>] spa_sync+0x37f/0xb30 [zfs]
[81223.827636]  [<ffffffff810a6c12>] ? default_wake_function+0x12/0x20
[81223.827687]  [<ffffffffa02fac1a>] txg_sync_thread+0x3ba/0x620 [zfs]
[81223.827738]  [<ffffffffa02fa860>] ? txg_delay+0x160/0x160 [zfs]
[81223.827747]  [<ffffffffa01fef61>] thread_generic_wrapper+0x71/0x80 [spl]
[81223.827755]  [<ffffffffa01feef0>] ? __thread_exit+0x20/0x20 [spl]
[81223.827762]  [<ffffffff8109be38>] kthread+0xd8/0xf0
[81223.827767]  [<ffffffff8102c782>] ? __switch_to+0x2d2/0x630
[81223.827773]  [<ffffffff815f823f>] ret_from_fork+0x1f/0x40
[81223.827779]  [<ffffffff8109bd60>] ? kthread_worker_fn+0x170/0x170
[94923.528386] CE: hpet increased min_delta_ns to 20115 nsec
[102727.918958] INFO: task rsync:5577 blocked for more than 120 seconds.
[102727.919092]       Tainted: P           O    4.8.13-1-ARCH #1
[102727.919170] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[102727.919277] rsync           D ffff8801dfe9ba10     0  5577   5576 0x00000000
[102727.919287]  ffff8801dfe9ba10 00000000ffffffff ffffffff81a0d500 ffff8802157c3400
[102727.919293]  ffffffffa0340567 ffff8801dfe9c000 ffff88021ed41148 ffff88021ed41180
[102727.919298]  ffff88021ed41170 0000000000000000 ffff8801dfe9ba28 ffffffff815f40ec
[102727.919303] Call Trace:
[102727.919352]  [<ffffffffa0340567>] ? zio_destroy+0xb7/0xc0 [zfs]
[102727.919358]  [<ffffffff815f40ec>] schedule+0x3c/0x90
[102727.919367]  [<ffffffffa0203e1f>] cv_wait_common+0x10f/0x130 [spl]
[102727.919373]  [<ffffffff810c0450>] ? wake_atomic_t_function+0x60/0x60
[102727.919380]  [<ffffffffa0203e55>] __cv_wait+0x15/0x20 [spl]
[102727.919414]  [<ffffffffa02b814b>] dmu_tx_wait+0x9b/0x340 [zfs]
[102727.919448]  [<ffffffffa02b847b>] dmu_tx_assign+0x8b/0x490 [zfs]
[102727.919482]  [<ffffffffa0333d09>] zfs_write+0x3f9/0xc80 [zfs]
[102727.919514]  [<ffffffffa0340567>] ? zio_destroy+0xb7/0xc0 [zfs]
[102727.919544]  [<ffffffffa0343c78>] ? zio_wait+0x138/0x1d0 [zfs]
[102727.919579]  [<ffffffffa0328510>] ? zfs_range_unlock+0x1a0/0x2c0 [zfs]
[102727.919610]  [<ffffffffa034926c>] zpl_write_common_iovec+0x8c/0xe0 [zfs]
[102727.919640]  [<ffffffffa0349497>] zpl_write+0x87/0xc0 [zfs]
[102727.919646]  [<ffffffff81208797>] __vfs_write+0x37/0x140
[102727.919652]  [<ffffffff810c7be7>] ? percpu_down_read+0x17/0x50
[102727.919656]  [<ffffffff81209566>] vfs_write+0xb6/0x1a0
[102727.919660]  [<ffffffff8120a9e5>] SyS_write+0x55/0xc0
[102727.919665]  [<ffffffff815f8032>] entry_SYSCALL_64_fastpath+0x1a/0xa4
```

Happened last night during backups.",2017-01-18T21:52:50Z,30030
1018,openzfs/zfs,40463575,277640892,"Okay, so I've replaced the drive which had the high await time, and also increased the memory to 16GB, still having issues:

```
[12785.566973] CE: hpet increased min_delta_ns to 20115 nsec
[25560.317294] INFO: task txg_sync:1392 blocked for more than 120 seconds.
[25560.317451]       Tainted: P           O    4.9.6-1-ARCH #1
[25560.317542] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[25560.317668] txg_sync        D    0  1392      2 0x00000000
[25560.317678]  ffff88041b855c00 0000000000000000 ffff8804104b2700 ffff88042fc180c0
[25560.317686]  ffffffff81a0e500 ffffc9000828fad8 ffffffff81605cdf ffff880411a4c080
[25560.317692]  00ffffffa0341360 ffff88042fc180c0 0000000000000000 ffff8804104b2700
[25560.317698] Call Trace:
[25560.317713]  [<ffffffff81605cdf>] ? __schedule+0x22f/0x6e0
[25560.317719]  [<ffffffff816061cd>] schedule+0x3d/0x90
[25560.317726]  [<ffffffff81608fd3>] schedule_timeout+0x243/0x3d0
[25560.317781]  [<ffffffffa033fbb1>] ? zio_taskq_dispatch+0x91/0xa0 [zfs]
[25560.317821]  [<ffffffffa033fbd2>] ? zio_issue_async+0x12/0x20 [zfs]
[25560.317859]  [<ffffffffa0343569>] ? zio_nowait+0x79/0x110 [zfs]
[25560.317867]  [<ffffffff810f7b81>] ? ktime_get+0x41/0xb0
[25560.317873]  [<ffffffff81605a44>] io_schedule_timeout+0xa4/0x110
[25560.317884]  [<ffffffffa01eccd1>] cv_wait_common+0xb1/0x130 [spl]
[25560.317891]  [<ffffffff810c4200>] ? wake_atomic_t_function+0x60/0x60
[25560.317900]  [<ffffffffa01ecda8>] __cv_wait_io+0x18/0x20 [spl]
[25560.317938]  [<ffffffffa034337c>] zio_wait+0xac/0x130 [zfs]
[25560.317984]  [<ffffffffa02cf408>] dsl_pool_sync+0xb8/0x480 [zfs]
[25560.318035]  [<ffffffffa02e9e1f>] spa_sync+0x37f/0xb30 [zfs]
[25560.318041]  [<ffffffff810aa4a2>] ? default_wake_function+0x12/0x20
[25560.318091]  [<ffffffffa02fa6aa>] txg_sync_thread+0x3ba/0x620 [zfs]
[25560.318096]  [<ffffffff8102d752>] ? __switch_to+0x2d2/0x630
[25560.318145]  [<ffffffffa02fa2f0>] ? txg_delay+0x160/0x160 [zfs]
[25560.318154]  [<ffffffffa01e7f22>] thread_generic_wrapper+0x72/0x80 [spl]
[25560.318161]  [<ffffffffa01e7eb0>] ? __thread_exit+0x20/0x20 [spl]
[25560.318167]  [<ffffffff8109e8f9>] kthread+0xd9/0xf0
[25560.318171]  [<ffffffff8102d752>] ? __switch_to+0x2d2/0x630
[25560.318176]  [<ffffffff8109e820>] ? kthread_park+0x60/0x60
[25560.318180]  [<ffffffff8109e820>] ? kthread_park+0x60/0x60
[25560.318184]  [<ffffffff8160a995>] ret_from_fork+0x25/0x30
[25683.204571] INFO: task txg_sync:1392 blocked for more than 120 seconds.
[25683.204722]       Tainted: P           O    4.9.6-1-ARCH #1
[25683.204814] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[25683.204940] txg_sync        D    0  1392      2 0x00000000
[25683.204950]  ffff88041b855c00 0000000000000000 ffff8804104b2700 ffff88042fc180c0
[25683.204958]  ffffffff81a0e500 ffffc9000828fad8 ffffffff81605cdf ffff880411a4c080
[25683.204965]  00ffffffa0341360 ffff88042fc180c0 0000000000000000 ffff8804104b2700
[25683.204970] Call Trace:
[25683.204985]  [<ffffffff81605cdf>] ? __schedule+0x22f/0x6e0
[25683.204992]  [<ffffffff816061cd>] schedule+0x3d/0x90
[25683.204999]  [<ffffffff81608fd3>] schedule_timeout+0x243/0x3d0
[25683.205055]  [<ffffffffa033fbb1>] ? zio_taskq_dispatch+0x91/0xa0 [zfs]
[25683.205095]  [<ffffffffa033fbd2>] ? zio_issue_async+0x12/0x20 [zfs]
[25683.205134]  [<ffffffffa0343569>] ? zio_nowait+0x79/0x110 [zfs]
[25683.205142]  [<ffffffff810f7b81>] ? ktime_get+0x41/0xb0
[25683.205148]  [<ffffffff81605a44>] io_schedule_timeout+0xa4/0x110
[25683.205159]  [<ffffffffa01eccd1>] cv_wait_common+0xb1/0x130 [spl]
[25683.205166]  [<ffffffff810c4200>] ? wake_atomic_t_function+0x60/0x60
[25683.205175]  [<ffffffffa01ecda8>] __cv_wait_io+0x18/0x20 [spl]
[25683.205213]  [<ffffffffa034337c>] zio_wait+0xac/0x130 [zfs]
[25683.205259]  [<ffffffffa02cf408>] dsl_pool_sync+0xb8/0x480 [zfs]
[25683.205310]  [<ffffffffa02e9e1f>] spa_sync+0x37f/0xb30 [zfs]
[25683.205315]  [<ffffffff810aa4a2>] ? default_wake_function+0x12/0x20
[25683.205365]  [<ffffffffa02fa6aa>] txg_sync_thread+0x3ba/0x620 [zfs]
[25683.205371]  [<ffffffff8102d752>] ? __switch_to+0x2d2/0x630
[25683.205419]  [<ffffffffa02fa2f0>] ? txg_delay+0x160/0x160 [zfs]
[25683.205428]  [<ffffffffa01e7f22>] thread_generic_wrapper+0x72/0x80 [spl]
[25683.205436]  [<ffffffffa01e7eb0>] ? __thread_exit+0x20/0x20 [spl]
[25683.205441]  [<ffffffff8109e8f9>] kthread+0xd9/0xf0
[25683.205445]  [<ffffffff8102d752>] ? __switch_to+0x2d2/0x630
[25683.205450]  [<ffffffff8109e820>] ? kthread_park+0x60/0x60
[25683.205454]  [<ffffffff8109e820>] ? kthread_park+0x60/0x60
[25683.205458]  [<ffffffff8160a995>] ret_from_fork+0x25/0x30
[26051.866268] INFO: task txg_sync:1392 blocked for more than 120 seconds.
[26051.866424]       Tainted: P           O    4.9.6-1-ARCH #1
[26051.866516] ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message.
[26051.866642] txg_sync        D    0  1392      2 0x00000000
[26051.866652]  ffff8803e6f2c000 0000000000000000 ffff8804104b2700 ffff88042fc180c0
[26051.866660]  ffff88041b50b400 ffffc9000828fad8 ffffffff81605cdf ffff880411a4c080
[26051.866667]  00ffffffa0341360 ffff88042fc180c0 0000000000000000 ffff8804104b2700
[26051.866673] Call Trace:
[26051.866687]  [<ffffffff81605cdf>] ? __schedule+0x22f/0x6e0
[26051.866694]  [<ffffffff816061cd>] schedule+0x3d/0x90
[26051.866700]  [<ffffffff81608fd3>] schedule_timeout+0x243/0x3d0
[26051.866756]  [<ffffffffa033fbb1>] ? zio_taskq_dispatch+0x91/0xa0 [zfs]
[26051.866796]  [<ffffffffa033fbd2>] ? zio_issue_async+0x12/0x20 [zfs]
[26051.866834]  [<ffffffffa0343569>] ? zio_nowait+0x79/0x110 [zfs]
[26051.866842]  [<ffffffff810f7b81>] ? ktime_get+0x41/0xb0
[26051.866847]  [<ffffffff81605a44>] io_schedule_timeout+0xa4/0x110
[26051.866859]  [<ffffffffa01eccd1>] cv_wait_common+0xb1/0x130 [spl]
[26051.866866]  [<ffffffff810c4200>] ? wake_atomic_t_function+0x60/0x60
[26051.866874]  [<ffffffffa01ecda8>] __cv_wait_io+0x18/0x20 [spl]
[26051.866913]  [<ffffffffa034337c>] zio_wait+0xac/0x130 [zfs]
[26051.866959]  [<ffffffffa02cf408>] dsl_pool_sync+0xb8/0x480 [zfs]
[26051.867009]  [<ffffffffa02e9e1f>] spa_sync+0x37f/0xb30 [zfs]
[26051.867015]  [<ffffffff810aa4a2>] ? default_wake_function+0x12/0x20
[26051.867065]  [<ffffffffa02fa6aa>] txg_sync_thread+0x3ba/0x620 [zfs]
[26051.867070]  [<ffffffff8102d752>] ? __switch_to+0x2d2/0x630
[26051.867119]  [<ffffffffa02fa2f0>] ? txg_delay+0x160/0x160 [zfs]
[26051.867128]  [<ffffffffa01e7f22>] thread_generic_wrapper+0x72/0x80 [spl]
[26051.867135]  [<ffffffffa01e7eb0>] ? __thread_exit+0x20/0x20 [spl]
[26051.867140]  [<ffffffff8109e8f9>] kthread+0xd9/0xf0
[26051.867145]  [<ffffffff8102d752>] ? __switch_to+0x2d2/0x630
[26051.867149]  [<ffffffff8109e820>] ? kthread_park+0x60/0x60
[26051.867153]  [<ffffffff8109e820>] ? kthread_park+0x60/0x60
[26051.867157]  [<ffffffff8160a995>] ret_from_fork+0x25/0x30
```",2017-02-06T10:20:35Z,30030
1019,openzfs/zfs,40463575,277641361,"Running

```
% pacman -Q | egrep ""zfs|spl""
spl-linux 0.6.5.9_4.9.6_1-2
spl-utils-linux 0.6.5.9_4.9.6_1-2
zfs-linux 0.6.5.9_4.9.6_1-2
zfs-utils-linux 0.6.5.9_4.9.6_1-2
```",2017-02-06T10:22:26Z,30030
1020,openzfs/zfs,40463575,277641952,"Memory available is okay:

```
% free -h
              total        used        free      shared  buff/cache   available
Mem:            15G         12G        2.7G        692K        630M        3.0G
Swap:          4.0G          0B        4.0G
```

iostat seem okay (sde is samsung 840 pro)

```
% iostat -mx 10
Linux 4.9.6-1-ARCH (hinoki) 	06/02/17 	_x86_64_	(2 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           0.69    0.00    2.26    3.18    0.00   93.88

Device:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00     0.00    4.39   13.96     0.19     0.23    47.25     0.06    3.16   11.42    0.57   2.28   4.18
sdb               0.00     0.00    4.39   13.69     0.19     0.22    46.63     0.06    3.13   11.19    0.55   2.26   4.09
sdc               0.00     0.00    4.29   13.58     0.19     0.23    47.97     0.06    3.61   12.91    0.66   2.58   4.62
sdd               0.00     0.00    4.32   13.80     0.18     0.23    46.40     0.06    3.18   11.60    0.55   2.29   4.15
sde               0.00     0.30    1.46   11.25     0.02     0.36    61.82     0.01    0.41    0.78    0.36   0.22   0.28

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          10.43    0.00   47.68   28.80    0.00   13.08

Device:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00     0.00   54.20    0.80     3.27     0.01   122.01     0.31    5.70    5.77    0.88   3.10  17.06
sdb               0.00     0.00   54.60    0.80     3.09     0.01   114.60     0.43    7.70    7.80    0.75   3.62  20.07
sdc               0.00     0.00   54.20    1.00     3.15     0.01   117.16     0.39    6.98    7.11    0.00   3.80  21.00
sdd               0.00     0.00   56.30    1.00     3.08     0.01   110.27     0.43    7.57    7.70    0.30   3.87  22.17
sde               0.00     5.60   27.30  159.50     0.08     7.38    81.70     0.06    0.34    0.21    0.36   0.22   4.20

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           5.69    0.00   43.65   30.28    0.00   20.38

Device:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00     0.00   43.30    0.90     2.45     0.01   113.81     0.35    7.86    8.03    0.00   4.38  19.34
sdb               0.00     0.00   40.30    0.90     2.31     0.01   114.99     0.36    8.68    8.87    0.44   4.47  18.40
sdc               0.00     0.00   41.80    0.80     2.39     0.01   115.08     0.34    8.08    8.22    0.38   4.68  19.93
sdd               0.00     0.00   41.20    0.80     2.30     0.01   112.34     0.44   10.39   10.58    0.88   4.56  19.16
sde               0.00     3.10   25.80  136.70     0.07     5.80    74.00     0.05    0.28    0.48    0.25   0.21   3.47

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           3.42    0.00   38.83   28.72    0.00   29.02

Device:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sda               0.00     0.00   26.70    0.90     1.58     0.01   117.86     0.26    9.37    9.63    1.78   5.51  15.20
sdb               0.00     0.00   28.50    0.90     1.53     0.01   107.54     0.25    8.59    8.86    0.00   5.04  14.83
sdc               0.00     0.00   27.40    0.90     1.41     0.01   103.12     0.27    9.34    9.65    0.00   5.49  15.53
sdd               0.00     0.00   28.60    0.90     1.43     0.01   100.18     0.24    8.21    8.47    0.00   4.84  14.27
sde               0.00     0.80   27.30   84.50     0.08     2.67    50.38     0.02    0.21    0.27    0.19   0.19   2.07
```",2017-02-06T10:24:59Z,30030
1021,openzfs/zfs,40463575,277699228,"Just wanted to give a short update to my post in this issue from last year: in the mean time I have upgraded to ZFS 0.6.5.8 from Debian's backports, still using Debian 8. Unfortunately I still get the exact same timeout in the kernel logs.",2017-02-06T14:33:43Z,776801
1022,openzfs/zfs,40463575,277712973,"@kpande if you're going to assert that, can you please describe what's big enough?

I have resources 4X the average described in this thread and I also see the same issues, in this case with the combination of zfs send and nfsd load.",2017-02-06T15:16:52Z,3001850
1023,openzfs/zfs,40463575,277816461,"> severely undersized hardware

You may be right.

I have 4x 4TB drives, and one 256 SSD as OS/Cache. 16GB memory, 2 core CPU (1.5Ghz Atom). This system is almost exclusively used for RSync backups. I do feel that this is pretty reasonable for my needs.

The ARC cache is 12GB, the OS has 4GB left over. The L2 cache, if enabled , is about 170GB.

I'll have to check how many files it is, but I'm not sure if it's multi-millions or not.",2017-02-06T21:18:21Z,30030
1024,openzfs/zfs,40463575,278011680,"@kpande you are right I forgot to give any relevant infos about my underlying hardware. Sorry about that, here are hopefully all the relevant infos:

My server is a virtualization Server running Xen 4.4 with currently 6 virtual machines which all have their logical volumes (LVM) stored on a RAIDZ1 volume with 3x 2TB Seagate SATA enterprise disks (ST32000645NS). The Debian 8 OS is independent and located on two internal SATA-SSD disks of both 16 GB in RAID1 using Linux MD for mirroring. The CPU is an Intel E5-2620 v3 @ 2.40GHz with 6 cores/12 threads. Out of these 6 cores 4 vCPUs have been pinned to the host/hypervisor/dom0 using the `dom0_max_vcpus=4 dom0_vcpus_pin` Linux kernel options. The server has 64 GB of TruDDR4 ECC memory and out of this 64 GB of memory 6 GB has been reserved to the host/hypervisor/dom0 using the respective Linux kernel options `dom0_mem=6G,max:6G`. Finally I have reserved 2 GB RAM of these 6 GB for the ARC using the following `zfs_arc_max=2147483648` zfs module option. I have also disabled ZFS prefetch if that is of any relevance (`zfs_prefetch_disable=1`).

Below is the output of an actual ARC summary (server has bee rebooted 5 days ago):
```
ZFS Subsystem Report				Tue Feb 07 15:11:32 2017
ARC Summary: (HEALTHY)
	Memory Throttle Count:			0

ARC Misc:
	Deleted:				3.05m
	Mutex Misses:				7
	Evict Skips:				7

ARC Size:				77.69%	1.55	GiB
	Target Size: (Adaptive)		100.00%	2.00	GiB
	Min Size (Hard Limit):		1.56%	32.00	MiB
	Max Size (High Water):		64:1	2.00	GiB

ARC Size Breakdown:
	Recently Used Cache Size:	93.75%	1.88	GiB
	Frequently Used Cache Size:	6.25%	128.00	MiB

ARC Hash Breakdown:
	Elements Max:				171.83k
	Elements Current:		97.97%	168.34k
	Collisions:				68.60m
	Chain Max:				6
	Chains:					12.12k

ARC Total accesses:					102.96m
	Cache Hit Ratio:		48.88%	50.32m
	Cache Miss Ratio:		51.12%	52.63m
	Actual Hit Ratio:		48.88%	50.32m

	Data Demand Efficiency:		0.00%	49.34m

	CACHE HITS BY CACHE LIST:
	  Most Recently Used:		63.44%	31.92m
	  Most Frequently Used:		36.56%	18.40m
	  Most Recently Used Ghost:	0.08%	42.37k
	  Most Frequently Used Ghost:	0.00%	0

	CACHE HITS BY DATA TYPE:
	  Demand Data:			0.00%	167
	  Prefetch Data:		0.00%	0
	  Demand Metadata:		100.00%	50.32m
	  Prefetch Metadata:		0.00%	0

	CACHE MISSES BY DATA TYPE:
	  Demand Data:			93.74%	49.34m
	  Prefetch Data:		0.00%	0
	  Demand Metadata:		6.26%	3.29m
	  Prefetch Metadata:		0.00%	145




ZFS Tunable:
	metaslab_debug_load                               0
	zfs_arc_min_prefetch_lifespan                     0
	zfetch_max_streams                                8
	zfs_nopwrite_enabled                              1
	zfetch_min_sec_reap                               2
	zfs_dbgmsg_enable                                 0
	zfs_dirty_data_max_max_percent                    25
	zfs_arc_p_aggressive_disable                      1
	spa_load_verify_data                              1
	zfs_zevent_cols                                   80
	zfs_dirty_data_max_percent                        10
	zfs_sync_pass_dont_compress                       5
	l2arc_write_max                                   8388608
	zfs_vdev_scrub_max_active                         2
	zfs_vdev_sync_write_min_active                    10
	zvol_prefetch_bytes                               131072
	metaslab_aliquot                                  524288
	zfs_no_scrub_prefetch                             0
	zfs_arc_shrink_shift                              0
	zfetch_block_cap                                  256
	zfs_txg_history                                   0
	zfs_delay_scale                                   500000
	zfs_vdev_async_write_active_min_dirty_percent     30
	metaslab_debug_unload                             0
	zfs_read_history                                  0
	zvol_max_discard_blocks                           16384
	zfs_recover                                       0
	l2arc_headroom                                    2
	zfs_deadman_synctime_ms                           1000000
	zfs_scan_idle                                     50
	zfs_free_min_time_ms                              1000
	zfs_dirty_data_max                                624856268
	zfs_vdev_async_read_min_active                    1
	zfs_mg_noalloc_threshold                          0
	zfs_dedup_prefetch                                0
	zfs_vdev_max_active                               1000
	l2arc_write_boost                                 8388608
	zfs_resilver_min_time_ms                          3000
	zfs_vdev_async_write_max_active                   10
	zil_slog_limit                                    1048576
	zfs_prefetch_disable                              1
	zfs_resilver_delay                                2
	metaslab_lba_weighting_enabled                    1
	zfs_mg_fragmentation_threshold                    85
	l2arc_feed_again                                  1
	zfs_zevent_console                                0
	zfs_immediate_write_sz                            32768
	zfs_dbgmsg_maxsize                                4194304
	zfs_free_leak_on_eio                              0
	zfs_deadman_enabled                               1
	metaslab_bias_enabled                             1
	zfs_arc_p_dampener_disable                        1
	zfs_object_mutex_size                             64
	zfs_metaslab_fragmentation_threshold              70
	zfs_no_scrub_io                                   0
	metaslabs_per_vdev                                200
	zfs_dbuf_state_index                              0
	zfs_vdev_sync_read_min_active                     10
	metaslab_fragmentation_factor_enabled             1
	zvol_inhibit_dev                                  0
	zfs_vdev_async_write_active_max_dirty_percent     60
	zfs_vdev_cache_size                               0
	zfs_vdev_mirror_switch_us                         10000
	zfs_dirty_data_sync                               67108864
	spa_config_path                                   /etc/zfs/zpool.cache
	zfs_dirty_data_max_max                            1562140672
	zfs_arc_lotsfree_percent                          10
	zfs_zevent_len_max                                64
	zfs_scan_min_time_ms                              1000
	zfs_arc_sys_free                                  0
	zfs_arc_meta_strategy                             1
	zfs_vdev_cache_bshift                             16
	zfs_arc_meta_adjust_restarts                      4096
	zfs_max_recordsize                                1048576
	zfs_vdev_scrub_min_active                         1
	zfs_vdev_read_gap_limit                           32768
	zfs_arc_meta_limit                                0
	zfs_vdev_sync_write_max_active                    10
	l2arc_norw                                        0
	zfs_arc_meta_prune                                10000
	metaslab_preload_enabled                          1
	l2arc_nocompress                                  0
	zvol_major                                        230
	zfs_vdev_aggregation_limit                        131072
	zfs_flags                                         0
	spa_asize_inflation                               24
	zfs_admin_snapshot                                0
	l2arc_feed_secs                                   1
	zio_taskq_batch_pct                               75
	zfs_sync_pass_deferred_free                       2
	zfs_disable_dup_eviction                          0
	zfs_arc_grow_retry                                0
	zfs_read_history_hits                             0
	zfs_vdev_async_write_min_active                   1
	zfs_vdev_async_read_max_active                    3
	zfs_scrub_delay                                   4
	zfs_delay_min_dirty_percent                       60
	zfs_free_max_blocks                               100000
	zfs_vdev_cache_max                                16384
	zio_delay_max                                     30000
	zfs_top_maxinflight                               32
	ignore_hole_birth                                 1
	spa_slop_shift                                    5
	zfs_vdev_write_gap_limit                          4096
	spa_load_verify_metadata                          1
	spa_load_verify_maxinflight                       10000
	l2arc_noprefetch                                  1
	zfs_vdev_scheduler                                noop
	zfs_expire_snapshot                               300
	zfs_sync_pass_rewrite                             2
	zil_replay_disable                                0
	zfs_nocacheflush                                  0
	zfs_arc_max                                       2147483648
	zfs_arc_min                                       0
	zfs_read_chunk_size                               1048576
	zfs_txg_timeout                                   5
	zfs_pd_bytes_max                                  52428800
	l2arc_headroom_boost                              200
	zfs_send_corrupt_data                             0
	l2arc_feed_min_ms                                 200
	zfs_arc_meta_min                                  0
	zfs_arc_average_blocksize                         8192
	zfetch_array_rd_sz                                1048576
	zfs_autoimport_disable                            1
	zfs_arc_p_min_shift                               0
	zio_requeue_io_start_cut_in_line                  1
	zfs_vdev_sync_read_max_active                     10
	zfs_mdcomp_disable                                0
	zfs_arc_num_sublists_per_state                    4
```
Do you need any more information? and what do you think about this setup? is my hardware undersized?",2017-02-07T14:15:54Z,776801
1025,openzfs/zfs,40463575,278163009,I checked and the majority of my backups are < 200k files and < 20GBytes.,2017-02-07T22:27:00Z,30030
1026,openzfs/zfs,40463575,374879892,"I hope it helps:

> user@server ~ $ cat /var/log/syslog | grep zfs
> Mar 21 04:01:11 server kernel: [204939.534145]  dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:01:11 server kernel: [204939.534234]  dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:01:11 server kernel: [204939.534339]  zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:01:11 server kernel: [204939.534439]  ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:01:11 server kernel: [204939.534539]  ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:01:11 server kernel: [204939.534648]  zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:01:11 server kernel: [204939.534752]  zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:03:12 server kernel: [205060.371676]  dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:03:12 server kernel: [205060.371764]  dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:03:12 server kernel: [205060.371869]  zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:03:12 server kernel: [205060.371969]  ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:03:12 server kernel: [205060.372069]  ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:03:12 server kernel: [205060.372177]  zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:03:12 server kernel: [205060.372282]  zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:05:13 server kernel: [205181.209034]  dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:05:13 server kernel: [205181.209123]  dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:05:13 server kernel: [205181.209228]  zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:05:13 server kernel: [205181.209329]  ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:05:13 server kernel: [205181.209429]  ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:05:13 server kernel: [205181.209537]  zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:05:13 server kernel: [205181.209641]  zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:07:13 server kernel: [205302.045999]  dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:07:13 server kernel: [205302.046088]  dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:07:13 server kernel: [205302.046192]  zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:07:13 server kernel: [205302.046293]  ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:07:13 server kernel: [205302.046392]  ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:07:13 server kernel: [205302.046501]  zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:07:13 server kernel: [205302.046605]  zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:09:14 server kernel: [205422.882950]  dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:09:14 server kernel: [205422.883039]  dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:09:14 server kernel: [205422.883144]  zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:09:14 server kernel: [205422.883244]  ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:09:14 server kernel: [205422.883343]  ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:09:14 server kernel: [205422.883452]  zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:09:14 server kernel: [205422.883556]  zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:11:15 server kernel: [205543.719843]  dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:11:15 server kernel: [205543.719933]  dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:11:15 server kernel: [205543.720037]  zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:11:15 server kernel: [205543.720137]  ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:11:15 server kernel: [205543.720237]  ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:11:15 server kernel: [205543.720345]  zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:11:15 server kernel: [205543.720449]  zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:13:16 server kernel: [205664.556609]  dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:13:16 server kernel: [205664.556698]  dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:13:16 server kernel: [205664.556802]  zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:13:16 server kernel: [205664.556903]  ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:13:16 server kernel: [205664.557003]  ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:13:16 server kernel: [205664.557112]  zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:13:16 server kernel: [205664.557216]  zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:15:17 server kernel: [205785.393394]  dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:15:17 server kernel: [205785.393483]  dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:15:17 server kernel: [205785.393587]  zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:15:17 server kernel: [205785.393687]  ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:15:17 server kernel: [205785.393787]  ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:15:17 server kernel: [205785.393896]  zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:15:17 server kernel: [205785.394000]  zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:17:18 server kernel: [205906.229973]  dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:17:18 server kernel: [205906.230062]  dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:17:18 server kernel: [205906.230166]  zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:17:18 server kernel: [205906.230289]  ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:17:18 server kernel: [205906.230389]  ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:17:18 server kernel: [205906.230498]  zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:17:18 server kernel: [205906.230602]  zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 04:19:18 server kernel: [206027.066643]  dmu_tx_wait+0x36f/0x390 [zfs]
> Mar 21 04:19:18 server kernel: [206027.066732]  dmu_tx_assign+0x83/0x470 [zfs]
> Mar 21 04:19:18 server kernel: [206027.066837]  zfs_write+0x3f6/0xd80 [zfs]
> Mar 21 04:19:18 server kernel: [206027.066937]  ? rrw_exit+0x5a/0x150 [zfs]
> Mar 21 04:19:18 server kernel: [206027.067037]  ? rrm_exit+0x46/0x80 [zfs]
> Mar 21 04:19:18 server kernel: [206027.067146]  zpl_write_common_iovec+0x8c/0xe0 [zfs]
> Mar 21 04:19:18 server kernel: [206027.067250]  zpl_iter_write+0xae/0xe0 [zfs]
> Mar 21 07:54:33 server systemd[1]: zfs-import-cache.service: Main process exited, code=exited, status=1/FAILURE
> Mar 21 07:54:33 server systemd[1]: zfs-import-cache.service: Failed with result 'exit-code'.
> user@server ~ $ cat /etc/issue
> Ubuntu Bionic Beaver (development branch) \n \l
> 
> user@server ~ $ uname -a
> Linux server 4.15.0-12-generic #13-Ubuntu SMP Thu Mar 8 06:24:47 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
> user@server ~ $ dpkg -l zfs*
> Desired=Unknown/Install/Remove/Purge/Hold
> | Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend
> |/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)
> ||/ Name           Version      Architecture Description
> +++-==============-============-============-=================================
> un  zfs            <none>       <none>       (no description available)
> un  zfs-dkms       <none>       <none>       (no description available)
> un  zfs-dracut     <none>       <none>       (no description available)
> un  zfs-fuse       <none>       <none>       (no description available)
> un  zfs-initramfs  <none>       <none>       (no description available)
> un  zfs-modules    <none>       <none>       (no description available)
> ii  zfs-zed        0.7.5-1ubunt amd64        OpenZFS Event Daemon
> un  zfsutils       <none>       <none>       (no description available)
> ii  zfsutils-linux 0.7.5-1ubunt amd64        command-line tools to manage Open
> user@server ~ $
> user@server ~ $ df -h /data
> Filesystem      Size  Used Avail Use% Mounted on
> data             14T   11T  3.1T  78% /data
 

it was mv process from one zfs to another in the same pool (actually it behaves as cp, not faster). the process hangs forever (at least few hours) and not allow to terminate with a signal. zpool reports errors-free. Now I've started `zpool scrub`, let's see what it will report.

UPDATE:
> user@server ~ $ zpool status
>   pool: data
>  state: ONLINE
>   scan: scrub repaired 0B in 18h22m with 0 errors on Thu Mar 22 02:32:52 2018
> config:
> 
>         NAME        STATE     READ WRITE CKSUM
>         data        ONLINE       0     0     0
>           raidz2-0  ONLINE       0     0     0
>             sdf     ONLINE       0     0     0
>             sdg     ONLINE       0     0     0
>             sdh     ONLINE       0     0     0
>             sdi     ONLINE       0     0     0
>             sdj     ONLINE       0     0     0
>             sdk     ONLINE       0     0     0
>             sdl     ONLINE       0     0     0
>             sdm     ONLINE       0     0     0
> 
> errors: No known data errors

Let me know if you need other infos.
With best regards, Ag",2018-03-21T09:39:27Z,37616567
1027,jashkenas/coffeescript,544017763,544017763,"CoffeeScript:
```coffeescript
class Cat
  
  [@symbol]: =>
```

Output Javascript:
```javascript
class Cat {
  constructor() {
    this.[this.symbol] = this.[this.symbol].bind(this);
  }

  [this.symbol]() {}
}
```

But must:
```javascript
class Cat {
  constructor() {
    this[Cat.symbol] = this[Cat.symbol].bind(this);
  }

  [Cat.symbol]() {}
}
```

Two bugs:
===

1) `this` in constructor is not `this` in class body
2) not need dot after `this` keyword, `this.[exp]` is syntax error

Reason:
===
```coffeescript
class {
  [EXP PROP NAME IN CLASS BODY NOT WORKING CORRECT]() {}
}
```",2019-12-30T21:32:10Z,58376230
1028,jashkenas/coffeescript,544017763,569935650,"Hey, why you close it?",2019-12-31T14:17:57Z,58376230
1029,dokuwiki/dokuwiki,315863357,315863357,"Next month (May 2018), a new EU regulation will come into effect, concerning the privacy/ data protection of user data. This new regulation comes with very high potential sanctions: up to 20 Million EUR or 4% of a years revenue, depending on which is _higher_.

This also poses some questions on whether DokuWiki is GDPR-compliant (and what does that even mean?):

- [ ] If a user deletes their account, do we have to delete their username from changelogs/meta?
  - Question asked here: https://law.stackexchange.com/q/27795/17677
  - [ ] Would that potentially conflict with the license, if that license requires attribution?
- [ ] Do we have to delete IP-addresses from changelogs, maybe after some time?
  - [ ] Can we do that in a way that still lets us tell different anonymous users apart from each other?
- [ ] Do we need to show some message / do we need some legally worded user opt-in for some things? (Subscriptions?)
- [ ] Do we need a Privacy Statement for DokuWiki.org?
  - [ ] should we provide examples other users can adjust?
  - [ ] would it be desirable to extend that with technology (eg. automatically list what data is collected when, why and how long) and provide a way for plugins to hook into that?

Maybe some of you have expertise in this matter or work in a company with legal resources to answer such questions -- Input would be greatly appreciated 🙏",2018-04-19T12:51:42Z,7372507
1030,dokuwiki/dokuwiki,315863357,382905108,Do we have any donated money left to potentially pay a lawyer to help answer some questions?,2018-04-19T22:55:17Z,108893
1031,dokuwiki/dokuwiki,315863357,383005684,"Potentially yes. However since all this law *really* achieves is feeding lawyers, I would prefer to not contribute any more to that for ideological reasons ;-)

If anyone out there is already paying a lawyer to answer their GDPR questions, it would be nice if they could sneak in our questions though...",2018-04-20T07:20:41Z,86426
1032,dokuwiki/dokuwiki,315863357,383101429,"As long as the logs only contain a user ID (ie a numerical reference to their user account) then you'd only have to hash their user details in the central account record (where the numerical user ID, user name and email address are associated) to remove the ""personally identifiable information"" (Pii).    If the user account is ""deleted"" by marking their account as deleted and replacing their Pii with a one way hash of the information in their account details then there is no longer anything that personally identifies them.  The posts could be detected as having a hashed user ID and shown as authored by ""[deleted user]"". If the user rejoined the service with the same details then the a check on the new account's one way hash would match with the deleted accounts hash and the an question asked of the user ""Do you want to associate your previous posts with your new account?"" 

But I checked the logs and they contain the _actual_ user name, not a numerical reference. So that would be a real chore to update the logs (but potentially still possible).

And then of course you have the issue where the user's Pii is included in the body / text of the posts...
eg in a todo item where the username is listed
",2018-04-20T13:47:48Z,6104226
1033,dokuwiki/dokuwiki,315863357,383132933,"Even though thus is a EU regulation, the actual implementation is in national law so answers will vary across legislations. 
Starting with a privacy statement or agreement in which you explain what is collected and why and is accessible to whom and when it will be deleted is always a good start.",2018-04-20T15:28:12Z,1165786
1034,dokuwiki/dokuwiki,315863357,383168648,"From some fresh readings, this regulation is a law that applies to all EU members with extraterritorial involvements (for outside EU working with Europeans),  probably with some variation by land, but not so much (e.g. sensitive data ). 
To get an quick overview, WordPress has a [roadmap](https://make.wordpress.org/core/2018/03/28/roadmap-tools-for-gdpr-compliance) to address GDPR and its first [implementations](https://core.trac.wordpress.org/query?status=!closed&keywords=~gdpr). 
It is quite complex stuff but seems logic and normal (see [Max Schrems](https://en.wikipedia.org/wiki/Max_Schrems ) ). Privacy and consent by design, right to erasure, personal data backup, encrypted data, ...etc,  just good sense. At this stage, I am not sure a lawyer is necessary but companies would need to use a DokuWiki core and plugins GDPR-compliant. 
",2018-04-20T17:35:17Z,12547689
1035,dokuwiki/dokuwiki,315863357,383922750,I started a privacy policy at https://www.dokuwiki.org/privacy -- keeping it understandable (as requested by the GDPR) and complete is quite hard. Any hint on what's missing is welcome.,2018-04-24T13:05:14Z,86426
1036,dokuwiki/dokuwiki,315863357,384096436,"> I started a privacy policy at https://www.dokuwiki.org/privacy -- keeping
> it understandable (as requested by the GDPR) and complete is quite hard.
> Any hint on what's missing is welcome.

I'm rewriting ours tomorrow.  I'll take a look and make some suggestions.

UPDATE : The one at the above link seems to have most of it covered well.  The additional sections that we have in ours are mostly to do with marketing which is most likely not relevant.",2018-04-24T22:11:49Z,6104226
1037,dokuwiki/dokuwiki,315863357,385056832,"[off-topic]: @splitbrain: I assume this is the page do you mean at the top of https://www.dokuwiki.org/privacy ?!
I already corrected the link there.",2018-04-27T18:33:04Z,38754074
1038,dokuwiki/dokuwiki,315863357,388551842,"An excellent article in Bozho's tech blog: [GRPD - a practical guide for developers ](https://techblog.bozho.net/gdpr-practical-guide-developers/). Also, the Drupal GDPR Compliance Team gives a lot of links on their [dedicated page](https://www.drupal.org/project/drupal_gdpr_team)",2018-05-12T12:28:41Z,12547689
1039,dokuwiki/dokuwiki,315863357,388653361,"@splitbrain About the privacy page, I think you have to ensure somehow the users data is save and that how it is kept save has to be described somehow internally for accountability.

Serverlogs, do they contain a user ticket number that is related to their account and visible or knowable by google analytics, if so you should mention that possibly.

GDPR is not done something done by lawyers, but rather accountants, they check if their customers are GDPR compliant. 

Good starting point: https://gdprchecklist.io
",2018-05-13T20:21:41Z,38727432
1040,dokuwiki/dokuwiki,315863357,388761817,"Everyone, please refrain from posting more links to pages ""that explain everything"". If you want to help out, do one of the following:

* extend the privacy policy at https://www.dokuwiki.org/privacy
** feel free to ask specific questions for things you can not answer (eg. details of the server setup)
* post answers to the questions in the original post, with references to the exact text of the applicable laws",2018-05-14T09:54:27Z,86426
1041,dokuwiki/dokuwiki,315863357,390864029,"What about the cookie nag box? DW uses cookies, so DW would need such a box. I noticed that the bootstrap3 template offers to activate a cookie nag box, so why not integrate such a thing into the DW core?",2018-05-22T05:05:39Z,15716793
1042,dokuwiki/dokuwiki,315863357,390962391,"@daumling Cookies which are required in order to fulfill the requests of the website visitor do not require explicit user consent. But any others — including those used for general use statistics eg tracking — do require it.

On our local dokuwiki as far as i can see the cookies are only functional to dokuwiki.",2018-05-22T11:51:43Z,38727432
1043,dokuwiki/dokuwiki,315863357,390965152,"Not sure about that. Session cookies are considered personal data AFAIK, and consent is required. See e.g. this article: https://www.cookiebot.com/en/gdpr-cookies/ - it mentions session cookies.

There is a cookielaw plugin, but it is rudimentary.",2018-05-22T12:02:49Z,15716793
1044,dokuwiki/dokuwiki,315863357,392180629,"AFAIK, https://www.dokuwiki.org/privacy is missing the required bits of [Information to be provided where personal data are collected](http://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A32016R0679#d1e2254-1-1) GDPR §13, especially the mention of the data subject's rights.

Edit: Link to law replaced w/ official URL.",2018-05-25T20:45:03Z,176672
1045,dokuwiki/dokuwiki,315863357,392522327,"I updated the last part and renamed it to ""Your Rights"" that should make it more clear.

Regarding the ""where personal data are collected"" - is that referring to which country? That would be France (Hetzner's servers are located there). Not sure where to put that though.

@xrat can you make changes to the page where you think clarification is needed?",2018-05-28T13:07:08Z,86426
1046,dokuwiki/dokuwiki,315863357,392622963,"For your inspiration, I've edited Greebo (the installed release) to make the *DOKU_PREFS* cookie a session cookie. A session cookie means no permanent storage, so no user consent required.

The second commit removes recording of IP addresses from the logs. Quite some places need code removal, still the result works just fine. All new changelog entries no longer receive the IP address, so nothing can go wrong. Some retrocompatibility code for dealing with older records is also included.

As making a pull request on Github is a chore and Github refuses to accept patches, I made a Gist: https://gist.github.com/Traumflug/74fd0b4c8968fd0184e503d221b13310 with both patches.

With these patches applied the privacy statement reduces to about this (DokuWiki markup):

----
==== General Data Protection Regulation (GDPR) ====

We're neither interested in personal data, nor do we try to collect or use such data. In detail:

  * Pages at reprap-diy.com do not use trackers.
  * Visiting pages at reprap-diy.com stores up to three cookies in your browser to follow the session. These cookies get deleted when the session ends (when you close your browser).
  * Creating an account at reprap-diy.com stores your email address, content of the //Real name// field and an encrypted hash of your password.
  * Logging into an account and checking the //Remember me// checkbox stores another, permanent cookie (valid for one year) to keep you logged in. To remove this cookie, log out.
  * Each page edit stores the username of the user who did the edit. This information cannot get removed, but if the related account was removed, it also cannot be mapped to an email address or other personal data.
  * During page editing your IP address is used to lock the page against a competing edit. The address gets removed when the edit gets saved.
  * Some of the pages on reprap-diy.com may contain external videos. For YouTube we use the ""privacy enhanced"" youtube-nocookie.com domain that will not track your visit. Your IP address will be visible to the server providing the video, though. 
  * To view the data stored about you at reprap-diy.com, look at your  [[start?do=profile|user profile]].
  * To remove this data, go to your [[start?do=profile|user profile]] and delete your account.
----
Voilá, no user consent required, problem solved.

The only issue which might remain is fighting spammers. No IP address, no entry into blacklists. But we all have secured account registration against spammers, right?

----
In case somebody doesn't believe that session cookies need no user consent, he may have a look at this pretty official page: http://ec.europa.eu/ipg/basics/legal/cookies/index_en.htm#section_2. It states:

> Cookies clearly exempt from consent according to the EU advisory body on data protection include:
>
> * user‑input cookies (session-id) such as first‑party cookies to keep track of the user's input when filling online forms, shopping carts, etc., for the duration of a session or persistent cookies limited to a few hours in some cases
> * authentication cookies, to identify the user once he has logged in, for the duration of a session
    user‑centric security cookies, used to detect authentication abuses, for a limited persistent duration
> * multimedia content player cookies, used to store technical data to play back video or audio content, for the duration of a session
> [...]
",2018-05-29T00:40:24Z,318581
1047,dokuwiki/dokuwiki,315863357,392716963,"If I understand it correctly, I don't think we need to do anything about cookies for GDPR (apart from informing the user about them which is already handled by the privacy statement).
The only cookie which contains personally identifiable information is the `DW<hash>` cookie, and users can choose to delete that one at the end of each session by not ticking ""Remember me"". The `DOKU_PREFS` cookie is not used for personally identifiable information (although plugins could potentially abuse it).",2018-05-29T09:42:08Z,108893
1048,dokuwiki/dokuwiki,315863357,392722396,"Sorry for posting more links, but these are more relevant because this is how two big wiki projects handle GDPR:

* [GDPR in Wikimedia's issue tracker](https://phabricator.wikimedia.org/T194901) gives an overview but doesn't really say that much about what actually needs to be done and includes links to a lot of speculation. Like so many others, Wikipedia has changed their Privacy Policy. I'm not sure how they're (not) dealing with [public contributions](https://wikimediafoundation.org/wiki/Privacy_policy#your-public-contribs) is really compliant?
* [OpenStreetMap's GDPR Position Paper](https://wiki.openstreetmap.org/wiki/File:GDPR_Position_Paper.pdf) is more interesting as it seems quite thorough. Their process around [account removal](https://wiki.openstreetmap.org/wiki/Privacy_Policy#Account_Removal) is not as helpful for us, though, as they don't allow anonymous edits (and use a database which makes that task easier).
",2018-05-29T10:01:09Z,108893
1049,dokuwiki/dokuwiki,315863357,392724710,">  The DOKU_PREFS cookie is not used for personally identifiable information

The sheer existence of a cookie means personally identifiable information, because they come with and IP address / DNS entry attached. Content doesn't matter, much less encrypted content.

> Sorry for posting more links, but these are more relevant

D'oh. Those pages pointing to some volunteering efforts are more relevant than an official page. Ouch.

I certainly see this GDPR panic mode everywhere. People try extremely hard to stick to what they're used to, providing endless text blobs in the hope to walk around the problem somehow with lawyer fineprint. Instead of simply fixing the software.",2018-05-29T10:09:31Z,318581
1050,dokuwiki/dokuwiki,315863357,392733145,"> The sheer existence of a cookie means personally identifiable information, because they come with and IP address / DNS entry attached.

I don't think this specific cookie comes with IP address and DNS entry attached. That cookie and its contents is not stored on the server but only in the browser.

> D'oh. Those pages pointing to some volunteering efforts are more relevant than an official page. Ouch.

No need to become personal, especially not dissing ""volunteering efforts"" in any Open Source project.

I meant it is much more relevant to us as in no-one else (apart from other version control software or services, like git or GitHub) deals with the one question which none of the official pages deal with: how to deal with user contributions that are intrinsic to the software.

> Instead of simply fixing the software.

If anyone of us would know what is needed to fix the software, we would do it. Can you point out what needs fixing? I don't think that is possible, most certainly not ""simple"".
I have the feeling no-one really understands any specifics about GDPR (and that includes the big guys like Google and Facebook). I think the majority of what's out there is misinformation.

I like how OpenStreetMap (who ""have received professional counsel"") say in their paper:

> Naturally estimating the impact of the GDPR introduction and consequences before it is
actually in force are fraught with the problem that we have to guess how the legislation will
be applied in practice and there is a danger of both over- and underreacting.
",2018-05-29T10:43:34Z,108893
1051,dokuwiki/dokuwiki,315863357,392736227,"Changing `DOKU_PREFS` into a session does not fix anything but breaks the usability imho. `DOKU_PREFS` is used e.g. for storing the size of the edit window and this should persist across sessions imho. As Anika says, cookies do not store the IP (or even DNS entry) of the user, the IP address is instead sent with every request.

Not storing IP addresses is also not the solution as there is a very valid reason to store them at least temporarily: detect and remove vandalism (by IP address you can identify the connection between several edits, possibly even several user accounts) and to be able to identify the author (at least in court) if the content posted was illegal and the site owner gets sued because of that.

What I think would be a good thing is to have some automatic way to remove IP addresses after some time, at least for changes where the user has been logged in (this could be a plugin of course). For anonymous edits I'm not sure if the IP address can be interpreted as an author identification that needs to be stored because of the license (but this of course depends on the selected license).

Concerning the removal of the user name: my personal (non-lawyer) interpretation is that due to the license of the content (creative commons license at least with attribution), DokuWiki has a legitimate interest to store this attribution as it otherwise cannot use the content and as the Wikimedia issue tracker says ""the right of erasure only exists when the processing is not necessary for some legitimate interest of the data controller"".",2018-05-29T10:57:23Z,198317
1052,dokuwiki/dokuwiki,315863357,392738048,"Thanks @michitux for pointing out the usability aspects. I was just about to do that.

@selfthinker thanks for the links on how other wikis handle it. I'll have a look later.

Regarding removing IP addresses after while, there is now the aptly named gdpr plugin which does exactly that. It also replaces user names in change logs for deleted users.

I will close this ticket now. We will probably not ever get definite answers to all the questions asked in the original issue. And it's an issue people love to discuss for the sake of discussing without getting any further.

For now we should simply focus on having a useful privacy policy for the 0.1 percent of users who care about that. So please, if you think the privacy policy needs adjustments just go ahead and do it.",2018-05-29T11:04:55Z,86426
1053,dokuwiki/dokuwiki,315863357,392738379,"> I don't think this specific cookie comes with IP address and DNS entry attached. That cookie and its contents is not stored on the server but only in the browser.

It's the very nature of any cookie to come with IP address or DNS records attached. All of them are stored in the browser, only. Still GDPR considers them to be personal data, which is why they have to become session cookies or ask for user content before being placed. Fairly simple basics.

> If anyone of us would know what is needed to fix the software, we would do it.

Code is provided above. Instead of looking at the code and commenting it, all the extensive comments sum up to ""Go away, we have to find a harder way"".

Very apparently, some people here *want* to stick their head in the sand. Instead of applying these patches and enjoying a GDPR compliant wiki. Enjoy it!

And I just see how you closed the issue to make extra sure nobody sees this solution. Extra compliment to that much stupidity!",2018-05-29T11:06:23Z,318581
1054,dokuwiki/dokuwiki,315863357,392738963,"> would it be desirable to extend that with technology (eg. automatically list what data is collected when, why and how long) and provide a way for plugins to hook into that?

I would definitely say Yes to that. That should also include templates, not just plugins. (I know e.g. some templates include Google Analytics.) Maybe have a hook per section (cookies, third party, etc)?",2018-05-29T11:09:07Z,108893
1055,dokuwiki/dokuwiki,315863357,392741148,"> GDPR considers [cookies] to be personal data

That is simply factually not true. The [official original legal text](http://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32016R0679) says only one thing about cookies:

> (30) Natural persons may be associated with online identifiers provided by their devices, applications, tools and protocols, such as internet protocol addresses, cookie identifiers or other identifiers such as radio frequency identification tags. This may leave traces which, in particular when combined with unique identifiers and other information received by the servers, may be used to create profiles of the natural persons and identify them.

This means that cookies only need to be considered **if** they contain personal data or can be used to obtain them in any way. That is not the case with `DOKU_PREFS`.
",2018-05-29T11:18:15Z,108893
1056,dokuwiki/dokuwiki,315863357,392742576,"This topic is now locked. @Traumflug that kind of language is not welcome here

To clarify some last points:

* the GDPR is only about Personal Data. Cookies are not per se personal data (they might be if they can be used to identify people - the doku_prefs cookie does not)
* cookies do not ""come with IP address or DNS records attached"" that is technically nonsense. Cookies are sent back by the browser when the domain they are saved for matches
* The IP address of the originating request is always visible to the server. That's how TCP/IP works and nothing to do with cookies or anything
* There is legitimate reason to save the IP addresses for a certain amount of time to react on malicious activity
* making doku_prefs a session cookie makes the UX of the application much worse without improving privacy at all
* if you want code to be discussed for integration into DokuWiki core, open a proper pull request",2018-05-29T11:24:19Z,86426
1057,symfony/symfony,985447779,985447779,"**Symfony version(s) affected**: latest

**Description**

According to the documentation (https://symfony.com/doc/current/service_container.html),

```
// checks if a parameter is defined (parameter names are case-sensitive)
$container->hasParameter('mailer.transport');
```

However, running this code gives:

`Notice: Undefined variable: container`

Which is obviously what would happen, since there is no variable called ""$container"" and you forgot to document how to obtain that variable. ",2021-09-01T18:09:25Z,10137
1058,symfony/symfony,985447779,910560529,"Thank you for your report. This is not a bug, as you already noticed yourself. This is why I'm closing this issue.

If you believe, the documentation could be improved, feel free to open an issue or pull request over at https://github.com/symfony/symfony-docs/",2021-09-01T18:40:14Z,1506493
1059,symfony/symfony,985447779,910569983,"It **is** a bug if following the exact steps of the documentation leads to an error.

There is NO resource in the entire internet that tells how to access the $container variable, and consequently how to use bundles and service containers. It is not merely a matter of documentation needing to improve, it is a matter of your piece of software being completely unusable because the developer cannot possibly guess the information about how make something work when the documentation step by step leads to internal server errors.

Notice that I am following your own suggestion of JMSSerializerBundle for serialization, and it seems completely impossible to use this piece of software because there is no information available on the internet about how to access this ""$container"" variable that the Symfony docs also mention. Where is it? Where does it live?",2021-09-01T18:48:22Z,10137
1060,symfony/symfony,985447779,910601376,"Well then, I pointed you to the documentation repository where you're free to suggest improvements to the documentation. There's nothing I can do for you here because we're only tracking bugs and feature requests on this tracker.

If you're looking for support, feel free to open a discussion here: https://github.com/symfony/symfony/discussions",2021-09-01T19:07:01Z,1506493
1061,symfony/symfony,985447779,910603433,"@derrabus You could at least tell me how to access the $container variable that is mentioned dozens of times throughout the docs, without they ever thinking about telling how to get that variable in the first place!!!",2021-09-01T19:09:02Z,10137
1062,symfony/symfony,985447779,910608519,Thank you for telling me what I could or could not do. Please use discussions for support. Thank you for understanding.,2021-09-01T19:14:03Z,1506493
1063,symfony/symfony,851350444,851350444,"**Symfony version(s) affected**: 4.4.*

**Description**
`framework.templating` is deprecated in 4.3 and should be removed in 5.0. 

However, the feature is de-facto ""removed"" in 4.4 already.

As can be seen in dependencies here:
https://packagist.org/packages/symfony/twig-bundle#v4.3.11 -> symfony/twig-bridge: ^4.3
https://packagist.org/packages/symfony/twig-bundle#v4.4.20 -> symfony/twig-bridge: ^4.4|**^5.0**

Updating twig-bundle to 4.4.* also updates twig-bridge to **^5.0**. This is a breakage, because it removes the `TwigEngine` (implementation of `Symfony\Component\Templating\EngineInterface`) even if you have symfony/* version 4.4.*.

**How to reproduce**
mkdir test && cd test
composer init
composer require symfony/templating:4.4.* symfony/twig-bundle:4.4.*

verify twig-bridge is ^5.0 and there is no `Symfony\Component\Templating\EngineInterface`-implementation.

You can also do the same with flex-recipe constrained to symfony 4.4.*. It neither defines the twig-bridge dependency explicitly, hence gets it updated to the breaking version.",2021-04-06T11:56:40Z,2409779
1064,symfony/symfony,851350444,814068336,"If you depend on Twig bridge 4, you need to require it explicitly in your `composer.json` file. The `require` section in the Twig bundle only states that the bundle works with both versions.",2021-04-06T12:08:48Z,1957048
1065,symfony/symfony,851350444,814091431,"I do not depend on twig-bridge. The twig-bundle depends on it. If it wants to delegate this dependency to me, it should not declare it itself first place. I never `use` any of the twig-bridge classes in my App's code. In fact I only depend on `EngineInterface` of the templating component, which has the purpose of me not depending the implementation (=twig-bridge in this case). 

ps. adding twig-bridge in root is actually the way I workarouned this BC-break, but anyway...",2021-04-06T12:46:39Z,2409779
1066,symfony/symfony,851350444,814100946,"As you mention @akomm (which is was @xabbuh suggested), you need to explicitly add `twig-bridge` in your root `composer.json` file.",2021-04-06T13:01:04Z,47313
1067,symfony/symfony,851350444,814136675,"But you do realize, that this is a workaround and not a fix?",2021-04-06T13:50:57Z,2409779
1068,symfony/symfony,851350444,814192856,"This is the fix. If you depend on the `TwigEngine` class, you should require the package that provides it explicitly.",2021-04-06T15:01:56Z,1957048
1069,symfony/symfony,851350444,814203505,"> This is the fix. If you depend on the `TwigEngine` class, you should require the package that provides it explicitly.

Did you just ignore my post? I do not depend on TwigEngine. I do depend on FrameworkBundle and TwigBundle. I also do depend on the EngineInterface from the templating component.

- FrameworkBundle utilizes the templating component to load an implementation (compiler pass)
- TwigBundle provides the implementation via TwigEngine class. It does depend on twig-bridge by extending the TwigEngine class from the bridge
- My app never messes with TwigEngine class of the twig-bridge. It even does not mess with the TwigEngine from the twig-bundle, but it depends on the bundle due to implementation provided

If twig-bundle assumes itself not a provider of the EngineInterface implementation, it should not depend on twig-bridge and delegate this to the consumer...


",2021-04-06T15:15:50Z,2409779
1070,symfony/symfony,851350444,814227959,"I don't think, we're getting anywhere here. As mentioned already, there isn't much we can do for you. You have been provided with an explanation of and a solution to your problem.

I'm locking this thread now. Thank you for understanding.",2021-04-06T15:47:27Z,1506493
1071,symfony/symfony,741692283,741692283,"| Q             | A
| ------------- | ---
| Branch?       | 5.x
| Bug fix?      | no
| New feature?  | no
| Deprecations? | no
| Tickets       | -
| License       | MIT
| Doc PR        | -

*EDIT: Although I didn't intend any harm at all, this PR wasn't well-received (see comments below). I invite the reader to check https://github.com/Ocramius/ProxyManager/issues/630 for a possible follow up.*

The versioning policy of `ocramius/proxy-manager` has been proven quite painful over the years.
Everybody can feel it hard in various ways these days with Composer 2 and PHP 8 coming.

I created the [`suimarco/proxy-manager`](https://github.com/suimarco/proxy-manager) fork to provide the same code and API, but with a versioning policy that is friendly to continuous migrations.

This fork is a drop-in replacement for `ocramius/proxy-manager` that is tested from PHP 7.1 to 8.0.
It also ships with a few fixes that were not merged upstream but are still needed, and that we had to monkey-patch. The related monkey-patching can now be removed.",2020-11-12T15:25:16Z,243674
1072,symfony/symfony,741692283,726713739,"Seems completely backwards to me. Perfectly OK from a MIT license PoV: your package, my freely provided code.

Discussion also at https://github.com/suimarco/proxy-manager/pull/1, but overall, instead of helping maintainers with their OSS work, forks are being done, and dependency graphs are turned into a mess.

As I mentioned there, there's a way to support the upstream work @ https://github.com/Ocramius/ProxyManager/tree/f65ae0f9dcbdd9d6ad3abb721a9e09c3d7d868a4#ocramiusproxy-manager-for-enterprise, and bugfixes for critical stuff are being backported anyway :shrug: ",2020-11-13T11:29:55Z,154256
1073,symfony/symfony,741692283,726716953,"And btw, thanks to the symfony ecosystem for ruining the mood again: was going to work on PHP 8 support this weekend, but yet again I find myself being nervous, angry and frustrated at people treating weekend projects (**LITERALLY DOING IT IN THE WEEKEND**)  like paid-for products.",2020-11-13T11:37:53Z,154256
1074,symfony/symfony,741692283,726718779,Dick move.,2020-11-13T11:42:09Z,613376
1075,symfony/symfony,741692283,726723131,"There's no reason to create a organiation like `suimarco`. It's a really **toxic** movement and totally apart of the technical implementation. Personal discussions should live apart. Authors can do whatever they want with their packages, if it's that critical and needs to be versioned differently then let's move it inside symfony organisation to follow same policies and maintain it there. 
I understand the reasons, I don't like how. Stay away from personal attacks. ",2020-11-13T11:52:44Z,4022187
1076,symfony/symfony,741692283,726731121,"As other have said, the naming here of the org/user is severely aggressive and makes it a _personal attack_ on @Ocramius . Call it `nicolas-grekas/proxy-manager` or something, and _allow issues to be reported_ on it. I don't want to pile on, but this is really working _against_ the PHP community by making aggressive moves like this.

@nicolas-grekas you might not personally like @Ocramius, but this is really a personal attack and is 100% out of order coming from a Symfony representative.",2020-11-13T12:12:00Z,496145
1077,symfony/symfony,741692283,726731290,Will this be PHP 4 compatible ?,2020-11-13T12:12:24Z,5034671
1078,symfony/symfony,741692283,726731901,"To be clear, this [was not a personal attack](https://twitter.com/nicolasgrekas/status/1327249300263493633) at all, I admire the work of @Ocramius at many levels.
I'm going to rename that package and delete that org. ",2020-11-13T12:13:45Z,243674
1079,symfony/symfony,741692283,726733171,"Im closing this PR and locking the discussions. Technical reasons aside, making this PR could be (and probably have been) perceived as a personal attack which is a violation of the Code of Conduct. ",2020-11-13T12:16:39Z,1275206
1080,symfony/symfony,619762251,619762251,"I removed old Inflector, tried laravel inflector, tried to copy Symfony/Component/String/EnglishInflector manually, and even CakePhp inflector, or my own inflector...

There's no inflector who returns
singular('Congress') -> ""Congress"" | Actual result is ""Congres""
singular('Status') -> ""Status"" | Actual result is ""Statu""

Of course i wrapped with ""irregular words""...
Maybe there's not a bug, because word is already singularized when passed to inflector.

Then there's expected something like ""isSingular()"" or ""isPlural()"" when function is not idempotent...",2020-05-17T17:56:39Z,26746829
1081,symfony/symfony,619762251,630270020,"IIRC, the method does not claim to be idempotent. So regarding the ""congress"" example, I wouldn't classify this issue as bug because you're passing a singular form to a function that expects a plural.

Regarding ""status"" however, it's a different story because ""status"" is in fact a valid plural form of ""status"". So in that case, ""status"" would be the expected result.",2020-05-18T15:46:58Z,1506493
1082,symfony/symfony,619762251,630978453,"How do you provide if i pass from console arguments the ""word"", i dont know is it a plural or singular. I think i able to pass it to plural or singular function.

Maybe just update the docs with yellow block with this warning about idempotence, prevents the people for searching ""the good one"" (i mean - spending time)

About status - same story. Status is in singular form ""status"", trying to pass it into singular() gives incorrect ""statu"", when pass to plural -> ""statuses"" as expected. (Status is plural form of status? First time see the point)",2020-05-19T17:46:56Z,26746829
1083,symfony/symfony,619762251,631313685,"This doesn't look like a bug to me. `singularize()` expects a plural, `pluralize()` expects a singular (I am not even sure we can reliably determine if a word is singular or plural).",2020-05-20T08:11:06Z,1957048
1084,symfony/symfony,619762251,631471336,"> This doesn't look like a bug to me. `singularize()` expects a plural, `pluralize()` expects a singular

Exactly. That's how the methods are specified. The component uses heuristics to determine singular/plural forms __under the assumption that the input is correct__.

NLP is hard and I don't think this component aims to solve NLP problems. :wink: ",2020-05-20T13:24:18Z,1506493
1085,symfony/symfony,619762251,631583865,It doesn't look like there is actually anything to fix. So I am going to close this issue as explained before. Thank you for understanding.,2020-05-20T16:28:02Z,1957048
1086,symfony/symfony,619762251,631844054,"We show the movie how the user problem converts into *we already have best quality, dont be stupid* as a result some lalala and issue gone, noone understand, noone try. Ok no problem",2020-05-21T02:39:34Z,26746829
1087,symfony/symfony,619762251,631980289,"I'm sorry that this issue was not resolved the way you expected it to be. What you've presented might be a valid use-case, it's just not one that this component tries to solve.

That being said, I'd like to ask you to use a respectful language when commenting or filing issues on this tracker.",2020-05-21T09:18:09Z,1506493
1088,symfony/symfony,619762251,632101801,"Ohh, if we use on our project ""respectful language"" we never push anything.

Class theory means ""expect respectful language from different class, not from your employees"". Thats the quality recept... sorry me for unexpected recommendations. There's only two classes: worker and seller. If you're worker you should expect respectful language from sellers. I do not want to sell your work. I want same as you - make it better.

If you expect respect from guys who wants to improve you - you never improve. As the fact.
===

Again, the problem is: user pass to me unknown plural/singular form as a word. Forcing user to pass both forms made Pluralizer useless. The library can solve it, but *there's already good library* phenomenon found.

And also some likes and agreements for you because of idol love.",2020-05-21T13:58:50Z,26746829
1089,symfony/symfony,619762251,632107786,"Please note that human language is very difficult to grasp in programming logic. Developers usually are no language experts. I think it's quite amazing that there is programming logic that is able to singularize/pluralize words.
If you think it's possible to detect if a word is singular and plural, please change the code and create a pull request. That's the greatness of open source: combining the knowledge of the every developer that is happy to share their knowledge.

At last, I want to make you aware of the [Symfony Code of Conduct](https://symfony.com/coc). That's how the Symfony community has decided to communicate with eachother (as every developer lives in very different cultures, it's important to have one standard code of conduct for the community).",2020-05-21T14:10:17Z,749025
1090,symfony/symfony,619762251,632144059,"I never did that before, but it seems like sometimes you'll have to do everything for the first time. If using a respectful language stops you from being productive, I am sure you will be able to create your own open source project where this is the way to progress. But I am tired of spending (a massive amount of) my free time improving software that is given for free to everyone who wants to use it and then wasting my time reading things like that. That's why I am locking the conversation here as there's nothing to add.",2020-05-21T15:17:29Z,1957048
1091,symfony/symfony,485217326,485217326,"
You can redirect back to previous url in controller, just like in Ruby On Rails:

```
def show
   redirect :back
end
```

In symfony maybe can archive same way:

```
public function show(){
  return $this->redirectBack();
}
```",2019-08-26T12:44:24Z,3498786
1092,symfony/symfony,485217326,524909369,"Hello,

I will try to start if we need this feature :)",2019-08-26T15:35:15Z,19558543
1093,symfony/symfony,485217326,525199246,How would this feature work? Because there's no reliable way of determining where to send back to as the referrer header is not foolproof,2019-08-27T08:31:54Z,1754678
1094,symfony/symfony,485217326,525242551,@linaori we can get it from `$_SERVER['HTTP_REFERER']`.,2019-08-27T10:31:58Z,19558543
1095,symfony/symfony,485217326,525245185,"That's not reliable. If you expect a return to the previous page, you'd need something reliable. That value can be spoofed or hidden.",2019-08-27T10:40:33Z,1754678
1096,symfony/symfony,485217326,525268571,"We can save the previous URL in the session and, if `HTTP_REFERER` is empty, get it from there.",2019-08-27T11:58:37Z,19558543
1097,symfony/symfony,485217326,525299881,"If you do this, make sure it's behind a feature flag, as it would make the session a requirement on each page, while it might not be necessary. It's also important to not save it on requests that had a non 2xx status.",2019-08-27T13:25:31Z,1754678
1098,symfony/symfony,485217326,525302836,"the session is not working for that either: if you open 2 tabs, they will share the same server session, but not the same browser history.

Note that Rails (suggested as inspiration for the feature) implements it based on the Referer header, with a fallback location and an optional filtering of the allowed host for the Referer (using the fallback location if the filter rejects the referer).",2019-08-27T13:32:34Z,439401
1099,symfony/symfony,485217326,526688802,Closing as the corresponding PR has been rejected.,2019-08-30T17:44:13Z,243674
1100,symfony/symfony,485217326,558219567,"Are you kidding me? Laravel has that already. They even got a helper function for that, simple as 

`return back();`",2019-11-25T15:57:59Z,9727382
1101,symfony/symfony,484737447,484737447,"Anyone have any idea why ParamConverterListener is taking 5s on my local development environment? Actually, most of the loading process is slow. I'm not looking for someone to hold my hand and debug for days, I just wanted to know if this was common and, if so, what the fix is.

I'm on Windows 10 with Apache 2.4, PHP 7.3.5. The project is Symfony 4.3.3

![image](https://user-images.githubusercontent.com/2415447/63627293-0d19a080-c5d5-11e9-895a-e4c755386393.png)

",2019-08-23T22:37:35Z,2415447
1102,symfony/symfony,484737447,524484909,"What param converters do you have? What do your controllers look like that have those action params?
There is nothing we can do here for you. For support questions, please have a look at 
http://symfony.com/support ",2019-08-23T22:47:30Z,610090
1103,symfony/symfony,484737447,524485353,"As a ""10x developer"" (https://htmlguy.com/about) this should be pretty easy for you to debug.",2019-08-23T22:50:21Z,610090
1104,symfony/symfony,484737447,524487328,"""There is nothing we can do here for you"". Then why have an issue type specifically for support?
Ah, the description below the support type says to see the link you shared. ",2019-08-23T23:01:05Z,2415447
1105,symfony/symfony,484737447,524487411,Love the personal attack too. Nice to know if I ever want to deal with pompous assholes I can just create a Symfony issue.,2019-08-23T23:01:30Z,2415447
1106,symfony/symfony,484737447,524488738,"@Tobion congrats, you won an award for the best support of the month",2019-08-23T23:09:09Z,566953
1107,symfony/symfony,484737447,524489576,"FYI, clearing var/cache/dev brought it down from 5 seconds to 1.",2019-08-23T23:13:26Z,2415447
1108,symfony/symfony,484737447,524493910,"@eventhorizonpl It seems you missed the point again. We don't use github issues for support questions which should be very clear if you take the time to read https://github.com/symfony/symfony/issues/new/choose
And even as a support question, the report is not given any information that would be required to help. If you are better at answering this support question, why did you provide any help at all?",2019-08-23T23:43:12Z,610090
1109,symfony/symfony,484737447,524496919,"@Tobion Regardless of who is right or wrong, how about using some tact? I'm not looking for you to determine my exact problem. I don't want to waste anyone's time with that. No matter how experienced I am, I can't possibly keep up with all the changes you guys make to every component, or how it all works behind the scenes. Symfony documentation is designed for using the framework, not understanding the backbone. All I'm looking for is something like ""I've seen this before, it's just X. All you have to do is Y"" or ""If you have a lot of annotations, it's probably taking a long time to cache them prior to serving your request"". You guys have the inner-workings memorized. I don't. In an effort to save time, I thought it was worth posting a quick issue here. Sorry I didn't notice that the support topic told me to bugger off and pay for assistance. I know it's an environment issue because it's much slower on this dumb Windows machine than it was on my Mac (I'm stuck on Windows while my Macbook is in for the battery recall). So, in a way, it's not really Symfony, but then again, it is, because a static site loads very quickly.",2019-08-24T00:06:32Z,2415447
1110,symfony/symfony,484737447,524497327,"@tobion ""It seems you missed the point again"" oh, so you know me well and you know how many times I missed a point. and it is now my fault. I'm sorry that you had a bad day... ",2019-08-24T00:10:26Z,566953
1111,symfony/symfony,484737447,524497477,"We're all here to help one another. I have open source plugins that I give away for free and provide basic support. Regardless of Symfony's official stance on issues and assistance, is it really that hard for someone who's seen this before to jump in and give me a quick tip so I can save time? I could go to stack overflow, but...#shudders",2019-08-24T00:11:54Z,2415447
1112,symfony/symfony,484737447,524497568,"![image](https://user-images.githubusercontent.com/2415447/63629783-5d4b2f80-c5e2-11e9-83fb-df0fd2cba65d.png)
",2019-08-24T00:12:45Z,2415447
1113,symfony/symfony,484737447,524497871,And so it's clear...I inherited this Windows machine with WAMP as a temporary solution while I wait for my Mac to come back. I'm not going to bother digging into this issue deeper because I'm 99% sure it'll be fixed when I setup Docker or Vagrant. I'm going that route tomorrow. Have a good night.,2019-08-24T00:15:28Z,2415447
1114,znc/znc,976125179,976125179,"Hi !

I’ve been wanting to remove the `webadmin` module and fully disable ZNC from being able to respond anything meaningful over HTTP, because it listens to the same port as the IRC protocol one and so I cannot just block the port. Either this is a behaviour not achievable or I am missing a setting somewhere after searching the wiki for a solution.

I tried setting `AllowWeb` to `false` by hand on my only listener but it doesn’t work, as ZNC still serves an html page saying that `Web Access have is not enabled.`

```
<Listener listener0>
    AllowIRC = true
    AllowWeb = false
    IPv4 = true
    IPv6 = true
    Port = ****
    SSL = true
    URIPrefix = /
</Listener>
```
![名称未設定](https://user-images.githubusercontent.com/37584624/130321317-f36e1f7e-10d6-4e93-a206-ce85d7e7d056.png)

[From this changelog](https://wiki.znc.in/ChangeLog/0.090), I can see that the http server got embedded in ZNC so that modules other than webadmin can serve pages over http. Is there a way to tell ZNC to not load it?

Serving a page *isn’t* that big of a deal so I’m okay with this for now. But from a rigorous standpoint, if I don’t want my bouncer to communicate over HTTP I wish I could tell it to not do it, at all.

I am on Debian Bullseye, using their package of ZNC. I took a quick glance over the other issues but I have to say there was too much of them to see if this was an issue already raised.

```
$ znc --version
ZNC 1.8.2+deb2+b1 - https://znc.in
IPv6: yes, SSL: yes, DNS: threads, charset: yes, i18n: yes, build: cmake
```



Thanks in advance for your time,",2021-08-21T12:22:15Z,37584624
1115,znc/znc,976125179,903114933,"`Web Access have is not enabled.` is the only thing it replies. What error do you want to see instead? ""Connection refused"" is not possible, because the port is still open.",2021-08-21T13:13:42Z,325092
1116,znc/znc,976125179,903278390,"I don’t think you understand the behaviour I would like to have so I’m going to try my best to reformulate:

I don’t want to see any error/warning/dismissing message in a html page when I make a http request to my bouncer. I want the bouncer to not respond, at all, to any kind of http request, and my web browser to tell me It was unable to connect to the url I gave it, like so :
![名称未設定2](https://user-images.githubusercontent.com/37584624/130358864-c7285ce4-5b0f-44da-8a30-f2e8b92f6f71.png)
Is this behaviour achievable as of today? Would the code architecture support that kind of feature without having to make major breaking changes?",2021-08-22T14:34:15Z,37584624
1117,znc/znc,976125179,903315682,"The port needs to be open to be able to accept IRC connections. And SSL handshake happens even before the client (IRC client or web browser) sends any data to server. ZNC can't distinguish IRC vs HTTP client until at least some data got communicated; in case of SSL, communicated both ways).

So no, ""Unable to connect"" can't be done.

Why exactly do you need this?",2021-08-22T19:04:39Z,325092
1118,znc/znc,976125179,903654107,"I need this because I don’t want to serve anything HTTP and so I don’t want the HTTP server to be in my way.

ZNC may not be able to distinguish IRC vs HTTP until at least some data data got communicated, but at some point it knows the client want HTTP instead of IRC then right? I don’t see why it wouldn’t be feasible to make ZNC not respond anything at this point.

You’re coming to me saying your a client. I acknowledge you and I don’t know what you want yet. Then you’re going to tell me you want HTTP. I’m going to ignore you, pretend I didn’t hear anything and that this event never happened at all. ",2021-08-23T10:45:24Z,37584624
1119,znc/znc,976125179,903659593,"> I don’t want to serve anything HTTP and so I don’t want the HTTP server to be in my way.

Why?

> pretend I didn’t hear anything and that this event never happened at all.

accept() and SSL handshake happened already, you can't undo them.

> I don’t see why it wouldn’t be feasible to make ZNC not respond anything at this point.

You can patch it to close the socket instead of returning HTTP error.",2021-08-23T10:55:08Z,325092
1120,znc/znc,976125179,903668146,"God please make an effort to not be a huge freaking pain in the ass. You don’t have to care about why I want something. If I want something it’s for a good reason and I’ve been writing about it since my first message.

I can’t undo handshakes because I’m not a freaking magic time traveling wizard good job smartass did you figured that out yourself?

Now just tell me how to close the damn socket so I can flag this issue as closed and move on to something more productive than having to interact with an antisocial edgelord who can’t go straight to the point.",2021-08-23T11:08:03Z,37584624
1121,znc/znc,976125179,903688304,"> God please make an effort to not be a huge freaking pain in the ass. 

Great way to request a feature. Or not.

> You don’t have to care about why I want something.

Yes, I do. Without a good reason, I won't change this in the code.

> I can’t undo handshakes because I’m not a freaking magic time traveling wizard

That's what I'm trying to tell you since the first message.

> did you figured that out yourself?

I thought it's pretty obvious, but apparently it was not, therefore I had to say it directly.

> smartass

Oh, insults will help your cause for sure.

> Now just tell me how to close the damn socket

I'm sure google search will help you.",2021-08-23T11:40:27Z,325092
1122,znc/znc,976125179,903689367,Here's one more link for you to read: https://en.wikipedia.org/wiki/XY_problem,2021-08-23T11:42:11Z,325092
1123,znc/znc,976125179,903695562,"> > I can’t undo handshakes because I’m not a freaking magic time traveling wizard
> 
> That's what I'm trying to tell you since the first message.

If you want someone to understand something, you can tell it straight away you know? Tell me, how hard would have it been to write from the start : ""you can’t do exactly the behaviour you want because there are handshakes happening and you cannot make them not happen. However, instead you could either return an HTTP error or just simply close the socket. Here is where you should look into if you want to add this feature""

But you had to play your little game of being a smug winnie for no apparent reason.",2021-08-23T11:53:03Z,37584624
1124,znc/znc,976125179,903698216,"I did.

> And SSL handshake happens even before

> So no, ""Unable to connect"" can't be done.",2021-08-23T11:57:47Z,325092
1125,znc/znc,976125179,903726585,"No you didn’t.

You just told facts here and there, without any intention to go out of your way to help me understand the blind spots I have in my understanding. It took one day of back and forth reformulations to get to a single piece of possible solution : apparently, I could make ZNC close a socket.

You didn’t take the time to show me how my reasoning doesn’t comply with how protocols are supposed to work, you didn’t explain how to change my reasoning to find a middle ground between knowing what’s possible and not possible and being satisfied with a behaviour that meets my need, and you didn’t even try to guide me towards the parts of ZNC that would need to be patched if this was something that wouldn’t introduce breaking changes.

As of right now, I am still figuring out the whole timeline of events between ZNC and a http client. Here is something you would find dead obvious : how an SSL handshake can’t lead to a browser telling its user they were unable to connect to the website? This isn’t a call for help by the way. It has been a day since I knew I would have to do and understand everything by myself, way before we got to here.

Facts are not the sole metric in a conversation. You’re intent will show. You felt dismissive.

",2021-08-23T12:38:29Z,37584624
1126,znc/znc,976125179,903744454,"> to go out of your way

Why would I do that? First, I need to understand what you're actually trying to do, before showing you the direction: otherwise the direction I show would be completely wrong. But you're refusing to answer my questions! Again, please read about the XY problem I linked above.

> understand the blind spots I have in my understanding

How am I supposed to know what parts of protocols you know and what parts you don't know? Do you expect me to teach you the whole computer science from ""2+2=4"", or? The two facts ""And SSL handshake happens even before"" and ""So no, ""Unable to connect"" can't be done."" imply that you obviously can't go back in time. Or I should expect that you don't know physics and also teach you that?

> you didn’t even try to guide me towards the parts of ZNC that would need to be patched

You think the correct answer always is a patch? Wrong!

The first step is to figure out what you actually want. Then show you the existing configuration option (or e.g. a module) to do what you want. Then, if there's no such opiton, a patch, and not just on your side, but in upstream ZNC, so that it benefits everyone, not just you. Of course, if this is useful at all.",2021-08-23T13:02:06Z,325092
1127,znc/znc,976125179,903901089,"I'm also not sure what exactly you are trying to achieve. Also, I don't think you deserve more help than DG already tried to provide. But okay....

It *seems* like you want some kind of error message in your web browser. As you already heard, the error that you wanted to get is not possible, because the internet does not work that way. If you want an ""connection closed unexpectedly"" error message, you can just remove the call to `Write()` here:
https://github.com/znc/znc/blob/e0ffdddd473e97cb843f2bc8ad4fa16cf47c65b4/src/Listener.cpp#L149-L151
If you want another kind of error message, you can try replacing this with `Write(""this is definitely not http\n"");`. No idea how browsers react to that, but perhaps one of these two options solves the problem you are actually trying to solve.

I subscribed to your issue in the mlemnc repository. I might or might not write more answers there, in case you write some questions.

Edit:

> You didn’t take the time

Why would DG have to take the time? As far as I know, this is a hobby of theirs and there are no reasons why you get to dictate how people spend their free time.",2021-08-23T15:58:22Z,89482
1128,tinymce/tinymce,1636711509,1636711509,"`Uncaught (in promise) Error: Failed to get RTC instance not yet initialized.`

Seems in the process of attempting to vendor-lock with the CDN based implementations you've broken the library itself.

...

```
import tinymce from 'tinymce';

import 'tinymce/icons/default';

import 'tinymce/themes/silver';
import 'tinymce/models/dom';

import 'tinymce/skins/ui/oxide/skin.css';

import 'tinymce/plugins/advlist';
import 'tinymce/plugins/code'; // FIXME: Not picking up changes made in editor view? NOTE: Works again in v5
import 'tinymce/plugins/emoticons';
import 'tinymce/plugins/emoticons/js/emojis';
import 'tinymce/plugins/link';
import 'tinymce/plugins/lists';
import 'tinymce/plugins/table';

import contentUiSkinCss from 'tinymce/skins/ui/oxide/content.css';
import contentCss from 'tinymce/skins/content/default/content.css';
```


```
tinymce.init({
	selector: `#mg-wysiwyg-${this._uid}`,
	height: parseInt(this.height),
	readonly: this.disabled,
	//min_height: this.height,
	placeholder: this.placeholder,

	resize: 'both',

	menubar: false,
	plugins: ['advlist', 'code', 'emoticons', 'link', 'lists', 'table'],
	toolbar: 'undo redo | bold italic forecolor backcolor | bullist numlist checklist table | link emoticons | code',
	model: 'dom', // FIXME: Forcing ""dom"" because non-existant RTC ""Premium plugin"" getting in the way

	// Configuration required for local self-install
	skin: false,
	content_css: false,
	content_style: contentUiSkinCss.toString() + '\n' + contentCss.toString(),

	//promotion: false, // Oh yeah we really want adverts!

	// Bound to ""change keyup"" events as per https://github.com/tinymce/tinymce-vue/blob/b41c2a47eb8d9629eb01a41d6c6c633651f2d078/src/main/ts/Utils.ts#L115-L119
	init_instance_callback: editor => {
		editor.on('change keyup', e => {
			this.data = editor.getContent({ format: this.syntax })
		})
	},
});
```
",2023-03-23T01:33:31Z,213575
1129,tinymce/tinymce,1636711509,1480551624,"This appears to be a cynical attempt to drive people into vendor-locking with your CDN.

Your desire to be paid for working on Open Source has cost me a lot of my time.

Was it the intention to waste my time or did you expect me to just hand over a monthly subscription?

I have decided I will not be supporting such a dark-pattern by moving to the CDN approach. Instead I will likely fork the project and do whatever I have to do in order to remove this  ""RTC"" nonsense.",2023-03-23T03:29:37Z,213575
1130,tinymce/tinymce,1636711509,1480565569,"`5.10.7` works as it should and does not include these most recent attempts to break the library.

Those choosing not to vendor-lock themselves into unnecessary pointless parasitic CDN subscriptions can use this version and avoid any future _""upgrades""_.

This might explain why v5 has 3x the weekly downloads v6 does.",2023-03-23T03:57:22Z,213575
1131,tinymce/tinymce,1636711509,1480686064,"If you're just going to go around our tracker posting about vendor lock-in you're the one wasting our time.

If you would like to continue using v6 - which I recommend as support for v5 [ends in a month](https://www.tiny.cloud/blog/tinymce-end-of-support/) - I'm happy to work with you to figure out what might be causing this error.",2023-03-23T06:53:18Z,298292
1132,tinymce/tinymce,1636711509,1480706687,"During the TinyMCE startup process the editor checks whether the `rtc` plugin is loading, with the aim of allowing RTC to take over key sections of editor functionality. If RTC isn't in the plugin list it redirects everything back to the core.

https://github.com/tinymce/tinymce/blob/ece3bf774b9ae20910d073af5351f162c4fb52d5/modules/tinymce/src/core/main/ts/Rtc.ts#L294-L306

This was added in TinyMCE 5.3 so the source of the exception isn't recent, there's probably just some code trying to use one of those core features during setup instead of waiting for the `PreInit` event.

I am open to collaborating if you'd like to help track down this mistake. An unminified stack trace would be a good place to start, or a replication case.",2023-03-23T07:18:25Z,298292
1133,tinymce/tinymce,1636711509,1481874912,"> I am open to collaborating

* The bug is caused by efforts to lock certain features and wouldn't exist otherwise
* The ticket is closed as if the reported bug does not exist as only the commercial version matters
* Given this is a commercial product only one of us is incentivised to contribute
* v5 will work fine for decades to come
* Have other fish to fry, wasted enough time on what should have been a simple install
",2023-03-23T20:46:33Z,213575
1134,tinymce/tinymce,1636711509,1482060775,"If you're just going to twist my words don't bother replying. This isn't an airport, you don't need to announce your departure.",2023-03-23T23:54:37Z,298292
1135,hashicorp/vagrant,532874644,532874644,"Seeing you are happy to close issues and never respond to replies in closed issues, I am forced to open new issue. I'm going to be brief this time.

In https://github.com/hashicorp/vagrant/commit/1b0148bc783298c7aa16a519e133bb26bcc1cc9f you removed function argument, which broke` vagrant-gatling-rsync` plugin. This is not a fault of plugin, but you breaking API compatibility. Please respect semver and do such changes in major version. Until then, please revert the BC break.

refs: #11229 #10974",2019-12-04T19:02:30Z,496233
1136,hashicorp/vagrant,532874644,561809857,"Hi @ostrolucky - I would like to remind you that we are a small team of maintainers, and it has been less than or barely 24 hours since I responded to your issue yesterday. There are a lot of other folks that we are supporting at the same time as you, and we are responding as fast as we can. Please know that we aren't ignoring you, nor are we happy to close your issues and never respond. Please have patience with us while we try to assist you and others. Thanks :heart: 

I'm happy to make a temporary fix to support the two parameter method for this third party plugin using an internal library to Vagrant, but as I mentioned yesterday you will likely get more traction and a quicker fix if you open an issue on the plugin side and fix it there. Our Vagrant release cadence isn't too frequent, and there are lots of other issues and features that we're trying to get in for each release.

If you wish to get this moving faster, there is already a pull request on the plugin: https://github.com/smerrill/vagrant-gatling-rsync/pull/37 I recommend pinging the maintainer on there.

For fixing this today on your side, you should still be able to use the built in rsync synced folder feature, as this is the recommended way to use rsync and Vagrant. Alternatively, downgrading to Vagrant 2.2.5 and using that plugin until a fix has been met is also a possibility. You could also bring in the changes locally and run and install the plugin from source.

We can leave this open to track the issue for now in case others find it. If our next release time comes around and it's still not fixed on their side, I'll apply a patch that fixes this so that it works in 2.2.7. Thanks!",2019-12-04T19:46:50Z,810277
1137,hashicorp/vagrant,57258770,57258770,"When relying on vagrant to download a box I frequently see connection speeds like this:

```
default: Downloading: http://boxes.example.com/vagrant/boxes/c6/packer_c6_2.5.2_virtualbox.box
default: Progress: 20% (Rate: 179k/s, Estimated time remaining: 0:41:37)
```

(Rate: **179k/s**)

Yet when I use wget to the same URL:

```
wget http://boxes.example.com/vagrant/boxes/c6/packer_c6_2.5.2_virtualbox.box
--2015-02-10 09:52:12--  http://boxes.example.com/vagrant/boxes/c6/packer_c6_2.5.2_virtualbox.box
Resolving boxes.example.com... 10.1.0.17
Connecting to boxes.example.com|10.1.0.17|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 830674320 (792M) [text/plain]
Saving to: 'packer_c6_2.5.2_virtualbox.box'

packer_c6_2.5.2_virtualbox.bo   0%[                                                         ]   7.12M   696KB/s   eta 19m 50s
```

(Rate: **696KB/s**) or often higher.

This particular example was pulled when on Wifi and connected to an IPSEC VPN.
",2015-02-11T00:12:16Z,129629
1138,hashicorp/vagrant,57258770,73818901,"Hi @spkane 

Some boxes are hosted on Atlas and sometimes Atlas is just acting as a proxy to a user-hosted box. If you give more information on the specific box(es) you're downloading, we can do some research.
",2015-02-11T01:16:30Z,408570
1139,hashicorp/vagrant,57258770,74010969,"@sethvargo This box is actually a box I built using packer and it is hosted on a remote server. I'm trying to understand why the download in significantly slower using vagrant then using wget to the exact same URL.
",2015-02-12T03:17:26Z,129629
1140,hashicorp/vagrant,57258770,74088452,"@spkane sorry - I misread your original issue.

I would suspect (and maybe @mitchellh could elaborate more) a few things:
1. Ruby is slow and somehow throttling the subprocess
2. Wget is faster than curl (which is what Vagrant is using)
3. Vagrant is also allocating time to unpack the box
4. Wget is allowing for some type of compressed download

It would be helpful if you could benchmark this with curl for reference.
",2015-02-12T15:22:55Z,408570
1141,hashicorp/vagrant,57258770,75818017,"I really can't explain this. Vagrant doesn't do anything during the subprocess Ruby-wise: it subprocesses to `curl`. It doesn't even do the download in Ruby. Perhaps wget is using multiple connections to download multiple parts? I really don't know, but unless we get more information I have to assume that Vagrant is fine here. 

Is `curl` just as slow? Vagrant is just subprocessing to curl until it completes.
",2015-02-24T18:40:57Z,1299
1142,hashicorp/vagrant,57258770,161091836,"I'm experience the same slow experience. Anyone can try aria - http://aria2.sourceforge.net/ and http://stackoverflow.com/questions/3430810/wget-download-with-multiple-simultaneous-connections

It's seems a little bit faster, but, man, you can set this up using default vagrant download mechanism and take a walk or make yourself a sandwich. Get way from screen for a little bit.
",2015-12-01T20:52:31Z,5209820
1143,hashicorp/vagrant,57258770,175213111,"Having the same problem here:
1. Upload a box manually to atlas
2. Create a new Vagrantfile with just `vm_cfg.vm.box_url = <user>/box-name`
3. `vagrant up` - box downloads slowly
4. wget box url from atlas (see `vagrant up` output) - box downloads lightening fast
",2016-01-26T20:31:56Z,3980984
1144,hashicorp/vagrant,57258770,178110677,"I wish there was just a +1 for this. Me too. Same connection for all 3 attempts. VPN turned off.
- `vagrant up` took 25+ minutes.
- `wget` took 3 minutes.
- `curl` took 4 minutes. 
",2016-02-01T18:26:29Z,1070185
1145,hashicorp/vagrant,57258770,178823565,"Ubuntu vivid64 is downloading at ~56kbps. I'm on a 100mbit symmetric connection.
edit: it timed out before it could finish.
edit2: I can confirm that https://atlas.hashicorp.com/ubuntu/boxes/vivid64/versions/20160128.0.0/providers/virtualbox.box downloads dramatically faster over wget than via ""vagrant up"".
",2016-02-02T21:13:55Z,660997
1146,hashicorp/vagrant,57258770,180714186,"I'm trying to download the scotch/box and current download speeds using vagrant are less than 10kbps.

default: Progress: 0% (Rate: 2603/s, Estimated time remaining: 33:17:38)

However just as bad using wget.
",2016-02-06T07:41:08Z,10137
1147,hashicorp/vagrant,57258770,184283188,"ditto; some popular boxes are very slow to download - i'm updating ubuntu/trusty64 as we speak and it's dropping below 1Kb/s. Been seeing this for a couple wks now.
",2016-02-15T16:28:42Z,398237
1148,hashicorp/vagrant,57258770,193369924,"+1 -- exact same as last comment
",2016-03-07T17:54:37Z,2024145
1149,hashicorp/vagrant,57258770,194075475,"Same here:

```
$ vagrant box add lazygray/heroku-cedar-14
==> box: Loading metadata for box 'lazygray/heroku-cedar-14'
    box: URL: https://atlas.hashicorp.com/lazygray/heroku-cedar-14
==> box: Adding box 'lazygray/heroku-cedar-14' (v1.0.6) for provider: virtualbox
    box: Downloading: https://atlas.hashicorp.com/lazygray/boxes/heroku-cedar-14/versions/1.0.6/providers/virtualbox.box
==> box: Box download is resuming from prior download progress
    box: Progress: 3% (Rate: 281k/s, ...
```
",2016-03-09T02:25:51Z,304958
1150,hashicorp/vagrant,57258770,196060860,"same here

```
vagrant box update
==> default: Checking for updates to 'laravel/homestead'
    default: Latest installed version: 0.4.1
    default: Version constraints: >= 0
    default: Provider: vmware_desktop
==> default: Updating 'laravel/homestead' with provider 'vmware_desktop' from version
==> default: '0.4.1' to '0.4.2'...
==> default: Loading metadata for box 'https://atlas.hashicorp.com/laravel/homestead'
==> default: Adding box 'laravel/homestead' (v0.4.2) for provider: vmware_desktop
    default: Downloading: https://atlas.hashicorp.com/laravel/boxes/homestead/versions/0.4.2/providers/vmware_desktop.box
    default: Progress: 0% (Rate: 42210/s, Estimated time remaining: 6:10:54))
```
",2016-03-13T21:57:57Z,3443226
1151,hashicorp/vagrant,57258770,196242779,"Is there any way to use something like axel to stream downloads in quicker?
",2016-03-14T10:23:23Z,5149834
1152,hashicorp/vagrant,57258770,197252297,"I guess there's nothing preventing people from sharing boxes via torrent.  For example, below is a magnet link for the heroku-cedar-14 box:

> magnet:?xt=urn:btih:5bb1480d5316f229bb71be55b56b06278de41a67&dn=heroku-cedar-14.box&tr=http%3A%2F%2F9.rarbg.com%3A2710%2Fannounce&tr=http%3A%2F%2Fannounce.torrentsmd.com%3A6969%2Fannounce&tr=http%3A%2F%2Fbt.careland.com.cn%3A6969%2Fannounce&tr=http%3A%2F%2Fexplodie.org%3A6969%2Fannounce&tr=http%3A%2F%2Fmgtracker.org%3A2710%2Fannounce&tr=http%3A%2F%2Ftracker.tfile.me%2Fannounce&tr=http%3A%2F%2Ftracker.torrenty.org%3A6969%2Fannounce&tr=http%3A%2F%2Ftracker.trackerfix.com%2Fannounce&tr=http%3A%2F%2Fwww.mvgroup.org%3A2710%2Fannounce&tr=udp%3A%2F%2F9.rarbg.com%3A2710%2Fannounce&tr=udp%3A%2F%2F9.rarbg.me%3A2710%2Fannounce&tr=udp%3A%2F%2F9.rarbg.to%3A2710%2Fannounce&tr=udp%3A%2F%2Fcoppersurfer.tk%3A6969%2Fannounce&tr=udp%3A%2F%2Fexodus.desync.com%3A6969%2Fannounce&tr=udp%3A%2F%2Fglotorrents.pw%3A6969%2Fannounce&tr=udp%3A%2F%2Fopen.demonii.com%3A1337%2Fannounce&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.glotorrents.com%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.leechers-paradise.org%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce&tr=udp%3A%2F%2Ftracker.publicbt.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker4.piratux.com%3A6969%2Fannounce

Anyone know a good website where one can search for torrents of vagrant boxes?
",2016-03-16T10:32:10Z,304958
1153,hashicorp/vagrant,57258770,197372009,"@wkretzsch - I personally don't know at the moment about any torrent sites - but for me I wouldn't want to trust torrent links as the source for my infrastructure testing. It's a possible option but security is also important. For me official vagrant boxes from folks like puppetlabs hosted on Atlas are so slow to download at times that I wish this issue could be resolved. For internal vagrant boxes that I build for my company we have the option to host on S3 or Artifactory or private Atlas org.

@mitchellh - yes - curl is just as slow (for me). I don't think it is a Vagrant issue - but a backed server hosting issue. Granted - not a Vagrant issue per se. 
",2016-03-16T15:03:39Z,398237
1154,hashicorp/vagrant,57258770,197734440,"![screenshot from 2016-03-17 14-04-58](https://cloud.githubusercontent.com/assets/7142025/13838783/486408ba-ec49-11e5-9903-19cc1e031395.png)

Yes, this is because curl can only use one of my 3 connections at the same time.  No, that's not the connection's rated speed.  The rated speed is 45mbps.  Yes, bittorrent does perform better.  Just sayin-- your rationale for not supporting bittorrent is kinda thin here.  
",2016-03-17T07:06:42Z,7142025
1155,hashicorp/vagrant,57258770,197847596,"@tehmaspc surely there must be a way for a website to publish the hash of their box along with a torrent link?  
",2016-03-17T12:01:38Z,304958
1156,hashicorp/vagrant,57258770,199516958,"I wish in general, there was a way to have incremental images, like docker images, with vagrant boxes.  For the provisioners, which bootstrap (cfengine, chef, salt, puppet, docker, etc) by downloading their platform, I wish there was a way to download a packaged up installer, so that other fresh images that use that provisioner, e.g. ubuntu + docker, would not need to download the goods again.  Box updates and provisioner downloads were already painful, but recently, have been beyond notoriously slow.
",2016-03-21T22:28:33Z,7537882
1157,hashicorp/vagrant,57258770,207881297,"Just went to update my box for the first time (trusty64 - noticed the warning on my vagrant up command output), and it's going to take my 1.5 hours on a 150MBps connection - pathetic. It's 2016 - I don't know the specifics of what's going on here, but surely we can fix this, like, by the end of next week? The tech that goes into modern technologies like vagrant is amazing, something this basic should be overcome in mere hours.
",2016-04-09T23:31:46Z,1664812
1158,hashicorp/vagrant,57258770,207896576,"Amen, Matt, Amen.  This is about UX.

There should be a recognition that line speed != line speed and practical
steps can be taken to overcome the daunting issue of line speed != line
speed.

Jacob Gadikian
E-mail: faddat@gmail.com
SKYPE: faddat
Phone/SMS: +84 167 789 6421

On Sun, Apr 10, 2016 at 6:32 AM, Matt Porter notifications@github.com
wrote:

> Just went to update my box for the first time (noticed the warning on my
> vagrant up command output), and it's going to take my 1.5 hours on a
> 150MBps connection - pathetic. It's 2016 - I don't know the specifics of
> what's going on here, but surely we can fix this, like, by the end of next
> week? The tech that goes into modern technologies like vagrant is amazing,
> something this basic should be overcome in mere hours.
> 
> —
> You are receiving this because you commented.
> Reply to this email directly or view it on GitHub
> https://github.com/mitchellh/vagrant/issues/5319#issuecomment-207881297
",2016-04-10T01:53:28Z,7142025
1159,hashicorp/vagrant,57258770,208706127,"I just tried asking Vagrant to download ubuntu/trusty64, and was getting speeds of <= 5 KiB/sec. I killed it and tried again using the exact same command, and got 29 MiB/sec. 

I think @mitchellh is correct in that this doesn't really seem like a Vagrant issue. If anything, it seems more like an Atlas issue (so possibly the ELB and/or whatever's sitting behind it). I highly doubt it has anything to do with the routes or hops between end-users and the ELB VIPs -- you wouldn't typically see such a polarizing set of speeds in that case, especially considering both VIPs terminate in us-east-1.

If for no other reason, it'd be highly desirable to see these made available through a CDN rather than a centrally-located ELB. Then again, I'm just one guy (who isn't paying for this service), so take that for what it's worth. Pretty thankful it's there either way.
",2016-04-12T04:58:02Z,32032
1160,hashicorp/vagrant,57258770,209974166,"It's not just an Atlas issue. I have boxes and metadata.json on S3, with a Fastly CDN in front and regularly have the exact same issue: sometimes vagrant downloads at 100kbps and sometimes it downloads at > 5mbps. You can cancel a slow download and half the time a retry gets you the faster speeds. 
",2016-04-14T14:36:16Z,3980984
1161,hashicorp/vagrant,57258770,210578465,"I contacted support about this around the same time I chimed in here initially. Their response is that Vagrant uses curl to download things so they don't see this as a Vagrant problem. IMO that's an unprofessional cop-out because they chose to use curl, know that there are problems and aren't considering swapping out with an alternative to eliminate the problem for their users.
",2016-04-15T18:29:28Z,660997
1162,hashicorp/vagrant,57258770,213288936,"I can confirm that this is still an issue. All my peers also report times of >1h, while the connection here for other connections is around 200MB/s.

```
vagrant up
Bringing machine 'default' up with 'virtualbox' provider...
==> default: Box 'ubuntu/trusty32' could not be found. Attempting to find and install...
    default: Box Provider: virtualbox
    default: Box Version: >= 0
==> default: Loading metadata for box 'ubuntu/trusty32'
    default: URL: https://atlas.hashicorp.com/ubuntu/trusty32
==> default: Adding box 'ubuntu/trusty32' (v20160406.0.0) for provider: virtualbox
    default: Downloading: https://atlas.hashicorp.com/ubuntu/boxes/trusty32/versions/20160406.0.0/providers/virtualbox.box
    default: Progress: 11% (Rate: 43801/s, Estimated time remaining: 1:36:50)
```
",2016-04-22T06:41:30Z,6270048
1163,hashicorp/vagrant,57258770,213619117,"While I am unsure of the origin of the problem, I really do wish that Hashicorp would get back to its unrelenting focus on user experience with this one.  **Muli-hour downloads (that should take 1-10 minutes)==bad ux.**
",2016-04-22T23:03:20Z,7142025
1164,hashicorp/vagrant,57258770,214512394,"Currently downloading an image for the 5th time (@13Xk/s, even with `wget`). Keep disconnecting me while around 50-90%. But it ALWAYS downloads at full speed either early morning / late night EST.  Assuming it is a traffic  issue, but regardless very bad UX.

```
    box: Progress: 47% (Rate: 106k/s, Estimated time remaining: 0:14:50)
```
",2016-04-25T20:34:51Z,10047844
1165,hashicorp/vagrant,57258770,215832266,"I have been trying for 2 day's now and still can not get it to download... its a shame.. it is really not impressing new comers to  laravel .. i can only get 34ks speed.........
",2016-04-29T18:04:31Z,6499340
1166,hashicorp/vagrant,57258770,216503206,"Speeds ok from the UK:

```
Bringing machine 'default' up with 'virtualbox' provider...
==> default: Box 'bento/centos-7.2' could not be found. Attempting to find and install...
    default: Box Provider: virtualbox
    default: Box Version: >= 0
==> default: Loading metadata for box 'bento/centos-7.2'
    default: URL: https://atlas.hashicorp.com/bento/centos-7.2
==> default: Adding box 'bento/centos-7.2' (v2.2.6) for provider: virtualbox
    default: Downloading: https://atlas.hashicorp.com/bento/boxes/centos-7.2/versions/2.2.6/providers/virtualbox.box
    default: Progress: 11% (Rate: 7728k/s, Estimated time remaining: 0:01:19)
```

What is your location?
",2016-05-03T11:42:46Z,5149834
1167,hashicorp/vagrant,57258770,216505071,"Also, https://atlas.hashicorp.com/ URL's are delivered from Amazon Web Services (atlas-frontend-atlas-230110478.us-east-1.elb.amazonaws.com) so I doubt they are tight for bandwidth ;-)

Are the slow downloads being made from locations a long distance away from the AWS us-east-1 DC, perhaps thats the root cause of the issue?

Maybe the AWS CDN could be used to cache files around the world?
",2016-05-03T11:47:35Z,5149834
1168,hashicorp/vagrant,57258770,216507036,"I'm located in Vermont, which is pretty us-east-1 last I checked :dart: 
",2016-05-03T11:59:19Z,1664812
1169,hashicorp/vagrant,57258770,216520958,"I am in Brazil.... got it to download.... 10m connection here took 4.6 hours!!!!! my wife just gave birth to our 8th little girl.. It only took her 40 minutes !!!!! lol There is a big problem with there download!!!!!
",2016-05-03T13:07:32Z,6499340
1170,hashicorp/vagrant,57258770,217642878,"Same here, I have a 50Mbps connection...

`
    default: Progress: 44% (Rate: 102k/s, Estimated time remaining: 0:15:01))
`
",2016-05-07T15:01:31Z,6663430
1171,hashicorp/vagrant,57258770,217673852,"Sign me upp here, 100mb symmetric connection (Fiber) sloooow as shit, doing 150kb/s
",2016-05-07T22:55:25Z,1127626
1172,hashicorp/vagrant,57258770,217725427,"after looking at the years of complaints of slow download with no effort of resolving the issue,,, i think its time to start emailing Laravel to stop endorsing homestead until the issue is resolved..... maybe that will get their attention!!!! this is a real problem... 15 retries and then 4.6 hours to download a file is irresponsible on their part........
",2016-05-08T15:02:09Z,6499340
1173,hashicorp/vagrant,57258770,218598736,"I bet it gets closed, but if you don't ask you don't get:

https://github.com/mitchellh/vagrant/issues/7307
",2016-05-11T21:40:41Z,5149834
1174,hashicorp/vagrant,57258770,225645550,"Just snagged a box at 85mbit.  You all fix something recently?  Much better than it used to be.
",2016-06-13T17:08:37Z,1097509
1175,hashicorp/vagrant,57258770,230386144,"This is painful to do anything on any more - On 100mbps synchronous connection and getting 168kb, either overloaded servers or throttling
",2016-07-05T04:38:56Z,1065098
1176,hashicorp/vagrant,57258770,233045389,"In looking to debug curl being slow -- I found a [stackoverflow post  ](http://stackoverflow.com/questions/30984641/debugging-slow-download-with-curl) that suggests that --trace-ascii /dev/null makes your curl go at the speed you'd expect.  For me, I'm trying to download [CentosOS 7](http://cloud.centos.org/centos/7/vagrant/x86_64/images/CentOS-7-x86_64-Vagrant-1606_01.VirtualBox.box) and here are my results:

NO --trace-ascii option:

```
$ curl http://cloud.centos.org/centos/nt/x86_64/images/CentOS-7-x86_64-Vagrant-1606_01.VirtualBox.box -o CentOS-7-x86_64-Vagrant-1606_01.VirtualBox.box
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0  483M    0  489k    0     0  77724      0  1:48:44  0:00:06  1:48:38 69721
```

With trace-ascii the first time:

```
$ curl http://cloud.centos.org/centos/7/vagrant/x86_64/images/CentOS-7-x86_64-Vagrant-1606_01.VirtualBox.box -o CentOS-7-x86_64-Vagrant-1606_01.VirtualBox.box --trace-ascii /dev/null
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  5  483M    5 24.5M    0     0  4259k      0  0:01:56  0:00:05  0:01:51 4599k
```

Does anyone else see the same behavior?
",2016-07-15T19:15:06Z,2091058
1177,hashicorp/vagrant,57258770,240311022,"The download is extremely slow on my end too. I'm trying vagrant for the very first time. Might ditch this software and go back to my native apache2 instead.

<img width=""913"" alt=""screen shot 2016-08-17 at 12 31 40 pm"" src=""https://cloud.githubusercontent.com/assets/4960876/17724673/926a8510-6476-11e6-82ab-fb7276c448c4.png"">
",2016-08-17T04:32:31Z,4960876
1178,hashicorp/vagrant,57258770,268513565,Help. I have same problem. I can't wait 3 hours! Very slow! Stupid!,2016-12-21T12:39:22Z,24268974
1179,hashicorp/vagrant,57258770,268532571,i gave up a year ago..download to slow.. problems after down load.. have to download for 3 hours again.... Vagrant will not fix the problem that has been there for several years now.. you would think that after 3 or 4 years of this problem they would address the issue.....,2016-12-21T14:15:13Z,6499340
1180,hashicorp/vagrant,57258770,268540307,8 hours to download! I hate you all!,2016-12-21T14:47:48Z,24268974
1181,hashicorp/vagrant,57258770,268653949,"Lol, I hate you too @daryn-k :)",2016-12-21T22:04:56Z,304958
1182,hashicorp/vagrant,57258770,269612087,"Guys why is this issue closed? This is still an outstanding issue and needs to be addressed ASAP. I am experiencing the same issue.
",2016-12-29T10:41:14Z,18344557
1183,hashicorp/vagrant,57258770,272380689,Wow! Downloading boxes is painful please fix this. PLEASE?,2017-01-13T07:42:33Z,1530209
1184,hashicorp/vagrant,57258770,272505864,"I think it really has to do with time of day, traffic, alignment of the planets, etc. I haven't had slow speeds in a while. It seems very hit or miss. In fact, if it is slow you and start and stop it with the chance of getting a better connection. I'm not sure this is really the fault of the vagrant framework as much as it is the nature of large bottlenecked downloads.",2017-01-13T18:07:31Z,2024145
1185,hashicorp/vagrant,57258770,273679937,"@mitchellh These errant speed symptoms from hashicorp's servers could be indicative of bumping up against AWS's IOPS credits for GP2 filesystems.  https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html#IOcredit

We had some testing infrastructure on drupal.org that would run fine for a long time, then suddenly drop to a crawl because we had ""spent"" all of our IO credits.  It could be possible that hashicorps' servers are bumping up against the same limit. 

`sar -b` could give some insight as to whether or not this explains the random performance drops.
",2017-01-19T04:34:25Z,101536
1186,hashicorp/vagrant,57258770,273765494,It could be an idea for Hashicorp to move the images for download into S3 and use that for downloads... that would save on running instances specifically for downloads.,2017-01-19T12:33:26Z,5149834
1187,hashicorp/vagrant,57258770,279201272,"Downloading boxes used to be quick, now it's so slow it makes vagrant a no-go for quick and simple developer environments.  ",2017-02-12T07:06:26Z,379628
1188,hashicorp/vagrant,57258770,280154083,"Trying to download ubuntu/xenial64. Download speed maxes out at 150 KB/s on a 1 Gbps symmetrical fiber connection. WTF. Remaining time 1 hour? I could probably download the ISO, read the guide on how to set up my own box, and finish earlier.

EDIT: Interestingly, speed went up by factor 10 when I tried to download the same box in the browser simultaneously.",2017-02-15T22:00:51Z,6877273
1189,hashicorp/vagrant,57258770,287034242,">use latest devops tools to speed things up
>spend days watching max 420k/s download speeds",2017-03-16T11:49:14Z,3893
1190,hashicorp/vagrant,57258770,288285617,"Same here, I have a 30Mbps connection

> default: Adding box 'ubuntu/trusty64' (v20170313.0.5) for provider: virtualbox
default: Downloading: https://atlas.hashicorp.com/ubuntu/boxes/trusty64/versions/20170313.0.5/providers/virtualbox.box
default: Box download is resuming from prior download progress
default: Progress: 0% (Rate: 80568/s, Estimated time remaining: 1:26:22)",2017-03-22T03:08:18Z,13390855
1191,hashicorp/vagrant,57258770,289866508,"@DeadlySystem We have the same experience, when I download the same box (url) using `curl` from the commandline (during the `vagrant up`)",2017-03-28T18:44:50Z,3392962
1192,hashicorp/vagrant,57258770,290985383,"Any update on this, fetching box from Hashicorp is painfully slow.

![screenshot_2017-04-02_21-08-15](https://cloud.githubusercontent.com/assets/1684989/24587412/8f0329d8-17e8-11e7-9743-f793458b7daf.png)
",2017-04-02T13:08:58Z,1684989
1193,hashicorp/vagrant,57258770,291019093,"Hey, quick thought:

If this uses curl (not libcurl) through some sort of ruby-controlled, bash-mediated process, why not just remove curl for one of:

* ipfs
* aria2

Both would do the job better than curl.  ",2017-04-02T22:15:37Z,7142025
1194,hashicorp/vagrant,57258770,291026688,"It honestly looks like they dont give a shit, rules this out as an option
for me!

On 3 Apr 2017 8:15 AM, ""Jacob Gadikian"" <notifications@github.com> wrote:

> Hey, quick thought:
>
> If this uses curl (not libcurl) through some sort of ruby-controlled,
> bash-mediated process, why not just remove curl for one of:
>
>    - ipfs
>    - aria2
>
> Both would do the job better than curl.
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/mitchellh/vagrant/issues/5319#issuecomment-291019093>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ABBAihR9ng4t2Jq1XTmAjMyMCnlEFtxRks5rsB4WgaJpZM4Deq5d>
> .
>
",2017-04-03T00:37:21Z,1065098
1195,hashicorp/vagrant,57258770,292199156,"I'm on a 150Mbps line.

vagrant up = HOURS
vagrant box add = HOURS
browser download /wget = HOURS

May not be a vagrant issue per se, BUT IT IS.   If your infrastructure can't handle it then your product is broken.

BAD UX",2017-04-06T14:51:44Z,7947364
1196,hashicorp/vagrant,57258770,296575098,"I opened a ticket about customizing the download tool, but it got rejected as too complicated to impliment ;-(

That said, my recent download speeds have been ok from the UK for a while. 
This example downloaded just now:

```
    default: Box Provider: virtualbox
    default: Box Version: >= 0
==> default: Loading metadata for box 'bento/ubuntu-16.04'
    default: URL: https://atlas.hashicorp.com/bento/ubuntu-16.04
==> default: Adding box 'bento/ubuntu-16.04' (v2.3.4) for provider: virtualbox
    default: Downloading: https://atlas.hashicorp.com/bento/boxes/ubuntu-16.04/versions/2.3.4/providers/virtualbox.box
    default: Progress: 12% (Rate: 8940k/s, Estimated time remaining: 0:01:06)
```

Maybe you guys are just too far from their AWS instances for a good download speed?
All their download servers look to be in New York with no CDN to distribute content.

```
$ dig atlas.hashicorp.com
; <<>> DiG 9.10.3-P4-Ubuntu <<>> atlas.hashicorp.com
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 44169
;; flags: qr rd ra; QUERY: 1, ANSWER: 4, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4000
;; QUESTION SECTION:
;atlas.hashicorp.com.           IN      A

;; ANSWER SECTION:
atlas.hashicorp.com.    120     IN      CNAME   atlas.hashi.co.
atlas.hashi.co.         60      IN      A       52.206.86.0
atlas.hashi.co.         60      IN      A       52.200.255.5
atlas.hashi.co.         60      IN      A       52.55.203.197

;; Query time: 42 msec
;; SERVER: 10.1.1.14#53(10.1.1.14)
;; WHEN: Mon Apr 24 09:20:24 DST 2017
;; MSG SIZE  rcvd: 124
```
http://geoiplookup.net/ip/52.55.203.197
http://geoiplookup.net/ip/52.206.86.0
http://geoiplookup.net/ip/52.200.255.5

Maybe its worth the people getting slow downloads detailing what ISP they are using? Maybe you are all on an ISP with a high contention ratio?",2017-04-24T08:29:20Z,5149834
1197,hashicorp/vagrant,57258770,296677436,https://github.com/mitchellh/vagrant/issues/8434#issuecomment-291979521,2017-04-24T14:00:08Z,266674
1198,hashicorp/vagrant,57258770,296875645,"> Maybe its worth the people getting slow downloads detailing what ISP they are using?

Any ISP, in any state I've traveled to in the last couple years.

This problem is squarely on whatever Hashicorp is doing for hosting, and that's where it needs to be fixed.  If they're unable or unwilling to fix it, a torrent solution would certainly help without taking their resources aside from development for adding a torrent client to the tool.  If all this stuff is on S3 anyway, AWS provides torrent seeding out of the box.",2017-04-25T02:40:09Z,1097509
1199,hashicorp/vagrant,57258770,296891633,"I was getting painfully slow speeds and so I decided to see if upgrading Vagrant would change anything. Before I updated I was getting speeds of around 50-200kbps. After the update I was using the full 70mbps of my connection.

So for those of you that have slow speeds, try updating to v1.9.4 if you aren't already running it.",2017-04-25T03:47:51Z,4453733
1200,hashicorp/vagrant,57258770,297826485,"Hi, I'm on the latest Vagrant 1.9.4 and the download speed is so slow that keeps disconnect. No way to download latest laravel homestead 2.1.0 box...
When I downloaded the 2.0 with Vagrant 1.9.3 (or previous, can't remember) it was flawless",2017-04-27T20:14:32Z,9018738
1201,hashicorp/vagrant,57258770,306632475,"same issue here, very slow download rates
Rate: 35033/s (keeps jumping between 20000 and 60000...)
Estimated time remaining: 9:26:13 :(
(on vagrant 1.9.5)",2017-06-06T22:12:17Z,5557790
1202,hashicorp/vagrant,57258770,307085722,Now it's fast again... really depends on time of day and luck (as stated above...).,2017-06-08T12:16:01Z,5557790
1203,hashicorp/vagrant,57258770,310799797,"I'm having this same problem when trying to download Bento boxes. I'm using Vagrant 1.9.5 with 5.1.22. I've tried several times, during various days. My normal download speed is 1.7mb~ but when dowloading boxes I can't get pass 40kb.

If I try downloading with wget I get the same low speed.",2017-06-24T00:15:08Z,1458916
1204,hashicorp/vagrant,57258770,311167669,"Maybe the move will help:

https://www.vagrantup.com/docs/vagrant-cloud/vagrant-cloud-migration.html",2017-06-26T20:08:48Z,5149834
1205,hashicorp/vagrant,57258770,313185187,"Gonna try again tonight to see if the migration helped. Anyway I noted that with a different ISP the download rates were fine, so I guess it's not a problem with Vagrant itself (like many users already reported).",2017-07-05T18:21:04Z,1458916
1206,hashicorp/vagrant,57258770,313638204,"Well, nothing has changed, it still downloads at a snail's pace given that i am on a 125 Mbps connection!",2017-07-07T09:48:07Z,1678111
1207,hashicorp/vagrant,57258770,315969569,Sooooooooooooooo slowly！！！！:(,2017-07-18T06:23:12Z,7239021
1208,hashicorp/vagrant,57258770,315973158,"create a new issue, this has been closed for ages",2017-07-18T06:45:03Z,3893
1209,hashicorp/vagrant,57258770,326277819,"i just downloaded latest scotch-box vagrant box, spent ~1hour for that, and then i found an issue which could be fixed only by downgrading to previous version of box, so i'm here again downloading second box for next hour of my working time. So.. it's good since i got per-time payment, but on the other hand probably i'll have some problems explaining why i spen last day downloading stuff from the internet ",2017-08-31T12:17:04Z,18402060
1210,hashicorp/vagrant,57258770,326939886,I'm also facing the slow download issue.,2017-09-04T11:31:49Z,3977193
1211,hashicorp/vagrant,57258770,327102154,"Try downloading from a wired connection, it's exponentially faster!

On 4 September 2017 at 17:02, Omar Tariq <notifications@github.com> wrote:

> I'm also facing the slow download issue.
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/mitchellh/vagrant/issues/5319#issuecomment-326939886>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ABmbHxWHtAnCnEw34FB4oXX8lTQrOsNRks5se9-_gaJpZM4Deq5d>
> .
>



-- 
Best regards,
Shivaprabhu Wali.
",2017-09-05T08:04:39Z,1678111
1212,hashicorp/vagrant,57258770,332805316,"Typical issue, been around for ages, everybody's moaning about it, and nothing is being done.",2017-09-28T11:13:18Z,11272013
1213,hashicorp/vagrant,57258770,333338552,"Came here via Google and this might help some people:

When I run `vagrant init [box] && vagrant up`, it loads super slow.
But when I run `vagrant box add [box]`, it's faster.

Going from 120 kb/s to >1000 kb/s.",2017-09-30T21:55:26Z,17178613
1214,hashicorp/vagrant,57258770,334906895,@PascalAOMS Thanks for the tip. At least it worked for me.,2017-10-07T03:20:43Z,10437330
1215,hashicorp/vagrant,57258770,338771287,"Today I've tried [aria2](https://aria2.github.io/) with 16 simultaneous connections to download laravel/homestead managed to get up to 9.6MiB

```
Download Results:
gid   |stat|avg speed  |path/URI
======+====+===========+=======================================================
80fa38|ERR |   7.8MiB/s|C:/development/homestead/47dce273-9892-4691-a746-c4f351ae44a5
```",2017-10-23T19:32:16Z,1244112
1216,hashicorp/vagrant,57258770,342169126,From last two days i'm trying to download vigrant box but every time i try to download it gives me error and downloading spedd is very slow. I also tried to install it manually but it also didn't worked.For newbie to Laravel it pretty frustrating and i'm still not able to download it.Don't know what should i do now.,2017-11-06T14:43:36Z,26194239
1217,hashicorp/vagrant,57258770,342774968,"I'm seeing the same problem. Running latest Vagrant 2.0.0. When downloading a box (configured using external URL on Vagrant Cloud, so it's actually redirecting to Rackspace Cloud Files) it's painfully slow. 

If I use cURL or Chrome to download the same URL (https://vagrantcloud.com/joomlatools/boxes/box/versions/1.5.0/providers/virtualbox.box) from the same machine it only takes a couple of seconds to complete the download. What could be the issue here? ",2017-11-08T10:25:35Z,77368
1218,hashicorp/vagrant,57258770,345481819,"Using terminal to add a box is extremely slow, more than 1 hour with 100kb/s. If I use a browser to directly download the box it is much faster, just a few minutes with 600kb/s. What's the reason for having such a huge difference? Seems as adding the box through terminal won't use all the available bandwidth.",2017-11-19T00:22:44Z,6256009
1219,hashicorp/vagrant,57258770,345567579,"I am trying to download boxes more quickly, and following the advice of some of the above comments, I used [aria2](https://aria2.github.io/) through [homebrew](http://brewformulas.org/Aria2) to download much faster in parallel. Sample command (url was from original box add attempt from Vagrant):

    $ aria2c -x16 https://vagrantcloud.com/ubuntu/boxes/xenial64/versions/20171118.0.0/providers/virtualbox.box

My speed was around 100k/s with Vagrant's download, up to 1 MB/s with aria2.

Then when you finish downloading that, you [can add the box using](https://stackoverflow.com/a/26655618/532524):

    $ vagrant box add ubuntu/xenial64 ../xenial64.box

I think you can remove the downloaded box after this since it will be copied to [the standard box storage path](https://stackoverflow.com/a/10226134/532524) after adding it. Hope this helps save someone else some time.",2017-11-20T01:17:30Z,51120
1220,hashicorp/vagrant,57258770,346974011,"@panozzaj's comment above doesn't quite work if a `Vagrantfile` expects a specific version of a box (that command will add a box without a version). Instead, you can do the below.

# Steps to get a specific box at a specific version without using `vagrant` to perform the download

I'm using laravel/homestead in this example because it's what I was trying to get.

1. Download the box using a better download client (e.g. your browser, curl, wget, whatever).

2. Create a new json file (anywhere), add this to it (note, tweak the `name`, `version` and `url` keys to match the box you want, don't worry about the `checksum` key yet. For `url`, use the path to the file you just downloaded, example below):
```
{
    ""name"": ""laravel/homestead"",
    ""description"": ""Whatever you want"",
    ""versions"": [{
        ""version"": ""4.0.0"",
        ""providers"": [{
                ""name"": ""virtualbox"",
                ""url"": ""file://c:/users/madmatt/Downloads/47dce273-9892-4691-a746-c4f351ae44a5"",
                ""checksum_type"": ""sha1"",
                ""checksum"": ""abc123""
        }]
    }]
}
```

3. In your `Vagrantfile`, add the following lines `vm.box` and `vm.box_url` keys):
```
Vagrant.configure(""2"") do |config|
    config.vm.box = ""laravel/homestead""
    config.vm.box_url = ""file://c:/users/madmatt/path/to/the/json-file-you-created-in-step-2.json""
end
```

4. Run `vagrant up` as normal.

5. `vagrant` will complain that the sha hash doesn't match (`The checksum of the downloaded box did not match...`). Take the string that appears next to 'Expected', copy and paste that into the `checksum` key of the json file you created in step 2.

6. Run `vagrant up` again, this time it should load from the local file, store it as the correct name and version, and successfully run it.

Hopefully someone finds these steps useful... funnily enough I have done all this research (having never used vagrant before), have tested it a bunch of times, and the original `vagrant box add laravel/homestead` command that I started running 3 hours ago is still only 8% complete, even though in that time I've downloaded the box file 8 times outside of vagrant.

The rest of this is just my experience, no need to read further ;)

# My experience (aka. why is vagrant so slow to download boxes)

Was getting 420b/sec (yes, that's bytes per second) on a gigabit connection downloading https://vagrantcloud.com/laravel/boxes/homestead/versions/4.0.0/providers/virtualbox.box.

I downloaded the same file via browser, curl and wget with speeds varying between 12 and 27MB/sec. I then tried doing both at the same time - I was able to download via Chrome, Firefox, curl and wget before ```vagrant box add laravel/homestead``` had downloaded 1%.

The URL I got in the browser was ```https://vagrantcloud-files-production.s3.amazonaws.com/archivist/boxes/47dce273-9892-4691-a746-c4f351ae44a5?X-Amz-Algorithm=<snip>&X-Amz-Credential=<snip>&X-Amz-Date=<snip>&X-Amz-Expires=<snip>&X-Amz-SignedHeaders=<snip>&X-Amz-Signature=<snip>```

I don't know what the problem here is, but I can think of a couple:
* Whatever UA vagrant uses is bad, and AWS severely limits it
* Whatever vagrant uses to download isn't following redirects, or somehow never ends up downloading from AWS infrastructure, instead using some other terribly overloaded server/proxy",2017-11-25T23:52:30Z,893117
1221,hashicorp/vagrant,57258770,351668465,how is this closed? still a major issue,2017-12-14T10:17:46Z,2160666
1222,hashicorp/vagrant,57258770,352158611,"Perhaps it's time to fork.  This project is core to a lot of development environments, leaving us all subject to the whims of HashiCorp... and on this issue we seemingly cannot even get a reasonable official response.

Vagrant is MIT-licensed.  Boxes could easily be distributed via torrent, or we can even just specify URLs in our Vagrantfile.  We don't need the Vagrant Cloud dependency if some basic enhancements around box handling are made.  Are there any maintained forks already in existence?

Any thoughts from others?",2017-12-16T03:55:35Z,1097509
1223,hashicorp/vagrant,57258770,352944302,Definitely still an issue!,2017-12-20T02:10:36Z,11791
1224,hashicorp/vagrant,57258770,353840877,"Are there any trusted torrent links available? I am trying to download bentoo/ubuntu16.04 and i am getting 12 kbps and upto 15kbps when i try to do wget, i am sure that my connection is fine!",2017-12-25T07:19:03Z,24985760
1225,hashicorp/vagrant,57258770,354071483,"Still an issue here, have to update my box at ~150k/s, not able to do a lot of useful work in the meanwhile..",2017-12-27T08:01:38Z,7504556
1226,hashicorp/vagrant,57258770,354400270,Still an issue.,2017-12-29T05:33:45Z,150670
1227,hashicorp/vagrant,57258770,354400348,@bradisbell The real solution is probably a docker-based development environment. Just waiting for that ecosystem to settle down .,2017-12-29T05:34:56Z,150670
1228,hashicorp/vagrant,57258770,354400471,"🍺 🍺 🍺 🍺 🍺 🍺 

I'm just going to leave these here for any poor sod waiting for this download. ",2017-12-29T05:37:08Z,2160666
1229,hashicorp/vagrant,57258770,354402948,"I found a root cause and a solution for my situation. I increased my download speed 100x:

Root cause: Slow speed of `ubuntu.com` cloud box servers
Solution: Switch to another trusted source for Ubuntu boxes, Puppet Labs.

I've been using `ubuntu/xenial64` for developing apps that must deploy to Ubuntu 16. (Heroku) And download speeds have become ridiculously slow, or failed entirely. I used wget and watched it follow the redirects and saw that it's actually hitting `ubuntu.com`. I tried several ways to download from Ubuntu, but they all resulted in speeds around 50 KB/s. 

So I went back to the Vagrant box listing to see who else might have a Xenial 64 box, and if I'd trust them. Turns out Puppet does, and I trust them as much as (or even more than) Ubuntu. My `Vagrantfile` now has the config line:

```
config.vm.box = ""puppetlabs/ubuntu-16.04-64-nocm""
```

And it downloaded in just a few seconds, as opposed to possibly an hour or never.

",2017-12-29T06:11:07Z,150670
1230,hashicorp/vagrant,57258770,355681813,@dogweather Thanks for this! My download time estimate went from 3 hours to 3 minutes when I switched to a box from the list at https://app.vagrantup.com/puppetlabs/ ,2018-01-05T22:23:03Z,24866055
1231,hashicorp/vagrant,57258770,355689564,"@tristanmason You're welcome! 

Ironically: In the past week I switched to Docker Compose, and it's **AMAZING**. I can't believe I've waited this long to use it. I used this [Quickstart for Rails](https://docs.docker.com/compose/rails/) doc.

* Configuration is 1/10 the size and complexity of Vagrant for my typical Rails app
* Build and Boot times are much faster
* On my Linux desktop in particular, there is a 10x speed increase vs. running on Mac.
* There's a huge reduction in RAM required. 
* I'm going to save a ton of money on development computers. I.e., 8GB RAM is plenty for a Docker-based dev machine. (Not so good for Vagrant+VirtualBox though.) And a top-tier i7 isn't necessary anymore either to get good speeds.

I was able to setup a Docker-based dev environment on four computers in a fraction of the time it takes me to create a single Vagrant setup from scratch.

I'm going to write up a blog with detailed metrics - I'll post it here.
  ",2018-01-05T23:05:31Z,150670
1232,hashicorp/vagrant,57258770,357775196,"@dogweather Vagrant and Docker seems similar but they are different things. Also, you can use a Docker provider within Vagrant.",2018-01-15T19:55:47Z,1458916
1233,hashicorp/vagrant,57258770,357778686,"@oncet Yep, very different! But for my use case, they're equivalent and commodified: Ways I can create and launch an isolated dev environment with just one or two commands. 

Docker Compose works great and is far simpler than Vagrant (note, this not just ""Docker"" per se, which only launches containers one by one) and I'd need to hear about a compelling reason to try a Vagrant+Docker solution, which sounds pretty damn complex. ;-)",2018-01-15T20:11:55Z,150670
1234,hashicorp/vagrant,57258770,357875973,Pain. This is a pain. I'll never be able to download the 5.0.1 I guess,2018-01-16T07:32:58Z,9018738
1235,hashicorp/vagrant,57258770,358685317,"On a 100mbps connection in Manila, with Vagrant 2.0.1, I was trying to download:

https://vagrantcloud.com/ubuntu/boxes/trusty64/versions/20180110.0.0/providers/virtualbox.box

...and I was getting speeds of anywhere from 1k-150k via `vagrant up` or even `wget`. Total download time: **12 hours!**

Then I did one thing: **I VPN'd to California.**

Suddenly, my download took only 3 mins. So that's something worth trying possibly.",2018-01-18T15:40:54Z,6813314
1236,hashicorp/vagrant,57258770,360672358,"Same here, I thought this is a personal problem lol.
Even update box still get really poor download speed, hashicorp please fix :(

```
==> default: Updating 'ubuntu/xenial64' with provider 'virtualbox' from version
==> default: '20171221.0.0' to '20180122.0.0'...
==> default: Loading metadata for box 'https://vagrantcloud.com/ubuntu/xenial64'
==> default: Adding box 'ubuntu/xenial64' (v20180122.0.0) for provider: virtualbox
    default: Downloading: https://vagrantcloud.com/ubuntu/boxes/xenial64/versions/20180122.0.0/providers/virtualbox.box
==> default: Box download is resuming from prior download progress
    default: Progress: 13% (Rate: 52999/s, Estimated time remaining: 0:31:46)
```",2018-01-26T03:10:32Z,20561837
1237,hashicorp/vagrant,57258770,360958536,"Ignoring such persistent issue means something on Hashicorp's end, I believe.",2018-01-27T04:14:40Z,1684989
1238,hashicorp/vagrant,57258770,361026239,"@dqlopez from the previous comments, this is a solved issue: it's purely the speed of ubuntu's servers. The solution is to use some other org's images.",2018-01-28T00:03:23Z,150670
1239,hashicorp/vagrant,57258770,361033888,"Confirming that when I used `bento/ubuntu-16.04` instead of `ubuntu/xenial64` ([why?](https://github.com/hashicorp/vagrant/issues/6616#issuecomment-227776489)), I got pretty good download speeds. The box downloaded in less than a minute, on a 300 Mbps fibre optic connection. (Unfortunately I didn't think to copy and paste the log.)",2018-01-28T02:48:44Z,11791
1240,hashicorp/vagrant,57258770,362883206,"I promised I'd report back on my transition to Docker, so here goes. I spent a ton of time creating a config that delivers the one-liner `vagrant up` experience. It's `docker-compose up`. And there's only one dependency, Docker. Not two (VM _plus_ Vagrant).

My post about performance of a Docker dev environment on Mac and Linux: [medium.com](https://medium.com/@dogweather/dev-environment-performance-tests-docker-vs-native-mac-vs-linux-old-vs-new-883399d05182)

Repos with my configuration, tailored for [Ruby on Rails](https://github.com/dogweather/rails-docker-compose) and [Phoenix](https://github.com/dogweather/phoenix-docker-compose) development. Very very similar. Can be tailored for any language, I'd imagine.

cc: @fredngo ",2018-02-04T05:57:18Z,150670
1241,hashicorp/vagrant,57258770,362884368,"@dogweather Frankly, I don't see how your commentary on docker has anything to do with the issue at hand.  Please stop taking this thread off-topic.",2018-02-04T06:25:32Z,1097509
1242,hashicorp/vagrant,57258770,362910549,"Docker is not an alternative to Vagrant. They work completely differently and are intended for different environments. While I'm all for a discussion on the merits of when each should be used, this thread is not really the place for it.

Edit: I love both and use them both in development and production environments.",2018-02-04T14:28:01Z,2126835
1243,hashicorp/vagrant,57258770,368481937,"Haven't read the full thread, but it was a significant difference between using PowerShell and Git Bash. ",2018-02-26T12:16:08Z,865493
1244,hashicorp/vagrant,57258770,370191268,"Well, I cant download a single box tonite.  I'm in the UK.
```
The box 'puppetlabs/ubuntu-16.04-64-puppet' could not be found or
could not be accessed in the remote catalog. If this is a private
box on HashiCorp's Atlas, please verify you're logged in via
`vagrant login`. Also, please double-check the name. The expanded
URL and error message are shown below:

URL: [""https://atlas.hashicorp.com/puppetlabs/ubuntu-16.04-64-puppet""]
Error: The requested URL returned error: 404 Not Found
```

> config.vm.box = ""puppetlabs/ubuntu-16.04-64-nocm""
> config.vm.box = ""bento/ubuntu-16.04""
> config.vm.box = ""ubuntu/xenial64""
> 

none of those work.

BUT if you download the box manually via wget, all works fine...

```
nick@TX200-S5:~/workspaces/elk-vagrant$ wget https://app.vagrantup.com/puppetlabs/boxes/ubuntu-16.04-64-nocm/versions/1.0.0/providers/virtualbox.box
--2018-03-04 00:30:45--  https://app.vagrantup.com/puppetlabs/boxes/ubuntu-16.04-64-nocm/versions/1.0.0/providers/virtualbox.box
Resolving app.vagrantup.com (app.vagrantup.com)... 50.17.237.77, 54.221.226.80, 54.243.175.62, ...
Connecting to app.vagrantup.com (app.vagrantup.com)|50.17.237.77|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://s3.amazonaws.com/puppetlabs-vagrantcloud/ubuntu-16.04-x86_64-virtualbox-nocm-1.0.0.box [following]
--2018-03-04 00:30:46--  https://s3.amazonaws.com/puppetlabs-vagrantcloud/ubuntu-16.04-x86_64-virtualbox-nocm-1.0.0.box
Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.32.82
Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.32.82|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 666915336 (636M) [application/vnd.previewsystems.box]
Saving to: ‘virtualbox.box’

100%[==========================================================================================>] 666,915,336 2.23MB/s   in 4m 48s 

2018-03-04 00:35:35 (2.21 MB/s) - ‘virtualbox.box’ saved [666915336/666915336]

nick@TX200-S5:~/workspaces/elk-vagrant$ vagrant box add bento/ubuntu-16.04
The box 'bento/ubuntu-16.04' could not be found or
could not be accessed in the remote catalog. If this is a private
box on HashiCorp's Atlas, please verify you're logged in via
`vagrant login`. Also, please double-check the name. The expanded
URL and error message are shown below:
URL: [""https://atlas.hashicorp.com/bento/ubuntu-16.04""]
Error: The requested URL returned error: 404 Not Found


nick@TX200-S5:~/workspaces/elk-vagrant$ wget -O bento-ubuntu-16.04 https://app.vagrantup.com/bento/boxes/ubuntu-16.04/versions/201802.02.0/providers/virtualbox.box
--2018-03-04 00:37:41--  https://app.vagrantup.com/bento/boxes/ubuntu-16.04/versions/201802.02.0/providers/virtualbox.box
Resolving app.vagrantup.com (app.vagrantup.com)... 54.243.252.123, 50.19.252.69, 54.243.137.45, ...
Connecting to app.vagrantup.com (app.vagrantup.com)|54.243.252.123|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://archivist.vagrantup.com/v1/object/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJrZXkiOiJib3hlcy80NzYzZDNiMy04Yzk0LTQ2YmMtYTQxNy02MDEwYjkxYzhlZjIiLCJtb2RlIjoiciIsImV4cGlyZSI6MTUyMDEyNDc2MX0.At50HVbqsvj9bfhDrbkzH7G5ON5RCcnYHwm5Xx1GXzA [following]
--2018-03-04 00:37:41--  https://archivist.vagrantup.com/v1/object/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJrZXkiOiJib3hlcy80NzYzZDNiMy04Yzk0LTQ2YmMtYTQxNy02MDEwYjkxYzhlZjIiLCJtb2RlIjoiciIsImV4cGlyZSI6MTUyMDEyNDc2MX0.At50HVbqsvj9bfhDrbkzH7G5ON5RCcnYHwm5Xx1GXzA
Resolving archivist.vagrantup.com (archivist.vagrantup.com)... 50.16.237.173, 23.21.92.233, 54.221.212.171, ...
Connecting to archivist.vagrantup.com (archivist.vagrantup.com)|50.16.237.173|:443... connected.
HTTP request sent, awaiting response... 307 Temporary Redirect
Location: https://vagrantcloud-files-production.s3.amazonaws.com/archivist/boxes/4763d3b3-8c94-46bc-a417-6010b91c8ef2?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIA4WZ7ZDX3WM4HDQ%2F20180304%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20180304T003742Z&X-Amz-Expires=60&X-Amz-SignedHeaders=host&X-Amz-Signature=871143b6e5a7078c775fba1f262ad4b3cd31e065ecde0d4ff0c4da2081aec7e7 [following]
--2018-03-04 00:37:42--  https://vagrantcloud-files-production.s3.amazonaws.com/archivist/boxes/4763d3b3-8c94-46bc-a417-6010b91c8ef2?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIA4WZ7ZDX3WM4HDQ%2F20180304%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20180304T003742Z&X-Amz-Expires=60&X-Amz-SignedHeaders=host&X-Amz-Signature=871143b6e5a7078c775fba1f262ad4b3cd31e065ecde0d4ff0c4da2081aec7e7
Resolving vagrantcloud-files-production.s3.amazonaws.com (vagrantcloud-files-production.s3.amazonaws.com)... 52.216.164.43
Connecting to vagrantcloud-files-production.s3.amazonaws.com (vagrantcloud-files-production.s3.amazonaws.com)|52.216.164.43|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 428203828 (408M) [binary/octet-stream]
Saving to: ‘bento-ubuntu-16.04’

100%[==========================================================================================>] 428,203,828 2.09MB/s   in 3m 6s  

2018-03-04 00:40:48 (2.19 MB/s) - ‘bento-ubuntu-16.04’ saved [428203828/428203828]

```

A direct box add works too

```
nick@TX200-S5:~/workspaces/elk-vagrant$ vagrant box add ""bento/ubuntu-16.04"" https://app.vagrantup.com/bento/boxes/ubuntu-16.04/versions/201802.02.0/providers/virtualbox.box
==> box: Box file was not detected as metadata. Adding it directly...
==> box: Adding box 'bento/ubuntu-16.04' (v0) for provider: 
    box: Downloading: https://app.vagrantup.com/bento/boxes/ubuntu-16.04/versions/201802.02.0/providers/virtualbox.box
    box: Progress: 7% (Rate: 2247k/s, Estimated time remaining: 0:03:13)

```",2018-03-04T00:25:44Z,11463820
1245,hashicorp/vagrant,57258770,374049900,"I would like to know how can I manually add the .box file in an offline manner. It is not clear in the documents on how to do that. I think the correct place for modification is Vagrantfile. However, I can not figure out which parameter should I change.",2018-03-18T21:29:31Z,11626212
1246,hashicorp/vagrant,57258770,374318439,@mahmoodn Your question is totally unrelated to this thread. The [box documentation](https://www.vagrantup.com/docs/boxes.html) should help you with your question.,2018-03-19T18:31:13Z,2126835
1247,hashicorp/vagrant,57258770,375837495,"```
https://vagrantcloud.com/laravel/boxes/homestead/versions/5.2.0/providers/virtualbox.box
    homestead-7: Progress: 36% (Rate: 95492/s, Estimated time remaining: 0:54:16)
```
99% of the time, this is what I'm dealing with. On a stable fibre connection. No WiFi. Every now and again I get a short burst of speed that pushes progress up around 5%, but overall, I'm left waiting several hours. This is insane. People have proposed work arounds and mitigations but those aren't solutions. When this issue was first closed, it was implied that `curl` might be to blame. Perhaps this is true, and if it is, then it should be switched out for a different solution.",2018-03-24T01:41:41Z,17376090
1248,hashicorp/vagrant,57258770,377623779,"I suffer the same problem also 

My connection on speedtest.net 6.6Mbps while the download takes only 0.6 Mbps of my bandwidth 


![image](https://user-images.githubusercontent.com/33742499/38153904-5e74252a-346f-11e8-9623-e891b6bc3466.png)

![image](https://user-images.githubusercontent.com/33742499/38153877-41067678-346f-11e8-9a15-3ef259286fa9.png)
",2018-03-30T21:08:30Z,33742499
1249,hashicorp/vagrant,57258770,383594627,"Hi everyone,

This issue overall is very complicated in that it is not easily reproducible and is very dependent on location, network health, and the actual download location of the target box. In many cases the box being downloaded is located on a third party system. One feature that was introduced in Vagrant a couple releases ago was the notification of redirect to show when boxes are being downloaded from a third party server.

There have also been claims that the embedded curl is downloading files slower than the system curl. I have not been successful in validating this claim and do not see a difference in download speed between my system curl and the embedded curl. However, as of Vagrant 2.0.4, the [`VAGRANT_PREFER_SYSTEM_BINS`](https://www.vagrantup.com/docs/other/environmental-variables.html#vagrant_prefer_system_bin) environment variable is available for all platforms. When enabled, Vagrant will use local system binaries if available instead of the embedded binaries. If you observe faster download speeds with your local curl binary, this provides a switch to easily enable Vagrant to use the local binary.

My apologies for the frustration this issue has caused. It is something I have investigated multiple times, but the number of variables involved makes this extremely difficult to get a valid reproduction. I have updated Vagrant's functionality to make the underlying cause more easily understandable where available (redirect notifications) or easier to work around. I am continuing to investigate our options for box downloads from Vagrant Cloud in different regions of the world (which have occasionally presented with issues). However, with box downloads from third party locations, there is little I am able to do to control their available bandwidth or any kind of throttling they may impose.

Cheers!",2018-04-23T14:26:45Z,266674
1250,mitmproxy/mitmproxy,1656534314,1656534314,"Firstly, I just discovered `mitmproxy` and I'm pretty excited to make use of it.  Thank you for maintaining it.

#### Problem Description

I'm trying to use mitmproxy as part of an integration test in a project that depends on `cryptography`, specifically version `40.0.0`.  I see that mitmproxy requires cryptography at `<38.1`.  I can automate the creation of a separate venv for the mitmproxy stuff, but that's kind of clunky.  I'd rather just make it a dependency and let poetry handle it.  For that to happen, `mitmproxy` needs to be compatible with higher versions of cryptography.

I recognize that the `<38.1` constraint might be there for a good reason, but not knowing that reason I figured I'd file an issue and cross my fingers.  Thanks for at least considering it.

#### Steps to reproduce the behavior:

1. Have a `pyproject.toml` like this:
```
...

[tool.poetry.dependencies]
python = ""^3.10""
cryptography = ""^40.0.0""

[build-system]
requires = [""poetry-core""]
build-backend = ""poetry.core.masonry.api""
```

2. Run `poetry add mitmproxy`
3. See error:
```
Because no versions of mitmproxy match >9.0.1,<10.0.0
 and mitmproxy (9.0.1) depends on cryptography (>=38.0,<38.1), mitmproxy (>=9.0.1,<10.0.0) requires cryptography (>=38.0,<38.1).
So, because foo depends on both cryptography (^40.0.0) and mitmproxy (^9.0.1), version solving failed.
```

#### System Information
```
Mitmproxy: 9.0.1
Python:    3.10.9
OpenSSL:   OpenSSL 1.1.1s  1 Nov 2022
Platform:  Linux-5.15.86-x86_64-with-glibc2.35
```",2023-04-06T01:34:13Z,5834582
1251,mitmproxy/mitmproxy,1656534314,1498535463,"We already upgraded, you have to wait for the next release.

https://github.com/mitmproxy/mitmproxy/blob/be02b1e298dfb1b218007f824c4cf345c53b0cee/pyproject.toml#L37
https://github.com/mitmproxy/mitmproxy/commit/8c6ec5cb56fbf4961806ed27b4974c440df59e87

https://github.com/search?q=repo%3Amitmproxy%2Fmitmproxy+cryptography&type=issues
https://github.com/mitmproxy/mitmproxy/issues/5966

Feel free to re-open if I miss something. I have no idea what `poetry` is.",2023-04-06T06:01:59Z,679144
1252,mitmproxy/mitmproxy,1656534314,1563938565,"@Prinzhorn @mhils Just curious why are we not releasing the fix even though [CVE-2023-0286](https://nvd.nist.gov/vuln/detail/CVE-2023-0286)   is a **High** severity vulnerability ? Developers who are already using library `cryptography > 39.0.1` won't be able to use `mitmproxy`. 

Other libraries have already released the fix. For example, https://pypi.org/project/pyOpenSSL/23.1.0/

",2023-05-26T07:33:57Z,80242052
1253,mitmproxy/mitmproxy,1656534314,1563961771,"https://github.com/mitmproxy/mitmproxy/issues/5966#issuecomment-1455045917

> I don't think either of the posted vulnerabilities affects mitmproxy users in a significant way, so I don't think we will ship a patch release just for that. :)",2023-05-26T07:52:54Z,679144
1254,mitmproxy/mitmproxy,1656534314,1636320181,"@Prinzhorn Do you have a target date for the next release?  Our customers in regulated industries (banking, federal, etc.) are prohibited by regulations from using s/w with known High and Critical severity vulnerabilities. I'm trying to formulate our case for waiting, but need some kind of target date.",2023-07-14T19:27:41Z,440156
1255,mitmproxy/mitmproxy,1656534314,1636352355,"@FrugalGuy: If you are interested in timely patch releases to fix your company's compliance requirements, I'm happy to set up a support contract. Email is on my profile. :-)",2023-07-14T19:58:38Z,1019198
1256,mitmproxy/mitmproxy,1656534314,1636814071,"Since this has been picked up by some news sites I'd like to provide some additional context here: I don't mind the release question at all, it's valid! But the context should ideally be along the lines of ""we have an interest in this, how can we help make it happen"" (contributions or $) and not ""you are causing problems for our customers"". I don't want the requestor to have a miserable time because of a badly-worded comment, I want large companies to have a healthy relationship with FOSS.",2023-07-15T16:17:30Z,1019198
1257,mitmproxy/mitmproxy,1656534314,1636827703,"@FrugalGuy has just sent me genuine apology, which I truly appreciate. Please be nice and assume good intentions. ❤",2023-07-15T17:08:40Z,1019198
1258,mitmproxy/mitmproxy,595484179,595484179,"#### Problem Description
mitmproxy --mode socks5 --listen-port 1080 --ssl-insecure

""Error in HTTP connection: Certificate verification error for None: self signed certificate in certificate chain (errno: 19, depth: 2)
clientdisconnect
clientconnect
Certificate verification error for None: self signed certificate in certificate chain (errno: 19, depth: 2)
Invalid certificate, closing connection. Pass --ssl-insecure to disable validation.""

#### Steps to reproduce the behavior:
Just open anything with self-signed cert, socks5 mode.

#### System Information
> Paste the output of ""mitmproxy --version"" here.
Mitmproxy: 5.0.1
Python:    3.8.2
OpenSSL:   OpenSSL 1.1.1f  31 Mar 2020
Platform:  macOS-10.15.3-x86_64-i386-64bit
",2020-04-06T23:13:40Z,2836790
1259,mitmproxy/mitmproxy,595484179,611039159,"I can't reproduce this.
```
λ curl -k --socks5-hostname localhost:1080 https://self-signed.badssl.com/
```
works fine. Here's how I invoke mitmproxy:
```
λ mitmdump --mode socks5 --listen-port 1080 --ssl-insecure
Proxy server listening at http://*:1080
127.0.0.1:52598: clientconnect
::ffff:127.0.0.1:52598: Certificate verification error for self-signed.badssl.com: self signed certificate (errno: 18, d
epth: 0)
::ffff:127.0.0.1:52598: Ignoring server verification error, continuing with connection
127.0.0.1:52598: GET https://self-signed.badssl.com/
              << 200 OK 502b
127.0.0.1:52598: clientdisconnect
```

",2020-04-08T15:51:52Z,1019198
1260,mitmproxy/mitmproxy,595484179,611334177,"You kidding me, right? There is a bunch of reports **of the same issue** in your issue tracker. I've posted an exact output of the log. If you can't reproduce, than this **is solely your problem**, but it doesn't meant bug doesn't exist. Perhaps you should follow what I have wrote, that it's on MACOS.
Also why don't you try to access a naked ip host with self signed cert? Did I mention it has to be domain?
I used ssl split and it worked fine, comparing to mitmproxy. Bye!",2020-04-09T05:21:06Z,2836790
1261,mitmproxy/mitmproxy,595484179,611335212,"Whoever will find this issue and gets pissed off, because author doesn't bother to fix it for years, **ditch mitmproxy and use SSL SPLIT**",2020-04-09T05:24:50Z,2836790
1262,mitmproxy/mitmproxy,595484179,611360996,"I'm glad you have found something that works for you! 

Closing this as a duplicate of #3865.",2020-04-09T06:49:03Z,1019198
1263,curl/curl,993768928,993768928,"<!-- Only file bugs here! Ask questions on the mailing lists https://curl.se/mail/

     SECURITY RELATED? Post it here: https://hackerone.com/curl

     There are collections of known issues to be aware of:
     https://curl.se/docs/knownbugs.html
     https://curl.se/docs/todo.html       -->

### I did this
In MS Windows Command Prompt:
curl.exe -C 1 --compressed -o HTML.txt https://curl.se/windows/
** Resuming transfer from byte position 1
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
 37  2395   37   904    0     0    221      0  0:00:10  0:00:04  0:00:06   222
curl: (61) Error while processing content unencoding: incorrect header check

### I expected the following
Error №2 or №3.

### curl/libcurl version
Build: 7.78.0_6
[curl -V output]
curl 7.78.0 (x86_64-pc-win32) libcurl/7.78.0 OpenSSL/3.0.0 (Schannel) zlib/1.2.11 brotli/1.0.9 zstd/1.5.0 libidn2/2.3.2 libssh2/1.10.0 nghttp2/1.44.0 libgsasl/1.10.0
Release-Date: 2021-07-21
Protocols: dict file ftp ftps gopher gophers http https imap imaps ldap ldaps mqtt pop3 pop3s rtsp scp sftp smb smbs smtp smtps telnet tftp
Features: alt-svc AsynchDNS brotli gsasl HSTS HTTP2 HTTPS-proxy IDN IPv6 Kerberos Largefile libz MultiSSL NTLM SPNEGO SSL SSPI TLS-SRP UnixSockets zstd
### operating system
MS Windows 10-21H1
<!-- On Unix please post the output of ""uname -a"" -->
",2021-09-11T06:58:54Z,53446248
1264,curl/curl,993768928,917359968,"~~AFAIK there's no way to resume compressed data like that which is why you see bad header.~~ The data is needed from the start to decompress. We should probably warn about it or stop it entirely.

slight edit: There is a way transfer encoding but it's hardly supported. curl could ask for the range of resource and then that range of resource is compressed (as opposed to requesting it compressed first then the requesting the range of that)",2021-09-11T07:34:04Z,965580
1265,curl/curl,993768928,917405947,"> The data is needed from the start to decompress.

Here is what experts who configure servers write.  Experts, not diletants.
curl.exe --compressed -I https://curl.se/windows/
HTTP/2 200
server: nginx/1.17.6
content-type: text/html
x-frame-options: SAMEORIGIN
last-modified: Thu, 09 Sep 2021 03:25:04 GMT
etag: ""1eea-5cb878b724db1-gzip""                                 ← Even here «gzip» mentioned.
cache-control: max-age=60
expires: Fri, 10 Sep 2021 18:41:54 GMT
content-encoding: gzip                                 ← «gzip» again.
x-content-type-options: nosniff
content-security-policy: default-src 'self' curl.haxx.se www.curl.se curl.se www.fastly-insights.com fastly-insights.com; style-src 'unsafe-inline' 'self' curl.haxx.se www.curl.se curl.se
strict-transport-security: max-age=31536000
via: 1.1 varnish, 1.1 varnish
accept-ranges: bytes                                 ← Resume (i.e. option «--continue-at») is supported.
date: Sat, 11 Sep 2021 12:55:10 GMT
age: 0
x-served-by: cache-bma1632-BMA, cache-fty21367-FTY
x-cache: MISS, HIT
x-cache-hits: 0, 1
x-timer: S1631364910.118245,VS0,VE408
vary: Accept-Encoding
content-length: 2396

If you want to be a specialist, read the full☺ manuals (i.e. RTFM).",2021-09-11T13:15:01Z,53446248
1266,curl/curl,993768928,917416556,"From manual:
«43     Internal error. A function was called with a bad parameter.»

For this case instead of error №61 or №3.",2021-09-11T14:27:24Z,53446248
1267,curl/curl,993768928,917481544,"I don't think the error code is the primary problem here. The problem is rather that if the combination of options doesn't work, curl should warn about it earlier...",2021-09-11T21:15:26Z,177011
1268,curl/curl,993768928,917486526,"> curl should warn about it earlier

Of course you are right.  Thank you!",2021-09-11T21:57:50Z,53446248
1269,curl/curl,993768928,917490126,"To Badger: please, look at combination of options «-C» and «-JO»:
curl.exe -C 1 -J -L -O https://github.com/PowerShell/vscode-powershell/releases/download/v2021.9.0/powershell-2021.9.0.vsix
curl: (23) Failed writing header

HEADER, not output.  Real dump of headers terminates on the sixth line of the second header.",2021-09-11T22:04:05Z,53446248
1270,curl/curl,993768928,917517699,"To Badger: please, look at combination of options «-JO» and main page of your site:
curl.exe -J -O https://curl.se/windows/
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0Warning: Remote filename has no length!
 11  7914   11   928    0     0    360      0  0:00:21  0:00:02  0:00:19   360
curl: (23) Failure writing output to destination

But the «size_download» NOT equals to zero.  Downloaded, but NOT SAVED.  What is the point?
And I can not get the warning «Remote filename has no length!» from the script.  Only the «errormsg» is available.",2021-09-12T01:06:43Z,53446248
1271,doctrine/migrations,1863541325,1863541325,"### Bug Report

<!-- Fill in the relevant information below to help triage your issue. -->

|    Q        |   A
|------------ | ------
| BC Break    | yes
| Version     | I have now idea how to find it

#### Summary

I can't rename migration file from Version{date} to Migration{date}

#### Current behavior

I HAVE to use illogical naming Version. But I'm extending Migration. It does not make any sence.

#### How to reproduce

Rename Version{date}.php to Migration{Date}.php as it SHOULD be.


#### Expected behavior

Find what whatever file I want

",2023-08-23T15:15:13Z,8470476
1272,doctrine/migrations,1863541325,1690576622,Manners.,2023-08-23T20:14:36Z,657779
1273,mrdoob/three.js,1137539452,1137539452,"<!-- Ignoring this template may result in your bug report getting deleted -->

**Describe the bug**

I'm building a small game that uses some primitive shapes for things.
Whenever I use a PolyhedronGeometry (like DodecahedronGeometry or IcosahedronGeometry) in an instancedMesh, my GPU tanks to ~5FPS (depending on the amount of instances).

I've spent hours figuring out what is happening and yesterday it looked like `powerPreference: 'high-performance'` was the issue (disabling solved the issue), but today it's back again with the same issues.

So this might not be threejs directly, but rather a webgl problem?

it's very hard to reproduce, because in FireFox this doesn't happen, only in Chrome, on my 2016 MacBook pro, so I feel like it might even be the webgl implementation of Chrome itself.

Interestingly enough FPS are just fine with other geometries (like box, or cone, etc), just not any of the PolyhedronGeometry based ones.

**To Reproduce**

You can clone my repo (https://github.com/TheDutchCoder/atc-gjk) and on the `convert-to-native-js` branch you can run `npm run dev`.

If you see good FPS, please let me know which browser and OS you're using, because again I feel like this might even be a browser problem.

Disclaimer: code is a bit of a mess currently because I'm moving things around and trying to fix this issue.

***Code***

See repo.

***Live example***

N/A (see repo)

**Expected behavior**

When using a PolyhedronGeometry I should not see a huge drop in FPS.

**Screenshots**

![image](https://user-images.githubusercontent.com/1668207/153903237-5989947d-16c9-422a-b737-63fea8f00ab4.png)
Note that the performance stats seem normal

**Platform:**

 - Device: Desktop, MacBook Pro 2016
 - OS: MacOS 12.1
 - Browser: Chrome 98.0.4758.80
 - Three.js version: r137 (latest)
",2022-02-14T16:23:16Z,1668207
1274,mrdoob/three.js,1137539452,1039289429,"Sorry, we are not able to clone repos and debug user's code.
Please, provide a jsfiddle that shows the issue. Closing until then.",2022-02-14T16:27:03Z,97088
1275,mrdoob/three.js,1137539452,1039291730,"Well that's unfortunate, because it really does happen. Here's a GPU profile:
![image](https://user-images.githubusercontent.com/1668207/153904707-571fc306-d648-4d95-a291-98270223555b.png)

The throttled start is using DodecahedronGeometry, the spiked usages is using BoxGeometry.

I'm not sure if this is reproducible in a service like JSFiddle, as it's not the same as running a local instance, but I might give it a shot.",2022-02-14T16:29:08Z,1668207
1276,mrdoob/three.js,1137539452,1039301833,"> Disclaimer: code is a bit of a mess currently because I'm moving things around and trying to fix this issue.

We try to give you (and all the library users) clear code.
Would be nice if you could help us back by giving us clear code too.
",2022-02-14T16:38:21Z,97088
1277,mrdoob/three.js,1137539452,1039311346,"It's clear enough to see what's going on. If you want I can make a branch with the bare minimum (it would just have certain files removed).

I'm not sure why sharing a repo is an issue? It's quite literally the same as standing up a new one as a repro ;)",2022-02-14T16:47:13Z,1668207
1278,mrdoob/three.js,518811026,518811026,"Updated from v92 to v110 and have flicker on the edges.  I use PlaneBufferGeometry with null z values to represent unknown elevations(holes).  The sparse grid shows a flicker where the nulls' edges are.  Was not seen with flatShading: true or v92:

![image](https://user-images.githubusercontent.com/18248938/68339808-812af700-00a2-11ea-8f01-ec761d009580.png)

I know you are going to say NaN are not supported in geometries, but any chance you could fix this or point me in proper direction?


##### Three.js version

- [ ] Dev
- [x ] r110
- [ ] ...

##### Browser

- [x] All of them
- [ ] Chrome
- [ ] Firefox
- [ ] Internet Explorer

##### OS

- [ ] All of them
- [x] Windows
- [ ] macOS
- [ ] Linux
- [ ] Android
- [ ] iOS

##### Hardware Requirements (graphics card, VR Device, ...)
",2019-11-06T21:36:20Z,18248938
1279,mrdoob/three.js,518811026,550519724,"Its something that occurred between 103 and 104.  103 works, 104 flickers.... Looking at change log",2019-11-06T21:59:27Z,18248938
1280,mrdoob/three.js,518811026,551033188,I'm afraid a screenshot is not sufficient to debug this issue. Please always provide live examples when reporting such problems.,2019-11-07T11:06:42Z,12612165
1281,mrdoob/three.js,518811026,551102563,"thank you for your help;  Flicker is seen when not moving.

https://jsfiddle.net/b4xzmqcr/

",2019-11-07T14:28:39Z,18248938
1282,mrdoob/three.js,518811026,551115996,"The issue gets worse when multiple 3js windows are open and active(open the fiddle in 2 windows).  Also noticeably when ""lightShot"" screen capture was opened.  Also noticed when whats app message received.  I have a dev team of 4 guys finding inconsistencies of producing on different graphics cards with different results, but all see a flicker at different times",2019-11-07T15:00:14Z,18248938
1283,mrdoob/three.js,518811026,551129187,"Sorry, but I can't see any flickering on my system. Tested with Chrome 78.0.3904.97, FF 70.0.1 and macOS 0.14.6. Are you on Windows?",2019-11-07T15:30:56Z,12612165
1284,mrdoob/three.js,518811026,551142905,"video: Multiple instances of the window seem to be the issue, even with duplicate fiddles it appears.

[here](https://drive.google.com/open?id=1zgscsAoXE1DNdWyIW1qORhXgatCRhKL1)

windows version 1809 OS Build 17763.805
NVIDIA GeForce RTX 2080 SUPER

I have also tested on msi lapiop with NVIDIA GeForce GTX 980M and the problem is probably worse.

All other devs have windows machines, but we could check on a mac if that helps
",2019-11-07T16:00:01Z,18248938
1285,mrdoob/three.js,518811026,551155765,"looks like a Problem with the nvideo driver?  i tested this on two Systems, with amd driver (linux) without any problem 


",2019-11-07T16:27:41Z,14173
1286,mrdoob/three.js,518811026,551164574,"I haven't gone through the diff of 103-104, but ideas on where to look that would make this happen?

",2019-11-07T16:47:40Z,18248938
1287,mrdoob/three.js,518811026,551177321,"Was able to replicate on Windows with NVidia GPU, but not on Mac with intel integrated GPU. 

Couldn't find anything on the change logs from `r103 -> r104` that would explain that. 

But, then again, you are making use of unsupported behavior, as you said it yourself.",2019-11-07T17:18:08Z,3927951
1288,mrdoob/three.js,518811026,551184591,Seems very odd that separate instances of three are causing it.  Even though unsupported seems like it could be a canary for a scoping issue?,2019-11-07T17:35:41Z,18248938
1289,mrdoob/three.js,518811026,551397312,"why not just map NaNs to some big but finite values to move the triangles out of the view. that would mean removing the index attribute as well, however (rough idea: https://jsfiddle.net/uxwqyjvm/ )",2019-11-08T06:02:51Z,242577
1290,mrdoob/three.js,518811026,551619400,"In any event, I don't see a `three.js` bug here. I suppose this happens also with pure WebGL and with any other 3D engine so I think it's more correct to close the issue and move the dicussion to stackoverflow or the forum.",2019-11-08T11:25:20Z,12612165
1291,mrdoob/three.js,518811026,551764216,"@makc We are constantly updating the surface with new known position.  Might be worth testing but would take a considerable amount of work.  If anything this would double the overhead(two geometries needed) of multi-million point grid geometries.  Do you have any docs for ExplodeModifier?  Also we have to have bufferGeometry.  These are very large surface models.

@Mugen87 I do not think that is true because the issue is not seen in r103.  There was a change in the code base that introduced this bug.  If it never worked, then i would agree it is a  webGL/GPU issue, but it was working up to r103 and we have it in production at r92 for a pretty long time(since r92 release)",2019-11-08T13:08:08Z,18248938
1292,mrdoob/three.js,518811026,551851903,"So there were two shader changes in the revision in the build.  
EDIT

single change then adding that change in so this is messing it up.
sorry my github skills suck , we are svn:

it is here in light_pars_begin lines 1-40:
```

export default /* glsl */`
uniform vec3 ambientLightColor;
uniform vec3 lightProbe[ 9 ];
// get the irradiance (radiance convolved with cosine lobe) at the point 'normal' on the unit sphere
// source: https://graphics.stanford.edu/papers/envmap/envmap.pdf
vec3 shGetIrradianceAt( in vec3 normal, in vec3 shCoefficients[ 9 ] ) {
	// normal is assumed to have unit length
	float x = normal.x, y = normal.y, z = normal.z;
	// band 0
	vec3 result = shCoefficients[ 0 ] * 0.886227;
	// band 1
	result += shCoefficients[ 1 ] * 2.0 * 0.511664 * y;
	result += shCoefficients[ 2 ] * 2.0 * 0.511664 * z;
	result += shCoefficients[ 3 ] * 2.0 * 0.511664 * x;
	// band 2
	result += shCoefficients[ 4 ] * 2.0 * 0.429043 * x * y;
	result += shCoefficients[ 5 ] * 2.0 * 0.429043 * y * z;
	result += shCoefficients[ 6 ] * ( 0.743125 * z * z - 0.247708 );
	result += shCoefficients[ 7 ] * 2.0 * 0.429043 * x * z;
	result += shCoefficients[ 8 ] * 0.429043 * ( x * x - y * y );
	return result;
}
vec3 getLightProbeIrradiance( const in vec3 lightProbe[ 9 ], const in GeometricContext geometry ) {
	vec3 worldNormal = inverseTransformDirection( geometry.normal, viewMatrix );
	vec3 irradiance = shGetIrradianceAt( worldNormal, lightProbe );
	return irradiance;
}
```






",2019-11-08T14:37:30Z,18248938
1293,mrdoob/three.js,518811026,551859368,"looks like probe light is causing this issue @WestLangley  @sciecode :

Added support for light probes. [#16223](https://github.com/mrdoob/three.js/pull/16223)

Edit as i walk through for future refrence:
1.  Light probe is part of ambient light so removing ambient light fixes the issue. 
2. light probe is a measure of ambient light not a child.  So no ambient light no light probe?
3. Hemisphere lighting does not show flicker
4. Hemisphere white light is super flickery
5.  Looks like the more white the light the worse the flicker, equally bad on hemisphere and ambient.",2019-11-08T14:56:42Z,18248938
1294,mrdoob/three.js,518811026,551883083,"updated fiddle with super flicker:
https://jsfiddle.net/sxjemofg/1/

turn off ambient light for no flicker",2019-11-08T15:57:19Z,18248938
1295,mrdoob/three.js,518811026,551904425,"RESOLVED

fixed it.  Is this worth committing?  Someone who knows get and three should commit this 👍 @WestLangley 
@Mugen87 


```
vec3 shGetIrradianceAt( in vec3 normal, in vec3 shCoefficients[ 9 ] ) {
	// normal is assumed to have unit length
	float x = normal.x, y = normal.y, z = normal.z;
	// band 0
	vec3 result = shCoefficients[ 0 ] * 0.886227;

      //FIX
      // in webgl 2.0 this could be replaced with  isnan(val)
      if(
              (! ( x < 0.0 || x > 0.0 || x == 0.0 )) || 
              (! ( y < 0.0 || y > 0.0 || y == 0.0 )) || 
              (! ( z < 0.0 || z > 0.0 || z == 0.0 ))
        ){ return result; };
      // response from dev who helped fix this:
      // ""it is a bug because they should always account for NaN 
      // whether it is intended or not ;)""
      //END FIX

	// band 1
	result += shCoefficients[ 1 ] * 2.0 * 0.511664 * y;
	result += shCoefficients[ 2 ] * 2.0 * 0.511664 * z;
	result += shCoefficients[ 3 ] * 2.0 * 0.511664 * x;
	// band 2
	result += shCoefficients[ 4 ] * 2.0 * 0.429043 * x * y;
	result += shCoefficients[ 5 ] * 2.0 * 0.429043 * y * z;
	result += shCoefficients[ 6 ] * ( 0.743125 * z * z - 0.247708 );
	result += shCoefficients[ 7 ] * 2.0 * 0.429043 * x * z;
	result += shCoefficients[ 8 ] * 0.429043 * ( x * x - y * y );
	return result;
}
```",2019-11-08T16:51:14Z,18248938
1296,mrdoob/three.js,518811026,551911419,"> I use PlaneBufferGeometry with null z values to represent unknown elevations(holes). 

You have NaN's in your position data.

You also have NaN's in your vertex normals. Hence their length is undefined.

three.js does not support NaNs in data pushed to the GPU.",2019-11-08T17:09:10Z,1000017
1297,mrdoob/three.js,518811026,551912932,"one should always account for NaN whether it is intended or not.  And its a simple fix.  We need this to work for our production and it doesnt harm anything having a check in.   Could we please add, is there any harm?  @WestLangley ",2019-11-08T17:13:23Z,18248938
1298,mrdoob/three.js,518811026,551917382,">one should always account for NaN whether it is intended or not.

No, YOU need to account for it in your app and pass valid data.

",2019-11-08T17:25:42Z,1000017
1299,mrdoob/three.js,518811026,551921717,"Yes, this is something that needs to be fixed on app level.

> three.js does not support NaNs in data pushed to the GPU.

NaN geometry data also corrupt operations on the CPU like intersection tests or the computation of bounding volumes. So again, this is no library bug.",2019-11-08T17:37:44Z,12612165
1300,mrdoob/three.js,518811026,551925337,"We have a valid use case for putting NaN's in our data set that as far as I can tell can not be done in any other way, efficiently.  In any real world application dealing with NaN's is just a fact of life .

@Mugen87 Bounding Volumes was actually my last post.  It all stems from not handling NaN's properly.  Just a simple test for isNaN would resolve most these.  Why is there such push back and not a desire to resolve this issue?

Are there any suggestions for dealing with real world data that has NaN's?  We have bandied around alpha mapping, but then we increase overhead with the map and need to access the alpha map and the vertex positions.  

Also what if the numbers are very large and overflows?  Will that cause a NaN that might need to be dealt with.",2019-11-08T17:48:06Z,18248938
1301,mrdoob/three.js,518811026,551929007,"> Why is there such push back and not a desire to resolve this issue?

`NaN` values is an app level problem from our point of view since the user is in some sense the data producer. Geometric data like position, normal or color data are meant to be numerical. `NaN` data are no numerical data.

Handling `NaN` values is use case specific since proper default values depend on what the application actually does.

This is not only an issue in graphics. When you process huge data in context of KDD and machine learning, data cleanup is one important preparation step before you execute the actual algorithm (which is not responsible for handling missing, undefined or corrupt values).",2019-11-08T17:58:42Z,12612165
1302,mrdoob/three.js,518811026,551934130,"so this is a ""no"" on resolving?  Is there a reason not to other than ""we don't support this""?  Its a one line PR.

I guess we will be patching every update from here out.",2019-11-08T18:13:55Z,18248938
1303,mrdoob/three.js,518811026,551935362,"three.js does not validate data -- much less accommodate invalid data. That is the app's responsibility.

Also, your patch accommodates your use case, but it does not accommodate an arbitrary use case with invalid data. So, your patch is not a ""fix"".",2019-11-08T18:17:37Z,1000017
1304,mrdoob/three.js,518811026,551936446,">I guess we will be patching every update from here out.

One way to handle your examples is to add an ""invalid"" attribute: 0 if valid; 1 if invalid. Then discard fragments for which the interpolated attribute value is non-zero.",2019-11-08T18:20:48Z,1000017
1305,mrdoob/three.js,518811026,551939312,"> Also, your patch accommodates your use case, but it does not accommodate an arbitrary use case with invalid data. So, your patch is not a ""fix"".

 what case would it not fix?  It fixes unknowns in position array from flickering. Could I add a switch to turn off probe lighting?

> One way to handle your examples is to add an ""invalid"" attribute: 0 if valid; 1 if invalid. Then discard fragments for which the interpolated attribute value is non-zero.

Where woudl you implement this.  Seems alot like an alpha map but culling vertices else where?

Edit: thanks for the help.  

> three.js does not validate data -- much less accommodate invalid data. That is the app's responsibility.

its semantics(and not my project)  but in my mind ""unknown"" != ""invalid"". ",2019-11-08T18:28:52Z,18248938
1306,mrdoob/three.js,518811026,551948923,"My [suggestion](https://github.com/mrdoob/three.js/issues/17882#issuecomment-551936446) is similar to alpha-testing and would require injecting code into the fragment shader. Without knowing the details of your use case, I can only offer it as a suggestion.

Please use the three.js forum if you need additional help.",2019-11-08T18:57:12Z,1000017
1307,mrdoob/three.js,518811026,551994180,"@kpetrow 

> Just a simple test for isNaN would resolve most these. Why is there such push back and not a desire to resolve this issue?

You may have more luck by doing a PR with that ""simple test"" rather than asking us to do it for you.",2019-11-08T21:21:42Z,97088
1308,mrdoob/three.js,518811026,552050146,"@kpetrow,
> two geometries needed... ExplodeModifier

of course you could just create un-indexed buffer geometry to begin with, and not use ExplodeModifier or convert geometries otherwise.

> RESOLVED... Is this worth committing?

well, you know, a bunch of IF-s in the shader that almost noone would need any way... so, probably not.

> updated fiddle with super flicker

no flicker here at all",2019-11-09T01:17:21Z,242577
1309,mrdoob/three.js,518811026,552056351,"> You may have more luck by doing a PR with that ""simple test"" rather than asking us to do it for you.

yes the debt is too big to resolve, so lets never fix it @mrdoob .  Why not start here?  If I do a PR to fix this issue will get any traction? Or if i add a switch to light probe to turn on/off like shadows and transparency? 

> well, you know, a bunch of IF-s in the shader that almost noone would need any way... so, probably not.

@makc 
Single if.  You also turn off other features like shading, and transparency.  Why not make this new undocumented featured(only example) that effect primary lighting switchable?

> no flicker here at all

@makc 
Well great let me just ship your computer around so everyone can use it. Or even better we will buy everyone a plane ticket and then can come stand in line to use your computer where the software does work.",2019-11-09T02:19:51Z,18248938
1310,mrdoob/three.js,338480646,338480646,"This is the current (july 2018) GLTF status.
Test case: the same skinned mesh exported from 3DS Max 2018.
Standard material: Diffuse + Specular + Normal

1) SEA3D exporter + SEA3D importer:
http://necromanthus.com/Test/html5/Lara.html 
SEA file size: 658 KB
Result: close to perfect

2) Babylon3D GLTF exporter + GLTF importer:
http://necromanthus.com/Test/html5/Lara_gltf.html 
GLB file size: 1,850 KB
Result: messed up materials

I've also tested the FBX2GLTF utility (by Facebook): the same wrong results

Important note: there is nothing wrong with THREE.js and PBR materials:
http://necromanthus.com/Test/html5/Lara_PBR.html 

In any case, PBR was a bad choice for GLTF and also, all the current converters are collection of bugs.
",2018-07-05T08:46:40Z,11772787
1311,mrdoob/three.js,338480646,402707962,"I don't think this repo is the right place for this post. It's neither a feature request, nor a bug. So my question is: What are you trying to accomplish with this issue? Bashing `glTF`?

If you encounter problems with an exporter or converter, I suggest you open an issue at the respective github repo.

> In any case, PBR was a bad choice for GLTF and also, all the current converters are collection of bugs.

I generally reject such Trump-like statements. They have a provocative nature and are not objective at all.",2018-07-05T12:37:30Z,12612165
1312,mrdoob/three.js,338480646,402709204,"> I don't think this repo is the right place for this post.

It's the best place for sure and it shows the current GLTF status.

> I generally reject such Trump-like statements.

Really?
Here is a statement from Trump: Google already failed with UTF8.

> If you encounter problems with an exporter or converter, I suggest you open an issue at the respective github repo.

Many of them blame THREE.js for bad GLTF results.
The posted samples prove they are wrong.",2018-07-05T12:42:38Z,11772787
1313,mrdoob/three.js,338480646,402742155,@RemusMar are you suggesting any particular actions we should take? If not I vote to close this issue.,2018-07-05T14:31:57Z,5307958
1314,mrdoob/three.js,338480646,402745305,"> RemusMar are you suggesting any particular actions we should take?

Three possible causes for the wrong GLTF results:
1. the Babylon3D exporter is buggy (they say it's not)
2. the FBX2GLTF converter is buggy (they say it's not)
3. the GLTF importer is buggy.

But you and Mugen87 want to close the topic because there is no issue and everytbody is happy ...",2018-07-05T14:41:18Z,11772787
1315,mrdoob/three.js,338480646,402748668,"> the GLTF importer is buggy

You mean the GLTFLoader? Can you identify what the bug is? It would be especially helpful if you can find a very simple model that demonstrates the bug.

>  (they say it's not)

If you've made bug reports on the Babylon3D exporter and FBX2GLTF can you link to them here?",2018-07-05T14:51:12Z,5307958
1316,mrdoob/three.js,338480646,402751403,"> You mean the GLTFLoader? 

Yes.
Importer = Loader + Parser

> It would be especially helpful if you can find a very simple model that demonstrates the bug.

You have everything you need to study the issue.
Any PHONG material (Diffuse + Specular + Normal) exported or converted to GLTF gives wrong results.
p.s.
PHONG represents 50-60% of the current samples, compared to Physical less than 1%.
That's why I said that PBR was a bad choice for GLTF.",2018-07-05T14:59:04Z,11772787
1317,mrdoob/three.js,338480646,402754614,"> You have everything you need to study the issue

In other words you want someone else to do the work for you 🙄

Regarding Phong materials and glTF, I agree that this makes glTF a bad choice for converting older models - especially models originally exported as FBX which only supports Phong or Lambert shading.",2018-07-05T15:08:13Z,5307958
1318,mrdoob/three.js,338480646,402755130,"> n other words you want someone else to do the work for you

I don't have enough spare time for ""GitHub activities"".
You got the report and the working samples.
That's all for now.
cheers",2018-07-05T15:09:50Z,11772787
1319,mrdoob/three.js,338480646,402758865,"In that case, I still don't consider this to be a complete or actionable bug report and I continue to vote to close this issue.",2018-07-05T15:20:58Z,5307958
1320,mrdoob/three.js,338480646,402912258,"Hey @RemusMar,

Thanks for reporting this. Some notes...

**File size**
I'm not sure where you're getting these numbers, this is what I see:

```
Lara.sea: 1,032,525 bytes 
Lara.glb: 1,850,164 bytes

Lara.sea.gz: 1,031,840 bytes
Lara.glb.gz: 919,712 bytes 
```

**Materials**
Your model uses `Diffuse + Specular`. Unfortunately, seems like the specular texture is not being exported. GLTF supports 2 PBR modes: `Metalness + Roughness` and `Specular + Glossiness`. You want to export your model using the second mode. `GLTFLoader` supports both but maybe the Babylon.js doesn't have an option to export in that mode? In that case you may want to do a feature request on their project.

Let us know what you find out.",2018-07-06T03:06:29Z,97088
1321,mrdoob/three.js,338480646,402944564,"Hi Ricardo,

1. File size.
If you download the files (with Firefox) you'll get:
Lara.sea: 658,901 bytes
Lara.glb: 1,850,964 bytes
Even more: the GLB file does not even contain the equivalent (Metalness) of the Specular texture !

2) Materials
As I said before, the original MAX  and FBX files contain a standard PHONG material:
Diffuse + Specular + Normal 
None of the current GLTF exporters and converters is able to generate a correct Physical material.
PHONG is way more popular than Physical.

p.s.
I'm not interested in the GLTF format (SEA3D is better from any point of view).
I just want to help other poeple.",2018-07-06T06:56:27Z,11772787
1322,mrdoob/three.js,338480646,402991611,"Another userful sample:
Here is a NATIVE Physical material in 3DS Max 2018 exported to GLTF with Babylon3D exporter:
http://necromanthus.com/Test/html5/Lara_gltf_physical.html 
Now the metalness is present, but the result is still wrong:
- it looks emissive (but it's not)
- it has a red color bump

However, this GLB file looks better (compared to THREE) in the Babylon sandbox ( https://sandbox.babylonjs.com ).
Again, this is how the Physical material should look in THREE:
http://necromanthus.com/Test/html5/Lara_PBR.html 

So let's forget now about PHONG and buggy exporters and converters.
We should investigate why the GLB file looks better in the Babylon sandbox.
Buggy GLTF Loader in THREE ?",2018-07-06T10:11:28Z,11772787
1323,mrdoob/three.js,338480646,403126429,"If one could quickly prototype some hacks over the existing phong / standard implementations i bet it would be pretty useful 😉 

",2018-07-06T19:30:26Z,4681282
1324,mrdoob/three.js,338480646,403279309,"@RemusMar Seems like the glb includes a `AmbientLight`.

<img width=""766"" alt=""screen shot 2018-07-08 at 7 44 30 pm"" src=""https://user-images.githubusercontent.com/97088/42419015-7a49ede0-82e7-11e8-8067-ae27f0b0937a.png"">

If you set `visible` to `false` to the imported `AmbientLight` the character starts to look less red.

The last thing to do is setting `renderer.gammaOutput = true`. (Needed when using GLTF #12766)

<img width=""634"" alt=""screen shot 2018-07-08 at 7 48 00 pm"" src=""https://user-images.githubusercontent.com/97088/42419034-dca797b2-82e7-11e8-8548-e9f204aa7b54.png"">
",2018-07-08T10:53:47Z,97088
1325,mrdoob/three.js,338480646,403285464,"> Seems like the glb includes a AmbientLight.

Good catch Ricardo! :thumbsup: 

> If you set visible to false to the imported AmbientLight the character starts to look less red.

Something better (no wasted resources):
```javascript 
		gltf.scene.remove( gltf.scene.children[2] );
```

> The last thing to do is setting renderer.gammaOutput = true. (Needed when using GLTF #12766)

That indicates buggy GLTF Loader (and it has to be fixed).
If I use that for other loaders, I get wrong colors for the loaded models.
Just think about this scenario: use various loaders for the same scene and one of them is GLTF.
It will mess up the entire scene!

Anyway, after removing that ambient light and using this workaround, this is the result:
http://necromanthus.com/Test/html5/Lara_gltf_physical.html 
Much better compared to the initial GLTF one, but the material quality is far away from this one:
http://necromanthus.com/Test/html5/Lara_PBR.html 
Impressive lighting response and great metalness for bra and bikini.

At this stage I won't use GLTF in any serious project.
",2018-07-08T12:46:45Z,11772787
1326,mrdoob/three.js,338480646,403293414,Can you try adding a `envMap` cubemap to these examples too? PBR looks the best when a `envMap` is supplied.,2018-07-08T14:56:45Z,97088
1327,mrdoob/three.js,338480646,403295862,"> Can you try adding a envMap cubemap to these examples too?
> PBR looks the best when a envMap is supplied.

I've added offline.
Of course it looks better, but PHONG with Environment map still looks WAY better (in THREE).
Also, the envMap does not fix the GLTF Loader issue(s).",2018-07-08T15:34:07Z,11772787
1328,mrdoob/three.js,338480646,403301052,"> I've added offline.

Could you update the online samples?",2018-07-08T16:57:42Z,97088
1329,mrdoob/three.js,338480646,403344309,"The glTF format supports PBR and unlit shaders at this time. Whether the BabylonJS and FBX2GLTF tools do the Phong-to-PBR conversion in a way that preserves Phong specular maps, I don't know — that would be a question for the repos of those tools. If you are trying to preserve the exact appearance of models using classic Phong shaders, you may have an easier time with other formats.

> > The last thing to do is setting renderer.gammaOutput = true.
> 
> That indicates buggy GLTF Loader (and it has to be fixed).

This is a deliberate decision and not a bug. Base color and emissive textures in glTF (and, typically, diffuse textures in any format...) are in sRGB colorspace. GLTFLoader marks them as such (`material.map.encoding = THREE.sRGBEncoding`), so that they're converted to linear colorspace for correct PBR lighting calculations. Finally colors should be converted back to sRGB (e.g. `renderer.gammaOutput=true`).

If you skip all of this, with any format, lighting calculations are incorrect. SEA3DLoader, FBXLoader, and ColladaLoader never touch the `.encoding` property of any texture, and leave it to the end user to change texture colorspace and renderer colorspace. I'm pretty confident that the large majority of three.js users are passing sRGB colors into three.js without converting, despite the fact that renderer lighting calculations assume linear colorspace, and getting results that are ""good enough"" but inconsistent with other engines and authoring environments. For correct results you should be using `renderer.gammaOutput=true`, and marking sRGB textures as sRGB.

None of these issue are specific to glTF (see https://github.com/mrdoob/three.js/issues/11337), but with GLTFLoader we're trying to achieve consistency with other engines and 3D authoring environments, and have chosen to treat all sRGB textures as sRGB for a first step. If you're mixing models from other formats in the scene, then yes it's awkward, and you'd need to either mark the diffuse textures of those formats as sRGB or mark the colorspace on the glTF models to linear (the latter is incorrect for all model formats involved, but may look good enough if you don't need precise colors).

***

It does not seem like there is anything actionable here, unless there are specific issues we can report to the tools mentioned. @RemusMar if you are happy with your SEA3D workflow, that's great — I'm not interested in debating formats or persuading you to change from something that is already working well for you.",2018-07-09T02:43:59Z,1848368
1330,mrdoob/three.js,338480646,403376409,"> Could you update the online samples?

Ricardo,
PHONG looks great with Diffuse + Specular only.
PBR does not look great with BaseColor + MetallicRoughness only.
That's the main problem here.
The Normal/Bump and Environment textures are irrelevant at this point.
On top of that: more texture layers = bigger file size and performances drop

> If you skip all of this, with any format, lighting calculations are incorrect. SEA3DLoader, FBXLoader, and ColladaLoader never touch the .encoding property of any texture

That was a wise decision.

> I'm not interested in debating formats or persuading you to change from something that is already working well for you.

Don,
I'm not debating the ""PBR only"" bad choice for GLTF.
This topic shows GLTFLoader design flaws.
We don't reinvent the wheel here, so ""renderer.gammaOutput = true"" is not an option now, when GLTF represents less than 1% of the market.
cheers",2018-07-09T06:46:25Z,11772787
1331,mrdoob/three.js,338480646,403401100,"> Can you try adding a envMap cubemap to these examples too? PBR looks the best when a envMap is supplied.
> Could you update the online samples?

Because you asked me to:

SEA3D + Phong: http://necromanthus.com/Test/html5/Lara_envMap.html 
vs
GLTF + PBR: http://necromanthus.com/Test/html5/Lara_gltf_envMap.html 

The quality drop is obvious.
Also, in 3DS Max 2018 the Physical material looks WAY better than the GLTF result.",2018-07-09T08:28:15Z,11772787
1332,mrdoob/three.js,338480646,403461202,"> None of these issue are specific to glTF (see #11337), but with GLTFLoader we're trying to achieve consistency with other engines and 3D authoring environments,

That's completely wrong Don!
Here we're talking about GLTFLoader and THREE.js
The users are interested in the best results with THREE.
Other engines and 3D authoring environments are irrelevant here.

And you still don't understand the main problem here.
For the last time:

1) JSONLoader (or SEA3D loader) + PBR = GOOD results (close to Phong):
http://necromanthus.com/Test/html5/Lara_PBR.html 

2) GLTFLoader + PBR = BAD results
http://necromanthus.com/Test/html5/Lara_gltf_physical.html 

p.s.
In the first sample you don't even need ""renderer.gammaOutput = true"" to get good results !!!",2018-07-09T12:25:36Z,11772787
1333,mrdoob/three.js,338480646,403513554,"
> The users are interested in the best results with THREE. Other engines and 3D authoring environments are irrelevant here.

Being able to author a PBR model in Substance Painter or download one from Sketchfab, and have the model appear as the artist designed it, is good for three.js users. I don't think there's any definition of ""best result"" where that sort of consistency can be dismissed.

> This topic shows GLTFLoader design flaws. We don't reinvent the wheel here, so ""renderer.gammaOutput = true"" is not an option now...

This isn't reinventing any wheels, and it isn't a design flaw. PBR calculations are done in linear space, with every engine I'm aware of. If you pass sRGB data into the renderer and pretend it's linear, the lighting and blending math will come out wrong. The difference is not huge, and so this not a major concern for many three.js users, but nevertheless it is not as good as it could be. For that reason, your ""good"" result example is not actually correct. But as you've said before, backward-compatibility is important, so I'm not here to advocate for changing any three.js defaults. But because glTF is a new format, and because we're trying to get PBR right, we're going to mark sRGB textures as sRGB, even if other loaders are not doing so.

See [this article about Unreal](http://artbyplunkett.com/Unreal/unrealgamma.html) — 

> ...textures that are used for color information should have the sRGB flag checked, and textures that are used for masks and numerical calculations in shaders and effects (like Normal maps) should have it unchecked. And if you follow this simple guideline you mostly get the best effect.

This is precisely what we are doing.",2018-07-09T15:11:57Z,1848368
1334,mrdoob/three.js,338480646,403579264,"> ...I'm pretty confident that the large majority of three.js users are passing sRGB colors into three.js without converting, despite the fact that renderer lighting calculations assume linear colorspace

You are talking about PBR materials with glTF, but I assume this is just as much a problem with a Phong material?

> ...because glTF is a new format, and because we're trying to get PBR right, we're going to mark sRGB textures as sRGB, even if other loaders are not doing so.

@donmccurdy should other loaders be doing so? It seems like this inconsistency between loaders is a point of confusion for users, and it would make sense for all of them to treat sRGB textures the same way if possible. ",2018-07-09T18:39:25Z,5307958
1335,mrdoob/three.js,338480646,403590236,"> You are talking about PBR materials with glTF, but I assume this is just as much a problem with a Phong material?

Yes, the problem is the same for Phong materials or PBR materials loaded in any other format.

> ...should other loaders be doing so? It seems like this inconsistency between loaders is a point of confusion for users, and it would make sense for all of them to treat sRGB textures the same way if possible.

If we had a time machine, yes, the other loaders should also be marking sRGB textures containing color data as sRGB. But making the change now would cause confusion and break backward-compatibility, and the `gammaOutput=true` setting needed to fix output is not intuitive — I don't think changing other loaders can be justified given those issues.

Let's keep an eye on https://github.com/mrdoob/three.js/issues/11337. I hope the resolution there will make color workflows more intuitive. With that and NodeMaterial, there may be opportunities to fix some existing issues without breaking anyone's existing applications.",2018-07-09T19:16:30Z,1848368
1336,mrdoob/three.js,338480646,403717195,"> Being able to author a PBR model in Substance Painter or download one from Sketchfab,

They are irrelevant.
I get much better results in 3DS Max and that tells me that the GLTFLoader and/or your PBR model are not properly implemented.

> The difference is not huge, and so this not a major concern for many three.js users, but nevertheless it is not as good as it could be.

Your girlfriend looks bad but you're happy because your boss told you that's normal.
OMG ...

> For that reason, your ""good"" result example is not actually correct.

In fact you should fix your ""correct"" example to look good.",2018-07-10T06:40:10Z,11772787
1337,mrdoob/three.js,338480646,405027509,"> If we had a time machine, yes, the other loaders should also be marking sRGB textures containing color data as sRGB.  

Yeah, it's unfortunate but I agree that it's not worth breaking backwards compatibility over this. 

However, as I've been working with larger FBX scenes consisting of multiple models, animated cameras and so on I've found myself wishing that the output of the loader was something more like GLTFLoader's output - that is, it should return an `fbx` object with properties:

```
fbx.animations; // Array<THREE.AnimationClip>
fbx.models; // Array <THREE.Group, THREE.Mesh, THREE.SkinnedMesh>
fbx.cameras; // Array<THREE.Camera>
fbx.asset; // Object
```

There may be other loaders that would benefit from a similar change.  We should add this to the backburner (and certainly wait on #11337), but if any loaders do have breaking changes made for whatever reason, then we can use that as opportunity to apply this change as well. 

Perhaps we should open a new issue to keep track of this? ",2018-07-14T14:36:45Z,5307958
1338,mrdoob/three.js,338480646,405029116,"> Perhaps we should open a new issue to keep track of this?

Please do. That's better than resume the conversation in this closed thread.",2018-07-14T15:05:50Z,12612165
1339,mrdoob/three.js,338480646,406552359,"Just removed the FORCED sRGB encoding in the GLTFLoader.
```javascript
//			if ( material.map ) material.map.encoding = THREE.sRGBEncoding;
//			if ( material.emissiveMap ) material.emissiveMap.encoding = THREE.sRGBEncoding;
//			if ( material.specularMap ) material.specularMap.encoding = THREE.sRGBEncoding;
```

The result is better from any point of view:
- better lighting and material quality
- no need of ""renderer.gammaOutput = true"" anymore (a bad idea anyway)
- now you can use the GLTFLoader with other loaders for the same scene (renderer)

SEA3D + Phong: http://necromanthus.com/Test/html5/Lara_envMap.html
vs
GLTF + PBR: http://necromanthus.com/Test/html5/Lara_gltf_envMap.html
",2018-07-20T09:58:03Z,11772787
1340,mrdoob/three.js,338480646,406664594,"I've addressed each of those points in https://github.com/mrdoob/three.js/issues/14419#issuecomment-403513554 — we will not be removing the sRGB encoding assignment to sRGB textures in GLTFLoader. If you would like to override that, it is easy to change the texture encoding after loading the model.

If there are no other actions to take here, this issue should be closed.",2018-07-20T17:07:02Z,1848368
1341,mrdoob/three.js,338480646,406776895,"> we will not be removing the sRGB encoding assignment to sRGB textures in GLTFLoader.

That bad choice is yours, but the GLTFLoader is part of THREE, so it's up to Ricardo ( @mrdoob ) if they will be removed or not.
In any case, you (and @looeee ) continue to be on the wrong path.
Again: your workflow is not good and is not backwards compatible.
You should start learning from the more experienced people:
https://forum.allegorithmic.com/index.php?topic=8745.0 

**In Unity you don't have to do anything**. Maps that are placed in the metallic/smoothness, ambient occlusion are treated as linear by the shader and maps in the albedo are treated as sRGB. What the shader does for the albedo and specular is ""linearize"" the maps. It removes the gamma-encoded values from the map in the shader code by applying an inverse gamma to it of 0.4545. In the Unity workflow, this is done automatically and **you don't need to flag the images as sRGB**. ",2018-07-21T07:18:45Z,11772787
1342,mrdoob/three.js,338480646,406781388,">  In the Unity workflow, this is done automatically and you don't need to flag the images as sRGB.

Unity is treating textures as sRGB automatically rather than leaving it up to the user. This is exactly what the GLTFLoader does, and what we are arguing the other loaders should have been doing from the start. ",2018-07-21T08:44:29Z,5307958
1343,mrdoob/three.js,338480646,406781871,">That bad choice is yours, but the GLTFLoader is part of THREE, so it's up to Ricardo ( @mrdoob ) if they will be removed or not.

Errr.... this is extremely fuzzy. `GLTFLoader` appears to be a community provided example, that lives in `/examples`. If you load three.js alone (`three.min.js`) there wont be any mention of GLTF.

If you use three.js off the shelf you get a scenegraph and various webgl abstractions. In this context, GLTF is just another one of many many examples of how you can translate some generic 3d data / scene file into three.js's structs. 

So, at a glance, three.js seems like this generic library, that draws stuff to screen. It doesn't care if you fetch that data from some remote server, and it doesn't care how you parse it (collada, fbx, sea3d, gltf... and 40 others). At the end of the day, you are rendering a `THREE.Mesh` with `THREE.Geometry` and `THREE.Material`. **Absolutely all of the loaders share this feature. All of them result in this data structure.**

However, in practice, this is not the case, and you seem to be in the right. 

GLTF is a ""first class citizen"" of three.js. @mrdoob wrote that several times. SEA3d is some **random format** written by **some unknown people** while GLTF has the backing of THE khronos group. On top of that, it's probably meant to be the backbone of the whole VR/AR revolution, hence so much backing by other big players. 

This is just an unfortunate circumstance that three.js found itself in as the most widely used webgl library. People want to do 3d, which three.js solves really well with it's scene graph and other abstractions (things like `Mesh`, `Geometry`, `Material`, `Texture` etc.). Unfortunately, people also want to make experiences and expect three.js to be able to do that. This is of course complicated, hence, favoring one particular format and giving it preferential treatment makes sense. It may somewhat hurt the very essence of the library (draw stuff on screen) but it's a trade off. 

 If you care to read all the guidelines, there's a document called [owners](https://github.com/mrdoob/three.js/wiki/Owners). @donmccurdy owns that particular loader, so his word carries as much weight as @mrdoob's, so good luck there :)


The directive right now is:

>three.js must support gltf

So anything that the khronos group comes up with has to be reflected in three.js. If you look at the discussion historically, @mrdoob doesn't really follow what's going on and @donmccurdy is the authority on all things khronos/gltf related.

So for example, khronos has defined a specification for gltf ""extensions"". Out of infinitely many extensions, one involves texture transformations. It has been ratified by the khronos group and because of that, **three.js absolutely must support it**. This warrants a PR with **11 thousands line of code** and increasing the size of the library by 1/3. 

I think you're wasting a lot of effort head butting a wall here, and that this issue should be closed.  






",2018-07-21T08:54:16Z,4681282
1344,mrdoob/three.js,338480646,406782145,"> I think you're wasting a lot of effort head butting a wall here, and that this issue should be closed.

good point
As I said before, Google already failed with UTF8.
In the current development status, GLTF (and PBR) follow the same path.",2018-07-21T08:59:30Z,11772787
1345,mrdoob/three.js,338480646,406782214,">Unity is treating textures as sRGB automatically rather than leaving it up to the user. This is exactly what the GLTFLoader does, and what we are arguing the other loaders should have been doing from the start.

Unity has an editor which is a foreign concept for three.js. When you ""load"" a texture into unity, you bring it into the editor which can do all kinds of transformations before actually storing it for game's use.  I think that three.js has a different paradigm, since it doesn't care how you create the assets nor how you store them. An analogy would be if three.js came with folder `/utils` and then `/textureTools` or something like that, where you could prepare the assets for three.js. 

This difference should possibly be considered. 

The whole gamma correct workflow is extremely complicated, and if i recall correctly you may lose some precision if you store the textures in one way versus another. With three.js this is still kinda low-levelish so i'd prefer to leave multiple options to the user. ",2018-07-21T09:00:37Z,4681282
1346,mrdoob/three.js,338480646,406782575,"> If you care to read all the guidelines, there's a document called owners.

Wow, I never say that before. And apparently I ""own"" the `FBXLoader`  😍 😆",2018-07-21T09:07:27Z,5307958
1347,mrdoob/three.js,338480646,406782700,"I think it's a software engineering term. You ""own""  a code base?",2018-07-21T09:09:54Z,4681282
1348,mrdoob/three.js,338480646,406782918,"> And apparently I ""own"" the FBXLoader

So in your opinion you're free to mess it up, isn't it?",2018-07-21T09:14:30Z,11772787
1349,mrdoob/three.js,338480646,406782971,"In my opinion everyone who owns something is free to mess it up. I'm not sure if there should be place for disagreement with this, in the modern world :). If you have a Ferrari and decide to put a brick on the gas pedal and have it drive off a cliff, i may think it's a waste but it's YOUR Ferrari, can't get simpler than that. Unless it's 1917 and you're in tsarist Russia. ",2018-07-21T09:15:37Z,4681282
1350,mrdoob/three.js,338480646,406783201,"That right there is disruptive content... :(


You're free to fork the project, the examples and do whatever work you deem fit. If it makes you feel any better, what the fbx loader owner did is not at all different - the loader did exist for a couple of years and was written by other people before he took ownership. Fork's no different than this. ",2018-07-21T09:20:44Z,4681282
1351,mrdoob/three.js,338480646,406783296,"I think the documents wouldn't hurt by having some guideline on how to author textures. The texture can still be in the wrong space regardless of the gltf format, like under calibrated or over calibrated. Something saying ""when making a texture in photoshop do that, when making it in GIMP do this"". ",2018-07-21T09:22:40Z,4681282
1352,mrdoob/three.js,338480646,406785156,"> Cut the ""disruptive content"" crap and grow up !!!

@RemusMar I have no idea what you are talking about. I've never marked yours or anyone else's comments as disruptive.  

However, I would like to say that, much as I have a thick skin and don't care about your opinions re usernames, ad hominem attacks of the kind that you frequently make are not acceptable and should be grounds for being excluded from this community. ",2018-07-21T10:03:17Z,5307958
1353,mrdoob/three.js,338480646,406785665,"@pailhead agreed, that would be a very useful addition. ",2018-07-21T10:13:54Z,5307958
1354,mrdoob/three.js,338480646,406788004,"> I've never marked yours or anyone else's comments as disruptive.

I'm the one who was doing this. As collaborators we have to moderate issues and ensure correct language and behavior.",2018-07-21T11:00:57Z,12612165
1355,mrdoob/three.js,338480646,406788336,"> I'm the one who was doing this. As collaborators we have to moderate issues and ensure correct language and behavior.

Grow up first!",2018-07-21T11:07:04Z,11772787
1356,mrdoob/three.js,338480646,406790352,"@Mugen87 I agree with this, and will start to be more vigilant in marking disruptive or otherwise useless comments. In this case, since I was the one being targeted, I didn't think it was appropriate to do so. 

However, @RemusMar has consistently exhibited what can only be described as disruptive behaviour over a long period of time and I don't think that marking comments is going to bring about any kind of change.

We're consistently having to deal with ad hominem attacks  from this user in the form of name calling or other vaguely aggressive statements, and it's quite frankly tedious and detrimental to the development of the three.js community. ",2018-07-21T11:50:45Z,5307958
1357,fluentmigrator/fluentmigrator,709556703,709556703,"I'll be taking my first vacation in awhile during this time, and plan to unplug from computers.  As a result, I won't be responding to issues on stackoverflow.com or github.com.",2020-09-26T15:24:11Z,447485
1358,pallets/flask,372714597,372714597,"**This issue tracker is a tool to address bugs in Flask itself.
Please use the #pocoo IRC channel on freenode or Stack Overflow for general
questions about using Flask or issues not related to Flask.**

If you'd like to report a bug in Flask, fill out the template below. Provide
any extra information that may be useful / related to your problem.
Ideally, create an [MCVE](http://stackoverflow.com/help/mcve), which helps us
understand the problem and helps check that it is not caused by something in
your code.

---

### Expected Behavior

```flask-tutorial
$ coverage run -m pytest
Name                 Stmts   Miss Branch BrPart  Cover
------------------------------------------------------
flaskr/__init__.py      22      0      2      0   100%
flaskr/auth.py          54      0     22      0   100%
flaskr/blog.py          54      0     16      0   100%
flaskr/db.py            24      0      4      0   100%
------------------------------------------------------
TOTAL                  154      0     44      0   100%
```

### Actual Behavior

```flask-tutorial
$ coverage run -m pytest
============================================================== test session starts ==============================================================
platform darwin -- Python 2.7.10, pytest-3.9.1, py-1.7.0, pluggy-0.8.0
rootdir: /Users/adamg9999/Development/flask-tutorial, inifile: setup.cfg
collected 24 items

tests/test_auth.py ........                                                                                                               [ 33%]
tests/test_blog.py ............                                                                                                           [ 83%]
tests/test_db.py ..                                                                                                                       [ 91%]
tests/test_factory.py ..                                                                                                                  [100%]

=========================================================== 24 passed in 1.15 seconds ===========================================================
```

for coverage > 4.0 need to first run `coverage run -m pytest` this will print the pytest output to STDOUT, then a followup `coverage report` will output the expected report:

```$ coverage report
Name                 Stmts   Miss Branch BrPart  Cover
------------------------------------------------------
flaskr/__init__.py      22      0      2      0   100%
flaskr/auth.py          54      0     22      0   100%
flaskr/blog.py          54      0     16      0   100%
flaskr/db.py            24      0      4      0   100%
------------------------------------------------------
TOTAL                  154      0     44      0   100%```

### Environment

$ python --version
Python 2.7.10 :: Continuum Analytics, Inc.
(py2.7.10) adamg9999 flask-tutorial
$ flask --version
Flask 1.0.2
$ coverage --version
Coverage.py, version 4.5.1 with C extension
",2018-10-22T21:46:33Z,564114
1359,pallets/flask,372714597,432042698,"IMO, `coverage run` $ `coverage report` is the normal usage.

See [ Coverage documentation](https://coverage.readthedocs.io/en/v4.5.x/) for more detail.",2018-10-23T00:48:18Z,12967000
1360,pallets/flask,372714597,432478397,"I didn't receive your reply and it's one day long, so I closed this. BTW, you can reopen it anytime at your will. Sorry for the inconvenience, so what do you mean actually? ",2018-10-24T01:23:50Z,12967000
1361,pallets/flask,372714597,432479230,"The roles of “expected” & “actual” are reversed in context of the tutorial.
The tutorial needs update according to your comment.

On Tue, Oct 23, 2018 at 6:24 PM Grey Li <notifications@github.com> wrote:

> I didn't receive your reply and it's one day long, so I closed this. BTW,
> you can reopen it anytime at your will. Sorry for the inconvenience, so
> what do you mean actually?
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/pallets/flask/issues/2959#issuecomment-432478397>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AAibkk-0ta5DbrlreZLnT-ttz_Jh4sbWks5un8FSgaJpZM4X0RJS>
> .
>
",2018-10-24T01:28:58Z,564114
1362,pallets/flask,372714597,432482730,"The behavior described by the [document ](http://flask.pocoo.org/docs/1.0/tutorial/tests/#running-the-tests) seems to be consistent with what you describe. Are you reading the latest documentation？

![image](https://user-images.githubusercontent.com/13200012/47400969-f8c90500-d771-11e8-8b9b-42948239141f.png)
",2018-10-24T01:49:26Z,13200012
1363,Automattic/mongoose,463261125,463261125,"<!-- *Before creating an issue please make sure you are using the latest version of mongoose -->

**Do you want to request a *feature* or report a *bug*?**
> *feature*

**What is the current behavior?**
> `findOneAndUpdate` query `toJSON` doesn't return the aliased keys. Rather it returns the original keys.

**If the current behavior is a bug, please provide the steps to reproduce.**
<!-- If you can, provide a standalone script / gist to reproduce your issue -->

**What is the expected behavior?**
`toObject` or `toJSON` should return the aliased keys instead of the raw keys. Or a custom function like [`reverseTranslateAliases`](https://mongoosejs.com/docs/api.html#model_Model.translateAliases) can be implemented.
Related to #5184

**What are the versions of Node.js, Mongoose and MongoDB you are using? Note that ""latest"" is not a version.**
mongoose: 5.6.2
node: 12.0.0
npm: 6.9.0

<!-- You can print `mongoose.version` to get your current version of Mongoose: https://mongoosejs.com/docs/api.html#mongoose_Mongoose-version -->
",2019-07-02T14:27:00Z,46988905
1364,Automattic/mongoose,463261125,507720189,"Adding the following to the mongoose schema options solves the issue. It removes all the raw keys and only aliases are left.

```js
{
    toJSON: {
      virtuals: true,
      transform: (doc: any, ret: any, opts: any) => {
        Object.keys(doc.schema.obj)
          .filter(k => doc.schema.obj[k].alias)
          .forEach(ak => delete ret[ak]);
      }
    }
}
```

Maybe a plugin can help?",2019-07-02T15:10:34Z,46988905
1365,Automattic/mongoose,463261125,508214198,"This is by design. The primary purpose of aliasing keys is to reduce storage in the database and network overhead between Node.js and MongoDB by shortening the keys when they're stored in the database, and converting the aliased keys back to long-form keys in Node.js.",2019-07-03T18:50:07Z,1620265
1366,Automattic/mongoose,463261125,597544545,"If it's by design, then why doesn't it work as designed?",2020-03-11T10:04:00Z,1116517
1367,Automattic/mongoose,463261125,597876919,"@klemensz if it doesn't work as designed, please open up a new issue and follow the issue template.",2020-03-11T21:02:25Z,1620265
1368,Automattic/mongoose,354147871,354147871,"After #6917 it should now be possible to clear up all deprecation warnings by:

* Replacing `update()` with `updateOne()`, `updateMany()`, or `replaceOne()`
* Replacing `remove()` with `deleteOne()` or `deleteMany()`
* Setting `mongoose.set('useFindAndModify', false);`
* Setting `mongoose.set('useCreateIndex', true);`
* Setting `mongoose.set('useNewUrlParser', true);`

Need a docs page to summarize this.",2018-08-27T01:13:47Z,1620265
1369,Automattic/mongoose,354147871,416134161,"If these are set, would it be incompatible with or breaking the current  program?

",2018-08-27T07:03:09Z,24730006
1370,Automattic/mongoose,354147871,416215268,"@isLishude the only differences we are aware of are:

1) Support for MongoDB 3.0.x
2) `updateOne()` and `updateMany()` do not support the [`overwrite` option](https://mongoosejs.com/docs/api.html#query_Query-update). If you use `update(filter, update, { overwrite: true })`, you should use `replaceOne(filter, update)`, **not** `updateOne(filter, update, { overwrite: true })`

If you run into any other problems, please don't hesitate to open up a GitHub issue.",2018-08-27T12:48:07Z,1620265
1371,Automattic/mongoose,354147871,416236492,"I set `mongoose.set('useCreateIndexes', true)` and still get `DeprecationWarning: collection.ensureIndex is deprecated. Use createIndexes instead`.

I've tried it before and after `mongoose.connect()`.",2018-08-27T14:00:48Z,388796
1372,Automattic/mongoose,354147871,416241442,@ulrichb Are you sure that you're using mongoose@5.2.10 that released an hour ago?,2018-08-27T14:16:30Z,5862369
1373,Automattic/mongoose,354147871,416246534,"I can replicate what @ulrichb is seeing on 5.2.10 with:

### 6922.js
```js
#!/usr/bin/env node
'use strict';

const mongoose = require('mongoose');
mongoose.connect('mongodb://localhost:27017/test', { useNewUrlParser: true });
mongoose.set('useCreateIndexes', true);

const conn = mongoose.connection;
const Schema = mongoose.Schema;

const schema = new Schema({
  name: {
    type: String,
    unique: true
  }
});

const Test = mongoose.model('test', schema);

const test = new Test({ name: 'one' });

async function run() {
  console.log(`mongoose version: ${mongoose.version}`);
  await conn.dropDatabase();
  await test.save();
  return conn.close();
}

run();
```
### Output:
```
issues: ./6922.js
mongoose version: 5.2.10
(node:7186) DeprecationWarning: collection.ensureIndex is deprecated. Use createIndexes instead.
issues:
```",2018-08-27T14:31:39Z,6015453
1374,Automattic/mongoose,354147871,416260778,"I believe it is `mongoose.set('useCreateIndex', true)`",2018-08-27T15:13:51Z,16786645
1375,Automattic/mongoose,354147871,416263289,updated the original comment to reflect @saagar2000's observation. Thanks @saagar2000 !,2018-08-27T15:21:21Z,6015453
1376,Automattic/mongoose,354147871,416801228,"@vkarpov15 is it possible to rename the document method `remove` to `delete` to be consistent as well? (Or provide an alias)?

Currently I'm refactoring all Model based usage of `remove` to `deleteOne` and `deleteMany`, but the document methods are still called `remove()` (which is also inconsistent with the `isDeleted()` method).

E.g.

```
const Foo = mongoose.model('Foo');
const foo = new Foo();
foo.remove(); //<-- create alias/rename to foo.delete()
```",2018-08-29T02:19:46Z,490562
1377,Automattic/mongoose,354147871,416981253,"I'm seeing the `collection.{remove|update} is deprecated` warning on `Document.{remove|update}` operations. 

```
const doc = await model.findById(someId);
await doc.update({ $set: { someProp: true }}); // Deprecation warning
await doc.remove() // Deprecation warning
```",2018-08-29T14:47:24Z,10137
1378,Automattic/mongoose,354147871,417083696,"@adamreisnz yeah we should add an alias `doc.delete()`, ditto for `doc.update()` @mycompassspins ",2018-08-29T19:53:45Z,1620265
1379,Automattic/mongoose,354147871,417989057,"mongoose.set('useCreateIndex', true) has no affect for me. 😞 
still getting the 'DeprecationWarning: collection.ensureIndex is deprecated. Use createIndexes instead.' ",2018-09-03T03:10:54Z,35219214
1380,Automattic/mongoose,354147871,418091324,@ParikshitChavan Did you set this option before calling `mongoose.model()`?,2018-09-03T11:47:29Z,5862369
1381,Automattic/mongoose,354147871,418166287,"Using mongoose 5.2.12. I also get this: `the options [useCreateIndex] is not supported` in addition to the deprecation warning. 

More info: I only get this extra message when pass `useCreateIndex` as a connection option. When i use `mongoose.set()` I don't get that message.",2018-09-03T17:30:22Z,13859249
1382,Automattic/mongoose,354147871,418196151,"looks like it should work properly when pass as connection option, but it doesn't. only after explicit `mongoose.set()`",2018-09-03T21:39:28Z,17675259
1383,Automattic/mongoose,354147871,418248684,"@Fonger @govindrai 
I tried doing this through connection options and though mongoose.set()
before and after connection to the DB.
the 'DeprecationWarning: collection.ensureIndex is deprecated. Use createIndexes instead.' always seems to persist.",2018-09-04T05:50:45Z,35219214
1384,Automattic/mongoose,354147871,418303496,"Same here, cant get rid of `collection.ensureIndex is deprecated`warning ... No matter if use set or as an option...",2018-09-04T09:32:47Z,2271201
1385,Automattic/mongoose,354147871,418333409,"@ParikshitChavan @govindrai @Avcajaraville 

You have to put `mongoose.set('useCreateIndex', true)` before **EVERY** `mongoose.model()` call
(Note: not `mongoose.connect()` ).

See the discussion in #6890. This is fixed in 5.2.13 (but it's not released yet).",2018-09-04T11:31:47Z,5862369
1386,Automattic/mongoose,354147871,418914130,"@Fonger @vkarpov15  after upgrade to 5.2.13 the issue still exists
```
(node:24769) DeprecationWarning: collection.ensureIndex is deprecated. Use createIndexes instead.
```

package.json:
```
""dependencies"": {
		""bcrypt"": ""^3.0.0"",
		""connect-mongo"": ""^2.0.1"",
		""ejs"": ""^2.6.1"",
		""express"": ""^4.16.3"",
		""express-session"": ""^1.15.6"",
		""mongodb"": ""^3.1.4"",
		""mongoose"": ""^5.2.13"",
		""morgan"": ""^1.9.0"",
		""passport-local"": ""^1.0.0""
	},
```

node and npm info:
```
$ node -v
v10.6.0

$ npm -v
6.4.1
```",2018-09-05T23:30:55Z,472374
1387,Automattic/mongoose,354147871,418925973,"@arastu can you create a reproducible example with one or more files where you demonstrate that you are using 5.2.13 and are seeing one or more of these warnings?

*Note that I am calling `mongoose.set()` **before** requiring my models*

For Example:

### index.js
```js
#!/usr/bin/env node
'use strict';

const assert = require('assert');
const mongoose = require('mongoose');
mongoose.set('useCreateIndex', true);
mongoose.set('useFindAndModify', false);
mongoose.connect('mongodb://localhost:27017/test', { useNewUrlParser: true });
const conn = mongoose.connection;


const One = require('./one');
const Two = require('./two');

const one = new One({ name: '1' });
const two = new Two({ name: '2' });

async function run() {
  assert.strictEqual(mongoose.version, '5.2.13');
  await conn.dropDatabase();
  await Promise.all([One.init(), Two.init()]);
  await one.save();
  await two.save();
  await One.findOneAndUpdate({}, { name: 'one' });
  await Two.findOneAndUpdate({}, { name: 'two' });
  console.log(`${mongoose.version} should show no deprecations with the new settings.`);
  return conn.close();
}

run();
```
### one.js
```js
'use strict';

const mongoose = require('mongoose');

const schema = new mongoose.Schema({ name: { type: String, unique: true } });

module.exports = mongoose.model('one', schema);
```
### two.js
```js
'use strict';

const mongoose = require('mongoose');

const schema = new mongoose.Schema({ name: { type: String, unique: true } });

module.exports = mongoose.model('two', schema);
```

### Output:
```
6922: ./index.js
5.2.13 should show no deprecations with the new settings.
6922:
```
",2018-09-06T00:42:08Z,6015453
1388,Automattic/mongoose,354147871,419594408,"working for me in 5.2.13. @lineus will useCreateIndex be supported as a connection option? Currently getting `the options [useCreateIndex] is not supported`

i.e.
 `mongoose.connect(
      process.env.MONGODB_URI,
      {
        useNewUrlParser: true,
        autoIndex: false,
        useCreateIndex: true,
      }
    )
`",2018-09-07T23:50:24Z,13859249
1389,Automattic/mongoose,354147871,420096898,"@govindrai right now you should do `mongoose.set('useCreateIndex', true)` as specified in the [deprecation warning docs](https://mongoosejs.com/docs/deprecations.html#summary). But we'll add that as well.",2018-09-10T23:41:24Z,1620265
1390,Automattic/mongoose,354147871,420406644,"I've upgraded to `5.2.14` and am still seeing deprecation warnings that don't make sense. For instance, `collection.update is deprecated . . .` when I'm actually using `document.update`. Same for the `remove` method of both the model and the document. Switching to `document.updateOne` produces` TypeError: document.updateOne is not a function`. Am I missing something here? ",2018-09-11T20:05:01Z,10137
1391,Automattic/mongoose,354147871,420591327,"@mycompassspins Did you track the stack producing the warning ?

For instance, in my case turns out the deprecation warning was happening due to [connect mongo](https://github.com/jdesboeufs/connect-mongo#connect-mongo).

If you follow the above instructions, at least for me, all deprecation warning where gone :)",2018-09-12T10:11:23Z,2271201
1392,Automattic/mongoose,354147871,420659027,"Here's the stack:

```
DeprecationWarning: collection.update is deprecated. Use updateOne, updateMany, or bulkWrite instead.
    at NativeCollection.(anonymous function) [as update] (/MyProject/node_modules/mongoose/lib/drivers/node-mongodb-native/collection.js:143:28)
    at NodeCollection.update (/MyProject/node_modules/mquery/lib/collection/node.js:66:19)
    at model.Query._updateThunk (/MyProject/node_modules/mongoose/lib/query.js:3233:23)
    at model.Query.Query._execUpdate (/MyProject/node_modules/mongoose/lib/query.js:3245:23)
    at process.nextTick (/MyProject/node_modules/kareem/index.js:333:33)
    at process._tickCallback (internal/process/next_tick.js:150:11)
```
Obviously, `Model.update` is actually executed, even though I'm calling `MongooseDocument.update`. This is what I'm doing, more or less:

```
async UpdateMyDocument(id:string, update:MyDocument):Promise<MyDocument>
{
	const doc = await MyModel.findById(id)
		.populate({ path: 'somePath', populate: [{ path: 'somePath.nested' }] });

	return await doc.update(update); // <= MongooseDocument.update is producing a Model.update deprecation warning
}
```",2018-09-12T14:00:23Z,10137
1393,Automattic/mongoose,354147871,421822608,@mycompassspins are you sure you're on 5.2.14? We [added `doc.updateOne(update)` in 5.2.13](https://github.com/Automattic/mongoose/blob/master/History.md#5213--2018-09-04),2018-09-16T19:58:00Z,1620265
1394,Automattic/mongoose,354147871,423497767,There's an internal configuration typo in `@5.2.16` that causes the deprecation notice for createIndexes to reemerge when you call createIndexes explicitly. PR opened☝️ ,2018-09-21T11:13:22Z,6015453
1395,Automattic/mongoose,354147871,427627940,"(node:6455) DeprecationWarning: GridStore is deprecated, and will be removed in a future version. Please use GridFSBucket instead. What about this ??.
   ""gridfs-stream"": ""^1.1.1"",
  ""mongoose"": ""^5.3.1"",
 ""multer"": ""^1.4.0"",
 ""multer-gridfs-storage"": ""^3.0.0""",2018-10-07T05:48:53Z,35519213
1396,Automattic/mongoose,354147871,428776931,"@venkyyPoojari that's an old one, predates mongoose 5, but worth addressing. We will fix.",2018-10-11T00:28:41Z,1620265
1397,Automattic/mongoose,354147871,428790529,"Thanks, I am going to use this for production, so please help as soon as possible",2018-10-11T01:51:06Z,35519213
1398,Automattic/mongoose,354147871,429640252,"> Thanks, I am going to use this for production, so please help as soon as possible

I have the same issue, I'm piping the readstream to the res object, is this what is wrong? I mean does it prevent the files to be read out?",2018-10-14T16:25:00Z,7512202
1399,Automattic/mongoose,354147871,429705048,@venkyyPoojari @romain10009 that's because gridfs-stream is out of date. The MongoDB driver has a new streaming API for GridFS that you should use instead: https://thecodebarbarian.com/mongodb-gridfs-stream,2018-10-15T04:28:07Z,1620265
1400,Automattic/mongoose,304302655,304302655,"**Do you want to request a *feature* or report a *bug*?**

Bug

**What is the current behavior?**
If I run .save on a mongoose schema without being connected to the database, the callback is never called. I don't get any errors either. The process seems to be running forever. Also, if I'd run an empty .save(), `message.save()` before the actual save, I get the 200 response back. Not sure if that's the expected behaviour or not.

**If the current behavior is a bug, please provide the steps to reproduce.**
Controller:
```
    var message = new Message({
        content: ""Much content""
    });
});

    // message.save(); <--- run this to get the 200 response
    message.save(function (err) {
        if (err){
            console.log('Something went wrong: ' + err);
            return res.status(500).send(""Something went wrong: "" + err);
        }
        return res.status(200).send(""Saved!"");
    });

```
Model:
```
var mongoose = require('mongoose');
var Schema = mongoose.Schema;
// Schema.set('validateBeforeSave', false); // to validate data manually

//mongoose.connect('mongodb://localhost:27017/userFeedback'); <--- don't connect to database
mongoose.set('debug', true);

var MessageSchema = new Schema({
    content: String
});

module.exports = mongoose.model('Message', MessageSchema);
```


**What is the expected behavior?**
Some sort of error message telling me that I'm not connected to the database and that the data will not be saved.

_I am using version 5.0.9 which is the latest version at this point._
",2018-03-12T09:51:20Z,13626169
1401,Automattic/mongoose,304302655,372285051,"Hi @beanilsson, This is the expected behavior per the connection docs [here](http://mongoosejs.com/docs/connections.html#buffering). Mongoose will buffer model commands internally until a connection is established by default. If you want to disable this behavior globally you can call 
```
mongoose.set('bufferCommands', false);
``` 
or optionally add the same option to your schema options to set it at the collection level.
```
const testSchema = new Schema ({
  name: String
}, { bufferCommands: false })
```",2018-03-12T11:55:10Z,6015453
1402,Automattic/mongoose,304302655,372313826,"Thanks for the clarification. Personally, I think that's a very strange default behaviour. But I do realize the benefits of it.",2018-03-12T13:44:07Z,13626169
1403,Automattic/mongoose,304302655,372829311,"It's a behavior that made a lot more sense when everybody used callbacks, but it's less useful now that we have async/await. As async/await becomes more prevalent we'll consider changing it.",2018-03-13T21:45:21Z,1620265
1404,Automattic/mongoose,304302655,541006561,In what world is this acceptable default behaviour? Please fix.,2019-10-11T10:17:57Z,47244673
1405,Automattic/mongoose,67171304,67171304,,2015-04-08T16:44:41Z,1620265
1406,Automattic/mongoose,67171304,91518043,"very good feature :+1: 
",2015-04-10T11:02:46Z,8848104
1407,Automattic/mongoose,67171304,209370249,"+1 for this feature : )
",2016-04-13T11:09:32Z,3648619
1408,Automattic/mongoose,67171304,217405106,"+1
",2016-05-06T10:20:45Z,2454954
1409,Automattic/mongoose,67171304,225381101,"+1
",2016-06-11T18:09:36Z,2848271
1410,Automattic/mongoose,67171304,240836488,"+1
",2016-08-18T19:51:58Z,622622
1411,Automattic/mongoose,67171304,240975948,"+1
",2016-08-19T09:47:33Z,5138121
1412,Automattic/mongoose,67171304,245510480,"+1
",2016-09-08T06:56:58Z,12946435
1413,Automattic/mongoose,67171304,248375944,":+1: would definitely be useful.
",2016-09-20T17:42:07Z,6350063
1414,Automattic/mongoose,67171304,249586119,"Is there and ETA for this?
",2016-09-26T14:29:33Z,6185849
1415,Automattic/mongoose,67171304,250920576,"Not at the moment
",2016-10-01T16:08:48Z,1620265
1416,Automattic/mongoose,67171304,274344463,+1,2017-01-22T17:21:39Z,17980973
1417,Automattic/mongoose,67171304,282194757,+1,2017-02-24T03:32:46Z,8632158
1418,Automattic/mongoose,67171304,298319628,+1,2017-05-01T11:09:52Z,11016257
1419,Automattic/mongoose,67171304,312899458,+1,2017-07-04T15:02:29Z,28922996
1420,Automattic/mongoose,67171304,316742259,+1 extreamly important ,2017-07-20T15:36:22Z,2615945
1421,Automattic/mongoose,67171304,325488764,+1,2017-08-28T21:32:35Z,2472064
1422,Automattic/mongoose,67171304,346844078,+1,2017-11-24T14:47:21Z,8837442
1423,Automattic/mongoose,67171304,348563127,+1,2017-12-01T17:54:56Z,6297396
1424,Automattic/mongoose,67171304,354258739,+1,2017-12-28T09:32:31Z,5276909
1425,Automattic/mongoose,67171304,368341294,+1,2018-02-25T20:23:34Z,20019305
1426,Automattic/mongoose,67171304,376025625,+1,2018-03-26T02:01:27Z,5395269
1427,Automattic/mongoose,67171304,376080849,+1,2018-03-26T08:08:01Z,303574
1428,Automattic/mongoose,67171304,377442952,+1,2018-03-30T04:35:04Z,18255217
1429,Automattic/mongoose,67171304,378294409,+1,2018-04-03T15:37:08Z,4521300
1430,Automattic/mongoose,67171304,378468869,when to achieve?,2018-04-04T03:27:23Z,8686739
1431,Automattic/mongoose,67171304,383946536,@vkarpov15 : any progress on this?,2018-04-24T14:12:34Z,1388742
1432,Automattic/mongoose,67171304,384628478,"Since there's still no support for this, I solved this by keeping a base model and exporting another schema to a separate file and then adding properties to it as needed.

Hope that helps:
```js
/** BaseModel.js **/
const mongoose = require('mongoose');
const myModel = new mongoose.Schema({
	name: {type: String, required: true},
});

module.exports = mongoose.model('Model', myModel);

/** schema.js **/
const mongoose = require('mongoose');
module.exports = function(props) {
	const params = {
		description: {type: String}
	};
	if(props) Object.assign(params, props);
	return new mongoose.Schema(params);
};

/** SubModel.js **/
const mongoose = require('mongoose');
const BaseModel = require('./BaseModel');
const BaseSchema = require('./schema');

const subModel = BaseSchema({
    myProp: {type: Number}
});

module.exports = BaseModel.discriminator('SubModel', subModel);
```

You create more sub models by creating more sub-schema factories.",2018-04-26T12:47:07Z,4521300
1433,Automattic/mongoose,67171304,389914281,+1,2018-05-17T15:47:48Z,12107014
1434,Automattic/mongoose,67171304,396814119,+1,2018-06-13T05:02:21Z,205634
1435,Automattic/mongoose,67171304,407966292,+1,2018-07-26T03:38:22Z,7054245
1436,Automattic/mongoose,67171304,423973962,+1,2018-09-24T13:27:34Z,3037518
1437,Automattic/mongoose,67171304,431799826,+1,2018-10-22T10:43:15Z,36060784
1438,Automattic/mongoose,67171304,449780122,+1,2018-12-25T00:18:52Z,14098106
1439,Automattic/mongoose,67171304,458991552,+1,2019-01-30T15:42:18Z,2612564
1440,Automattic/mongoose,67171304,543097704,+1,2019-10-17T09:49:38Z,8804334
1441,Automattic/mongoose,67171304,545230950,"I recently took the time to open source a solution to this issue, it's call [mongo-schematic-class](https://github.com/Omninox/SchematicClass).  It allows you to use native ES6 classes (or function classes, if you prefer) and normal inheritance to define your schemas.",2019-10-23T02:07:09Z,6350063
1442,Automattic/mongoose,67171304,576583300,Any updates?,2020-01-21T09:00:40Z,203980
1443,doctrine/orm,1843328693,1843328693,"Since release v2.16 this line of code won't save the entities in order of the loop anymore:

```
 foreach ($importDossier->getKlanten() as $klant) {
          $klant->setIsActive(true);
          $this->entityManager->persist($klant);
}
```

The last line to be executed will get an earlier id in the DB than the first iteration of the loop...which seems weird. Is this expected behaviour?

I noticed work was done on the commit order here: https://github.com/doctrine/orm/pull/10547",2023-08-09T14:07:34Z,5173714
1444,doctrine/orm,1843328693,1671453207,"Apparently this is now the new behaviour: 

```
    Do not make any assumptions in your code about the number of queries
    it takes to flush changes, about the ordering of ``INSERT``, ``UPDATE``
    and ``DELETE`` queries or the order in which entities will be processed.
```

and they don't consider it a BC breaking change lol",2023-08-09T14:22:49Z,5173714
1445,doctrine/orm,1843328693,1672103125,Damn right we don't! And the paragraph you're quoting does not document new behavior. It merely newly documents that using the ORM does not and never did guarantee any particular insertion order.,2023-08-09T20:29:17Z,657779
1446,doctrine/orm,1843328693,1672907400,"FTR, duplicate of #10864",2023-08-10T09:47:53Z,1506493
1447,doctrine/orm,1665591134,1665591134,"<!--
Before reporting a BC break, please consult the upgrading document to make sure it's not an expected change: https://github.com/doctrine/orm/blob/2.9.x/UPGRADE.md
-->

### BC Break Report

<!-- Fill in the relevant information below to help triage your issue. -->

|    Q        |   A
|------------ | ------
| BC Break    | yes
| Version     | 2.14.2

#### Summary

YES I UNDERSTAND that the relied upon feature (short namespace alias) is deprecated BUT upgrading in a MINOR release from 2.14.1 to 2.14.2 should not break an app that was working fine in 2.14.1 - that goes against SEMVER

#### Previous behavior

No error messages when using doctrine/orm 2.14.1 

#### Current behavior

after composer update to doctrine/orm 2.14.2 from 2.14.1 - a minor release, now getting app crashes due to relying on deprecated features (I know I know) 

This is caused by now throwing Exceptions where no exceptions were previously thrown (because of a bug) https://github.com/doctrine/orm/pull/10489

<img width=""1280"" alt=""ScreenShot-2023-04-13-03 44 02"" src=""https://user-images.githubusercontent.com/400092/231634531-65ba81f5-41f7-41af-b810-c8e54e973df5.png"">


#### How to reproduce

The related package is https://github.com/j-guyon/CommandSchedulerBundle which is a 3 year old release. Yes I know I know I know... but still, a minor release of doctrine/orm should not have breaking changes ",2023-04-13T02:47:52Z,400092
1448,doctrine/orm,1665591134,1506377545,"You are confusing MINOR and PATCH. 2.14.2 is not a minor release, it is a patch release. BUT. Short aliases are not a feature of `doctrine/orm`, they are a feature of `doctrine/persistence`, which means that if you rely on them, you (or rather, `jmose/command-scheduler-bundle`) should have `doctrine/persistence` in composer.json, and you should still be using `doctrine/persistence` 2 instead of `doctrine/persistence`3. If that were still the case, you would get a deprecation, not a crash.

A solution for you personally can be to downgrade to `doctrine/persistence` 2, and address the issue when you have the time.

> No error messages when using doctrine/orm 2.14.1

OK, there were no error messages, but was it working? I believe it either didn't, or worked by accident. If you were using persistence 3 at the time, then you were not using short aliases.

To fully understand the issue, it would be great to have [a stack trace](https://symfony.com/doc/current/contributing/code/stack_trace.html)",2023-04-13T05:43:01Z,657779
1449,doctrine/orm,1665591134,1506453682,"In my defence it was 4am (now 8am) zzz 

Personally I just removed the bundle and replaced with zenstruck scheduler within 20 mins and deployed that to production already - it's a far superior product anyway

You can close this as it doesn't really affect me now but the fact remains updating a PATCH version broke otherwise working (albeit old bundle) code from working ",2023-04-13T07:01:52Z,400092
1450,doctrine/orm,1665591134,1506456921,"As I said, I suspect it wasn't actually working. ",2023-04-13T07:04:38Z,657779
1451,doctrine/orm,1665591134,1506463530,"The scheduler has been running every min of every day for almost 10 years. It's the backbone of a service that has made me several million pounds... but hey, it's fixed now and I can go to bed. 

Off topic: Also, when someone sponsors you $100 it would be nice, the most minimal thing to do, is to acknowledge it... but you are not alone, many developers don't even acknowledge GitHub sponsorships - ah well. ",2023-04-13T07:10:35Z,400092
1452,doctrine/orm,1665591134,1506476132,"Maybe it was working by accident, but I cannot see how if you were using persistence 3, which does not support short aliases since 3.0: https://github.com/doctrine/persistence/blob/3.1.x/UPGRADE.md#bc-break-removed-support-for-short-namespace-aliases

Or maybe you're not using them at all, and there is a call to `addEntityNamespace` with a namespace that you don't use? We will never know since you neglected to  provide a stack trace.

Regarding the $100, I must admit I noticed them. In fact we all did, and wondered what was up with you. I thought it was  maybe your way to apologize, and they are the reason I decided to help you figure this out today. Are you going to make me regret it? Let's not find out and lock this, again.",2023-04-13T07:21:42Z,657779
1453,doctrine/orm,1656348603,1656348603,"https://github.com/doctrine/orm/blame/2.14.x/UPGRADE.md#L61

### BC Break Report

BaseLifecycleEventArgs

<!-- Fill in the relevant information below to help triage your issue. -->

|    Q        |   A
|------------ | ------
| BC Break    | yes
| Version     | 2.14.1

#### Summary

Hello. Please provide an interface

```php

    public function getSubscribedEvents(): array
    {
        return [
            Events::prePersist,
            Events::preUpdate,
        ];
    }

    public function preUpdate(PreUpdateEventArgs $args): void # <-- PreUpdateEventArgs
    {
        $this->index($args, false); # <--
    }

    public function prePersist(PrePersistEventArgs $args): void # <-- PrePersistEventArgs
    {
        $this->index($args, true); # <--
    }

    public function index($args, $skipOverwriteInitial): void # <-- [????????] $args ?
    {
        $entity = $args->getObject();
        if (!$entity) {
            return;
        }
        $this->entityDateUpdater->updateFields($entity, $skipOverwriteInitial);
    }
```

### Previously all types were LifecycleEventArgs

Right now I can't determine the type for `$args` in `index` method :(

",2023-04-05T21:58:38Z,9466814
1454,doctrine/orm,1656348603,1498217204,"> BC Break Report

For a deprecation that you don't agree with? Seriously?

> Right now I can't determine the type for `$args` in `index` method :(

It's `PreUpdateEventArgs | PrePersistEventArgs`.
",2023-04-05T22:06:25Z,1506493
1455,doctrine/orm,1656348603,1498224858,"
> It's `PreUpdateEventArgs | PrePersistEventArgs`.

Let's have the full version of your beautiful code

```php
public function index(PrePersistEventArgs | PreUpdateEventArgs | PreRemoveEventArgs | PostPersistEventArgs | PostUpdateEventArgs | PostRemoveEventArgs | PostLoadEventArgs $args, $skipOverwriteInitial): void
```

# Seriously?
",2023-04-05T22:11:42Z,9466814
1456,doctrine/orm,1367665070,1367665070,"...and don't throw exceptions when it's not found

Discussed here: https://github.com/doctrine/orm/discussions/10019#discussioncomment-3528300",2022-09-09T11:02:55Z,1481807
1457,doctrine/orm,1367665070,1241843545,For the record: I don't think that raising and catching an exception is a problem that deserves a fix.,2022-09-09T11:17:25Z,1506493
1458,doctrine/orm,1367665070,1241848057,"> For the record: I don't think that raising and catching an exception is a problem that deserves a fix.

Can you suggest any workaround for debugger stopping on internal ORM exceptions during debugging as discussed in the link above?",2022-09-09T11:22:15Z,1481807
1459,doctrine/orm,1367665070,1403589777,"I don’t think we should merge this. 

It’s a problem related to how you use the debugger, not the code itself.

for sure there’s a way how to configure your debugger to ignore this exception, not stop for it or similar.
",2023-01-25T13:14:25Z,1202333
1460,doctrine/orm,1367665070,1403746945,Let's close this then.,2023-01-25T14:55:44Z,657779
1461,doctrine/orm,1367665070,1404643803,"Typical PHP-world hair-brained ""solution""",2023-01-26T07:34:09Z,1481807
1462,doctrine/orm,1367665070,1404657057,"I guess, we PHP guys are just too stupid. Better move to go. 🤷🏻‍♂️ ",2023-01-26T07:53:57Z,1506493
1463,doctrine/orm,1110181012,1110181012,"See /usr/share/php/Doctrine/ORM/Configuration.php

",2022-01-21T08:17:59Z,270445
1464,doctrine/orm,1110181012,1018295291,"In short running doctrine-data-fixtures test suite
=> requires doctrine-orm
=> use doctrine-annotations",2022-01-21T08:40:07Z,270445
1465,doctrine/orm,1110181012,1018297879,"```
$ phpcompatinfo analyser:run lib/
...
Classes Analysis
----------------

    Class                                                                               REF        EXT min/Max PHP min/Max 
    ArrayIterator                                                                       spl        5.0.0       5.0.0       
    BadMethodCallException                                                              spl        5.1.0       5.1.0       
    DOMDocument                                                                         dom        5.0.0       5.0.0       
  U Doctrine\Common\Annotations\AnnotationReader                                        user                   5.3.0       
  U Doctrine\Common\Annotations\CachedReader                                            user                   5.3.0       
  U Doctrine\Common\Annotations\SimpleAnnotationReader                                  user                   5.3.0       
  U Doctrine\Common\Cache\ApcuCache                                                     user                   5.3.0       
  U Doctrine\Common\Cache\ArrayCache                                                    user                   5.3.0       
  U Doctrine\Common\Cache\Cache                                                         user                   5.3.0       
  U Doctrine\Common\Cache\MemcachedCache                                                user                   5.3.0       
  U Doctrine\Common\Cache\RedisCache                                                    user                   5.3.0       
 CU Doctrine\Common\ClassLoader                                                         user       0.1.0       5.3.0       
  U Doctrine\Common\Collections\ArrayCollection                                         user                   5.3.0       
  U Doctrine\Common\Collections\Collection                                              user                   5.3.0       
  U Doctrine\Common\Collections\Criteria                                                user                   5.3.0       
  U Doctrine\Common\Collections\Expr\Comparison                                         user                   5.3.0       
  U Doctrine\Common\Collections\Expr\CompositeExpression                                user                   5.3.0       
  U Doctrine\Common\Collections\Expr\Value                                              user                   5.3.0       
...

```

So doctrine-annotations is used",2022-01-21T08:43:55Z,270445
1466,doctrine/orm,1110181012,1018300386,"It is used for optional functionality. If the data fixtures test suite makes use of annotations, that's where the `doctrine/annotations` dependency should be added.",2022-01-21T08:47:48Z,1506493
1467,doctrine/orm,1110181012,1018305216,"Sorry can't agree... data fixtures don't use annotations, it useq orm, wich use annotations

BTW, do the fuck you want with your project, at least you are aware of the problem.

P.S. previously annotations was pulled by persistence (but dep was removed there)",2022-01-21T08:55:05Z,270445
1468,doctrine/orm,1110181012,1018323981,"> do the fuck you want with your project

Okay, closing the PR then.",2022-01-21T09:17:22Z,1506493
1469,doctrine/orm,985432324,985432324,"### Feature Request

<!-- Fill in the relevant information below to help triage your issue. -->

|    Q        |   A
|------------ | ------
| New Feature | yes
| RFC         | yes/no
| BC Break    | yes/no

#### Summary

This feature is EXTREMELY basic and the whole doctrine ORM is basically useless if you are unable to convert entities (select query results, etc) into json for returning in an API. Either the feature doesn't exist, which is honestly unbelievable, or there is a desperate need to add documentation for it, since not even stackoverflow requests have come to any reasonable conclusion in the past 10 years.
",2021-09-01T17:54:07Z,10137
1470,doctrine/orm,985432324,910566045,"Thank you for your friendly request. The feature that you're looking for is called `json_encode()`, see https://www.php.net/json_encode

If you need more sophisticated serialization solutions, there's Symfony Serializer, JMS Serializer and other high quality open source libraries. I don't think that Doctrine has to invent its own solution.",2021-09-01T18:45:06Z,1506493
1471,doctrine/orm,985432324,910575850,"@derrabus I don't see anywhere stated that, say, the Symfony serializer or JMS Serializer provides support for Doctrine. Notice that serialization of Doctrine Entities might lead to infinite recursion because of associations. Every ORM I have ever used throughout 10 years of web development career provide serialization methods because of this very issue. It is important to have a serializer that provides support for Doctrine, not merely a generic serializer. Or are Doctrine entities, query results, etc, with associations ready to be blindly serialized?",2021-09-01T18:50:54Z,10137
1472,doctrine/orm,985432324,910598112,The solutions I've provided you with are used quite commonly for serializing entity models hydrated with Doctrine ORM. Give them a try.,2021-09-01T19:03:44Z,1506493
1473,doctrine/orm,985432324,910606116,"@derrabus To use the JMS Serializer Bundle in Symfony, I need to access the ""$container"" variable in Symfony controllers, which there is no documentation about how to access that variable. There is no minimal documentation about how to use the JMS Serializer Bundle. Please notice that json serialization is an EXTREMELY BASIC feature, and it is not minimally documented anywhere.",2021-09-01T19:11:47Z,10137
1474,doctrine/orm,985432324,910612366,"As mentioned on symfony/symfony#42833, use Symfony's discussions for support on Symfony.

https://github.com/symfony/symfony/discussions",2021-09-01T19:17:38Z,1506493
1475,doctrine/orm,891698219,891698219,"If the OneToOneField is used with joinColumns it will try to create an unique index with the name of the annotated attribute. This can lead to crashes if two Entities have the same attribute name.
Currently there is no way to rename the generated index which makes the bug ugly.
So please either provide an easy way to rename the index or better: include the entity name prefix in this combination 

It is related to the question #7961.

Example:
```php
class Foo{

    /**
     * @ORM\Id
     * @ORM\Column(type=""string"", nullable=false)
     */
    protected string $id1;
    /**
     * @ORM\Id
     * @ORM\Column(type=""string"", nullable=false)
     */
    protected string $id2;
/**
     * @ORM\ManyToOne(targetEntity=""Foo"")
     * @ORM\JoinColumns(
     *      @ORM\JoinColumn(name=""id1"", referencedColumnName=""id1""),
     *      @ORM\JoinColumn(name=""id2"", referencedColumnName=""id2"")
     * )
     */
    private $foo;
}
class Foo2{

    /**
     * @ORM\Id
     * @ORM\Column(type=""string"", nullable=false)
     */
    protected string $id1;
    /**
     * @ORM\Id
     * @ORM\Column(type=""string"", nullable=false)
     */
    protected string $id2;
/**
     * @ORM\ManyToOne(targetEntity=""Foo2"")
     * @ORM\JoinColumns(
     *      @ORM\JoinColumn(name=""id1"", referencedColumnName=""id1""),
     *      @ORM\JoinColumn(name=""id2"", referencedColumnName=""id2"")
     * )
     */
    private $foo;
}

```
scheme updates or generations should now fail because of:

""General error: 1 index foo_uniq already exists"" or similar
",2021-05-14T07:46:20Z,658768
1476,doctrine/orm,891698219,1577025316,"I've run into the same issue with the fake clash in names. It's a legacy table so renaming one or other of the fields is not a viable workaround.

`An index with name '%s' was already defined on table '%s`",2023-06-05T15:31:49Z,12990914
1477,doctrine/orm,891698219,1578738748,"I found only one viable solution: don't use the **.
Two years have passed without any response.

Django is much nicer and even supports websockets.
Ah and for your use case there is even a model generator.

Sry for the rant, it was the hell with this framework, everything screwed up. I swore never to use it again.",2023-06-06T13:08:31Z,658768
1478,doctrine/orm,795993177,795993177,Look at the ORM on github; whats going on? is this project dead? not actively maintaining by project owners anymore?,2021-01-28T13:13:36Z,3320364
1479,doctrine/orm,795993177,769058652,"No. Better look on the page with releases, than number of issues.",2021-01-28T13:39:23Z,1568198
1480,doctrine/orm,795993177,769068827,"> No. Better look on the page with releases, than number of issues.

i already looked. total 19 issues solved in 2020. And there is 1k issues on tracker. I'm needing an answer from the project owners not from contributors.",2021-01-28T13:54:49Z,3320364
1481,doctrine/orm,795993177,769070731,"@mdogancay This project is open source. Feel free to help out by confirming issues, providing test cases, fixing bugs. If you watch the repository you will see that there is quite some activity, but more people helping out will help reduce the backlog of existing issues.",2021-01-28T13:57:48Z,2952726
1482,doctrine/orm,795993177,769074599,"> @mdogancay This project is open source. Feel free to help out by confirming issues, providing test cases, fixing bugs. If you watch the repository you will see that there is quite some activity, but more people helping out will help reduce the backlog of existing issues.

@dbrumann Thanks for the answer. The reason I'm asking this is because thousands of people are still actively using these projects. If the project is losing momentum (which it seems) we have to move towards an alternative choice. Developing a product with a dying project can result in serious financial losses! This is a serious situation. I trust the project managers to explain the situation when necessary.",2021-01-28T14:03:43Z,3320364
1483,doctrine/orm,795993177,769082694,"> The reason I'm asking this is because thousands of people are still actively using these projects. If the project is losing momentum (which it seems) we have to move towards an alternative choice.

You don't necessarily have to move. You could also incentivise contributors and maintainers by supporting their efforts either financially or through active participation.

> Developing a product with a dying project can result in serious financial losses!

The fact that there was a release roughly a month ago (2020-12-04) should tell you that the project is not inactive. The work on an upcoming 2.9.x release could also help indicate at some activity. Not to mention the activity in sister projects like dbal and the releases there.

If you consider this project vital it might make sense to let the maintainers work on it instead of wasting their time with frivolous claims or requests of a risk assessment for your projects or whatever this is supposed to be.

If you have an alternative that you feel is more stable and a better choice for your project, feel free to switch to it. Otherwise I suggest you put your energy in supporting the dependencies that you feel are vital to your projects so they don't become ""dying projects"".",2021-01-28T14:15:39Z,2952726
1484,doctrine/orm,795993177,769732215,Let me close this as a way to show you that we do have a look at issues.,2021-01-29T10:52:55Z,657779
1485,doctrine/orm,795993177,770226714,"> Let me close this as a way to show you that we do have a look at issues.

@dbrumann , @greg0ire 
I am speaking the truth here, but you are making politics. Instead of looking for a solution to the problem, you're treating me badly and trying to silence me. I'm sorry, but I don't think you are ""good people"" anymore ... Yeah, with your behavior, it became clear why this project is Dying ... Goodbye FOREVER...",2021-01-30T15:15:33Z,3320364
1486,doctrine/orm,795993177,770228608,"I am sorry you think that way. I admit I was snarky, but if you read back your title and initial question, I hope you will realize that you are accusing a bunch of people who spend their free time on a project that you rely on on not doing enough - for you, for free. I think that is rude, which is why I reacted as I did.

As to your concerns, I think I have made it clear that your premise of not much activity doesn't hold up. Yes there are open issues and some of them old, but this is not uncommon and it's not the only activity metric (as shown in my previous comment). If you look at projects like symfony/symfony you will see they have quite a few open issues as well, some of them dating back to 2012, almost 10 years, but you could hardly call Symfony dead or inactive.

Again, if these open issues concern you, you should help out and look at them, help verify them maybe produce a test case and even fix them. This, in my opinion, is the only way to address the backlog constructively.",2021-01-30T15:24:29Z,2952726
1487,doctrine/orm,795993177,770233227,"@mdogancay Doctrine's code base is over 10 years old and heavily matured. While we do have 1000 open issues, when I worked through 100 of them in December only a small number was actually bugs, most of them are help requests and users wrongly using or misconfiguring. A lot of issues are feature requests that we tend to be conserative about and not consider.

The complexity of Doctrine often makes it very time consuming finding the ultimate problem of the user. We provide free software, not free support. 

Yes, not much is happening code wise, but that also means not a lot of stuff is breaking or requires your constant attention, getting to the new APIs.

There isn't an ORM in PHP that comes even close to features that Doctrine ORM has. Due to the nature of ORMs being leaky abstraction, it is my firm believe working on ORMs for 15 years now, that this feature richness comes at the price of complex and sometimes ""fragile"" internals (you can trade this off for better internals at low performance). This proves to be a high hurdle for new contributors and it takes a while to understand the code to understand the side effects of a change. As such you should be happy that we are keeping it working as is.",2021-01-30T15:58:47Z,26936
1488,doctrine/orm,795993177,770250997,"- [x] clickbaity title, with word in all caps
- [x] IS PROJECT DEAD
- [x] 0 contributions to the project ([1 attempt](https://github.com/doctrine/doctrine-website/pull/379), to be fair)
- [x] wants to talk to ~your manager~ the maintainers, not the contributors
- [x] maintainers are bad people

> Goodbye FOREVER...

![that was always allowed](https://comb.io/oCQ8gV.gif)

I don't know why people in this thread are dignifying OP's post with the lengthy and thoughtful answers. All this post deserves is to be ~silenced~ locked. OP can come back after calming down and finding the right tone to phrase his otherwise valid concerns. Because yes, tone matters.",2021-01-30T17:51:38Z,657779
1489,doctrine/orm,530232265,530232265,"### Do not deprecate the `AbstractQuery::useResultCache`

<!-- Fill in the relevant information below to help triage your issue. -->

|    Q        |   A
|------------ | ------
| New Feature | no
| RFC         | no
| BC Break    | no

#### Summary

Please do not deprecate the `AbstractQuery::useResultCache`. 
Without this method the ""chainability"" gets broken:
`
$em->createQuery()->setParameters()->useResultCache($debug)->getResult()
`

Without the method we have no way of doing the above and honestly I do not understand the need to remove it.
",2019-11-29T09:07:24Z,323925
1490,doctrine/orm,530232265,559723347,"Encountered the same situation where the ""application context"" determines whether caching is enabled.

```php
$query->useResultCache($this->applicationContext->useCaching());
```

I do like the explicit enable and disable cache methods, but why deprecate the ""useResultCache()"" methods? They can all exist right?",2019-11-29T09:31:18Z,776405
1491,doctrine/orm,530232265,559772460,"Same here ☹️  I have to refactor all my `useResultCache()` calls with an ugly `if`s:

```
👌  Nice and smooth in ORM 2:

$res = $entityManager
  ->createQuery($dql)
  ->useResultCache($cache_enabled, $ttl)
  ->getResult();

----

🤮 Ugly in ORM 3:

$q = $entityManager->createQuery($dql);
if ($cache_enabled) {
  $q->enableResultCache($ttl);
}
$res = $q->getResult();
```",2019-11-29T12:19:32Z,3925630
1492,doctrine/orm,530232265,559859909,"The deprecation was introduced in #7701 , cc @someniatko .",2019-11-29T18:24:05Z,657779
1493,doctrine/orm,530232265,559885504,"@Ocramius it looks like people are complaining, milord
Should we revert the change?",2019-11-29T21:53:49Z,15856706
1494,doctrine/orm,530232265,559885740,"Not really: conditionally using result cache is secondary, mostly for ORM wrappers than for end consumers, so I'd say that the new API is indeed preferable.

In practice, in development environments, replace the cache with a volatile version, rather than passing a `$debug` flag down by multiple layers.",2019-11-29T21:56:41Z,154256
1495,doctrine/orm,530232265,559979136,"> replace the cache with a volatile version

That's clever! @moxival @holtkamp @ilianiv are you satisifed with this? 

```
👌  Nice and smooth in ORM 2:

$res = $entityManager
  ->createQuery($dql)
  ->useResultCache($cache_enabled, $ttl)
  ->getResult();

----

✨👌✨  Nicer and smoother in ORM 2:
// globally replace the cache with a volatile cache
// stop passing $cache_enabled through a bazillion layers

$res = $entityManager
  ->createQuery($dql)
  ->enableResultCache($ttl)
  ->getResult();
```",2019-11-30T15:01:47Z,657779
1496,doctrine/orm,530232265,560015478,"***EDIT***  _Lets make my comment less b**chy_

No I am not satisfied by this, when i submitted this issue I did not ask for a lecture in ""volatile cache"" I asked for you guys to leave a perfectly good method (that in no way will mess the ORM code).

If you can not leave the method by all means make us fill our code with ifs and elses for much more smoothness and volatile stuff 😸 ",2019-11-30T19:45:57Z,323925
1497,doctrine/orm,530232265,560016351,"Is this your way of telling us you don't understand what ""volatile cache"" means? If yes, there are nicer ways to do so. If not, then explain what it means to you, because when I read your last sentence , I'm under the impression that you don't… why would this cause lots of ifs and elses?

regarding your EDIT: at least you noticed you were not being very nice :P

",2019-11-30T19:59:54Z,657779
1498,doctrine/orm,530232265,560020351,"@Ocramius @greg0ire Lets say you want to implement an api that should handle GET parameter “nocache” that will make it hit the DB bypassing the cache. In this case replacing the cache is not so good. 

I think it’ll cost you nothing to add a proxy method useResultCache() that will call one of the two new methods and everyone will be happy.",2019-11-30T20:53:34Z,3925630
1499,doctrine/orm,530232265,560020788,"I think it would be simpler to have a cache decorator that is aware of the request stack (assuming Symfony context) and forwards calls to either a normal cache or a null cache (such as one of those: https://github.com/ThaDafinser/psr6-null-cache/blob/master/src/Adapter/ , but for doctrine/cache, we have not yet migrated to PSRs) depending on the value of this parameter. That way it works for the whole API just like you asked, and you did not modify n repositories with n calls to `useResultCache()` for that.",2019-11-30T21:00:02Z,657779
1500,doctrine/orm,530232265,560022587,"Dear @greg0ire this issue is not about me knowing what ""volatile cache"" is or isn't, I may know what volatile cache is (although in all honesty violate cache is just real fancy term) 'coz I was in the field for the last 15 years or I may have no idea coz I am 15 y.o. and I am just starting up with PHP  . May i remind you **milord** that PHP does not constrain itself to Symfony nor decorators (even less to decorators that are aware of stuff). 

If it is too challenging for you to keep the `useResultCache` please just say so _my liege_ we would totally understand. 

I also understand that ORM 3 sounds exhilarating and you feel like you should make all your loyal subjects miserable and have them change every single bit of code but maybe, just maybe you would let us (the stupid ones that have no clue what ""volatile cache"" is or how to manipulate their request stack with decorators while trying to implement PSR- 35672 at no avail) this little twinkle star of hope and leave the `useResultCache` alone because there is literally **0** (_zero_) effort involved on your part in doing so 😺 (_please note the emoji - that means my comment is not mean but witty and hilarious filed with warm feelings and melting caches_).",2019-11-30T21:26:55Z,323925
1501,doctrine/orm,530232265,560022846,Closing and locking here: not worth discussing further with this tone.,2019-11-30T21:31:22Z,154256
1502,doctrine/orm,398582372,398582372,,2019-01-12T19:04:27Z,22549507
1503,doctrine/orm,398582372,453773242,"Closing, erasing, locking and blocking the author. This is their first and last interaction on this communication level.",2019-01-12T19:09:03Z,154256
1504,doctrine/orm,313696087,313696087,"I migrate a mysql db to sqlAnywhere(sqla) to test my app developed with Doctrine2.

Acces to sqla works,
DQL like
 
$query = $em->createQuery('SELECT e FROM countries e WHERE e.conlng = :lng );

$query->setParameters(array(
    'lng' => 'DE'
            ));

$result = $query->getResult();

also runs funny.
 
$query = $em->createQuery('SELECT e FROM countries e WHERE e.connum = :num );

$query->setParameters(array(
    'num' => '4'
            ));

$result = $query->getResult();

also no problems!

## BUT!

$query = $em->createQuery('SELECT e FROM countries e WHERE e.conlng = :lng AND e.connum = :num');

$query->setParameters(array(
    'lng' => 'DE',
    'num' => '4',
            ));

$result = $query->getResult();

doesn't work!!

$result = empty!

with operator OR it's the same problem!

Any idea!
# #
",2018-04-12T12:00:06Z,38318899
1505,doctrine/orm,313696087,380834595,What's the executed SQL query? Can you run that manually and verify the output? Also make sure the parameter types match.,2018-04-12T14:54:51Z,154256
1506,doctrine/orm,313696087,381045479,"Interactive sql works!

![grafik](https://user-images.githubusercontent.com/38318899/38720441-91189f92-3ef6-11e8-9d20-b188b7e0eb70.png)

But if i change my test-pgm to this ..

![grafik](https://user-images.githubusercontent.com/38318899/38721092-197090d2-3ef9-11e8-9c87-2db46a9e2065.png)

look at the sql statement!

## SELECT e FROM countries e WHERE e.conlng = :lng  or e.connum = :num

and the result in the sceenshot.

It woorks with an OR operator??!

## I'm very irritated !

",2018-04-13T07:11:29Z,38318899
1507,doctrine/orm,313696087,381098664," > `SELECT e FROM countries e WHERE e.conlng = :lng or e.connum = :num`

Is this the same SQL statement that the ORM runs?
Use a [`DebugStack`](https://github.com/doctrine/dbal/blob/0f23ed9ba28db2b392eeaaf5938ce804e52084b9/lib/Doctrine/DBAL/Logging/DebugStack.php) logger to see if the SQL statement is the same.

The one difference I noticed is that sometimes your parameters are `string`, sometimes `int`: try looking for differences there as well as in the executed SQL string.

 > I'm very irritated !

This is not a support hotline: if you need to get irritated over volunteers helping you out, you can instead hire somebody to help you out, and be (contractually) allowed to be irritated instead.",2018-04-13T10:47:40Z,154256
1508,doctrine/orm,313696087,381116767,"![grafik](https://user-images.githubusercontent.com/38318899/38734156-d37fd23c-3f24-11e8-8f14-eae024415161.png)

The result ofDebugStack!?",2018-04-13T12:14:18Z,38318899
1509,doctrine/orm,313696087,381126371,@winnilein can you expand on `params` and `types` there?,2018-04-13T12:55:19Z,154256
1510,doctrine/orm,313696087,381163301,"![grafik](https://user-images.githubusercontent.com/38318899/38742174-1fc9735c-3f3c-11e8-9ea9-dc64dbcc730b.png)
",2018-04-13T15:00:22Z,38318899
1511,doctrine/orm,313696087,381163760,"Both are bound as `string` there - can you try passing in an integer and running the SQL statement manually with either `string` or `int`, and see if there is a difference?",2018-04-13T15:01:51Z,154256
1512,doctrine/orm,313696087,381237870,"native sql..
![grafik](https://user-images.githubusercontent.com/38318899/38754033-914eaabe-3f60-11e8-90ea-475087e3c644.png)

and the result of my script..

![grafik](https://user-images.githubusercontent.com/38318899/38754086-c5245d48-3f60-11e8-87b9-fd36462da3c9.png)


my script..

```
<?php
include ""Bootstrap.php"";
require_once 'Entities/countries.php';
error_reporting(E_ALL);

$stack = new \Doctrine\DBAL\Logging\DebugStack();
$stack->enabled;
$em->getConfiguration()->setSQLLogger($stack);

$query = $em->createQuery('SELECT e FROM countries e WHERE e.conlng = :lng AND e.connum = :num');

$LNG = 'DE';
$NUM = 4;

  $query->setParameters(array(
    'lng' => $LNG,
    'num' => $NUM,
            ));
 

$result = $query->getResult();
        
            if (!$result) {
            
                echo 'Noting found!';
                
            
            } else {
        
                foreach ($result as $row) {
                    $TXT=$row->getContxt();
                    echo $TXT;   
                }     
     
            }    
            
      echo '<pre>';
\Doctrine\Common\Util\Debug::dump($stack, 5);      

```",2018-04-13T19:24:39Z,38318899
1513,doctrine/orm,313696087,381240426,"## For your entertainment..

`$query = $em->createQuery('SELECT e FROM countries e WHERE e.conlng = :lng AND e.connum = :num');`

`$NUM = 4;`

result ->

![grafik](https://user-images.githubusercontent.com/38318899/38754410-01e1203a-3f62-11e8-8c54-a62626a5d075.png)

## and now !

`$query = $em->createQuery('SELECT e FROM countries e WHERE e.conlng = :lng or e.connum = :num');`

` $NUM = '4';`

result..

![grafik](https://user-images.githubusercontent.com/38318899/38754495-627ebbbe-3f62-11e8-9078-388115a1b4fe.png)


f****** web apps
i think i continue programming midrange applications",2018-04-13T19:35:53Z,38318899
1514,doctrine/orm,313696087,381270728,"Closing: nobody's spare time is worth following up on this attitude.

If you don't have the willpower to contribute to this conversation in a constructive way, then please consider hiring somebody doing it for you instead.",2018-04-13T21:58:20Z,154256
1515,doctrine/orm,120671163,120671163,"Jira issue originally created by user pcnc:

Good afternoon,

When hydrating an Embeddable with nullable attributes the result is an instance of the Embeddable class , this is obviously correct and expected behavior. 

If all the attributes are null the hydrator will still return an instance of the class with all of its properties null , even if I persist and flush my Entity with the Embeddable being set as null . 

For clarification :

```

class MyEntity
{
    protected $myEmbeddable;

    public function setMyEmbeddable(MyEmbeddable $myEmbeddable = null)
    {
        $this->myEmbeddable = $myEmbeddable;
    }
    [...]
}

$newEntity = new MyEntity();
$newEntity->setMyEmbeddable(null);

$em->persist($newEntity);
$em->flsuh($newEntity);

```

Calling $newEntity->getMyEmbeddable() will return an instance of the MyEmbeddable object with all of it's attributes set to null .

I expected $newEntity->getMyEmbeddable() to be NULL . 

Can someone clarify is this is expected behaviour ? In case it is , how can I achieve what I'm looking for ? 

Best regards
",2015-05-08T13:06:30Z,2414358
1516,doctrine/orm,120671163,162368530,"Comment created by eugene-d:

See https://github.com/doctrine/doctrine2/pull/1275
",2015-07-28T10:27:49Z,2414358
1517,doctrine/orm,120671163,170110183,"This is actually really confusing definitely when you combine it with the [docs](http://doctrine-orm.readthedocs.org/projects/doctrine-orm/en/latest/tutorials/embeddables.html#initializing-embeddables)
",2016-01-08T20:08:23Z,399895
1518,doctrine/orm,120671163,182756650,"I must say I find this rather a bug than an improvement... agree with @boekkooi regarding the docs...
",2016-02-11T08:02:02Z,1125168
1519,doctrine/orm,120671163,182907949,"Can anyone confirm if this is a dupe of #4670 and #4568?
",2016-02-11T15:15:28Z,154256
1520,doctrine/orm,120671163,182917775,"well this _is_ #4568; and #4670 is rather a duplicate of #1275, both actually ""features"" or ""improvements"".

This here however is a Bug because embeddables whose all properties are nullable and being null hydrate to an empty object when being stored as null. This is misleading (saving != retrieving) and not according to the docs.
",2016-02-11T15:31:43Z,1125168
1521,doctrine/orm,120671163,182934107,"@afoeder the order of the tickets is scrambled due to the fact that they were imported from Jira in December, heh
",2016-02-11T16:00:59Z,154256
1522,doctrine/orm,120671163,244704155,"Hi @Ocramius 

Has there been any further discussion on this topic? We've just hit into the same problem as described here on our first Doctrine project. 

Let me know if there's anything we can do to help — provide usage examples, code samples, discussions, etc.
",2016-09-05T09:39:36Z,377366
1523,doctrine/orm,120671163,245402433,"@Harrisonbro as it currently stands, doctrine will not support nullable embeddables. That functionality may be implemented later, by implementing embeddables as hidden one-to-one records.
",2016-09-07T20:14:13Z,154256
1524,doctrine/orm,120671163,245548631,"OK. Is there any way for us to implement this on a case-by-case basis (eg. by hooking into the hydration process of an embeddable somehow) so we can manually check whether specific embeddables have enough data in the database to be considered 'valid' and therefore hydratable? Obviously we're not keen to allow value objects to be instantiated in an invalid state and then check an `isValid` method.
",2016-09-08T09:49:37Z,377366
1525,doctrine/orm,120671163,245548941,"> so we can manually check whether specific embeddables have enough data in the database to be considered 'valid' and therefore hydratable? 

Then just use the lifecycle system to (de-)hydrate VOs on your own, no?
",2016-09-08T09:50:59Z,154256
1526,doctrine/orm,120671163,245583515,"> Then just use the lifecycle system to (de-)hydrate VOs on your own, 
> no?

Do you mean lifecycle callbacks — maybe `postLoad` — as shown in 
http://docs.doctrine-project.org/projects/doctrine-orm/en/latest/reference/events.html#lifecycle-callbacks? 
Looks like those only work on entities, not value objects, as far as I 
can see? Eg. if I used `postLoad` an embeddable will already have been 
hydrated with invalid data (if the data in the database is all `null`, 
for example). Alternatively, if I move the VO properties onto the entity 
directly I’ve lost the nice encapsulation that embeddable so usefully 
provides (eg. if I had a `Product` entity with a `SalePrice` with 2 
properties, `value` and `currency` I’d have to move those 2 properties 
onto the entity. Whilst I could then have those properties be private 
and do an `is_null` check for those 2 properties before instantiating 
and returning the VO from `getSalePrice() : SalePrice { … }` it does 
rather compromise my entity.

I’m almost certainly missing something here, sorry. Rather new to 
Doctrine so still learning!
",2016-09-08T12:37:49Z,377366
1527,doctrine/orm,120671163,245583905,"@Harrisonbro the idea is to NOT use embeddables there, and use a lifecycle listener to replace fields with embeddables then (manually). Doctrine will not implement nullability for embeddables for now.
",2016-09-08T12:39:34Z,154256
1528,doctrine/orm,120671163,245585486,"> @Harrisonbro the idea is to NOT use embeddables there, and use a lifecycle listener to replace fields with embeddables then (manually). Doctrine will not implement nullability for embeddables for now.

OK, gotcha.

So in the example I gave — a `Product` entity which wants to use a `SalePrice` VO with 2 fields, `amount` and `currency` — would you suggest simply putting a `sale_price_amount` and `sale_price_currency` property on the `Product` entity, make those private, and then have `Product::getSalePrice() : SalePrice` first check whether the 2 properties are `null` before attempting to instantiate and return the VO?

If so, that seems workable and means the entity is responsible for checking if the VO should be instantiated (rather than having the VO able to be ‘invalid’ and have to implement an `isValid()` method).

Example code of what I mean:

``` php
    class Product
    {
        private $sale_price_amount;
        private $sale_price_currency;

        public getSalePrice() : SalePrice
        {
            if (
                is_null($this->sale_price_currency) || 
                is_null($this->sale_price_amount)
            ) {
                return null;
            }

            return new SalePrice(
                $this->sale_price_currency, 
                $this->sale_price_amount
            );
        }
    }
```

Is that something like what you’re suggesting instead of nullable embeddables?
",2016-09-08T12:46:05Z,377366
1529,doctrine/orm,120671163,245585880,">  So in the example I gave — a `Product` entity which wants to use a `SalePrice` VO with 2 fields, `amount` and `currency` — would you suggest simply putting a `sale_price_amount` and `sale_price_currency` property on the `Product` entity, make those private, and then have `Product::getSalePrice() : SalePrice` first check whether the 2 properties are `null` before attempting to instantiate and return the VO?

Correct.

Basically, since this is a scenario that Doctrine can't cover right now (because of how RDBMS DDL works), you can just implement it in userland for the few times where it pops up.
",2016-09-08T12:47:41Z,154256
1530,doctrine/orm,120671163,245594377,"OK great, thanks a lot for the help.
",2016-09-08T13:19:26Z,377366
1531,doctrine/orm,120671163,245595610,"Thanks, I silently kept up reading your conversation :)
At the moment, my workaround is the following:

```
class Site
{
    /**
     * @var DomainName
     * @ORM\Embedded(class=""DomainName"", columnPrefix=""domain_"")
     */
    private $domainName;

    public function domainName()
    {
        return ((string)$this->domainName === '' ? null : $this->domainName);
    }
}

/**
 * @ORM\Embeddable
 */
class DomainName
{
    /**
     * Note this is only nullable in order to get the whole embeddable nullable (see [1] and [2]
     *
     * @var string
     * @ORM\Column(nullable=true)
     * @see http://doctrine-orm.readthedocs.org/projects/doctrine-orm/en/latest/tutorials/embeddables.html#initializing-embeddables [1]
     * @see https://github.com/doctrine/doctrine2/pull/1275 [2]
     */
    private $name;

    /**
     * Note this is only nullable in order to get the whole embeddable nullable (see [1] and [2]
     *
     * @var string
     * @ORM\Column(name=""escaped_name"", nullable=true)
     * @see http://doctrine-orm.readthedocs.org/projects/doctrine-orm/en/latest/tutorials/embeddables.html#initializing-embeddables [1]
     * @see https://github.com/doctrine/doctrine2/pull/1275 [2]
     */
    private $escapedName;

    public function __construct($name)
    {
        Assertion::notEmpty($name, 'The domain name must be provided.');
        Assertion::regex($name, '/^(?!www\.)([\pL\pN\pS-]+\.)+[\pL]+$/u', 'The domain name ""%s"" must be a valid domain name without the www. subdomain, but might have others.');

        $this->name = $name;
        $this->escapedName = static::escapeDomainName($name);
    }

    public function containsSubdomain()
    {
        return substr_count($this->name, '.') >= 2;
    }

    public static function escapeDomainName($name)
    {
        return preg_replace('/\./', '-', $name);
    }

    public function __toString()
    {
        return (string)$this->name;
    }
}

```
",2016-09-08T13:23:59Z,1125168
1532,doctrine/orm,120671163,245597857,"I like that approach, @afoeder — you still do an `is_null` check in the entity's getter method (`Site::domainName()` in your case) but you can still use an embeddable rather than having to hydrate your VOs yourself.

I suppose the major downside of your approach is that you do still have a VO in an inconsistent state, whereas if you don't let Doctrine hydrate the VO as an embeddable you avoid this; a bit more boilerplate & checking code, but you never have a VO in an invalid state.

Really it's just a trade-off between the 2 options. Others reading this should just be aware of the 2 options and their various merits.
",2016-09-08T13:31:37Z,377366
1533,doctrine/orm,120671163,277280210,"I would have preferred to comment on #1275, but the present issue has the benefit to be still open.

My 2 cents on the sensitive subject of  nullable embedded properties:

- when the Embeddable has at least one non-nullable `@Column`, and this field is null in the database, **there should be no ambiguity** and **`null` should be assigned to the embedded property**. Otherwise (currently!) you get an empty, invalid value object that has non-nullable properties set to `null`. IMHO, the current implementation is broken here.
- when all `@Column` in the Embeddable are nullable, there should be a boolean setting in the `@Embedded` annotation that controls whether or not you want an empty value object or a `null` value when all fields are `null` in the database. **Your choice**.

Finally, you only have a real problem when you have a fully nullable embeddable, **and** want to make the distinction between a `null` property and an empty object. People have suggested to add an extra column in the table, which would work, but would add a ton of complexity for what I think is an edge case. To clarify, **I think this edge case should not be supported by Doctrine**.

The previous two bullet points, however, **I would strongly suggest working on them ASAP**. I'll be happy to help, provided that lead developers are happy with the concept.",2017-02-03T15:40:20Z,1952838
1534,doctrine/orm,120671163,277317820,"Thanks for that clear explanation, @benmorel. I quite agree with your suggested specification of how Doctrine _should_ behave, and that it should be worked on. This issue is the top priority I'd like to see addressed in doctrine. 

I too would be happy to help out with the development and testing of this, if the Doctrine team agree. ",2017-02-03T18:02:37Z,377366
1535,doctrine/orm,120671163,289269154,"We came across the same issue, in our domain a `StreetAddress` is optional, but if given, it has to contain all fields. All fields on the `Embeddable` are `nullable: true`, so the DB is working. The domain is ensuring valid state. So I created that listener to make Doctrine load the objects the way the domain contains objects prior persisting: https://gist.github.com/havvg/602055f1488271f68e5bc82f9a828b4d

Well, it only requires knowledge on the embeddable itself, but easy workaround for now. I hope this helps other developers until the issue will be resolved by Doctrine.",2017-03-26T09:40:39Z,126898
1536,doctrine/orm,120671163,297694497,"@havvg How to use this workaround?

For example I have User entity with embeddable class Gender. Something like that:
https://gist.github.com/szepczynski/d3028eb9f92fd7aadd08a578c7a92ad3

I know that I need the User entity should have registered postLoad listener NullableEmbeddableListener but I have no idea how to register it. Can you provide any example? I guess that I need somewhere call addMapping? ",2017-04-27T12:05:22Z,231734
1537,doctrine/orm,120671163,299954666,"@BenMorel did you by any chance work on a PR for this? I'd really hope to have this ""bug"" resolved, but I don't understand doctrine well enough to be able to write a pretty PR for this issue.",2017-05-08T18:42:58Z,1267282
1538,doctrine/orm,120671163,300000046,"@Evertt Not yet, and I won't until I get the green light from lead developers. I've invested a lot of time in other pull requests, that have been open for years and are still not merged. I can't waste any more time on this project I'm afraid!",2017-05-08T21:45:25Z,1952838
1539,doctrine/orm,120671163,300000499,@BenMorel that's too bad and I totally understand! It's sad that huge projects like these at some point seem to slow down to a point that it just seems frozen.,2017-05-08T21:47:24Z,1267282
1540,doctrine/orm,120671163,300003098,"That's the dark side of open source: projects rely solely on the free time developers can invest in them, and at some point they're just too busy on other businesses and/or family life to carry on with developments.

I, too, feel like Doctrine is slowing down; it's just unfortunate that there aren't enough (available) lead developers to keep up the pace with pull requests: many developers are there to offer their help, but without enough consideration from project leaders, it's just wasted brain processing time.",2017-05-08T21:59:40Z,1952838
1541,doctrine/orm,120671163,300015633,"@BenMorel the pace did slow down a bit last year, but the last months have been quite active :) Also see #6211",2017-05-08T23:07:28Z,5175937
1542,doctrine/orm,120671163,300114518,doctrine 2.x is frozen because doctrine 3 is actively developing (I read somewhere post by @Ocramius),2017-05-09T09:42:02Z,231734
1543,doctrine/orm,120671163,300130032,@szepczynski except that there's no ETA for doctrine 3 so that could take years for all we know. If it really takes that long it would be nice for improvements to still be added to doctrine 2.,2017-05-09T10:57:42Z,1267282
1544,doctrine/orm,120671163,300130645," >  If it really takes that long it would be nice for improvements to still be added to doctrine 2.

It would make it a mess to migrate these additions to something completely redesigned. From what I can see in the last dozen releases, doctrine functionality already abundantly covers the 90% of use-case scenarios, so we could even call it ""feature complete"", if it wasn't for some rough edges that you encounter when you explore more shady features.",2017-05-09T11:01:01Z,154256
1545,doctrine/orm,120671163,300138604,"@Ocramius I wouldn't call this issue right here a ""shady feature"". I think this is a very essential part of the embeddables system.",2017-05-09T11:43:16Z,1267282
1546,doctrine/orm,120671163,300141600,"Right, and embeddables have barely been added in `2.5`, and are already removed in `develop` (`3.x`), as their fundamental internal working mechanisms need to be rewritten",2017-05-09T11:58:26Z,154256
1547,doctrine/orm,120671163,300148264,"@Ocramius Sorry to pollute this thread, but would the [transaction object](https://github.com/doctrine/dbal/pull/634) and [default lock mode](https://github.com/doctrine/doctrine2/pull/949) fit in 3.0?",2017-05-09T12:31:14Z,1952838
1548,doctrine/orm,120671163,300148480,"@BenMorel most likely, yes",2017-05-09T12:32:08Z,154256
1549,doctrine/orm,120671163,300172309,"> Right, and embeddables have barely been added in 2.5, and are already removed in develop (3.x), as their fundamental internal working mechanisms need to be rewritten

@Ocramius I'm not sure I understand you right. Do you mean they will come back in 3.x after their internal working mechanisms have been rewritten?",2017-05-09T13:56:05Z,1267282
1550,doctrine/orm,120671163,300174407,"@Evertt yes, but likely as completely rewritten/redesigned.",2017-05-09T14:03:09Z,154256
1551,doctrine/orm,120671163,300602464,"@Ocramius Are there ways we can help the development of Doctrine, either v2 or v3? It's a tool we all use so would love to,support development if a can. 

_(Sorry to pollute this thread but not sure where else to write.)_",2017-05-10T20:26:53Z,377366
1552,doctrine/orm,120671163,300603229,"@Harrisonbro https://github.com/doctrine/doctrine2/milestones/3.0

Let's please stop going further OT. If you have a question, make a new issue.",2017-05-10T20:29:50Z,154256
1553,doctrine/orm,120671163,320079515,FTR: there is a small 3rd party library that provides a listener for setting embedded entities that are all null to null (the gist that was discussed above): https://github.com/tarifhaus/doctrine-nullable-embeddable,2017-08-03T20:22:40Z,76576
1554,doctrine/orm,120671163,320163084,"@BenMorel With all due respect, I have to go r/quityourbullshit on you here:

> I've invested a lot of time in other pull requests, that have been open for years and are still not merged. I can't waste any more time on this project I'm afraid!

For Doctrine 2, [there are 26 pull requests from you](https://github.com/doctrine/doctrine2/pulls?utf8=%E2%9C%93&q=is%3Apr%20author%3Abenmorel): 2 Open, 2 Closed without merge (one was fixed differently, one would introduce a lot of pain with future pull requests), and 22 merged.
For DBAL, [there are 15 pull requests from you](https://github.com/doctrine/dbal/pulls?utf8=%E2%9C%93&q=is%3Apr%20author%3Abenmorel): 1 open, 2 Closed without merge, and 12 merged. A quick peek into other repositories (common, annotations, bundle, etc.) shows merged pull requests only.

Feel free to point out pull requests that you are waiting to get merged, but please don't say stuff like that without backing it up when other people sacrifice lots of free time to get you free software. Thank you.",2017-08-04T05:55:17Z,383198
1555,doctrine/orm,120671163,320377235,"@alcaeus Thanks for investing your time investigating my contributions to this repository.

You have already wonderfully pointed out my unmerged pull requests, you just forgot to mention their opening date:

- doctrine/dbal#634 : opened 3 years ago
- doctrine/doctrine2#949 : opened 3 years and 6 months ago

I did invest quite a lot of time on these two pull requests, and since 2 years I am getting next to no feedback, despite multiple attempts to draw attention from the team.

Yes, I had PRs merged as well (did I ever say I didn't?), but the lack of feedback on these last two blocked my motivation to contribute further to this project for now.

> [...] please don't say stuff like that without backing it up when other people sacrifice lots of free time to get you free software.

I hope I have backed it up to your taste, and rest assured that I know [quite well](https://github.com/BenMorel) what it's like to sacrifice some time on free software.

Cheers.
",2017-08-05T00:01:14Z,1952838
1556,doctrine/orm,120671163,320425791,"@BenMorel the way you made it sound was that people completely disregarded your work, which they don't. That's why I took the time to look through your contributions to see what's going on.

As for the pull request you mentioned, (please keep in mind that I'm not an ORM guy) it looks like it's a fairly large pull request that touches transaction logic in the DBAL, affecting most of what it (and thus ORM) does. Pull requests like that take time to review and evaluate. I'm not trying to make excuses for other people here, I'm just hoping you can bear with the people maintaining it. That said, there is currently a development push going for 3.0, so maybe there's an opportunity to get those pull requests merged.

Open source projects need contributors to move forward and evolve, especially when maintainers have little or in some cases no time for the project. However, writing pull requests is just a small part of that work - most of it is looking at issues, figuring out what's going on, evaluating pull requests and making sure your user base is not going to burn you at the stake because you messed up. That can be very time consuming and tiring, so unfortunately, large pull requests are often the first to stay open simply because of the effort it takes to review and merge them.",2017-08-05T08:03:40Z,383198
1557,doctrine/orm,120671163,360445243,"@BenMorel I agree with you. The Doctrine mantra I keep hearing seems to be along the line of ""Well, it kind of works in most cases and changing things is hard so let's not do it"".

They say a picture says a thousand words and I think this picture sums up my feelings about Doctrine right now: 
![](https://i.imgur.com/Rl0VmKc.jpg)

Disclaimer: Owners, don't take offense - it's meant in a lighthearted way and is just my personal opinion. I know you work hard on this, and for that I thank you. I just disagree a bit with the general negativity that I personally see towards any major changes. I know you're working on 3.0 but Symfony and others are leaving you in the dust. If there's too much work, consider giving other contributors more rights or bumping the major version more often so that there can be BC breaks.",2018-01-25T11:53:24Z,161332
1558,doctrine/orm,120671163,360445996,"@ryall I think that's the nail on the coffin then.

Here's the exit:

![selection_176](https://user-images.githubusercontent.com/154256/35387238-2c99dbda-01cf-11e8-8599-5740938bbc9d.jpg)

Closing and locking.

",2018-01-25T11:56:57Z,154256
1559,doctrine/orm,120669472,120669472,"Jira issue originally created by user johnconnor:

It seems that passing the limit to a subquery is not working

```
$subquery = $em->createQueryBuilder()->from('...')->where('...')->setMaxResults(5);
$query = $em->createQueryBuilder()->from('...')->where(
   $qb->expr()->in('p.id', $subquery->getDQL())
);
$query->getQuery()->getResult();
```

The query works but the is no specified limit in the resulting SQL.
I am aware that DQL does not support the limits and offsets, so i guess there should be another way to get this working?
",2014-01-07T23:57:13Z,2414358
1560,doctrine/orm,120669472,162364613,"Comment created by @deeky666:

Can you please tell which database platform you are using? Because limiting results heavily depends on the platform used.
",2014-01-08T00:00:27Z,2414358
1561,doctrine/orm,120669472,162364614,"Comment created by johnconnor:

MySql
",2014-01-08T00:05:48Z,2414358
1562,doctrine/orm,120669472,162364615,"Comment created by @deeky666:

Hmmm I am not quite sure if the limit/offset is invoked for subqueries but I don't see why it shouldn't. Also I think this is not a DBAL issue because the limit/offset support for MySQL is the easiest we have on all platform. See: https://github.com/doctrine/dbal/blob/master/lib/Doctrine/DBAL/Platforms/MySqlPlatform.php#L51-L63
The query doesn't have to be modified but instead only the limit clause is appended to the query. Can you maybe provide the generated SQL for that query?
",2014-01-08T00:13:59Z,2414358
1563,doctrine/orm,120669472,162364618,"Comment created by johnconnor:

I think if you try to build any query with QueryBuilder, set a limit to it with setMaxResults then call getDQL method, you should see that the output contains no info about limit.
So if you look at my code example , at $qb->expr()->in('p.id', $subquery->getDQL()), then you will see that the getDQL passes to the IN expression a query which already DOES NOT have limit. So this is the place where any info about limits and offsets gets lost.

So I fail to see what it has to do with any specific db engine,however I can provide the mysql resulting query if you want,though it looked perfectly normal to me,just lacks the LIMIT part.
",2014-01-08T00:33:47Z,2414358
1564,doctrine/orm,120669472,289810384,You any news on this ? please maybe a hack ? ,2017-03-28T15:36:43Z,8697604
1565,doctrine/orm,120669472,397550790,Just to note - it's still impossible to use subqueries with limits.,2018-06-15T08:26:10Z,891830
1566,doctrine/orm,120669472,903259409,"New note 3 years later, it is still impossible to use subqueries with limits. Any workaround ?",2021-08-22T12:11:11Z,16743500
1567,doctrine/orm,120669472,903270466,"@adrienpayen @vladimmi if I had a contribution graph like yours, I would refrain from complaining about a lack of contributions. You've done nothing to fix an issue that directly impacts you, so why would you expect people that are not impacted to do the work for you? Please stop the spamming and start being constructive.

You could provide a failing test case, or use a debugger to pinpoint where the issue is.",2021-08-22T13:37:27Z,657779
1568,doctrine/orm,120669472,903549117,"@greg0ire If I was such a tender snowflake, I would refrain from using public services having comments - especially bug trackers. And especially I would at least read ticket before posting anything there - because ""failing case"" was provided 7 years ago in the very first message and ""pinpointed where the issue is"" in the same day several messages later.",2021-08-23T08:21:15Z,891830
1569,doctrine/orm,120669472,903642274,"Well then you can move on to the next step: fix it!
If I had to read every thread when people ask for news, I wouldn't be able to make any progress. That being said, the first comment is not really what I meant by failing _test_ case. You can still work on that before attempting to fixing the bug. Make a PR with the code snippet above in a PHPUnit test.


> tender snowflake

Hmmmm… I think the discussion is too heated, don't you agree?",2021-08-23T10:27:05Z,657779
1570,kangax/compat-table,534580474,534580474,Reverts kangax/compat-table#1557,2019-12-08T18:25:04Z,2213682
1571,kangax/compat-table,534580474,562977970,"At this point we’re just going to go back and forth until one or both of us gets removed as collaborators.

Nothing lands in master that any collab blocks, this included. ",2019-12-08T18:28:15Z,45469
1572,kangax/compat-table,534580474,562978481,"Sorry, but you don't do anything for this project, only exhibit destructive activity, so your opinion does not matter here.",2019-12-08T18:33:08Z,2213682
1573,kangax/compat-table,534580474,562978750,"That’s not your decision to make - we’re all collaborators so all of our opinion matters equally, and without consensus, *nothing happens*.",2019-12-08T18:36:01Z,45469
1574,kangax/compat-table,534580041,534580041,"Reverts kangax/compat-table#1556

Until they can all be manually verified.",2019-12-08T18:21:26Z,45469
1575,nvm-sh/nvm,230340780,230340780,"I needed a global install of NVM because I have some node based cron jobs and a few legacy applications that are rather picky about which version of node they are able to work on. For a time I worked around this by sourcing the nvm script everywhere but that seems to be a somewhat unmaintainable solution. 

The requirements I have:
- NVM should just work in non-interactive sessions
- Every user should be able to select a installed version of node (or use ```$ nvm exec```)
- Some users should be able to install newer versions of node

This is the solution I came up with: (shout out to @icecoldphp, for the initial version)

0. I've done this on a debian 8 machine, as the root user
1. Create a group called ""nvm"", ```# groupadd nvm```
2. Add root to the nvm group ```# usermod -aG nvm root```
3. Goto the ```/opt``` directory and create a directory called nvm  
    - Make sure the groupd owner is nvm ```# chown :nvm ./nvm```
    - Set the permissions so that the group is allowed to write in there and all file will inherit the group ```# chmod g+ws ./nvm```
4. Follow the [git install](https://github.com/creationix/nvm#git-install) steps using ```/opt/nvm``` as the directory 
    - To make sure the group can also write aliases, cache downloads and install global packages make sure the directories exist and have the correct permissions:
    ```
    # mkdir /opt/nvm/.cache
    # mkdir /opt/nvm/versions
    # mkdir /opt/nvm/alias 
    
    # chmod -R g+ws /opt/nvm/.cache
    # chmod -R g+ws /opt/nvm/versions
    # chmod -R g+ws /opt/nvm/alias
    ```
5. Using the following snippet create ```/etc/profile.d/nvm.sh```:
    ```#/etc/profile.d/nvm.sh
    #!/bin/bash

    export NVM_DIR=""/opt/nvm""
    [ -s ""$NVM_DIR/nvm.sh"" ] && . ""$NVM_DIR/nvm.sh""

    ```
6. Ensure that the script is executable ```# chmod +x /etc/profile.d/nvm.sh```
7. If you want to use nvm in non-interactive sessions as well make sure to source the nvm file in ```/etc/bash.bashrc``` before the line saying ```# If not running interactively, don't do anything``` by adding ```. /etc/profile.d/nvm.sh```.
8. For bash completion (which is inherently interactive ;) add ```[ -s ""$NVM_DIR/bash_completion"" ] && \. ""$NVM_DIR/bash_completion""``` after the section about bash completion.

Every user can select a version of node (as the permissions for public are ```r-x```) and users in the nvm group can install and remove versions of node (permissions for the group are ```rwx```).

My questions are:
- As a developer I know next to nothing about linux, could this be improved, is it bad style, etc? Any feedback is welcome.
- Should this be documented in the NVM README.md?",2017-05-22T09:46:16Z,6936
1576,nvm-sh/nvm,230340780,303203372,"nvm is not intended to be global or system-wide - it's per-user, per-shell-session.

Thus, each user account must have its own `$NVM_DIR`. They can certainly share an `nvm.sh`, but I'd recommend they all have their own one of those too.

I would not want to document anything in the readme that encourages people to use nvm across user accounts - there's other tools for that.",2017-05-22T19:56:47Z,45469
1577,nvm-sh/nvm,230340780,303216648,"There is of course [n](https://github.com/tj/n) (with [n-install](https://github.com/mklement0/n-install)) which with a coaching could do the same. I'll give that a shot and create a gist of the process, what are the possible other tools, other than say apt-get, brew or some system level package manager which you are usable to manager the version of node in your shell?",2017-05-22T20:51:34Z,6936
1578,nvm-sh/nvm,230340780,303219792,"Yes, `n` is the sole system-wide node manager I'd recommend.

However, I'd suggest just installing `nvm` in the cronjob user, and invoking the cronjobs such that `nvm.sh` is sourced.",2017-05-22T21:04:53Z,45469
1579,nvm-sh/nvm,230340780,304429494,Yes it is always good to go with what nvm is intended for. per user. Go for per user installation. I have a similar kind of a situation (I used to install node without nvm previously) and did the same. ,2017-05-27T05:36:51Z,4600669
1580,nvm-sh/nvm,230340780,356898753,"> nvm is not intended to be global or system-wide - it's per-user, per-shell-session.

You know, for an util that should eliminate version discrepancies and staff it sure does increase it a lot... I mean I have server that deploys web projects on git pushes via hooks (i.e user = git). But sometimes I need to log in and redeploy the same things manually by invoking the git hooks manually (user = me). And sometimes my colleagues have to do the same (user = foo)... And then there is a process manager (PM2) that should be central for everyone, but it relies on node as well...

And everybody have it's own node&npm. Except root, so trying to sudo yields even less results, i.e ``node: command not found``. Ok, your util is the wrong one for these kind of  things, but NPM in it's [official docs explicitely says to use nvm](https://docs.npmjs.com/getting-started/installing-node) to avoid permission problems that unavoidably accompany multiuser usage.

",2018-01-11T10:52:38Z,3817672
1581,nvm-sh/nvm,230340780,357039868,"Yes - the hazard is ""multiuser usage"". In your use case, everyone should be running `node` as the same user.",2018-01-11T19:45:30Z,45469
1582,nvm-sh/nvm,230340780,410773213,"I'm writing a PHP wrapper for a NodeJS binary and I have this exact same problem. PHP is running as ""www"" so it **has no home directory**.

PHP can't ""see"" the NVM environment.

I had this exact same problem with getting node running in crontab but at least there I could control the user to be anything.

---
Thank you @AndreSteenveld 

This is yet another example of open-source project maintainer arrogance. Your use case doesn't matter so jump through these hoops.",2018-08-06T16:46:51Z,195216
1583,nvm-sh/nvm,230340780,410889181,"@hparadiz that's pretty hostile. I'm not being arrogant here, nor am I saying the use case doesn't matter - I'm saying that this project explicitly does not support this use case. ""Arrogance"" would be assuming that free labor on an open source project you don't pay for is obligated to support your use case.",2018-08-06T23:57:12Z,45469
1584,nvm-sh/nvm,230340780,410900877,"Literally didn't even call anyone out by name other than the one person who helped and I THANKED them. This issue is still a problem. Hasn't been closed. @Spown makes a good point too.

Yes, it *is* a project maintainer's hubris to dismiss people's use cases as invalid. Heck, I do it all the time. Just cause something is free doesn't excuse you from doing things people don't like. So it's free? I'm not allowed to complain about a vital feature being missing? I guess that makes me hostile. But remember for everyone 1 of me who bothered to write something here there's 99 others that find this page, get super annoyed, and move on with their day deciding on one solution or another.

People want to use node, nvm, and npm **system-wide** in a **global** context. This isn't rocket science. Why do people have to install it manually for every user on a system?

Like would it really kill you make a link in /usr/sbin and check the user that ran the command?

Congrats your tool is now an instrumental part of NodeJS development. It's time to level up and make it a system-wide command.",2018-08-07T01:14:08Z,195216
1585,nvm-sh/nvm,230340780,410942171,"@hparadiz I would prefer people with that desire and use case to use a different tool, since that's not what nvm is designed for. Just because you want to cut down a tree doesn't mean a pocket knife is the right tool for the job.",2018-08-07T05:57:08Z,45469
1586,nvm-sh/nvm,230340780,424688046,"Hey guys,
A bit of heresy from me over here that relates to this thread.
(but let's not launch the Spanish Inquisition (tm) just yet)
We needed properly managed and stable node version on our bespoke in-team-dev machines as well as on some specific live use-cases, and for that have forged an Ansible role to do just that. You may call it a hack, or you may not, it's your free will, but we're using NVM as the drop in replacement for the distro's `node` package, accessible globally. 
NVM turned out to be heavens apart in terms of stability than any `apt + n` combination we tried. I appreciate the tool (kudos and big thanks to you @ljharb) so it's the solution we incorporated.
For anyone interested (hope you don't mind sharing here bro):
https://github.com/grzegorznowak/ansible-nvm-node

For anyone else, please use the proper user-scoped approach. Whatever floats your servers chaps.
",2018-09-26T11:56:47Z,617275
1587,nvm-sh/nvm,230340780,424837006,"This problem exists when running node via cron as well since cron doesn't source any bash scripts and therefore won't see node installed via nvm. 

The solution for now tends to be to source the nvm bootstrap before every cron command:
`* * * * * source .nvmrc; node /my/script.js`

See: https://unix.stackexchange.com/questions/67940/cron-ignores-variables-defined-in-bashrc-and-bash-profile

The _real_ solution is still to install node globally but what do I know? I'm just a silly user.",2018-09-26T19:16:13Z,195216
1588,nvm-sh/nvm,230340780,424849636,"@hparadiz there's no need for the passive-aggressive comments. If you want to install node globally, go for it - but nvm isn't the right tool for that job.",2018-09-26T19:59:23Z,45469
1589,nvm-sh/nvm,230340780,424862720,"@hparadiz , cron is notorious for that. Be it node or not. You would want to put in full paths to whatever binary you get to use inside a cron.
Thanks to your question, I just realized cron doesn't even see /usr/local/bin, so will update my role to use the most common of paths: `/usr/bin/node`, and if you did similarly yourself then you could use the absolute path to your gulps etc, like:
`/var/lib/nvm/versions/node/v8.11.3/bin/gulp`
and the hack would be completed, I think.
",2018-09-26T20:42:07Z,617275
1590,nvm-sh/nvm,230340780,424877194,"Your assertion that nvm is the wrong tool for the job is a slap in the face of the paradigm created by BSD systems for the past three decades. 

If I install a binary on a machine it's expected that the binary is available to the entire system. Not just a single user. If I install ffmpeg on a machine I expect that I can run ffmpeg from my terminal but also from any running process. Expecting a process to source a bash script for every binary is madness. If everyone followed your paradigm the *nix eco system would be way worse off.

Here's [documentation](https://www.freebsd.org/doc/handbook/dirstructure.html) on BSD directory structure. Here's a [pretty good discussion](https://unix.stackexchange.com/questions/11544/what-is-the-difference-between-opt-and-usr-local) on where user level packages should be installed. Sadly nvm follows none of these well documented installation paradigms. If you did we wouldn't be having this discussion.

Furthermore if there are actually two users on a machine that need to use nvm you are duplicating binaries on-disk for each user for absolutely no reason. You could easily store them globally and link them to the user's home directory should you choose to continue to use your existing directory structure. My nvm folder on my Macbook Pro is already 479M with only two versions of node. If I told every developer to install node via nvm for every user I'd be looking at gigabytes of files on-disk for absolutely no reason. I tend to like efficiency and this is not it.

The only reason people use nvm is because node's release schedule is very rapid and apt, yum, brew, and other built in system package managers choose to not update fast enough. For this reason you have people using nvm even though they need node installed globally on a machine.

Which brings me to my next point: node is a run-time binary for running Javascript code. A run-time _should_ be installed globally. This is how literally every single run-time works.

By default when someone claims to have a piece of software that is a ""version manager"" I would expect it to do that on my system, not just within my bash terminal.

You might as well change the description of this project to:
> Node Version Manager - Simple bash script to manage multiple active node.js versions (for the current terminal user in bash compatible shells only).

You have multiple people in this thread asking for this and you claim that nvm isn't designed for this. Which is odd since nvm is obviously downloading node to a directory on the user's machine and managing directory links and aliases for them. Having it be global is a tiny additional feature on top of what you already have. You could simply soft link the directory where node is installed to /usr/local/ and be good to go (if root).

Finally for security reasons Linux [explicitly supports users](https://www.tecmint.com/add-users-in-linux/) (section 6) without a home directory. A common one in Ubuntu is `www-data` for web servers. In this thread you are explicitly saying you won't support this completely valid use case.

I'm not trying to be mean or passive aggressive. What you're sensing is disappointment.",2018-09-26T21:33:23Z,195216
1591,nvm-sh/nvm,230340780,424915534,"@grzegorznowak
Yes, cron is notorious but that's only because most people have no idea that the cron running environment is completely different from their terminal bash environment.

Make it tell you what's going on:
```bash
* * * * * (source ~/.bash_profile; node -v; date) >> ~/crontest.log
```

```
$ tail -f ~/crontest.log 
v8.4.0
Wed Sep 26 20:11:01 EDT 2018
```

You actually do *not* want full paths to the binaries in your crontab. Use [`env`](https://linux.die.net/man/1/env).

You can actually make your JS files themselves executable!
```bash
printf '#!/usr/bin/env node\nconsole.log(process);' > myscript.js
chmod +x myscript.js
./myscript.js
```
And yea... it's pretty dope.
<img width=""780"" alt=""image"" src=""https://user-images.githubusercontent.com/195216/46116560-bfc95f00-c1ca-11e8-9871-bcd1b9d7f375.png"">

tldr; make your crons executables themselves and use env",2018-09-27T00:33:07Z,195216
1592,nvm-sh/nvm,230340780,424924192,"@hparadiz that's not actually how BSD systems work. Installing a binary to a location provides the binary *only if it's available in the PATH*, and the PATH can be set per-user, or per shell session, so in fact contextual binary usage *is* the predominant paradigm - assuming that a binary is global everywhere is a slap in the face to how BSD systems work, and shows your ignorance of the same.

That nvm may not follow some conventions is irrelevant; they're *conventions*, not requirements.

I don't agree that a runtime should be installed globally, nor that it is. Many binaries are installed per-user.

That multiple people are asking for (free labor) doesn't mean they're entitled to it; I continue to claim that if you want this feature, YOU SHOULD NOT USE NVM FOR IT. That you're unhappy with that isn't really my concern.

If you're disappointed, I'm sorry for that - but there's no need to take that out on me, or this thread.",2018-09-27T01:27:49Z,45469
1593,nvm-sh/nvm,230340780,424924488,"Please see https://github.com/creationix/nvm/issues/1533#issuecomment-303203372 if you have any questions - `nvm` is not designed for, or intended for, multiple users, and will never support the same.

This issue remains open because there's clearly some documentation change that could be made in the readme to make this more clear. A PR to do so is welcome.",2018-09-27T01:29:38Z,45469
1594,rubygems/rubygems,1530248637,1530248637,"CentOS, Fedora, RHEL and so forth break without this.

Also, `Rakefile` doesn't need those options.",2023-01-12T07:40:35Z,224304
1595,rubygems/rubygems,1530248637,1379928051,~~Where the hack is the squash rebase button in Microsoft Github?~~ 🦺 ,2023-01-12T07:44:19Z,224304
1596,rubygems/rubygems,1530248637,1379935514,https://github.com/puma/puma/issues/2371 seems not related this. Can you explain it?,2023-01-12T07:53:17Z,12301
1597,rubygems/rubygems,1530248637,1379966860,@hsbt I misread puma/puma#2381 about offtopic puma/puma#2371. It's puma/puma#2381 only.,2023-01-12T08:24:09Z,224304
1598,rubygems/rubygems,1530248637,1380040010,This also fixes #6268 and #6204 ,2023-01-12T09:29:17Z,224304
1599,rubygems/rubygems,1530248637,1380180666,"I fixed both `bundle gem --ext=c foo` and `bundle gem --ext=rust bar` on several machines (CentOS 9 Stream x86_64, Ubuntu 22 LTS x86_64, and macOS 13 arm) I ran test benches against.

It's not ideal and probably not doing everything right, but it's closer to tried-and-true based on what already exists. I'm sure I'll see it's 100% wrong after I sleep.",2023-01-12T11:19:54Z,224304
1600,rubygems/rubygems,1530248637,1409090887,"This should've been fixed by #6298, so good to close, right?",2023-01-30T18:12:32Z,2887858
1601,rubygems/rubygems,1530248637,1409091256,(Thanks for your work regardless @steakknife),2023-01-30T18:12:47Z,2887858
1602,rubygems/rubygems,1530248637,1411036103,@deivid-rodriguez No worries. I'll give it a shot to validate.,2023-01-31T20:37:35Z,224304
1603,rubygems/rubygems,1530248637,1413267137,"@deivid-rodriguez Broken on mac and Linuxes. Absolutely no improvement. It's still not setting load paths correctly and it pollutes `lib/` with a binary shared object. And, it also isn't cleaned by `rake clean`. It belongs where native extensions always belong by maintaining convention over configuration and convention over useless churn. I have no horse in this race, so it can stay broken. ""Engineers"" who broken this should own it because it's a huge mess.",2023-02-02T07:26:42Z,224304
1604,rubygems/rubygems,727127817,727127817,"Many people don't install to the default directory, but to the user directory (--user-install), in fact, it's the default on many distributions.

Unfortunately this causes a mismatch between bundler and gem, since bundler doesn't read `gemrc` configuration files.

This problem has been explored in many issues: rubygems/bundler#710, rubygems/bundler#2565, rubygems/bundler#2667, chef/chef-dk#148. No satisfactory resolution from the development team.

I took the time to develop a simply and easy solution myself that should absolutely work for everyone, an environment variable: GEM_USER_INSTALL, which just works ™. And thus I am creating yet another issue ticket.",2020-10-22T07:29:52Z,8358
1605,rubygems/rubygems,727127817,714299222,"> Many people don't install to the default directory, but to the user directory (--user-install), in fact, it's the default on many distributions.

I don't think so. The users who are using the default package provided many of Linux distributions are minority. also see https://rails-hosting.com/2020/#ruby-rails-version-updates

I'm Ruby programmer over the 20+ years. I and my company also didn't use it. We use the custom runtime build by like ruby-build.

> No satisfactory resolution from the development team.

This comment is a little offensive.",2020-10-22T07:42:22Z,12301
1606,rubygems/rubygems,727127817,714312772,"@hsbt 

> I don't think so. The users who are using the default package provided many of Linux distributions are minority.

Really? Is that why 64% of Arch Linux users have [installed ruby gems](https://pkgstats.archlinux.de/packages#query=rubygems)? Is that also why Debian packages rubygems directly into the `ruby` package?

> https://rails-hosting.com/2020/#ruby-rails-version-updates

Rails developers != Ruby users.

> This comment is a little offensive.

You think reality is offensive?",2020-10-22T08:06:43Z,8358
1607,rubygems/rubygems,727127817,714333788,"Whatever the popularity of Linux distributions is, I agree we could do better here. What about honouring `--user-install` if bundler is installing to system gems (i.e., no custom `BUNDLE_PATH` configuration is given). I think it's reasonable to respect whatever rubygems considers as the preferred location to install system gems.",2020-10-22T08:42:47Z,2887858
1608,rubygems/rubygems,727127817,714344167,"@deivid-rodriguez I also created a patch for that, but it's so ugly it seems like a hack. Basically we would need to parse `Gem.configuration` and find the `--user-install` string. I did it only for `gem`, but the same has to be checked for `install`.

```diff
--- a/bundler/lib/bundler/rubygems_integration.rb
+++ b/bundler/lib/bundler/rubygems_integration.rb
@@ -163,6 +163,8 @@ def sources
     end
 
     def gem_dir
+      user_install = Gem.configuration[:gem]&.split(' ')&.include?('--user-install')
+      return Gem.user_dir if user_install
       Gem.dir
     end
 

```",2020-10-22T08:59:29Z,8358
1609,rubygems/rubygems,727127817,714372702,"Right, it _does_ feel like a hack, since these `gemrc` options are meant as permanent flags to the `gem` CLI commands. Not sure what the best solution is, need to think about it, but I don't like adding yet another environment variable.",2020-10-22T09:46:32Z,2887858
1610,rubygems/rubygems,727127817,714717763,"@deivid-rodriguez I've thought about it and tried many things. The environment variable is the only clean solution I see.

I don't think the configurations of `gem` and `bundler` were thought through.",2020-10-22T19:38:57Z,8358
1611,rubygems/rubygems,727127817,714726975,"Thanks for your feedback, I guess? :man_shrugging:

I tend to disagree and I currently believe it might be better to add the ugly code and make this just work without the need for setting yet another environment variable. We could also deprecate the `--user-install` flag and move this to a global configuration to make the code cleaner, since I believe people don't just want `--user-install` for single CLI invocations but for changing the location to install gems to a user location permanently.",2020-10-22T19:57:38Z,2887858
1612,rubygems/rubygems,727127817,714729614,I'd like to hear @duckinator's opinion since I'm aware she's a user of the `--user-install` flag and also a maintainer with better context :),2020-10-22T20:02:46Z,2887858
1613,rubygems/rubygems,727127817,714745406,"> I don't think the configurations of `gem` and `bundler` were thought through.

These libraries are like 15 years old. While I work on improving them I try to not make any judgement on past decisions, because te context at the time was completely different from the context now. To give you an example, when `--user-install` was added, maybe `bundler` didn't exist yet. Statements like the above feel quite arrogant to me, to be honest.",2020-10-22T20:33:41Z,2887858
1614,rubygems/rubygems,727127817,714777088,"The concept of users was known 15 years ago. If you had asked me 15 years ago what should have been the way gems were installed I could have told you then; make user installations the default. The system installation is the special case, which could have been enabled with `--system`.

There's no reason to have `Gem.user_dir`. Only two concepts are needed: 1. the location where gems are installed to, 2. the paths to look for gems (which includes `Gem.default_dir`).

It's **that simple**.

My statement may look arrogant to you, yet it's still true.",2020-10-22T21:38:28Z,8358
1615,rubygems/rubygems,727127817,714829728,"This is the final notice. Be honest, respectable, and collaborative.

",2020-10-23T00:11:08Z,12301
1616,rubygems/rubygems,727127817,714889906,"> Be honest, respectable, and collaborative.

I am.

- Honest: I am speaking the truth, that's called **honesty**.
- Respectable: I am **not disrespecting** anybody. Ideas on the other hand don't have feelings and are meant to be criticized.
- Collaborative: I've sent multiple patches and addressed the feedback on those patches. That is called **collaboration**.

> This is the final notice.

Or what? You are going to willingly harm the users of rubygems that have been reporting these issues for more than **ten years**?

What you do is up to you. If you fix this, great, if not, I will take this ticket as evidence and suggest a hacky solution downstream to my distribution maintainers, since you leave us no other choice.

What you are doing is called [tone policing](https://en.wikipedia.org/wiki/Tone_policing), and it distracts from the issue at hand. It would be much more productive if you concentrated on **the bug**, rather than my tone, but that's your choice.",2020-10-23T03:43:20Z,8358
1617,rubygems/rubygems,727127817,715303402,Let's have a fresh discussion about this at https://github.com/rubygems/rubygems/issues/4031.,2020-10-23T12:11:01Z,2887858
1618,rubygems/rubygems,633703607,633703607,"Eric Hodel publicly attacked and was very hostile against Bozhidar Batsov on Twitter:

https://twitter.com/bbatsov/status/1269490401054019584

I do not think this person would represent Ruby community and would be removed from rubygems organization.

As an autistic person and non-western person I don't feel being welcome and safe with person like Eric Holder in Ruby community.",2020-06-07T19:46:04Z,33736
1619,rubygems/rubygems,633703607,640298731,"Calm down, Please. ",2020-06-07T23:53:49Z,12301
1620,magit/magit,398264957,398264957,"When someone tries to refresh the refs buffer, they expect it to reflect the situation in the origin (beside other things).  For no reason, Git itself doesn't update this information if you do a fetch or similar... so, this needs extra work and fixing.  You already have the list of all origins, so it would be nice if refresh in this buffer would also synchronize your local view with the actual state of the origins.

Right now, if your colleague working on the same repository deleted a branch, you will never know it is gone, until you try to pull it / delete it etc, which confuses people and, sometimes will result in you restoring the branch deleted by your colleague from your (typically incomplete) history, so once the other person sees it they will be really puzzled about what had happened.",2019-01-11T12:15:30Z,3147276
1621,magit/magit,398264957,453517809,"You can configure Magit to always prune when fetching: `f` (enter popup), `- p` (set `--prune`), and finally `C-x C-s` (save the arguments).

> For no reason, Git itself doesn't update this information if you do a fetch or similar...

It makes it less likely that accidental deletions propagate unnoticed.

> When someone tries to refresh the refs buffer, they expect it to reflect the situation in the origin (beside other things).

What refreshing has always done in this and other buffers is to update to reflect the situation in the local repository.  *That* is what most users would expect and to instead perform a destructive operation would be very surprising and dangerous.

So I won't add the requested feature, but you can easily implement it using `magit-post-refresh-hook`.",2019-01-11T13:33:56Z,25046
1622,magit/magit,398264957,453543222,"You probably don't understand what `git remote prune origin` does...

It doesn't do anything destructive in the remote repository. It
synchronizes your *local* refs with the remote refs, taking *remote*
as the source of truth, and updating the local to reflect it.  Thus,
it doesn't do any destructive changes at all, neither locally nor
remotely.  The functionality of Git is just wrong when it does a
fetch, it should've also fetched the refs, but... it doesn't.  And I'm
just too tired of Git's boolshit to try to fight it.

Think about this operation as cache invalidation, all it does is it
instructs to invalidate the local cache of remote refs, which it
should've been doing by default anyways.


On Fri, Jan 11, 2019 at 3:34 PM Jonas Bernoulli
<notifications@github.com> wrote:
>
> You can configure Magit to always prune when fetching: f (enter popup), - p (set --prune), and finally C-x C-s (save the arguments).
>
> For no reason, Git itself doesn't update this information if you do a fetch or similar...
>
> It makes it less likely that accidental deletions propagate unnoticed.
>
> When someone tries to refresh the refs buffer, they expect it to reflect the situation in the origin (beside other things).
>
> What refreshing as always done in this and other buffers is to update to reflect the situation in the local repository. That is what most users would expect and to instead perform a destructive operation would be very surprising and dangerous.
>
> So I won't add the requested feature, but you can easily implement it using magit-post-refresh-hook.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or mute the thread.
",2019-01-11T14:56:52Z,3147276
1623,magit/magit,398264957,453626373,"> You probably don't understand what `git remote prune origin` does...

Every time I disagree with you, things escalate quickly. I do not want to do this again.",2019-01-11T19:15:31Z,25046
1624,ipython/ipython,1104666990,1104666990,"Version 8.0 added automatic formatting with Black. This was added in #13397 without much discussion - at least there is none linked.

IMO this is not desirable. It is surprising for a REPL to change what you wrote, and it introduces a large dependency (Black). As @rhettinger has pointed out on Twitter it reduces the utility of IPython in educational contexts: https://twitter.com/raymondh/status/1482225220475883522

Additionally the release note read:

> If `black` is installed in the same environment as IPython...

But this is misleading, implying that it's opt-in by installing Black. The change made IPython depend on Black, so it is always be installed.
",2022-01-15T10:18:18Z,857609
1625,ipython/ipython,1104666990,1013657384,💯 understand that it's really hard to know what users will think of any open source decision until release. Highly grateful for all your work on IPython @Carreau and others.,2022-01-15T10:21:59Z,857609
1626,ipython/ipython,1104666990,1013663347,"Please revert this.

```
In [2]: 2**3 + 4**5 + 6*7*8
```

becoming

```
In [2]: 2 ** 3 + 4 ** 5 + 6 * 7 * 8
Out[2]: 1368
```

is just terrible. Sure, it's a contrived example, but I think my point is clearly visible there...",2022-01-15T11:09:06Z,179599
1627,ipython/ipython,1104666990,1013664285,"Also, I think black should not be a hard dependency of ipython - make it an optional `[black]` extra instead. And still disable it by default regardless of whether black is installed or not :)

Usecase: My project adds ipython as a (non-development) dependency because it includes a REPL for some management tasks (`flask shell` like, just with ipython since it's much nicer than the default Python REPL). However, the project does NOT use black, and by default there's no black installed in the environment (when doing a dev setup), because well.. the project doesn't use black to auto-format code.

But now ipython pulls in black as a dependency and I think some editors might even detect this and suggest black to auto-format code...",2022-01-15T11:16:14Z,179599
1628,ipython/ipython,1104666990,1013665523,"Please don't pile on with comments like ""this is terrible"", it's not productive. Just indicate support with a thumbs-up on the issue.

> make it an optional `[black]` extra instead.

Yes, this is another path.

> I think some editors might even detect this and suggest black to auto-format code...

This is very true.",2022-01-15T11:26:14Z,857609
1629,ipython/ipython,1104666990,1013675915,"I would like to counter argument that black should be kept as default. 

I personally like the change. Even while prototyping, an code formatter can really help you keep your code clean and legible, and black is easily one of the most popular python code formatter. I would also argue that new users should come in contact with code formatters early on their learning journey, as writing readable code is just as important as writing functional ones. 

By keeping the feature as default, you make it more likely that new users will find this feature. Most people will not thoughtfully read all documentation and optional before starting to use a product, and by keeping it as default it's very easy to present the feature to new users. Keeping opt out as default is a good strategy for product rollout. 

Those who end up disliking the feature should have a easy time disabling it. According to the [docs](https://ipython.readthedocs.io/en/stable/config/intro.html#setting-configurable-options
), you can disable it inside the terminal, on the command line, on the profile files, on the default profile. There is just so many ways to do so, that should fit most needs. 

Lastly I do think making it a optional dependency is a good idea, but it's a bit of sidetracking: We can have black as default and black dependency as optional, just as we can revert black as default and keep black being a dependency. Those two are not interconnected. ",2022-01-15T12:42:59Z,49646718
1630,ipython/ipython,1104666990,1013677245,"> I personally like the change

Why not make it opt-in?

> keep your code clean 

If you do one-off tasks (let's say perfoming some maintenance in your app's REPL where all your DB models etc are available), it literally doesn't matter.

>  I would also argue that new users should come in contact with code formatters early on their learning journey

But not those that rewrite their code automatically. It's common to highlight formatting issue, provide a shortcut to auto-format and maybe even do it on commit. But right when typing? No, that's just confusing. ""why did my code just change to look completely different"" etc.

> Those who end up disliking the feature should have a easy time disabling it

Many people do not configure ipython, especially those who just use it a a simple REPL. Defaults should be as user-friendly (e.g. enabled syntax highlighting) and non-intrusive (no auto-formatting/rewriting of code) as possible.",2022-01-15T12:52:38Z,179599
1631,ipython/ipython,1104666990,1013678500,"I absolutely adore Black and use it in all of my projects, but I find Raymond's education argument and the `2**3 + 4**5 + 6*7*8` example provided above very convincing.

I'd love the ability to explicitly opt-in to Black formatting on a global or per-notebook basis, but turning it on by default doesn't feel right to me.",2022-01-15T13:00:38Z,9599
1632,ipython/ipython,1104666990,1013681047,"Personally, 90% of the time when I'm using the CLI, it's to run a quick test. And, when I'm testing something, the exact input that I've given is the exact input that I want to test.

Black's great, but it's not what I want to see in a CLI unless I've specifically enabled it.",2022-01-15T13:18:09Z,26678323
1633,ipython/ipython,1104666990,1013694403,"I love IPython, and use it every day, and wanted to add something to this conversation that I have noticed while testing out `black` integration in the past on my own projects.

Enabling `black` introduces inconsistencies with how the magic commands use the preceding optional `%` symbol, and may affect some user's current workflows (as well as confuse new users). For example, to see your history list, you'd type
```
%history
```
while leaving off the `%` works as well:
```
history
```
But if you wanted to list it with a numbered list, you pass in `-n`:
```
%history -n
```
...but if you leave the `%` off, `black` will reformat it from
```
history -n
```
to
```
history - n
```
which is no longer a valid magic command.
",2022-01-15T14:48:58Z,634230
1634,ipython/ipython,1104666990,1013695984,"I had no say in the decision to include Black in IPython so I'm opining here as a user. I understand that any decision is ultimately in the hands of IPython maintainers, and respect their right to choose whatever they find works best for their project.

I think having an option to auto-format code in the CLI and Jupyter cells is terrific.  That being said, it being the default is rallying people against auto-formatting so it's somewhat counterproductive to what we want to achieve.

Note that I don't necessarily agree with taking any change to Python, or third-party projects like in this case, hostage to some nebulous case of ""educating new users"".  It's trivial to demonstrate semicolon usage and other changes which would be auto-formatted away.  In fact, it would be also educational to inform users **why** those things are getting auto-formatted away.  Semicolons and backslashes are pretty uncontroversially discouraged in user code, no?

I know that the biggest point of contention is standardizing double quotes.  I don't want to reopen the single quotes discussion but [the choice was made after evaluating both variants](https://github.com/psf/black/issues/51#issuecomment-376204207).  Saying that ""Python itself prefers single quotes"" is anthropomorphicizing a piece of software.  It's silly.  Again, educating new users would be more complete with an explanation of the reasons why standardizing quotes is done and why one is better than the other.

But I digress.  The more important issue is auto-formatting scientific math-heavy lines.  There isn't much that can be magically done by a simple tool like Black here *in a consistent manner*.  We are evaluating improvements in this area, specifically in the most often brought up case of [hugging power operands](https://github.com/psf/black/pull/2726).  That being said, I don't think this will be an area where everybody will be happy whatever Black does.

So, the actions IPython maintainers can take now are as follows:
1. make it explicitly opt-in;
2. make it much easier to opt out;
3. stop depending on Black by default.

My personal preference would be to definitely do 3. but also 2.  Depending on Black brings a number of other dependencies to the venv which might not be in the versions people want.  And opting out even if Black is importable shouldn't require me to bring up the docs for the magical incantation every time.  It should be trivial to remember.  (something like `# fmt: off`)

But, again, this is ultimately the maintainers' choice.  I'm happy to help if you need anything.",2022-01-15T14:58:58Z,55281
1635,ipython/ipython,1104666990,1013714865,"Since you mentioned semicolons: It's not very uncommon to write concise one-liners - focus one ONE line - in the REPL when you want to do something quickly. For example importing and calling a function. This is a convenient one-liner in history then if you need to restart to apply code changes (`reload()` isn't particularly good if you changed multiple files).

But let's take a simple example: `foo = ""bar""; 2 * foo`

This is formatted to two lines:

```python
In [1]: foo = ""bar""
   ...: 2 * foo
Out[1]: 'barbar'
```

Now that's a multi-line statement in the history, which IMHO is actually less readable. If you do it with an actual import and function call it's even clearer.


BTW: In Jupyter Notebooks autoformatting may be more useful since that's something you want to share (but even there one-liners may be perfectly fine sometimes). But in the simple ipython REPL? Nope...",2022-01-15T16:50:14Z,179599
1636,ipython/ipython,1104666990,1013742058,"The way Raymond Hettinger complained on Twitter is personally deeply hurtful.
I hope he did not meant it the way I read it. 

I'm not going to reply to some comments (here or elsewhere), and may closing and locking all the issue and stop maintaining IPython for my own mental sanity for some time (expect some critical things). So apologies if this is not completely proofread and if words are ill-chosen, and does not get feedback for a while.

To all of those that have nice and constructive comments and discussion, thanks a lot for expressing your opinions and understanding the difficulty of maintaining a software. Also thank a lot for understanding that not everyone has the same needs and preferences, instead of assuming that your way of doing this is the best in the world.

> So, the actions IPython maintainers can take now are as follows:
> 
> 1. make it explicitly opt-in;

To this and similar suggestions, 'black' auto-formatting  has been opt-in for 2 years (may 1st 2020, IPython 7.14).

I had thought it might be problematic, but in two years received almost no bug reports. I tried a few time to say I was considering making it default and only got positive feedback. So I did it, with extensive alpha, beta, and RC time to complain and ask for modifications.

So here is my challenge, if I don't make it the default, no-one know about it. It's astonishing that no-one found the bug @ehamiter described above in 2 years ! That alone would have definitely delayed the release, and at least I would have had tried to fix it.

I've also seen a number of new users misformating Python code and taking really bad habits in the Repl, including folks that did not even realise IPython terminal was multiline. 

For many of those users black by default is much better. You get use to proper code formatting. So you learn to properly _read_ python code.
And it is much easier to deactivate something you don't like than even figure out it something that may exists. For many users this benefits to, having this option be opt-in would make black auto formatting be part of the [unknown unknowns].(https://en.wikipedia.org/wiki/There_are_known_knowns). So I will _never_ get feedback from these. This is in the same vein as ""but you can configure vim to do so"". 

> 2. make it much easier to opt out;

It's really hard to make it much easier, there have been a long standing issue to have persistent config, but that's far beyond the time and funds we have for that in IPython. We could borrow a nice configuration interface like ptpython for the UI if Someone want to take a shot at ti. 

> 3. stop depending on Black by default.

(As black is beta I agree that this is a problem I did not foresaw and will likely be removing at least the dependency, though I don't really like that either).

As many have pointed out, it's expected for major release to receive feedback because few users try `--pre`, and other channel, and it's ok I expect it. But I much prefer a reaction like:

https://twitter.com/jnuneziglesias/status/1478867554009452545

> Black formatting by default is 🤢 though 😜 — made myself a todo to turn it off. 😂 Can yapf be configured in its place?

Which I'm more likely to help with (and did in a thread). Than spitting on maintainers, which is painful and counter productive.

I'll try to restore some of my mental sanity. I was hoping to do a 8.1 around last Friday of January (release friday), we'll see what I can get in there. 







",2022-01-15T19:36:21Z,335567
1637,ipython/ipython,1104666990,1013749764,"@ambv:
>  Semicolons and backslashes are pretty uncontroversially discouraged in user code, no?

In modules, yes. But in IPython (7 and below), a semicolon will disable output:

```
IPython 7.26.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: 1 + 1;

In [2]: 1 + 1
Out[2]: 2
```
AFAIK, this is widely used in data processing (with Pandas), when the result of an expression is a huge table. Or with matplotlib+notebook, where the text `__repr__` of a graph is usually not useful (and the graph is drawn separately).

BTW, this use of the semicolon is mentioned [with the sphinx directive](https://ipython.readthedocs.io/en/stable/sphinxext.html?highlight=semicolon#writing-pure-python-code) in IPython docs; I assume that should be changed?
",2022-01-15T20:33:00Z,302922
1638,ipython/ipython,1104666990,1013751600,"Please revert this being the default. This is horrible for teaching purposes. It also mangles good pandas code for example. It’s not possible to go around to 200 students and have them disable this. Anybody who prefers something as strict as black can easily opt in. 

Strict dogmatic code styling is an advanced use case that should not be enabled by default! 

Thanks! 🙏

Edit: I didn’t do a full read of the issue before posting. Sorry if this was piling on. Always appreciate the work and efforts put into such a project as ipython that I used for years and years. Wanted to get my input in before bed after dealing with it over a zoom session with students. Cheers.",2022-01-15T20:46:53Z,11209690
1639,ipython/ipython,1104666990,1013758514,"> (As black is beta I agree that this is a problem I did not foresaw and will likely be removing at least the dependency, though I don't really like that either).

I do not know whether you were already aware of this but we are aiming to mark Black as stable by the end of the month (hopefully, staying on time is hard in OSS). This mostly means Black's style won't change until the start of every year[^1]. Bug fixes and enhancements will be released as they're landed. I don't know what this means for IPython but I wanted to make sure you were aware.

---

And finally, @Carreau if you feel you need to step back for a little bit then go for it. OSS is hard, and I understand the frustration when a) the decision you puts lots of effort in ended up being more controversial than prerelease polling suggested, and b) the dogpiling this mess trends towards. The world will not end if IPython 8.1 does not get released by the end of January (fwiw, we were quite hesitant doing black releases for a really long time, like over 8+ months without a release and yet today we're still doing ok 🙂 ). 

Thank you for all your hard work on IPython and Jupyter!

[^1]: to everyone who is worried about the poor math handling being locked in for a year, I don't have anything to promise but we are indeed aiming to improve power operation formatting _before the stable release_: https://github.com/psf/black/pull/2726",2022-01-15T21:42:38Z,63936253
1640,ipython/ipython,1104666990,1013764495,"This feature would be very welcome as a plugin.
Typing `pip install ipython[black]` or `pip install ipython ipython-black` would be the best way to opt-in to this amazing feature (as it would take no configuration other than installing the plugin).",2022-01-15T22:31:31Z,134005
1641,ipython/ipython,1104666990,1013780976,"@FlaviovLeal 

Ha! I understand your frustration. However, I don't share it. 

I have a two major problems with iPython configuration. 

1-st is configuration file and sync 

For me editing iPython's configuration and keep it similar on machines I work on is quite hard. I don't say that when I teach or present a new tool it's kind awkward to tell ""please, use this configuration as defaults are no good anymore"". 

2ns issue is configuration file is not a simple config. It's not a language agnostic file like INI, but it's an executable. I agree, that's it's more flexible this way, but… I don't trust executable code and wish to avoid it as much as possible. 

And now iPython tool changes sane default to forcibly use `black`. This formatted is good enough for general Python, but not every single code and way to use it. 

You tell that ""nobody knows""? How do this project advertises its' options? ""Generate default config file"" command? Man page? Simple HTML documentation? Wiki Page? An article on a popular blog platform or even Twitter? I don't see ""most useful configuration options for iPython or similar help pages. 

I see only complex html version ""go write a code and we don't care about your problems with lack or misplaced of information"". And yes, even I have black as a formatter I want to be able to configure it for iPython. 

You tell that you changed in alpha/rc/beta and so on and nobody complained? I don't know many people around me who really use any beta software, even if this software is indeed stable. Python itself is a language, where you have to check next version to make your library or tool to be compatible on day 1. IPython is a tool, which meant to be stable. If tool is meant to be stable, it means for me that defaults won't change without any security or vulnerability issues or at least without a very bold message shown to a user every time (s)he's running a tool",2022-01-16T00:20:36Z,485399
1642,ipython/ipython,1104666990,1013782730,"@ambv having black installed or available in path doesn't mean I want to use it for iPython. 

Also black constancy isn't really good for Pandas and other scientific applications. Yes, pep8 format is needed in one way or another, but not as forcibly as black applies. There's a lot of tricks you learn only by writing and solving by try and error, where black is like an elephant in Chinese shop. And even here I won't force its usage. 

I'd suggest to write a tool to format code as scientist would want to, keeping semicolons, keeping pandas-specific formatting and so on. And this tool most definitely won't be any good for general python programming. Even both tools would produce pep8 compatible code, but it would look different. ",2022-01-16T00:34:46Z,485399
1643,ipython/ipython,1104666990,1013792837,"> In modules, yes. But in IPython (7 and below), a semicolon will disable output

Hasn't the pythonic way to do this always been:

> _ = 1 + 1

?

> Also black constancy isn't really good for Pandas and other scientific applications.

I cannot even begin to tell you how much I disagree with this, speaking as a data scientist who works a lot with analysts who are new to python.

Pandas is a great library, but it almost encourages the writing of unreadable code. Since enabling `black` auto-formatting, I have found my own data processing to be 200% more legible (and has made typos, bugs and flaws 500% more obvious).

Beyond the toll on tech debt and throughput, needing to worry about the ""proper"" way to format code has a real psychological toll. An analyst once handed off a notebook containing a few hundred lines of code. They apologized for the ""how bad"" their code was. Literally all I had to do was run, `black-nb`, and suddenly I was staring at some of the most careful, clever and well-constructed python I'd ever seen, albeit with a ton of copy-paste (DRY would arguably have been an antipattern for this use case).

Similarly, I set up all of our team repos--including where we store our ""lab notebooks""--to run black (and isort) via pre-commit. I expected the junior team members to struggle with that. Instead, the feedback I received was gratitude that they didn't really have to learn or worry about all the various PEP8 rules; and relief that they didn't have to apply a dozen formatting rules during PRs.

In these ways, `black`-by-default leads not only to an essentially free jump in code quality and an easy ramp for new python programmers to get comfortable with the language's code conventions, but a demonstrable _empowerment_ of coders of all skill and experience levels to feel confident that their code is worthy of sharing without feeling self-conscious just because they don't always know the best place to put line breaks.

Thank you, IPython team, for doing this. This is a huge step for software integrity and for making python more accessible to the vast majority of programmers and python users who will benefit from having linted code out of the box. Let the experts who ""know what they're doing"" and want the software to ""get out of their way"" build their own customizations and workarounds.",2022-01-16T01:52:12Z,66568922
1644,ipython/ipython,1104666990,1013830605,"Again I think notebooks are completely different from the `ipython` REPL... for the latter I think it's not a good feature, for notebooks it seems way more reasonable and possibly useful.

BTW, regardless of the future defaults, how about disabling quote normalization? I think that's the single most controversial choice of black and least useful one in this context as well.

Also, in case the feature remains enabled by default: IIRC. sometimes the ipython config isn't loaded depending on how an application embeds it. Not sure if that's still the case with recent versions but if yes that'd mean you may sometimes not be able to disable it easily in every tool that embeds an ipython REPL. A simple env var to disable auto formatting could help in that case...",2022-01-16T08:11:17Z,179599
1645,ipython/ipython,1104666990,1013846323,"@ThiefMaster yeah the notebooks have toolbar, menu etc, wherein you could disable the reformatting. Or enable it, as you wish, easily. Or even have a different cell type for autoformatted code and so on.",2022-01-16T10:08:54Z,704200
1646,ipython/ipython,1104666990,1013853508,"@Carreau I can understand what you are going through. I feel like it was a perfectly fine decision to release 8.0 with black as a default. I have had similar experiences with Jedi, where some people got really frustrated with my decisions. It's probably perfectly fine to revert this change in 8.1 if you want to.

As an upside I would say that most people in this thread were nice and they actually care about IPython, so I hope you do not get too frustrated. Personally I don't do too much alpha/beta/RCs, because people don't use those anyway. So if it's too much work, I would probably just ditch that part. I feel like it's perfectly fine to just test default changes in major versions, because that's where you get feedback.

So whatever you do, note that 80% probably don't care at all about this default, 10% like it and will therefore never find this thread and 10% are against it. I personally would not enable it by default for two reasons:

- Some people get annoyed by this behavior and complain
- You get more bug reports, because it's an additional feature

Both are just wasting maintainer time and your mental sanity :) But feel free to do whatever you think is the right decision here.

> I've also seen a number of new users misformating Python code and taking really bad habits in the Repl, including folks that did not even realise IPython terminal was multiline.

I feel like a lot of those people are beyond saving :) I'm seeing more and more non-programmers using IPython/Jupyter in my circles, which is a good thing.",2022-01-16T10:56:43Z,748594
1647,ipython/ipython,1104666990,1013940419,"First and foremost, thank you @Carreau for stewarding IPython for many, many years. It's a useful REPL for scientific programming and beyond, because of your maintainer efforts and the time of many contributors.

I'm very happy that people have found IPython useful in education and in their day-to-day work. For over a decade, IPython and Project Jupyter have worked to enable interactive computing and computational narratives. To the many users and volunteer contributors, I thank you.

I have huge respect for @Carreau and the work that he has put in over the years. He has generously given many personal hours to maintain IPython.

In this challenging time with Covid, it's very important to be respectful of the people behind the projects. Please be thoughtful in your comments and suggestions and as generous with your appreciation as your criticism. We are truly working for the same goal, and we will be far more successful doing it thoughtfully.


",2022-01-16T19:50:14Z,2680980
1648,ipython/ipython,1104666990,1013942383,"I would donate $100 to see this PR closed unmerged.

AFAICT, there isn't anyone objecting to this change that that couldn't have better spent their time setting a single bit to restore the original behavior that they prefer.

",2022-01-16T20:02:51Z,579782
1649,ipython/ipython,1104666990,1013975665,"A problem that is about to surface is reformatting code in existing, published research and tutorials for Jupyter notebooks.  When a someone views the notebooks and runs them, the reformatting will defeat the original author's careful formatting.  Note, this is something the author cannot control. It happens on the client side.

Here are some examples from Peter Norvig's famous [Probability Tutorial](https://github.com/norvig/pytudes/blob/main/ipynb/ProbabilityParadox.ipynb):

```
@@ -1,6 +1,4 @@
-def joint(A, B, sep=''):
-    """"""The joint distribution of two independent probability distributions.
+def joint(A, B, sep=""""):
+    """"""The joint distribution of two independent probability distributions.
     Result is all entries of the form {a+sep+b: P(a)*P(b)}""""""
-    return ProbDist({a + sep + b: A[a] * B[b]
-                    for a in A
-                    for b in B})
+    return ProbDist({a + sep + b: A[a] * B[b] for a in A for b in B})

@@ -1,4 +1 @@
-S2b = {'BB/b?', 'BB/?b',
-       'BG/b?', 'BG/?g',
-       'GB/g?', 'GB/?b',
-       'GG/g?', 'GG/?g'}
+S2b = {""BB/b?"", ""BB/?b"", ""BG/b?"", ""BG/?g"", ""GB/g?"", ""GB/?b"", ""GG/g?"", ""GG/?g""}
```

In both cases, the reformatting defeats the author's effort to format data in a way that best communicates either what the code is doing or the structure of the data.

This applies broadly. Perhaps [critical research using Python2.7](https://www.gw-openscience.org/GW150914data/GW150914_tutorial.html) will be unaffected, but most tutorials and scientific papers will now look different to different viewers.  ",2022-01-16T23:19:34Z,1623689
1650,ipython/ipython,1104666990,1013979549,"Here's an example from the statistics tutorial:

```
-posterior_male = (prior_male * height_male.pdf(ht) *
-                  weight_male.pdf(wt) * foot_size_male.pdf(fs))
+posterior_male = (
+    prior_male * height_male.pdf(ht) * weight_male.pdf(wt) * foot_size_male.pdf(fs)
+)

-posterior_female = (prior_female * height_female.pdf(ht) *
-                    weight_female.pdf(wt) * foot_size_female.pdf(fs))
+posterior_female = (
+    prior_female
+    * height_female.pdf(ht)
+    * weight_female.pdf(wt)
+    * foot_size_female.pdf(fs)
+)
```

Note how the two posterior computations are no longer parallel to one another. When this happens in published Jupyter tutorials, it lowers the quality of presentation.  Again note that this cannot be controlled by the author.  Every **reader** of the notebooks would have to explicitly opt-out of reformatting the author's original code.

",2022-01-16T23:40:21Z,1623689
1651,ipython/ipython,1104666990,1013982594,"> A problem that is about to surface is reformatting code in existing, published research and tutorials for Jupyter notebooks. When a someone views the notebooks and runs them, the reformatting will defeat the original author's careful formatting. Note, this is something the author cannot control. It happens on the client side.

@rhettinger I share your concern that notebooks are different to modules. They are often intended to be executed rather than edited. Reformatting an existing notebook simply when a user executes it would therefore be a drastic change, if that were to happen.

But if I understand correctly, it hasn't happened. #13397 is a change to the IPython REPL (i.e. `IPython/terminal/interactiveshell.py`). It does not affect notebooks at all! I cannot reproduce the behavior you describe in a notebook, and the diff you post is not the diff of a notebook file but rather a diff of python code. Could you be more specific about the steps to reproduce what you posted?",2022-01-16T23:59:06Z,370930
1652,ipython/ipython,1104666990,1013984240,"I am incredibly confused by all this talk about how these changes would negatively impact classroom instruction and reference documentation. Any programming book I've read or data science tutorial I've ever found is written with the knowledge that languages evolve--especially when we're talking about packages outside the standard library. How many guides had to be rewritten or were rendered obsolete when [`pandas` deprecated](https://github.com/pandas-dev/pandas/issues/14218)--and soon after removed--`.ix`? I personally spent at least two weeks updating a SQLAlchemy toolkit to conform to the new [2.0 API](https://docs.sqlalchemy.org/en/14/changelog/migration_20.html). And how often does _boto_ break backwards compatibility?

In my experience, all the guides and tutorials I've found  _explicitly specify their versions_ and oftentimes contain instructions for setup, usually in my space in the form of a `conda` `environment.yml`. I won't even get into how containerization makes things even easier by making sandbox creation literally push-button. So if auto-formatting is really that big of a deal for your class--aren't your setup instructions already pinning your version of python, IPython and what-have-you? If not, well, that's really way more of a problem than anything else you're bringing up. Or [have you not been following the news](https://www.theverge.com/2022/1/9/22874949/developer-corrupts-open-source-libraries-projects-affected)?

<details><summary><b>Edit / Aside</b> <i>wrt</i> this behavior in notebooks</summary>

> I cannot reproduce the behavior you describe in a notebook, and the diff you post is not the diff of a notebook file but rather a diff of python code. 

Haha, phew--I thought I was the only one and figured there was a build or something that hadn't made it to `conda-forge` yet.

If you want to see how a Jupyter notebook _would_ reformat, [`jupyter_code_formatter` is my everything](https://jupyterlab-code-formatter.readthedocs.io/en/latest/)--and even there it's push-button, not automatic. And since I referenced pre-commit before, see also: [`black-nb`](https://github.com/tomcatling/black-nb).

</details>
",2022-01-17T00:05:23Z,66568922
1653,ipython/ipython,1104666990,1013988778,"@mikepqr ipython, an REPL, is a great tool to show off things if you don't want to create a Jupyter Notebook. Also I disagree with ""one true <...>"" thing where dots in triangle brackets could be anything. Or like a quote or semicolon at the end.

Forcing me to format with black by default makes a tool I use for years much less valuable for me and my students then the simple python REPL. 

The best solution I see from this dispute is to publish an official `ipython-black` package which just change the default to format using black and nothing else, leaving the setting inside iPython configuration for whom want to change it in their configuration files. And make it default if mounthly download report for a plugin is comparable to ipython itself 

@meawoppl you can bribe maintainers, but you cannot bribe whole community

The most edge case of where this dispute can go further is a creation of a fork where this setting is off and the rest is just aligned with this repo. And this would be a quite problematic solution for the whole community and people involved. 
This would lead to at least 2 repos, and community helping the project will actually struggle to where put efforts.  ",2022-01-17T00:20:57Z,485399
1654,ipython/ipython,1104666990,1013993521,"> @mikepqr ipython, an REPL, is a great tool to show off things if you don't want to create a Jupyter Notebook.

Sure. I think you're missing my point, which is that:

- the change we're talking about reverting does not affect notebooks (AFAICT!)
- [the scenario described in this comment](https://github.com/ipython/ipython/issues/13463#issuecomment-1013975665) does not happen (or at least I can't reproduce it)
- formatting of notebooks is not relevant to IPython (i.e. this repo)
",2022-01-17T00:36:17Z,370930
1655,ipython/ipython,1104666990,1014124732,"Yes.  I just confirmed this is IPython only.  Am not sure why, but the auto-reformatting doesn't affect the Jupyter notebooks.  So that bullet has been dodged.",2022-01-17T04:15:24Z,1623689
1656,ipython/ipython,1104666990,1014271158,"I don't miss your point and examples you mentioned are working just fine, eg forced to be reformatted. 

At class I show students various tools and I prefer keep their mess intact if this doesn't go further to a program as it's a part of a learning process. 

I also show various python features like semicolons, the quotes are the same, and many others. 

Why `_ = expression` doesn't work for me to suppress the output? `expression;` doesn't modify the `_` variable which is important in some cases. ",2022-01-17T08:45:08Z,485399
1657,ipython/ipython,1104666990,1014342629,"@rhettinger you may have missed it among all the comments here, but [the person who thought Black should be automatically applied to lines in the IPython CLI left a long comment here](https://github.com/ipython/ipython/issues/13463#issuecomment-1013742058), with an explicit reference to your unfortunate choice of words that set this whole discussion off on the wrong foot.

At this point there's clear pushback from the community, with plenty of technically-reasoned support for why this ended up being a controversial move. If maintainers want to revert the feature they already have the technical feedback they need right now. Adding further examples doesn't add to the discussion; on the contrary, piling on will only dig deeper trenches. Especially so if your further technical complaints are not well-founded.",2022-01-17T10:03:58Z,17914410
1658,ipython/ipython,1104666990,1014449047,"I'm locking the conversation, there is no point in discussing this further.

There are sane factual reason to disable it by default being that reformatting is sometime factually wrong (semicolon, and magics without `%`). That is sufficiently problematic in itself.

So the next release will likely revert it, depending on the time I have, other things on my plate, and more important things for this repository (like fixing CI first to not block all the pending Pull-requests first). 

I do not exclude to fix the magic and semi-colon problem and maybe re-enable black at some point in the future, even just because for some feature it seem the only way to get feedback and bug reports.

I would greatly appreciate any help to write better documentation, blog post on features, and getting feedback from the community, a better way to edit/persist [Configuration  in JSON files that has been supported for years](https://ipython.readthedocs.io/en/stable/config/intro.html?highlight=Json#json-configuration-files). Why not a %web_config that does like fish or xonsh and pop-up a web-browser that let you pick options (hey it can even be made as a separate package), or have a TUI client.

Also please stop hitting or vilifying anybody, and please help testing downstream project by making sure CI tests are run with `--pre` and similar, there are much better way to spend our time by making sure everything works well for everybody.
",2022-01-17T12:05:15Z,335567
1659,ipython/ipython,774727384,774727384,"<!-- This is the repository for IPython command line, if you can try to make sure this question/bug/feature belong here and not on one of the Jupyter repositories. 

If it's a generic Python/Jupyter question, try other forums or discourse.jupyter.org.

If you are unsure, it's ok to post here, though, there are few maintainer so you might not get a fast response. 

Ability of maintainers to spend time and resources on project like IPython is heavily influenced by US politics, and the current government policies have been harmful to the IPython Maintainers and Community. 

If you are on the fence on who to vote for or wether to vote, please cast your vote in for the democrat party in the US.
-->
Relevant traceback reads as follows: 
```
  File ""../venv/lib/python3.8/site-packages/IPython/core/completer.py"", line 2029, in _complete
    completions = self._jedi_matches(
  File ""../venv/lib/python3.8/site-packages/IPython/core/completer.py"", line 1373, in _jedi_matches
    interpreter = jedi.Interpreter(
  File ""../venv/lib/python3.8/site-packages/jedi/api/__init__.py"", line 725, in __init__
    super().__init__(code, environment=environment,
TypeError: __init__() got an unexpected keyword argument 'column'
```

sys info: 
```
{'commit_hash': '62779a198',
 'commit_source': 'installation',
 'default_encoding': 'utf-8',
 'ipython_path': '../venv/lib/python3.8/site-packages/IPython',
 'ipython_version': '7.18.0',
 'os_name': 'posix',
 'platform': 'Linux-4.15.0-128-generic-x86_64-with-glibc2.17',
 'sys_executable': '../venv/bin/python',
 'sys_platform': 'linux',
 'sys_version': '3.8.5 (default, Jul 20 2020, 19:50:14) \n[GCC 5.4.0 20160609]'}
```
same reported in jedi repo too",2020-12-25T12:54:33Z,11097395
1660,ipython/ipython,774727384,751273086,"https://github.com/ipython/ipython/commit/dcd9dc90aee7e4c5c52ce44c18e7518934790612

The code has been already updated, but `7.19.0` did not include this. :(",2020-12-25T17:15:20Z,10265739
1661,ipython/ipython,774727384,751273584,"As a temporary fix for anyone just trying to get things working again:

```
pip install jedi==0.17.2
```

----

It would be really nice if you could quickly release a 7.19.1. (It's already fixed on master).

Sorry for that. I did not realize that IPython with that fix was not released yet. I usually test IPython completions before doing a Jedi release, but not this time :/. It will probably also not happen in the future anymore, because I'm going to release Jedi 1.0 soon, so this is probably the last time for a long time that you have to deal with deprecations in Jedi.

Still wish you a Merry Christmas!",2020-12-25T17:20:36Z,748594
1662,ipython/ipython,774727384,751274036,"By the way, a `7.19.1` release with the dependency `jedi<0.18.0` would also suffice.",2020-12-25T17:26:00Z,748594
1663,ipython/ipython,774727384,751425126,@davidhalter Thanks for your solution. it finally work.,2020-12-27T04:33:25Z,1206342
1664,ipython/ipython,774727384,751476672,"I had the same problem with ipython and thanks to the solution of @davidhalter , it works again. :+1: ",2020-12-27T14:53:49Z,2304703
1665,ipython/ipython,774727384,751613487,Thank you for opening this issue.  Happy holiday!,2020-12-28T07:28:08Z,1468378
1666,ipython/ipython,774727384,751899801,Posted a pr to pin the dependency as suggested: #12746,2020-12-28T23:57:08Z,25695763
1667,ipython/ipython,774727384,751923463,Thanks a lot. I thought my shell was broken. Every time when I tried to use tab completion in IPython it crashed. Glad I found a solution.,2020-12-29T02:25:19Z,6715707
1668,ipython/ipython,774727384,751947852,Finally found a solution! thank you @davidhalter 🎉 ,2020-12-29T05:05:58Z,37798612
1669,ipython/ipython,774727384,753048651,When would the problem be fixed so that every Jedi version is compatible with ipython?,2020-12-31T19:47:45Z,3178925
1670,ipython/ipython,774727384,753227319,"@tik9 Does installing from master work for you?

    pip install git+https://github.com/ipython/ipython

If so, whenever ipython releases a new version, it will be fixed for everyone.",2020-12-31T23:52:55Z,730137
1671,ipython/ipython,774727384,753302449,"@NeilGirdhar , when doing
`pip install git+https://github.com/ipython/ipython`

It seems to work, I still have to update to the current Jedi.",2021-01-01T11:07:25Z,3178925
1672,ipython/ipython,774727384,753556986,"Got this issue as well. Pinning `jedi==0.17.2` worked for me, thanks @davidhalter.",2021-01-03T02:12:41Z,104550
1673,ipython/ipython,774727384,753614077,"> As a temporary fix for anyone just trying to get things working again:
> 
> ```
> pip install jedi==0.17.2
> ```
> 
> It would be really nice if you could quickly release a 7.19.1. (It's already fixed on master).
> 
> Sorry for that. I did not realize that IPython with that fix was not released yet. I usually test IPython completions before doing a Jedi release, but not this time :/. It will probably also not happen in the future anymore, because I'm going to release Jedi 1.0 soon, so this is probably the last time for a long time that you have to deal with deprecations in Jedi.
> 
> Still wish you a Merry Christmas!

Thank you for your solution.
I got the same issue and IPython worked after I installed Jedi",2021-01-03T13:05:27Z,34534823
1674,ipython/ipython,774727384,753719755,"installing 0.17.2 of jedi also worked for me.  Was chasing my tail trying to figure out why it wasn't working in a new virtual environment, glad to have found this!  Hope the fix is out soon.  Luckily this showed up as the top link in Google for me when searching ""ipython init got an unexpected keyword argument 'column'"".",2021-01-04T02:03:25Z,50895411
1675,ipython/ipython,774727384,754042613,"This works:
```console
$ pip install 'jedi<0.18'
```",2021-01-04T15:30:44Z,54780737
1676,ipython/ipython,774727384,754056158,"Thanks @bl-ue

For reference, the autocomplete on my Jupyter notebook wasn't working and I was getting this same error on the terminal.

And now it's working as expected after downgrading jedi to 0.17.2 by just executing the command @bl-ue mentioned.",2021-01-04T15:54:29Z,32599041
1677,ipython/ipython,774727384,754626312,Confirmed that downgrading jedi to `0.17.2` fixed the issue for me as well. Thanks @davidhalter ,2021-01-05T13:10:27Z,19918901
1678,ipython/ipython,774727384,754832463,"Hi everyone, I've stumbled on this issue while creating docker images for myself.
If I understand correctly from [this comment](https://github.com/ipython/ipython/issues/12740#issuecomment-751273086) the problem has already been solved on master, but so far no release including the fix has been issued.
In order to better organize my own work, may I know when do you plan a new release? Somebody [here](https://github.com/ipython/ipython/issues/12740#issuecomment-751273584) was suggesting to quickly release 7.19.1, including this patch, is it still an option?",2021-01-05T18:57:28Z,7643149
1679,ipython/ipython,774727384,756318031,"> As a temporary fix for anyone just trying to get things working again:
> 
> ```
> pip install jedi==0.17.2
> ```
> 
> It would be really nice if you could quickly release a 7.19.1. (It's already fixed on master).
> 
> Sorry for that. I did not realize that IPython with that fix was not released yet. I usually test IPython completions before doing a Jedi release, but not this time :/. It will probably also not happen in the future anymore, because I'm going to release Jedi 1.0 soon, so this is probably the last time for a long time that you have to deal with deprecations in Jedi.
> 
> Still wish you a Merry Christmas!

thanks a lot",2021-01-07T19:06:42Z,47916989
1680,ipython/ipython,774727384,757333211,pip install -U jedi==0.17.2 parso==0.7.1,2021-01-09T16:45:50Z,6576768
1681,ipython/ipython,774727384,757769588,"I wonder, when you have a dependency (jedi) that is in version 0.x.y (not reached major 1), if it is not wiser to pin the minor as >=0.17,<0.18.",2021-01-11T09:12:16Z,50870910
1682,ipython/ipython,774727384,758110002,FYI https://github.com/ipython/ipython/pull/12751#issuecomment-758065462,2021-01-11T17:39:25Z,25695763
1683,ipython/ipython,774727384,759827665,"> As a temporary fix for anyone just trying to get things working again:
> 
> ```
> pip install jedi==0.17.2
> ```
> 
> It would be really nice if you could quickly release a 7.19.1. (It's already fixed on master).
> 
> Sorry for that. I did not realize that IPython with that fix was not released yet. I usually test IPython completions before doing a Jedi release, but not this time :/. It will probably also not happen in the future anymore, because I'm going to release Jedi 1.0 soon, so this is probably the last time for a long time that you have to deal with deprecations in Jedi.
> 
> Still wish you a Merry Christmas!

note: in anaconda, this has to be done in the same kernel you're using from within the jupyter notebook, not the one you're using the `jupyter notebook` command from!

took me about 30 minutes to finally figure that one out once i found @davidhalter's pip install tip (i used `conda install jedi==0.17.2` instead of pip).

thank you all!",2021-01-13T23:40:31Z,7075901
1684,ipython/ipython,774727384,761073748,"Can you explain what you mean by ""same kernel ..."" vs ""using the jupyter notebook ...""   i.e.,  will conda install jedi==0.17.2 from the shell command line do the trick?  (it seems to)",2021-01-15T17:24:05Z,15945736
1685,ipython/ipython,774727384,761689616,"> Can you explain what you mean by ""same kernel ..."" vs ""using the jupyter notebook ..."" i.e., will conda install jedi==0.17.2 from the shell command line do the trick? (it seems to)

You can run Jupyter in one env and use it with a kernel from another env. See for example [this SO question](https://stackoverflow.com/questions/53004311/how-to-add-conda-environment-to-jupyter-lab). Jedi version of the latter env matters. If you are not aware of this, you probably use one conda env for jupyter and kernel.",2021-01-16T22:26:28Z,25516935
1686,ipython/ipython,774727384,763018119,"The following works in ipython/jupyter.   I assume it gets jedi out of the way of the built-in completer.   Good enough for me until fix is released.

%config Completer.use_jedi = False
",2021-01-19T18:00:21Z,9634782
1687,ipython/ipython,774727384,763627280,"It does work! 👍🏻 
Nice find @dowenk! 😍 ",2021-01-20T14:05:31Z,54780737
1688,ipython/ipython,774727384,763815141,"Because people at this point probably don't scroll up to my comment (https://github.com/ipython/ipython/issues/12740#issuecomment-751273584), this is what you should do temporarily:

```
pip install jedi==0.17.2
```",2021-01-20T17:37:46Z,748594
1689,ipython/ipython,774727384,766183653,When is the team going to fix this? In just a month hundreds of users have encountered this issue.,2021-01-23T21:39:34Z,54780737
1690,ipython/ipython,774727384,766183726,I guess pinning jedi to 0.17.2 should be added to the readme. Anyone think I should open a PR for that? 😸,2021-01-23T21:40:06Z,54780737
1691,ipython/ipython,774727384,766453106,"just adding a +1 to this issue, it totally disables tab completion in Jupyter, so worth pinning 0.17.2 until resolved",2021-01-24T22:46:30Z,49782545
1692,ipython/ipython,774727384,768163265,"> As a temporary fix for anyone just trying to get things working again:
> 
> ```
> pip install jedi==0.17.2
> ```
> 
> It would be really nice if you could quickly release a 7.19.1. (It's already fixed on master).
> 
> Sorry for that. I did not realize that IPython with that fix was not released yet. I usually test IPython completions before doing a Jedi release, but not this time :/. It will probably also not happen in the future anymore, because I'm going to release Jedi 1.0 soon, so this is probably the last time for a long time that you have to deal with deprecations in Jedi.
> 
> Still wish you a Merry Christmas!

Thanks",2021-01-27T09:42:44Z,40569235
1693,ipython/ipython,774727384,768270501,This issue should be pinned on the GitHub repo.  @Carreau?,2021-01-27T13:02:51Z,54780737
1694,ipython/ipython,774727384,768406516,"What the timeline looks like for this getting fixed for mainline conda users? 

I ask because I have an online outreach course spinning up in about four days, meaning that 600+ highschool students are going to be encountering the ""now hit `<TAB>` to autocomplete 😃"" portion of their virtual course, and it is going unceremoniously bomb them out of `ipython`.

Downgrading `jedi` is easy enough for me, but much harder for someone who has never written a line of code before.",2021-01-27T16:29:49Z,29104956
1695,ipython/ipython,774727384,768416652,"Releasing a new Jedi version might be easier than waiting for the next IPython release.

@davidhalter, how about making a one time exception in this case and reintroducing the deprecated features, the lack of which breaks IPython (and a few more packages) in a Jedi 0.18.1?",2021-01-27T16:45:36Z,4623504
1696,ipython/ipython,774727384,768417720,"> What the timeline looks like for this getting fixed for mainline conda users?

You might be successful if you ask the conda maintainers to yank jedi 0.18 from their repos.",2021-01-27T16:47:09Z,4623504
1697,ipython/ipython,774727384,768420082,"> You might be successful if you ask the conda maintainers to yank jedi 0.18 from their repos.

The last `ipython` builds on conda-forge [already](https://github.com/conda-forge/ipython-feedstock/blob/master/recipe/meta.yaml#L33-L34) depend on `jedi<0.18`, so this this shouldn't even happen if you create a new environment (or do `conda update ipython` first).",2021-01-27T16:50:35Z,33685575
1698,ipython/ipython,774727384,768420717,">The last ipython builds on conda-forge already depend on jedi<0.18, so this this shouldn't even happen if you create a new environment (or do conda update ipython first).

Presumably if you have set conda-forge as the default channel, correct?
",2021-01-27T16:51:31Z,29104956
1699,ipython/ipython,774727384,768424748,"Even Anaconda [followed](https://github.com/AnacondaRecipes/aggregate/commit/fdd96f6968d7d4b6b8ec68ff0caa4d8566e953f9#diff-b85a555c399c3f7c88b8c090a978f34b985a1b4d23db5d0fd8887dfc80b476d0) with this already, so that shouldn't matter.",2021-01-27T16:57:26Z,33685575
1700,ipython/ipython,774727384,768433164,"Hmm.. Then why is it that creating a new conda environment (with `conda 4.9.2`) gives me:

```
>>> conda create -n test_it python=3.8 ipython
The following NEW packages will be INSTALLED:
...
ipython            pkgs/main/win-64::ipython-7.19.0-py38hd4e2768_0
...
jedi               pkgs/main/win-64::jedi-0.18.0-py38haa95532_1
```

Am I missing something here?",2021-01-27T17:09:39Z,29104956
1701,ipython/ipython,774727384,768438937,"So the packages are definitely [there](https://anaconda.org/main/ipython/files) (notice the `_1` build number), but interestingly, are not seeing much (or even any) downloads. I don't know if your anaconda channel is behind a proxy somewhere (or indeed, if anaconda themselves do some more integration testing before a package goes ""live""), but in any case, you can work around this by adding `conda config --add channels conda-forge`.

Side note: For any user that's not under the strictest of regulatory requirements (where people want to be able to blame a provider like Anaconda if stuff goes bad), conda-forge is more than ready/stable/secure to be your default channel. But, fair warning, that's just my subjective opinion.",2021-01-27T17:17:25Z,33685575
1702,ipython/ipython,774727384,768439476,"> Am I missing something here?

Not you, but conda's dependency resolver. This is a problem and has been for years.  (Sorry for the off-topic, but I could not resist)",2021-01-27T17:18:18Z,4623504
1703,ipython/ipython,774727384,768450773,"> So the packages are definitely [there](https://anaconda.org/main/ipython/files) (notice the `_1` build number), but interestingly, are not seeing much (or even any) downloads

My totally uneducated guess (because I never understood the internals of the conda resolver):
conda sees the conflict for `jedi==0.18.0 and ipython-7.19.0-..._1` and instead of resolving it by `jedi==0.17.2 and ipython-7.19.0-..._1` it chooses `jedi==0.18.0 and ipython-7.19.0-..._0`",2021-01-27T17:34:17Z,4623504
1704,ipython/ipython,774727384,768451169,"Welp, spoke too soon, conda-forge has the same problem (basically, the higher build number seems to lose against having one dependency have a higher version, which is not how it _should_ work. A possible solution would be backporting https://github.com/ipython/ipython/commit/dcd9dc90aee7e4c5c52ce44c18e7518934790612 to the feedstock, but that really should happen here first).

> Not you, but conda's dependency resolver. This is a problem and has been for years. (Sorry for the off-topic, but I could not resist)

Have you heard of [mamba](https://github.com/mamba-org/mamba) already? It has a rewritten resolver but is fully compatible with the conda-ecosystem. Not only does it solve the biggest problem of the conda-solver (speed), but it also correctly sets up the environment here:

```
>mamba create -n test python=3.8 ipython
[...]
  ipython                             7.19.0  py38hc5df569_2      conda-forge/win-64       1 MB
  ipython_genutils                     0.2.0  py_1                conda-forge/noarch      21 KB
  jedi                                0.17.2  py38haa244fe_1      conda-forge/win-64     944 KB
[...]
```",2021-01-27T17:34:54Z,33685575
1705,ipython/ipython,774727384,768459710,"Thank you, both of you, for the useful insights and recommendations. I really appreciate it.

It sounds like I will go the route of putting a call-out box in my website that adds a hand-holding jedi-downgrading walkthrough (although, then I have to hope that the students have the awareness to repeat that step when creating new environments)

As it stands, I am dangerously close to making *two* ""but think of the children"" pleas in a single github issue, which is a violation of one of my new years resolutions.

(mamba looks cool!)",2021-01-27T17:49:07Z,29104956
1706,ipython/ipython,774727384,768471208,"@rsokl
I raised an issue with conda itself, since it's probably too much (though not impossible) to do this on the conda-forge packaging side (in a way that's also compatible across several jedi-versions) - raised an [issue](https://github.com/conda-forge/ipython-feedstock/issues/127) to discuss that too...

@Carreau [said](https://github.com/ipython/ipython/pull/12751#issuecomment-758065462) he'd make a new release soon, which would of course also solve things.

In the meantime, the ""hand-holding"" guide should be as easy as `conda install ipython ""jedi<0.18""` (the quotes are important though, as otherwise you end up piping to nowhere).",2021-01-27T18:07:31Z,33685575
1707,ipython/ipython,774727384,768898772,"@rsokl
There was one other way I hadn't thought about - fixing the conda-forge repodata directly. This was done yesterday, and means the problem is fixed _within_ conda-forge, i.e. when using strict channel priority (ignoring the main anaconda package that still has the same problem) - which unfortunately is not the out-of-the-box behaviour for conda though.

You can either do this for a single install (e.g. `conda create -n test --strict-channel-priority python=3.8 ipython`), but the recommended way (see e.g. [here](https://conda-forge.org/)) is persisting that setting with
```
conda config --set channel_priority strict
```",2021-01-28T08:50:46Z,33685575
1708,ipython/ipython,774727384,770072154,"> Thank you, both of you, for the useful insights and recommendations. I really appreciate it.
> 
> It sounds like I will go the route of putting a call-out box in my website that adds a hand-holding jedi-downgrading walkthrough (although, then I have to hope that the students have the awareness to repeat that step when creating new environments)
> 
> As it stands, I am dangerously close to making _two_ ""but think of the children"" pleas in a single github issue, which is a violation of one of my new years resolutions.
> 
> (mamba looks cool!)

@rsoki, can you post a link to your walkthrough?  I think some others among us have the same problem...",2021-01-29T21:57:33Z,822811
1709,ipython/ipython,774727384,770075806,"BTW, ""Effective Python"" (2nd edition) #89 suggests using the warnings module to handle things like deprecating arguments.",2021-01-29T22:06:15Z,822811
1710,ipython/ipython,774727384,770271052,"@gkuenning I just uploaded the downgrade instructions to Python Like You Mean It.

Under: [Information Introduction to Python](https://www.pythonlikeyoumeanit.com/Module1_GettingStartedWithPython/Informal_Intro_Python.html) and under [Jupyter Notebooks](https://www.pythonlikeyoumeanit.com/Module1_GettingStartedWithPython/Jupyter_Notebooks.html)",2021-01-30T19:49:49Z,29104956
1711,ipython/ipython,774727384,770277649,"@rsokl 
Actually, anaconda fixed their repo data as well, so now it works out of the box again 🥳 
",2021-01-30T20:42:13Z,33685575
1712,ipython/ipython,774727384,770284507,Nice! I just confirmed this on my end. I guess I'll keep those callout boxes up on PLYMI for a bit in case folks had already installed anaconda. ,2021-01-30T21:37:19Z,29104956
1713,ipython/ipython,774727384,770295999,"I'll try to release 7.20 soon; I've make it compatible with jedi 0.18 as pip also use a resolver and so pinning is not an option as pip would be free to downgrade IPython and keep jedi 0.18 which will brake. 

Right now the limiting factor for the release is writing the what's new. ",2021-01-30T23:21:56Z,335567
1714,ipython/ipython,774727384,770297420,"> I'll try to release 7.20 soon; I've make it compatible with jedi 0.18 as pip also use a resolver and so pinning is not an option as pip would be free to downgrade IPython and keep jedi 0.18 which will brake.
> 
> Right now the limiting factor for the release is writing the what's new.

I am really disappointed that the maintainer of a great product as Ipython has such a limited knowledge about the problem.
The fact that Ipython has not been pinning jedi means that any version of it released in the past that depends on jedi is at its merci to stop working as soon as a no backwards compatible change is introduced and used in Ipython.
By changing Ipython to make it jedi 0.18 compatible and releasing 7.20 you make that 7.20 operational today. If tomorrow jedi makes another backwards incompatible change We will be screwed again.
And you can't blame jedi for backwards incompatible changes because it is not even 1.x and you are not even pinning the major.
You are calling for trouble.",2021-01-30T23:36:04Z,50870910
1715,ipython/ipython,774727384,770299991,"> I am really disappointed that the maintainer of a great product as Ipython has such a limited knowledge about the problem.
> The fact that Ipython has not been pinning jedi means that any version of it released in the past that depends on jedi is at its merci to stop working as soon as a no backwards compatible change is introduced and used in Ipython.
> By changing Ipython to make it jedi 0.18 compatible and releasing 7.20 you make that 7.20 operational today. If tomorrow jedi makes another backwards incompatible change We will be screwed again.
> And you can't blame jedi for backwards incompatible changes because it is not even 1.x and you are not even pinning the major.
> You are calling for trouble.

I'm amazed that someone that contribute squat is telling me what i'm doing wrong without tryign to really think about the problem.

- Say I pin IPython 7.20 to jedi <0.19 , and david release jedi 0.19. 
- pip is no free to say ""hey let's install jedi 0.19 and downgrade IPython to 5.x"". 
- Bam you are wrong™️ . 💎 🚀 🌕 

And second the change of API were in jedi 0.17 with deprecation warnings and fix in master close to a year ago; I thought that by now 8.0 would be out. 

FYI about me know nothing about versioning:
  - I spend month implementing on pushing the `python_requires` logic in pypi, and in pip to make sure when IPython dropped it did not break for Python 2 users
  - I'm one of the regular advocate to have version medata data and package content to be separated in conda/pypi for this exact reason.
  - It's not because something seem obvious that it's correct.

You could have just asked in for a question ""why are not you pinning to <0.19 starting with IPython 7.20"", to which I would have responded.
So not only you are impolite and haven't done your research,  everybody see you incorrectly assume you know better than others and  you've lost all credibility.

✋ ⬇️ 🎤 ",2021-01-30T23:55:05Z,335567
1716,ipython/ipython,774727384,770300405,"@Carreau: dude, you rock!",2021-01-30T23:59:24Z,25695763
1717,ipython/ipython,774727384,770302515,"So essentially you say as there are problems with pip resolver and in some cases it will not be able to upgrade correctly I will completely ignore the problem and not try to solve other uses cases that I can.
Yes, fixing Ipython to work with the latest jedi is correct and pinning is needed as well.
Most use cases of Ipython will be install Ipython on a new environment. Should you pin, your 7.20 will work always. Ignore it and you depend on your dependencies.
The reason I didn't ask why you are planning to do what you are planning to do is because I know that solution is suboptimal.
It doesn't matter what work or knowledge has been accumulated in the past, that says nothing.
",2021-01-31T00:20:40Z,50870910
1718,ipython/ipython,774727384,770351128,"> pip is no[w] free to say ""hey let's install jedi 0.19 and downgrade IPython to 5.x"".

That would indeed be unfortunate, the resulting setup would be not only broken, but out-of-date to boot. Can that happen when running `pip install ipython` in a fresh environment? I would naively assume that this would fetch the latest version of IPython and figure out the dependencies from there, i.e. honoring `jedi < 0.19`. But that assumption is probably wrong?

As maintainer of my own small Python package with moderately complicated dependencies, I feel like I should read up on this :) Any tips on good material on this topic?",2021-01-31T09:12:14Z,2734517
1719,ipython/ipython,774727384,770354714,"@mostealth Obviously you have a different perspective on that matter. That's fine. However, it seems rather unproductive to dissipate the time and energy of productive people like @Carreau with pointless discussions. If you have deeper insights I see the following options: You can fork the project, file a PR or just sate your improvement suggestion in a way that is arguable, maybe in a dedicated issue.

For the rest of us I would suggest to basically ignore distracting comments which contain neither helpful information nor constructive criticism, but instead spread insults and negative emotion. Reactinv with :-1: should be enough attention for such comments.",2021-01-31T09:43:17Z,1315698
1720,ipython/ipython,774727384,770381551,"@Carreau Thanks a lot! I guess the next release is going to be 1.0 anyway, so it shouldn't happen again. We might then think about pinning `jedi < 2.0.0` (which is probably years away).

In general for all people like @mostealth: If you are worried about older versions breaking: **Pin your dependencies!** The Python packaging system is still in a bit of a problematic state. The devs are doing a good job trying to turn it around, but there is a lot of technical debt from years ago. Keep in mind they want to keep backwards compatibility with a lot of old and new things. Just a enumerating a few things to show how complicated it is: eggs, wheels, explicit namespace packages, setuptools, distutils, import hooks, import meta paths, importlib, easy_install, venvs, virtualenvs, zip imports, freezed packages, source packages for C/C++ libraries and a lot of other things that I don't even know or remember anymore.

Most of you guys complaining don't really understand how hard certain things are if you consider all consequences. Read about Chesterton's Fence and try to understand why you shouldn't want something changed unless you understand why it is that way in the first place.",2021-01-31T13:19:45Z,748594
1721,ipython/ipython,774727384,770436983,"Thank you @Carreau. Finally someone who does not follow the church of pinning stuff without thinking about the consequences. Here is another case why pinning to older versions is not always a good idea: https://github.com/palantir/python-language-server/issues/898#issuecomment-770399215.

Unfortunately the Jedi and Parso pair are a frequent cause for headaches for packagers. Not only pip and conda, but also Linux distributions. I am looking forward to a more stable API once they reach 1.0.",2021-01-31T19:31:25Z,4623504
1722,ipython/ipython,774727384,771176323,"Thanks everyone for the inputs; if we want to have discussion as to why/when pinning and why when not to pin, and other I suggest we don't use the bug tracker, maybe discourse.jupyter.org; but stop adding to this particular issue. 

IPython is a bit in a tough spot as it is both a library and application (in general you want to have a lock file to pin deps for applications, but be as liberal as possible when building a library). 

There is also a lot of issues that arise from the fact that maintainers time and desire to maintain can come and go; I definitely thought that by sept 2020 IPython 8 would be out, and that other maintainers like @davidhalter from jedi would also welcome more testing and help.

The Python package ecosystem is also quite old; and PyPI relatively recent; the requirements and needs of today's infrastructure can also be much different than what was expected even 5 years ago. That plus it works from HPC system to end-user  that are starting to program so there is a lot of difficulties there.

Thanks everyone, I've now released 7.20 feel free to open bug reports if you encounter any issues. 
For now I'm going to lock this thread to avoid getting much more off topic.
",2021-02-01T21:37:31Z,335567
1723,ipython/ipython,654775761,654775761,"Currently IPython uses `~/.ipython` which just clutters people's home directories and makes things like backing up configuration files much more hassle than it should be. 

This was last discussed more than five years ago and should be discussed again. Points made at that point in time frankly, don't really hold up. It is more consistent to follow platform specs, people learn where to look first. Secondly, for support reasons, you don't really have to ask what platform they're on, just mention the three paths in one sentence, it's actually rather easy. ",2020-07-10T13:17:17Z,5392141
1724,ipython/ipython,654775761,660626627,"For those looking for the previous discussion, it can be found [here](https://github.com/ipython/ipython/pull/4457).",2020-07-19T11:05:02Z,10088591
1725,ipython/ipython,654775761,661478060,"I believe if you move your folders in XDG compliant places it should work, though updating all the code and documentation all over the internet, plus the code to do the right thing is a lot of work; and XDG spec IIRC did not completely match or was not clear for all the types of files that could be present. 

There might be packages on PyPI that may help with ~/.config /User/.../Libraries/AppData. %appdata% depending on OS, and this will also likely need to touch all the jupyter ecosystem (ipykernel, jupyter_client, traitlets, etc, so I doubt there'll be an effort from core dev to push that forward.

THough if you have issues when files _exists_ in XDG placed and not found I""ll be happy to get fixes in. ",2020-07-21T00:17:54Z,335567
1726,ipython/ipython,654775761,661510731,"@Carreau 

You're being a bit facetious. There's no need to update ""all the code and documentation all over the internet"", nobody has every done that and never will. Neither would the updating even be necessary, the official documentation should contain the location of the configuration file, that's it. The third silly thing is that you're acting like IPython doesn't have any changes that break some pieces of code or tutorials out there. All in all it makes your comment seem more like irrational stubbornness than actual concerns.

Not to mention, if it's _that_ hard to change the folder, then that hardcoding seems like a massive code smell anyways and should be fixed anyways, and the choice to set the location of the folder should be up to the user.",2020-07-21T00:58:26Z,5392141
1727,ipython/ipython,654775761,661544204,"Hey, I was there for the first or changing default configuration directory, and all the issues it created; And I'm one of the person who has to deal both with public and private requests about the bits of documentation that users have tried on the internet and does not work because its inaccurate, either when it was written or because IPython changed, so yes I'm well aware of the consequences of changing. 

And so yes the official documentation is one of the last place users looks for that information, and yes sometime they look at the latest docs for 5 years old IPython version.

Yes `~/.ipython` is not optimal, and no it's not a code smell we removed all the complex logic because it was brittle and was super confusing, and because every time we were teaching software carpentry there was section with all the combinaisons of linux/mac/windows IPython version x.y.z

So yes I was of your opinion 10 years ago, and no to this days I still don't know the 3 path on each of the platform, even on Mac which I'm using everyday and which now hides ~/Library in the finder.

You can change the code location by setting environment variables, and I told you that if it was not working with configuration in xdg dir I would gladly accept patches for that so that you have  the choice. 

Now if it's to criticize the code without proper understanding of the technical and social reason of why it is the way it is, and bring XDG zealotry into the mix, I'm even less likely to make an effort.

PR still accepted but discussion closed.

 ",2020-07-21T01:43:18Z,335567
1728,doctrine/dbal,1418577350,1418577350,"Recently, I tried to enable checking exceptions in PHPStan and, besides a few other issues with the DBAL API, stumbled upon the one that shows what an awfully overcomplicated piece of work the Events API is:

1. `AbstractPlatform::getDropTableSQL()` dispatches an `onSchemaDropTable` event which can override the SQL used to drop the table.
2. The return type of `SchemaDropTableEventArgs::getSql()` is `?string`. If it returns `NULL`, the platform will throw an `UnexpectedValueException`.
3. There's no way to tell that a given method is an event handler unless it's documented as such or whoever reads the code knows that the methods with names starting with `on` are event handlers. Those methods are not invoked from anywhere in the code explicitly.

Here's a chain of questions that leads to the above conclusion:

1. Why is `SchemaDropTableEventArgs` allowed to return `NULL` if it's not even a valid value?
2. We could require that it returns a `string` but then what would it return if the handler of the event didn't call `setSql()`?
4. Putting aside the event-handling scenario, what should the `SchemaDropTableEventArgs` class do if it was instantiated and then got its `setSql()` invoked? Maybe throw a logical exception?
5. Then, should it throw this exception unconditionally or only if it got a call to `preventDefault()` first?

### Problems with the API design and implementation

What this API does, most likely could be solved just by extending the platform class and overriding the corresponding methods. Besides this awfully implicitly stateful logic, a few more concerns about DDL events:

1. It looks like it's an overengineered solution to what could be easily solved by extending the corresponding platform methods.
2. Unlike extending the platform, events don't allow invoking the default implementation (the method of the `parent` class).
3. Unlike class extension, events don't provide access to the original logic like a call to the `parent` method would.
4. The DDL events API is incomplete (e.g. one can override `DROP TABLE` but cannot override `DROP FOREIGN KEY`).
6. In event-driven systems and DDD, an event is an immutable object representing a fact that happened in the past. Which fact does an `onSchemaDropTable` event represent? Let's assume it's a fact that somebody intended to drop a table. Why is it mutable? Why does it carry any information about processing the event (prevent default) and even implementation of that processing (SQL)?
7. For 800 lines of production code, there are three unit tests with mocks and not a single integration test which would demonstrate how this API can be used and if it can be used at all.
8. The documentation shows only how to register handlers but not what they should to.

### Problems with the EventManager API as such

This API is unsafe from the standpoint of the types:

1. There's no way to enforce the fact that a given event (`onSomething`) is dispatched only with a specific type of arguments (`SomethingArgs`).
2. There's no way to guarantee that the given listener supports handling the given type of event. The dispatcher calls the method corresponding to the event name dynamically:
   ```php
   $listener->$eventName($eventArgs)
   ```

The mistakes in the code that implements the handling of events will lead to a type error at runtime.

### Types of events
1. Connection and transaction events: https://github.com/doctrine/dbal/blob/a85d913ca0d95fdadcd8967660dfe14165c11cd1/src/Events.php#L21 https://github.com/doctrine/dbal/blob/a85d913ca0d95fdadcd8967660dfe14165c11cd1/src/Events.php#L33-L35 The logic implemented on top of these events can be implemented using driver middlewares.
2. Schema manipulation events: https://github.com/doctrine/dbal/blob/a85d913ca0d95fdadcd8967660dfe14165c11cd1/src/Events.php#L23-L30 Depending on the use case, it should be advised to either extend the corresponding platform method, or modify the arguments being passed to the method before calling it.
3. Schema introspection events: https://github.com/doctrine/dbal/blob/a85d913ca0d95fdadcd8967660dfe14165c11cd1/src/Events.php#L31-L32 This is the hardest part: we allow users to hook into the very internals of the schema introspection, e.g. the API consumer is exposed to the SQL results representing raw introspected data. By definition, this API is very brittle since the raw results is an implementation detail that can and will change. The fact that we allow this extension complicates the refactoring of the Schema Manager and Platform API.",2022-10-21T16:21:12Z,59683
1729,doctrine/dbal,1418577350,1294799010,"I don't want to sound too negative, but that's a bit of a drive-by deprecation, no? Ticket was opened 7 days ago and 6 days ago it was merged. Would be nice to have a bit of a heads-up on stuff that requires consumers to rewrite their stuff. 

I am currently using the `onSchemaColumnDefinition` and `onSchemaCreateTable`, did I understand it correctly that now instead of using these events, I'm supposed to extend/override all the Platform classes I use?",2022-10-28T10:01:52Z,425166
1730,doctrine/dbal,1418577350,1294905860,"> Would be nice to have a bit of a heads-up on stuff that requires consumers to rewrite their stuff.

Isn't that exactly what a deprecation does? What would you do?

> did I understand it correctly that now instead of using these events, I'm supposed to extend/override all the Platform classes I use?

I don't think that's the right thing to do, see https://doctrine.slack.com/archives/CA9600CLC/p1666886106048959",2022-10-28T11:54:37Z,657779
1731,doctrine/dbal,1418577350,1294928288,"> Isn't that exactly what a deprecation does? What would you do?

I just mean the plan to deprecate was announced (i.e. this ticket opened) and one day later it was already implemented. It just seems like it would have been nicer to keep it open for a while to see if there is any user feedback (Also, some migration guide would really be appreciated).

> I don't think that's the right thing to do, see https://doctrine.slack.com/archives/CA9600CLC/p1666886106048959

That wants me to log into something. Is there a public version of it maybe?",2022-10-28T12:17:38Z,425166
1732,doctrine/dbal,1418577350,1294940017,"The deprecation of the event system happened in an effort to consolidate our extension points. But if we discover that this deprecation was premature, we can talk about it. Can you please open a new issue where you explain for what purpose you use the event system and why extending the platform would make that more difficult for you?",2022-10-28T12:29:04Z,1506493
1733,doctrine/dbal,1418577350,1294964977,"Sure, I can look into that. To be clear: I don't mind you deprecating stuff as long as there's an alternative way to implement the functionality I need. 

Just to make sure: @derrabus you think extending Platform (as in `Doctrine\DBAL\Platforms\SqlitePlatform` etc.?) would the the way to go, but @greg0ire you don't think so?

BTW, one scenario I use I described here:

https://github.com/doctrine/dbal/issues/4676#issuecomment-865151733",2022-10-28T12:53:17Z,425166
1734,doctrine/dbal,1418577350,1294983584,"According to https://doctrine.slack.com/archives/CA9600CLC/p1666886106048959, it isn't:

![2022-10-28_15-11](https://user-images.githubusercontent.com/657779/198601020-86205a28-773e-489f-b920-bbdb7ff46677.png)
",2022-10-28T13:11:36Z,657779
1735,doctrine/dbal,1418577350,1295042907,"@greg0ire thanks for the info! I just tried to figure this out, but somehow I'm stuck: AFAICT the `SchemaManager` to use gets determined in the `Platform`:

https://github.com/doctrine/dbal/blob/d3b8e804e5fa3995c9b3951ac8c4faed891957eb/src/Platforms/SqlitePlatform.php#L1481-L1484

So to use a custom `SchemaManager` I would have to extend the `Platform` to return my own instance. The `Platform` gets determined in the `Driver`:

https://github.com/doctrine/dbal/blob/d3b8e804e5fa3995c9b3951ac8c4faed891957eb/src/Driver/AbstractSQLiteDriver.php#L24-L27

And to override the `Driver`, I would have to use `driverClass` here:

https://github.com/doctrine/dbal/blob/d3b8e804e5fa3995c9b3951ac8c4faed891957eb/src/DriverManager.php#L245-L253

Which means I'd have to copy the `self::DRIVER_MAP` logic (because it's private), but then also the actual driver class is marked `final`, so I can't extend it:

https://github.com/doctrine/dbal/blob/d3b8e804e5fa3995c9b3951ac8c4faed891957eb/src/Driver/SQLite3/Driver.php#L9

Is there something I'm missing?
",2022-10-28T14:05:19Z,425166
1736,doctrine/dbal,1418577350,1295466116,"> The Platform gets determined in the Driver

… if you don't pass one via the `platform` option to the `DriverManager`.",2022-10-28T21:05:23Z,1506493
1737,doctrine/dbal,1418577350,1296881001,"@derrabus thanks, that helped! So for `onSchemaColumnDefinition`, I would need approximately this amount of boilerplate, right?

```php
$db_config = [/* config as before, driver set to pdo_mysql or pdo_sqlite */];

if ($db_config['driver'] == 'pdo_mysql') {
    $db_config['platform'] = my_mysql_platform::class;
} elseif ($db_config['driver'] == 'pdo_sqlite') {
    $db_config['platform'] = my_sqlite_platform::class;
} else {
    throw new Exception('Oh noes!!!');
}
/* startup connection as before */

class my_mysql_platform extends Doctrine\DBAL\Platforms\MySQLPlatform
{
    public function createSchemaManager(Doctrine\DBAL\Connection $connection): Doctrine\DBAL\Schema\MySQLSchemaManager
    {
        return new my_mysql_schemamanager($connection, $this);
    }
}

class my_sqlite_platform extends Doctrine\DBAL\Platforms\SqlitePlatform
{
    public function createSchemaManager(Doctrine\DBAL\Connection $connection): Doctrine\DBAL\Schema\SqliteSchemaManager
    {
        return new my_sqlite_schemamanager($connection, $this);
    }
}

class my_mysql_schemamanager extends Doctrine\DBAL\Schema\MySQLSchemaManager
{
    protected function _getPortableTableColumnList($table, $database, $tableColumns)
    {
        $tableColumns = onSchemaColumnDefinition($tableColumns);
        return parent::_getPortableTableColumnList($table, $database, $tableColumns);
    }
}

class my_sqlite_schemamanager extends Doctrine\DBAL\Schema\SqliteSchemaManager
{
    protected function _getPortableTableColumnList($table, $database, $tableColumns)
    {
        $tableColumns = onSchemaColumnDefinition($tableColumns);
        return parent::_getPortableTableColumnList($table, $database, $tableColumns);
    }
}

function onSchemaColumnDefinition($tableColumns)
{
    /* event listener code goes here */
    return $tableColumns;
}
```

Not exactly pretty, but I guess somewhat manageable, if those protected functions stay stable at least. The `onSchemaCreateTable` looks a bit more annoying to simulate though, because it's kind of called in the middle of a function, and it doesn't look like there's a way to just inject something without copying half the method's code (although, in fairness, my current listener also copies most of the `_getCreateTableSQL` function due to the way `preventDefault` works)",2022-10-31T10:21:07Z,425166
1738,doctrine/dbal,1418577350,1296898973,"Yes, that looks about right.",2022-10-31T10:36:34Z,1506493
1739,doctrine/dbal,1418577350,1297155068,"Hello, I am the author of the Slack question mentioned above. Thank you for your discussion and suggestions. I am trying to implement the solution proposed by @flack , but I am running into the fact that I will probably have to extend the Driver as well.

The reason is that although I pass the platform in the connection configuration, the original SchemaManager is created in the Driver's [getSchemaManager()](https://github.com/doctrine/dbal/blob/b66f55c7037402d9f522f19d86841e71c09f0195/src/Driver/AbstractPostgreSQLDriver.php#L81) method and my createSchemaManager() method is never called.

This brings me back to the fact that Doctrine\DBAL\Driver\PDO\PgSQL\Driver is final.

Did I understand that correctly, please? Thanks!",2022-10-31T14:14:07Z,450740
1740,doctrine/dbal,1418577350,1302618274,"@jaroslavlibal 
you can use decorator to wrap Driver and make doctrine use your custom schema manager
```php
use Doctrine\DBAL\Connection;
use Doctrine\DBAL\Driver;
use Doctrine\DBAL\Driver\API\ExceptionConverter;
use Doctrine\DBAL\Platforms\AbstractPlatform;
use Doctrine\DBAL\Platforms\PostgreSQLPlatform;

class YourCustomDriver implements Driver
{
    public function __construct(
        private readonly Driver $underlyingDriver
    ) {
    }

    public function getSchemaManager(Connection $conn, AbstractPlatform $platform)
    {
        return $platform->createSchemaManager($conn);
    }

    public function connect(array $params): Driver\Connection
    {
        return $this->underlyingDriver->connect($params);
    }

    public function getDatabasePlatform(): AbstractPlatform
    {
        return $this->underlyingDriver->getDatabasePlatform();
    }

    public function getExceptionConverter(): ExceptionConverter
    {
        return $this->underlyingDriver->getExceptionConverter();
    }
}
```

```php
use Doctrine\DBAL\Driver;
class Middleware implements Driver\Middleware
{
    public function wrap(Driver $driver): Driver
    {
        return new YourCustomDriver($driver);
    }
}
```
if you configured everything else correctly this code will make sure your custom schema manager is used

in my case, i've extended `PostgreSQLPlatform` and overriden `createSchemaManager` to return custom schema manager
you'll also need to edit your `doctrine.yaml` config with custom platform:
```yaml
doctrine:
  dbal:
    platform_service: <yourCustomPlatform>
```
",2022-11-03T20:17:14Z,80004828
1741,doctrine/dbal,1418577350,1304181315,"The effort and goal is clear, the way this has been done unfortunately is very rough.

API is inconsistent and could be done better: ok, we can agree on that. But deprecate an extension system in full without a single day of discussion is unpleasant, especially if done in a such popular project like this one.

On technical aspects:

- some events can be replaced by middlewares: upgrade path here is clear and sound, but they are only two. Most of those cannot be replaced this way.
- generated/custom sql is not the only information carried by the event object: tables and columns for example can be retrieved and modified in the event handler.
- you suggested to extend the platform, but it is obviously too overkill when the only thing you have to do is to add a column to a table. On the contrary, when you need to modify various tables it is simpler and more maintenable to have such code in different classes you could unit-test separately.
- you also suggested to modify the input before using schema manager/platform, but this is not always feasible, especially when using the ORM.
- DDD is not the goal of this library, immutability is not required for the events. Events here are an extension point, not something to be saved/retained as part of the normal flow.
- dynamic method call is unreliable and api of the event manager can be fragile. Ok, but meanwhile some community standards have been published and widely used. Using a PSR-compatible event dispatcher can help to clean up the code and can help you to move the maintenance burden of the event manager to others.

IMHO the way this thing has been done is not very nice to the community, and lacked a wider vision on how this project is used in the wild. What makes me sad is the low consideration of the community feedback: 24 hours issue is barely a communication and left no space for discussion. To me this is not acceptable in a such important project: it is widely used, production apps are written onto it and should take this responsibility seriously giving the community the time to evaluate such an important change and express an opinion.",2022-11-04T20:26:05Z,1257206
1742,doctrine/dbal,1418577350,1304775416,"> But deprecate an extension system in full without a single day of discussion is unpleasant, especially if done in a such popular project like this one.

I don't know if you realize that or if you just haven't read the previous comments, but we're actually having the discussion you're asking for. A deprecation is not an immediate removal. And we we can surely take it back if you give us good reasons to.

> generated/custom sql is not the only information carried by the event object: tables and columns for example can be retrieved and modified in the event handler.

Do you think, an event handler is the best way to expose this kind of extension point? If you designed this feature from scratch, how would you build the extension point that you need for your scenario?

> you suggested to extend the platform, but it is obviously too overkill when the only thing you have to do is to add a column to a table.

Why would you need to hook into the DBAL to add a column? Why can't you add it to the schema directly? Sorry if my question sounds stupid, but I'd like to understand you problem before we decide on an action here.

> you also suggested to modify the input before using schema manager/platform, but this is not always feasible, especially when using the ORM.

Can you give us examples? Is this something the ORM should address instead?

> Using a PSR-compatible event dispatcher can help to clean up the code and can help you to move the maintenance burden of the event manager to others.

The event manager is a trivial library. Sunsetting it is not our intention and certainly was not the motivation here.",2022-11-06T11:07:15Z,1506493
1743,doctrine/dbal,1418577350,1304807496,"> I don't know if you realize that or if you just haven't read the previous comments, but we're actually having the discussion you're asking for. A deprecation is not an immediate removal. And we we can surely take it back if you give us good reasons to.

From my POV, a discussion is needed before to take action, not after. This is a communication of what's have been already done, and doesn't matter if it can be un-done. A discussion can take place on a proposal, which this is not. This is a fact: events have been deprecated in the current public release and removed in the first beta of the next major version.

> Do you think, an event handler is the best way to expose this kind of extension point? If you designed this feature from scratch, how would you build the extension point that you need for your scenario?

Yes, why not? An entire framework (Symfony) has been written around an event manager, where handlers continuously modify and process information and structures, for example enhancing the request object, transforming the request body, etc.
Why DBAL cannot use the same flow?
To be clear: I don't mean that everything in the current event system is ok and does not need to be revisited/reworked, but surely I would retain the event system as a powerful extension point. 

> Why would you need to hook into the DBAL to add a column? Why can't you add it to the schema directly? Sorry if my question sounds stupid, but I'd like to understand you problem before we decide on an action here.
> Can you give us examples? Is this something the ORM should address instead?

I'll answer two questions here because they are connected. I'll give you some examples:

- I have an ORM entity with a ""priority"" field but I want to avoid that two records have the same exact value, so when you set a value that already exists, all the values after it needs to be updated. The ORM can handle it, but its very costly, so I made an optimization with a trigger that involves an additional column on that table. At the same time I want that column to be hidden from the ORM, it does not really exist, it's an implementation detail. Using the migrations library I cannot modify the schema in the middle of the process, I have to hook into DBAL and add the column when necessary.
- I wrote a Symfony Messenger transport that uses DBAL as storage backend. I want its tables and resources to be completely hidden from the ORM. In the bundle I have to register an event handler to generate the resources on schema creation so the doctrine's commands will work with no additional configuration.
- I have an application that uses a table with the ""ARCHIVE"" mysql engine. Using the ORM I cannot set it correctly, I have to hook into the schema generation process and modify the engine from ""InnoDB"" to ""ARCHIVE"".
- In an application I want to generate a VIEW resource alongside a normal table: this is something that cannot be done into the ORM. To work with schema commands and migrations I have to hook into DBAL in the middle of the schema generation.
- For a translation system I have to dynamically generate translation tables to be used as storage for the various ""translatable"" resources: again this is something that needs to be hidden from the ORM, but needs to work with schema commands and migrations.

> The event manager is a trivial library. Sunsetting it is not our intention and certainly was not the motivation here.

So those listed under ""Problems with the EventManager API as such"" are not valid reasons to deprecate and remove the event system? Because a PSR-compatible event dispatcher will solve both problems. And lately, will increase the integration level with the frameworks in general.
",2022-11-06T13:58:22Z,1257206
1744,doctrine/dbal,1418577350,1304837278,"> From my POV, a discussion is needed before to take action, not after.

And you would've participated in that discussion? Would you have noticed that it even happens? During the last months of DBAL development, I have barely seen any real discussion going on any PR. Users of the DBAL don't monitor this repository and that's absolutely okay. Trust me: We could've kept that PR open for two more weeks and the only difference would've been that you would have one thing less to complain about.

Really, I value every constructive feedback that we get from the community. But please think about how you address people who invest their free-time to build libraries that you can use for free. A phrase like ""this is not acceptable"" feels a bit inappropriate here, don't you think?

> Using the migrations library I cannot modify the schema in the middle of the process, I have to hook into DBAL and add the column when necessary.

I think, this is the issue we have to solve. We need to enable you to modify the schema _after_ the ORM has generted it from the metadata and _before_ it's being used for comparisons with the actual database.

> I have an application that uses a table with the ""ARCHIVE"" mysql engine. Using the ORM I cannot set it correctly, I have to hook into the schema generation process and modify the engine from ""InnoDB"" to ""ARCHIVE"".

Do you think, we should enable you configure vendor-specific tweaks like that in the ORM?

> In an application I want to generate a VIEW resource alongside a normal table: this is something that cannot be done into the ORM. To work with schema commands and migrations I have to hook into DBAL in the middle of the schema generation.

> For a translation system I have to dynamically generate translation tables to be used as storage for the various ""translatable"" resources: again this is something that needs to be hidden from the ORM, but needs to work with schema commands and migrations.

You should be abler to configure patterns of table names that should be ignored during schema comparisons, shouldn't you? I have to look that up though, not entirely sure about how to configure that.

> > The event manager is a trivial library. Sunsetting it is not our intention and certainly was not the motivation here.
> 
> So those listed under ""Problems with the EventManager API as such"" are not valid reasons to deprecate and remove the event system? Because a PSR-compatible event dispatcher will solve both problems.

No, if those were our only problems with the event system, we wouldn't have deprecated it. You are right, a PSR-4 dispatcher would be the solution here.

> And lately, will increase the integration level with the frameworks in general.

I agree. If you want to work on switching the ORM to PSR-14, don't hesitate to investigate a possible migration path.",2022-11-06T16:21:02Z,1506493
1745,doctrine/dbal,1418577350,1304861194,"> And you would've participated in that discussion? Would you have noticed that it even happens?
> Really, I value every constructive feedback that we get from the community. But please think about how you address people who invest their free-time to build libraries that you can use for free. A phrase like ""this is not acceptable"" feels a bit inappropriate here, don't you think?

Really, I don't want to open a flame, so just two things on these and then I want to discuss only the technical aspects: presuming that the discussion would pass unnoticed is wrong in my opinion, and no, *to me* is not inappropriate because is my point-of-view.
On the ""it's free/my free time and work"" that is used in every single open source project no matter how it's big and used, I completely disagree. But again, it's my opinion and here's not the right place for this discussion.

> I think, this is the issue we have to solve. We need to enable you to modify the schema _after_ the ORM has generted it from the metadata and _before_ it's being used for comparisons with the actual database.

That's the problem the event system resolved. If we can find an alternative way, then the migration path would be clear and sound and will not be a feature removal.

> Do you think, we should enable you configure vendor-specific tweaks like that in the ORM?

IMHO no, it's not a thing the ORM should handle. It is a detail that has no space in the ORM, but the underlying library (this one) should do.

> You should be abler to configure patterns of table names that should be ignored during schema comparisons, shouldn't you? I have to look that up though, not entirely sure about how to configure that.

I try to explain it better: I don't want to simply ignore some assets, I need to generate them alongside the tables and columns of the schema generated by the ORM. Solving the problem of modifying the schema generated by the ORM before passing it to DBAL will probably work for these two.
In any case, you should allow to modify and inject custom SQL in the process, or we have the same problem.

> No, if those were our only problems with the event system, we wouldn't have deprecated it. You are right, a PSR-4 dispatcher would be the solution here.

I did not write that they are the *only* problems, but that we shouldn't consider them as valid reasons for this change, so we can remove them from the discussion.

> I agree. If you want to work on switching the ORM to PSR-14, don't hesitate do investigate a possible migration path.

Ok, I'll do.
",2022-11-06T18:13:18Z,1257206
1746,doctrine/dbal,1418577350,1304863433,"> On the ""it's free/my free time and work"" that is used in every single open source project no matter how it's big and used, I completely disagree.

Well then I don't know why I waste any more of my free-time on having a this conversation with you. If we're not the first open-source project to tell you that, maybe your attitude is the problem?

> But again, it's my opinion and here's not the right place for this discussion.

The way you lecture us on how our development process should look like is really disrespectful. ""This is my opinion"" is really a weak argument for any conversation. If you just want to state your opinion, keep it for yourself next time.

> That's the problem the event system resolved.

No, the event system healed a missing extension point. You hooked into the DBAL because you could not hook into the ORMs schema generation.

> > Do you think, we should enable you configure vendor-specific tweaks like that in the ORM?
> 
> IMHO no, it's not a thing the ORM should handle. It is a detail that has no space in the ORM, but the underlying library (this one) should do.

All right.

> I try to explain it better: I don't want to simply ignore some assets, I need to generate them alongside the tables and columns of the schema generated by the ORM. Solving the problem of modifying the schema generated by the ORM before passing it to DBAL will probably work for these two. In any case, you should allow to modify and inject custom SQL in the process, or we have the same problem.

Okay, so we should really try to build that extension point in the ORM. Thank you for those answers, that was really helpful.",2022-11-06T18:25:32Z,1506493
1747,doctrine/dbal,1418577350,1304873269,"> No, the event system healed a missing extension point. You hooked into the DBAL because you could not hook into the ORMs schema generation.

I cannot hook into the ORM schema generation, but even if I could, it will not resolve the point of setting vendor-specific details on how the table/view/... should be created.

To be more clear, I'll give you an example: the `buildCreateTableSQL` method does not contain a way to completely override the generated SQL and the `Table` class does not contain something like the `columnDefinition` property. This means that if I want to override the way the asset is created, now I cannot do it, and the ORM hook after the schema generation will not help in doing so.

Also, appending custom SQL to the generated asset (schema/table/view/index/etc) is not possible with the current objects.
Hooking in the ORM processes will not solve these problems, not with the objects and tools we have at the moment.

I did not meant to hurt or offend anyone, but you're going down on a personal level in the last message.

> ""This is my opinion"" is really a weak argument for any conversation.

Sorry, but on if, when and how to deprecate something, every decision is opinion based. So ""this is my opinion"" is the only thing that counts, because there are no technical reasons to remove something that works and worked well for years.
Also you have never written ""IMHO"" under an issue/PR on github? You're really saying that everyone expressing his own opinion is wasting his time?

> maybe your attitude is the problem?
> keep it for yourself next time.

I really have to answer these statements? And I am the one who's disrespectful?",2022-11-06T19:17:38Z,1257206
1748,doctrine/dbal,1418577350,1304900173,"> I did not meant to hurt or offend anyone,

Well, but you have and you've made it very clear that you don't care.

> there are no technical reasons to remove something that works and worked well for years.

You'd be surprised.

> You're really saying that everyone expressing his own opinion is wasting his time?

I don't.

> > maybe your attitude is the problem?
> > keep it for yourself next time.
> 
> I really have to answer these statements?

No.

> And I am the one who's disrespectful?

Yes.",2022-11-06T21:38:03Z,1506493
1749,doctrine/dbal,1418577350,1305415530,"To anyone who had to witness this unfortunate exchange of messages:

The Doctrine project values constructive feedback from users of our libraries. If a change that we've shipped caused trouble in one of your projects, get in touch with us. We are very careful not to break downstream code, but if we do, it certainly does not happen on purpose.

⚠️ That being said, if I have to read that one of my fellow maintainers, who has invested a lot of work hours in maintaining and modernizing this library, ""lacks vision"" and that the way he managed one particular change is ""not acceptable"" – Crap like that that really gets me on the fence. In the last years, I have seen fine people retiring from open-source because of stress and mental health issues. Let me be crystal clear: I won't tolerate this kind of bullying here.

Back to topic: I understand that if you rely heavily on DBAL's events, the deprecation of the event system might appear a bit premature. But as of today, DBAL 3 is still maintained and the event system is still working. We've release DBAL 4 as a first beta and don't have a release date for a final version yet.

Right now, you can help us smoothen the upgrade path and shape the 4.0 release with us. The Doctrine project is maintained by very few people in their free-time. Any additional help is appreciated.

I have created issues for the 3.6.0 milestone from the feedback I've gathered after the 3.5 release. Comment on those or open new issues if you think, we've missed something. This issue will be locked now.",2022-11-07T10:41:31Z,1506493
1750,doctrine/dbal,1345626221,1345626221,"### Bug Report

For now 10 years  `public` has been returned by `getSchemaNames` generating a lot of bug and weird behaviour in higher level libraries relying on it.


|    Q        |   A
|------------ | ------
| Version     | 3.x

#### Summary

For now 10 years  since commit https://github.com/doctrine/dbal/commit/b89490a557584b61d575aaa67f8b386d833d5a0f (before that it was not working at all )

`public` is returned by PostgreSQLSchemaManager::getSchemaNames()

#### Current behaviour

<!-- What is the current (buggy) behaviour? -->

#### How to reproduce

(a reproducer is soon coming)

1.  connect to any postgresql (a vanilla postgres docker is fine)
2.  call PostgreSQLSchemaManager::getSchemaNames
-> public is returned among other user-defined schema

#### Expected behaviour

all application schema are returned , but not `public` (as well as other postgres-defined schema are also not returned


#### More context 

This bug is at least the root cause of these issues 


* https://github.com/doctrine/migrations/issues/441
* https://github.com/doctrine/migrations/issues/1196#issuecomment-1029706768
* https://github.com/doctrine/dbal/issues/1110
* https://github.com/doctrine/dbal/issues/1188#issuecomment-231751027
* https://github.com/doctrine/orm/issues/4777
* https://github.com/symfony/symfony/issues/44952

the more visible issue of it is with doctrine migration: 

1.  all migrations contains a `down` migration trying to `CREATE SCHEMA public`  that EVERYBODY deletes or workaround    ( :100:  if you
have done that too ) because it fails miserably (as it already exists)
2. to avoid dropping it ,  we don't drop schemas  *at all*  cf https://github.com/doctrine/dbal/pull/5604/files  , so if automated generated migrations are not  bijectives because `DROP SCHEMA` will never be generated. 

#### Why `public` should be considered a ""system schema"" 

1.  public is not created by the user
2.  everybody assume that it is present
3. even postgres assumes it is present 
4. it has a behaviour that no user-defined schema has  (i.e  omitting the name of the schema default to 'public' , i.e  when you do `CREATE TABLE foobar` you're actually doing `CREATE TABLE public.foobar` 
5. if a ill-advised user drop it,  a lot of libraries get broken  ( doctrine migration by default create is own internal table in the public namespace ,  so as this SQL runs  *BEFORE* the first migration, you can't even create back `public` in the first migration as it will fail before)
6.  weak argument: after 10 years of managing postgresql I never ever seen people dropping and even less (re)creating the public schema, the only case I could hypothetically see for it is a extremly ordered  DBA  that have severa applications on the same instance , and want each of them in its own schema to ease maintenance (but in that case it's the ""infrastrcture"" responsability to drop it once and for all, not one of the applications )

@morozov  I understand the issue is ""scary""  because this library is a high profile one and after a time even bugs become part of the API , so I understand we may want to be careful on that one,  so maybe we can find a deprecation strategy and changing this behaviour only on a  minor/major release (at least not just a bug fix one)


",2022-08-21T23:29:55Z,213167
1751,doctrine/dbal,1345626221,1221645973,"> it has a behaviour that no user-defined schema has (i.e omitting the name of the schema default to 'public' , i.e when you do CREATE TABLE foobar you're actually doing CREATE TABLE public.foobar

It's only because it's in the user's/session's [search_path](https://www.postgresql.org/docs/current/ddl-schemas.html#DDL-SCHEMAS-PATH).",2022-08-21T23:45:21Z,302295
1752,doctrine/dbal,1345626221,1221646847,"> It's only because it's in the user's/session's [search_path](https://www.postgresql.org/docs/current/ddl-schemas.html#DDL-SCHEMAS-PATH). 

thanks for pointing that out, I was looking if it was overridable and it was right under my nose.

However my other point still stands and this one is slighltly amended , as in the case of a DBA requesting the `search_path` to be overrided to avoid accidental use of publc  , it means even more that public should not be touched at all by the application layer 
",2022-08-21T23:50:32Z,213167
1753,doctrine/dbal,1345626221,1221647244,"for the records: I'm also affected by the bug for years and I agree with your points (just couldn't resist of being nit-picky, sorry!) :-D
",2022-08-21T23:52:20Z,302295
1754,doctrine/dbal,1345626221,1221648487,"> for the records: I'm also affected by the bug for years and I agree with your points (just couldn't resist of being nit-picky, sorry!) :-D

no offense taken ,  I also perfectly understand the maintainer point of view of being conservative and I think we're all on the same side of trying to take the best decision,  so your nitpicking is welcomed as at least if forces me to really find good arguments :) ",2022-08-21T23:57:15Z,213167
1755,doctrine/dbal,1345626221,1221648870,"Some other thoughts:

https://www.postgresql.org/docs/current/manage-ag-templatedbs.html

So theoretically a DBA may modify the template to NOT contain `public` schema.

But again, I agree that `public` exists in 99.999% installations and should be treated with exceptions.",2022-08-21T23:59:29Z,302295
1756,doctrine/dbal,1345626221,1255413818,"I'm closing the issue as it doesn't state an objective problem. The opinion of ""public should not be considered a ""application schema"" is subjective. The statement ""everybody assume that it is present"" doesn't have a proof.

All issues mentioned in the description are closed except for the one which is in a different repository.

If you believe that the current behavior needs to change, please file a new issue and state a problem, not an opinion.",2022-09-22T18:43:54Z,59683
1757,doctrine/dbal,1345626221,1255472461,"
i feel a bit disappointed , the stated issues are clear not a opinion, the ""opinion"" is a consequence of these issues, not the other way around,

> it doesn't state an objective problem. 

facts:

* right now dbal can not handle dropping schema 
* right now dbal always report it will create the public schema (causing the ""issue in an other repo""  it's because it's the only repo using this feature in a way that is tangible for the end user , it does not take a long time to see doctrine migration is just doing a very thin layer on top of this, so there's no way to fix it except here)  making people needing to edit their down migration to remove it 

when you tried to consider public as a normal schema  while adding the possibly for dbal to report SQL to drop schema , you had issues 

as a consequence of that  i see that the way to solve this is to consider public as special schema 

I'm not some kind of militan that think public should be considered special out of the blue and here find a way to spread my propaganda , i'm just a pragmatic people who try to find solution to problem 


so please first discard the facts before focusing on the proposed solution",2022-09-22T19:43:30Z,213167
1758,doctrine/dbal,1345626221,1255475094,"> All issues mentioned in the description are closed except for the one which is in a different repository.

the other issues have been closed by you these last days , so yes it's a self-fulling prophecy ...",2022-09-22T19:46:10Z,213167
1759,doctrine/dbal,1345626221,1255619077,"> If you believe that the current behavior needs to change, please file a new issue and state a problem

https://github.com/doctrine/dbal/issues/1110 <--- it already was explained 7 years ago, the problem hasn't changed since then.",2022-09-22T22:25:07Z,302295
1760,doctrine/dbal,1345626221,1255643904,">  ""everybody assume that it is present"" doesn't have a proof

1. postgresql default value for search_path is `public` ,  so  if you drop public `select * from articles` will break 
2. postgresql documentation states that ` By default such tables (and other objects) are automatically put into a schema named “public”. Every new database contains such a schema.` https://www.postgresql.org/docs/current/ddl-schemas.html#DDL-SCHEMAS-PUBLIC  so it read as .... well ""it is present in every new database"" , i don't know where's the subjective interpertration you can
3. doctrine migrations breaks if you drop the public schema because it is the default one it uses 

I'm really sorry but what kind of proof more than that do you need ,  if I rephrase it as ""major projects, including the database itself use it as a default value because except if you're doing very specific stuff,  it will be there "" would a more objective statement ? if not how do you describe my points ? 

also what's really frustrating is that these points are from my above comments, I really took the time to gather them, and you decide to discard them 

",2022-09-22T23:05:29Z,213167
1761,doctrine/dbal,1345626221,1256522701,"@allan-simon in the bugs you mentioned, is there one that doesn't relate to migrations? Any thoughts about https://github.com/doctrine/dbal/issues/1110#issuecomment-1091845105 ?",2022-09-23T18:09:37Z,657779
1762,doctrine/dbal,1345626221,1256530195,"@greg0ire  it's not linked in my issue here, but yes https://github.com/doctrine/dbal/issues/5596  is also affected by this  issue (doctrine:schema:drop , if you try to enable the reporting of drop schem statement in dbal which is for now conveniently disabled  ) 

so no I don't think it make sense to fix it in higher level library , as stated above,  doctrine migration on this part is actually simply a think layer on top of the method provided by dbal ",2022-09-23T18:19:08Z,213167
1763,doctrine/dbal,1345626221,1256573363,"Indeed, I've taken a look, and although it could be fixed in the ORM by tweaking `SchemaTool::getSchemaFromMetadata()`, that would make ORM schema creation attempt to create the `public` PG schema.

I see that DBAL has a `schemaAssetsFilter` configuration node, which I could not find documentation on but seems to be about making the DBAL ignore assets it should not manage. This leads me to formulating the following problem:

While it is possible to filter out assets in a schema, it's not possible to filter out schemas in a database.

A solution could be to add another configuration node just for this, and let the user configure it. That way we don't need to make any assumptions about what the user thinks about `public`. Assumptions should be made in Symfony recipes. That assumption should be made [here](https://github.com/symfony/recipes/blob/85dc7cd2f0049f9a86a6d020691ea217561f33e5/doctrine/doctrine-bundle/2.4/config/packages/doctrine.yaml#L2-L7), in the form of a commented out configuration node.",2022-09-23T19:03:36Z,657779
1764,doctrine/dbal,1345626221,1256584569,"could be a way yes, at the end of the day I just want the problems quoted to be fixed my ""opinion"" was just IMHO the most sensible solution but if yours makes everyone happy , yes let's go with it",2022-09-23T19:16:48Z,213167
1765,doctrine/dbal,1345626221,1256586069,"if we just can verify it works first ( because I think to have already come accross it, i don't think it worked) ",2022-09-23T19:18:31Z,213167
1766,doctrine/dbal,1345626221,1256615482,"After taking a deeper look, I'm afraid my proposal would break `getCurrentSchema()`: https://github.com/doctrine/dbal/blob/a5a58773109c0abb13e658c8ccd92aeec8d07f9e/src/Schema/PostgreSQLSchemaManager.php#L165-L209


Also, I'm not sure at what point in the schema comparison the method you're changing in #5600 is called. Maybe we should introduce another method that would return the filtered list of schemas when doing schema comparison, I don't know. I personally don't have to deal with the ORM or migrations so I'm less interested than you are in getting this fixed.",2022-09-23T19:53:08Z,657779
1767,doctrine/dbal,1345626221,1256717462,"I think having two separate method with different behaviour would introduce more bugs and will complexify things,  

if you consider that ""listSchemaNames"" returns all the user-created schema [0], then it make sense to have it (for the above reaons) to ignore `public`  as it's not user created , and the very few  who have all: 

1.  a DBA who deleted the public schema before handling it to developers
2. a DBA who allows application code have a postgres user with CREATE SCHEMA rights
3.  a DBA who allows also the `public` schema to be recreated 
4.  a DBA that doesn't complain that the application code who created the schema they didn't want in the first place, does not clean it on down migration 

should be a minority (sorry no hard statitics here [1], the same as I don't have a hard statistics on how many people eat soup with a fork but at some point it feel like pointless to argue...) 

 to the point I feel like these people (if they exists) will not complain that now they are the one that need to manually add the `public` schema in the list of returned schema by dbal (or if in the case of indirect use through doctrine migration the fact to have to manually add it in their very first migration , while right now people need to remove it in every single migration) 


[0] I know the documentation says ""     * Returns a list of the names of all schemata in the current database.""  but it's already wrong as it filter some schema ... 

[1] on AWS , Azure , GCP ,  on default ubuntu, debian ,  fedora ,  windows , Mac installer  , the postgres you got has a public schema,  so you really must have a DBA that trick the install to remove it before handling it ot ,  what if we make a poll at the next php conference  ? 
",2022-09-23T22:01:32Z,213167
1768,doctrine/dbal,1345626221,1256921268,"> it filter some schema

I think here you're referring to only one schema called `information_schema`. According to the official docs, it:
- [is defined in the SQL standard](https://www.postgresql.org/docs/9.6/information-schema.html)
- [is not in the schema search path](https://www.postgresql.org/docs/9.6/infoschema-schema.html)

> should be a minority

It seems like you are saying that the only thing we could break is `doctrine/orm` and `doctrine/migrations`. There is a lot more code that could break if we changed that method.

> it feel like pointless to argue

We can stop the discussion if you want, that's not an issue to me.


Anyway, as you can see, I have a proposal, you have a proposal, and there might be more solutions, but I might be wrong about my solution, because I don't know schema comparison that well. For instance, it's unclear where `listSchemaNames` kicks in.

Likewise, you might be wrong.

Also, the problem you're willing to solve is not worded clearly, I believe this is why this issue was closed. Shouldn't there be somewhere a sentence like: when I run a comparison between this schema and that schema, the diff should be empty, but isn't?

I think instead of jumping to solutions, you should do what @morozov asked and file a new issue with a clearly stated problem, expressed with the DBAL APIs (meaning high-level APIs). A test case would be best.

BTW, I'm talking about schema comparison, but it's unclear if the issue you are trying to fix is with schema comparison, with `Schema->toSql()` (used in `SchemaTool::getCreateSchemaSql()`, with `Schema->toDropSql()`, or a combination of those.",2022-09-24T09:23:30Z,657779
1769,doctrine/dbal,1345626221,1256971718,"> We can stop the discussion if you want, that's not an issue to me.

I'm arguying with @morozov  handling of it , not you, we can continue to discuss,  you're not ignoring my arguments or only handpicking the one that fit you. 


",2022-09-24T13:39:40Z,213167
1770,doctrine/dbal,1345626221,1256974100,"> Also, the problem you're willing to solve is not worded clearly,

it is, it's the summary of dozen of specific issues , which all boils down to the same underlying issue of public being returned , so I was thinking that creating a summary issue, stating all the angle of the problems would be enough (it's not like i throwed a title-only issue without takign the time to git bissect accross 10 years of commit ) 


> you should do what @morozov asked and file a new issue with a clearly stated problem

I'm sorry but https://github.com/doctrine/dbal/issues/1110  was worded clearly and very specific but it was closed without any argument

so hence my frustration, I have a guy in front of me that contradicts himself: 

   * ask for a clear problem  BUT close issue where the issues is stated clearly ( so what do i do, i reopen the same one, take hours of my life, for something that will get the same fate ? )
   *  ask for a PR  , BUT when provided  (https://github.com/doctrine/dbal/pull/5600)  is ignoring it (the PR itself re-explain everything and give the clear exemples he is asking for 
   *  ask for the root cause to be fixed https://github.com/doctrine/dbal/pull/5600#discussion_r950340413  BUT when I create this issue and the PR  and, he closes it ask for specific symptoms ...
  



",2022-09-24T13:52:00Z,213167
1771,doctrine/dbal,1345626221,1256974683,"> I think here you're referring to only one schema called information_schema. According to the official docs, it:

no `pg_` are also filtered 

> It seems like you are saying that the only thing we could break is doctrine/orm and doctrine/migrations. There is a lot more code that could break if we changed that method.

I'm not saying that ,  I'm saying the minority are people that have both a DBA who deleted the public schema but at the same time would allow people to recreate it ",2022-09-24T13:54:10Z,213167
1772,doctrine/dbal,1345626221,1257010369,"> which all boils down to the same underlying issue of public being returned 

Again, let's not jump to solutions, we don't know that it is the correct one. Not returning `public` might fix your issues, it might also result in other issues.

> I'm sorry but https://github.com/doctrine/dbal/issues/1110 was worded clearly and very specific

It's quite clear, yet it's not easy to reproduce with just the DBAL APIs. I would expect something like

```
$schema1 = …
$schema2->getMigrateFromSql($schema1)
```

From reading the issue, you can't tell if there is a bug in the DBAL, or if `doctrine/migrations` is misusing it.

> no pg_ are also filtered

Not sure how I missed that, that's indeed the case. The docs say this:

> Schema names beginning with pg_ are reserved for system purposes and cannot be created by users.

For public, you'll have to admit that it's different: it's kind of common for people to create table and indexes inside `public`, while they would never do that inside `information_schema` or schemas starting with `pg_`, right?

>  I'm saying the minority are people that have both a DBA who deleted the public schema but at the same time would allow people to recreate it

There could also be other packages (different from ORM or migrations) relying on that method to display schema names, for purposes we don't know. The DBAL has 4k dependends: https://packagist.org/packages/doctrine/dbal/dependents?order_by=downloads

Hard to tell who is using this API for what. Hence my proposal of having this filtering be opt-in, and affect a new API instead of an existing one.",2022-09-24T16:41:13Z,657779
1773,doctrine/dbal,1345626221,1257020519,"I'm locking this because it's starting to consume my energy as well, but anyone feel free to open another issue about this. If you do, make sure that you state a clear problem , preferably reproducible with the DBAL APIs that are used directly by `doctrine/orm` (specifically by `SchemaTool`) and by `doctrine/migrations`. And if you do, please leave your emotions at the door, or remarks about the issue being 10 years old. Avoid mentioning time of your life you lost with this when talking to maintainers that spend a lot more time working on Doctrine.",2022-09-24T17:13:30Z,657779
1774,doctrine/dbal,723058396,723058396,"<!-- Fill in the relevant information below to help triage your pull request. -->

|      Q       |   A
|------------- | -----------
| Type         | improvement
| BC Break     | no
| Fixed issues | no

#### Summary

- triggering on push to any branch is important to allow testing before PR is made
- test default branch hourly is important to provided actual build state for the default branch",2020-10-16T09:26:02Z,2228672
1775,doctrine/dbal,723058396,709974535,"> triggering on push to any branch is important to allow testing before PR is made

People can and should test locally, then they can make a draft PR IMO",2020-10-16T10:53:03Z,657779
1776,doctrine/dbal,723058396,709977176,"> 
> 
> > triggering on push to any branch is important to allow testing before PR is made
> 
> People can and should test locally, then they can make a draft PR IMO

yes, this is what it does - if you are in other repo, CI results are logged there, not here...",2020-10-16T10:59:21Z,2228672
1777,doctrine/dbal,723058396,709980153,"@greg0ire this is also what Symfony does.

doctrine/orm does trigger on PR only (not on push) - but I will submit there a PR shortly ;-)",2020-10-16T11:05:57Z,2228672
1778,doctrine/dbal,723058396,709988539,"> locally

I mean on their computer, not on their fork.",2020-10-16T11:26:08Z,657779
1779,doctrine/dbal,723058396,709989299,"I know ;-), but why not in GH which is the standard for almost every other project now :)",2020-10-16T11:28:00Z,2228672
1780,doctrine/dbal,723058396,719748334,Closing as I don't see a value in applying this change.,2020-10-30T19:18:59Z,59683
1781,doctrine/dbal,723058396,719928932,@morozov I am very disappointed as allowing CI to run on push in very and industry standard practice across broad spectrum of open-source projects. Please see https://github.com/doctrine/dbal/pull/4349#discussion_r506721571,2020-10-31T12:44:12Z,2228672
1782,doctrine/dbal,723058396,719929136,"> People can and should test locally, then they can make a draft PR IMO

@greg0ire please explain **SHOULD**, draft PR is definitelly not a good solution for anything that should not be merged",2020-10-31T12:45:59Z,2228672
1783,doctrine/dbal,723058396,719939733,I mean people should attempt to run the test they can run locally before opening a PR.,2020-10-31T14:13:55Z,657779
1784,doctrine/dbal,723058396,719941235,"> 
> 
> I mean people should attempt to run the test they can run locally before opening a PR.

Yes! Many of them just create a branch, push, and want to see the CI result. This is perfect for quick and small fixes when local setup migh be very time consuming even for one DB.

So this PR seems perfectly legit to me.",2020-10-31T14:26:31Z,2228672
1785,doctrine/dbal,723058396,719942629,"> Many of them just create a branch, push, and want to see the CI result.

Isn't that what draft PRs are for?",2020-10-31T14:38:17Z,657779
1786,doctrine/dbal,723058396,719943030,"> draft PR is definitelly not a good solution for anything that should not be merged

obviously no :)",2020-10-31T14:41:39Z,2228672
1787,doctrine/dbal,723058396,719945614,Says who? You?,2020-10-31T15:02:42Z,657779
1788,doctrine/dbal,723058396,719945934,"Yes, me! - like verify something etc. No need to create a draft PR and ""~polite~pollute"" the history",2020-10-31T15:05:02Z,2228672
1789,doctrine/dbal,723058396,719947207,"You mean pollute.

I don't think there are that many people just creating branches and never opening a PR, that seems unlikely. Besides, draft PRs are great in case you want to get early feedback and avoid wasting time on something that will never get merged. Of course, this works better when you start with documentation and explanations and finish with code rather than doing the opposite.",2020-10-31T15:14:58Z,657779
1790,doctrine/dbal,723058396,719948202,"I am saying, that there a cases, when PR is not needed. I think, we can agree, that every change/experiment does not need a push to PR nor needs an attention from others.

To be objective, I have provided links to popular repos - in https://github.com/doctrine/dbal/pull/4349#discussion_r506721571 - that this is ""very common practice"".",2020-10-31T15:23:02Z,2228672
1791,doctrine/dbal,723058396,719952307,"> I think, we can agree, that every change/experiment does not need a push to PR nor needs an attention from others.

We agree on that. We can also agree that it's a waste to have double the jobs when a maintainer pushes their branch to the origin repository.

> To be objective, I have provided links to popular repos - in #4349 (comment) - that this is ""very common practice"".

That's a bandwagon fallacy. Let's ignore that please.",2020-10-31T15:54:08Z,657779
1792,doctrine/dbal,723058396,719952700,"> We agree on that. We can also agree that it's a waste to have double the jobs when a maintainer pushes their branch to the origin repository.

this should really not be an argument as thanks to GH, GH minutes are cheap/free for open source

is there any other significant drawback except that?

> That's a bandwagon fallacy. Let's ignore that please.

what objective criteria do you honor then?",2020-10-31T15:57:29Z,2228672
1793,doctrine/dbal,723058396,719953275,"> what objective criteria do you honor then?

I'm not sure, but I try to ignore fallacies.

> this should really not be an argument as thanks to GH, GH minutes are cheap/free for open source

It is the argument made here nonetheless : https://github.com/doctrine/dbal/pull/4349#discussion_r506718451 discuss that with its author.",2020-10-31T16:02:13Z,657779
1794,doctrine/dbal,723058396,719953943,"Anyway, this PR should have been made against https://github.com/doctrine/.github",2020-10-31T16:08:13Z,657779
1795,doctrine/dbal,723058396,719954645,@morozov please review the discussing with @greg0ire here. I find this PR very helpfull as any push runs a CI test which is very helpfull information for developer,2020-10-31T16:14:25Z,2228672
1796,doctrine/dbal,723058396,719954716,"> 
> 
> Anyway, this PR should have been made against https://github.com/doctrine/.github

thanks, done, here https://github.com/doctrine/.github/pull/11",2020-10-31T16:14:57Z,2228672
1797,doctrine/dbal,723058396,719956435,"I'm locking this conversation as too heated and not bringing any value to the project. @mvorisek if you don't stop pushing your agenda, harassing maintainers, and creating an unnecessary sense of urgency, you will be banned from this project.",2020-10-31T16:28:15Z,59683
1798,doctrine/dbal,530975624,530975624,"### Bug Report

<!-- Fill in the relevant information below to help triage your issue. -->

|    Q        |   A
|------------ | ------
| BC Break    | yes
| Version     | 2.10

#### Summary

Some changes in 2.10 make a BC break. On line 281 in OCI8Statement, the column is not valid in array, so throws a Notice: Undefined index: test.

Before upgrade to 2.10 the line looks like:
`$column = $this->_paramMap[$column] ?? $column;`

After upgrade do 2.10 is:
`$column = $this->_paramMap[$column];`

#### Current behaviour

Undefined index in array. The null coalesce operator was removed (WHY, who accept this ?)

![obraz](https://user-images.githubusercontent.com/16488888/69944505-c696e480-14e7-11ea-9e3f-421f9efc3e2d.png)

#### How to reproduce

The EntityManager must be of OCI8 instance.

```
$query = $this->entityManager->getConnection()->prepare($prepareSQL);
$query->bindValue('test', 'testvalue');
$query->execute();
```

#### Expected behaviour

Should be work by using default column.

Gentlemen. A bit of partolite work with doctrine. You have very poor standards, these things should not appear. Remember: If something works, don't move it.

",2019-12-02T08:42:40Z,16488888
1799,doctrine/dbal,530975624,560298585," > You have very poor standards

Uhm, no, the change was introduced because we do have good and also strict standards, and that was highlighted (although incorrectly) by analysis.

Meanwhile, closing as duplicate of #3738",2019-12-02T08:59:11Z,154256
1800,doctrine/dbal,530975624,560300562,DAFUQ. So now will be throws exception ? Nice ... your standards.,2019-12-02T09:04:17Z,16488888
1801,doctrine/dbal,530975624,560301801,Nevermind.,2019-12-02T09:07:21Z,16488888
1802,doctrine/dbal,530975624,560302763,"Right, locking here: please refrain from opening further issues, if this is the level of discussion.",2019-12-02T09:09:50Z,154256
1803,doctrine/mongodb-odm,12894489,12894489,"I'm not sure if this is officially supported by Doctrine, since it is not documented, but it works - with an error notice.

For example, if I have documents A and B where document B has an embedded document, E, I can design the mapping as follows:

``` php
/** @Document */
class A {
    /** @ReferenceMany(targetDocument=""B"", mappedBy=""foo.bar"") */
    private $whatever = [];
}

/** @Document */
class B {
    /** @EmbedMany(targetDocument=""E"") */
    private $foo = [];
}

/** @EmbeddedDocument */
class E {
     /** @ReferenceOne(targetDocument=""A"", inversedBy=""whatever"") */
    private $bar;
}
```

However, reading A::$whatever generates the following notice:

> Notice: Undefined index: foo.bar in mongodb-odm/lib/Doctrine/ODM/MongoDB/Persisters/DocumentPersister.php on line 709

Despite this, the document collection is retrieved correctly.
",2013-04-07T12:07:10Z,470626
1804,doctrine/mongodb-odm,12894489,19156555,"This doesn't work if class `B` has a binary ID and `$bar` is a discriminator reference. I'm not sure what the culprit is here, but the symptomns are that `count(A::$whatever)` will never be > 1. That is, at most only one item is loaded into the collection on the field with an embedded document mapping.
",2013-06-08T21:43:50Z,470626
1805,doctrine/mongodb-odm,12894489,913841398,"This issue has been automatically marked as stale because it has not had any recent activity. It will be closed in a week if no further activity occurs. Thank you for your contributions.
",2021-09-06T20:23:55Z,26384082
1806,doctrine/mongodb-odm,12894489,913845504,Imagine thinking anyone cares about design flaws.,2021-09-06T20:37:54Z,470626
1807,doctrine/mongodb-odm,12894489,914030250,Imagine thinking anyone cares about Open Source being a joint effort.,2021-09-07T06:32:20Z,383198
1808,doctrine/mongodb-odm,12894489,914097655,Imagine thinking open source means other people work for you for free.,2021-09-07T08:23:56Z,470626
1809,bigbluebutton/bigbluebutton,620694982,620694982,"**Describe the bug**
If the waiting room is on, attendees can't be promoted to moderator

**To Reproduce**
Steps to reproduce the behavior:
1. Go to demo.bigbluebutton.org
2. Turn on 'Require moderator approval before joining'
3. Start the room, and admit an attendee
4. Click on user to bring up menu and click 'Promote to moderator'

**Expected behavior**
The user becomes a moderator

**Actual behavior**
Nothing happens

**BBB version (optional):**
BigBlueButton Server 2.2.11 (1944)
Also tested on demo.bigbluebutton.org

**Desktop (please complete the following information):**
 - OS: Linux
 - Browser: Firefox
 - Version: 68.7 ESR
",2020-05-19T05:44:00Z,111810
1810,bigbluebutton/bigbluebutton,620694982,631109995,"Can confirm this, we also had this issue with one of our customers today.
If you turn of the ""Require moderator approval"", a guest can be promoted to moderator role.",2020-05-19T22:08:53Z,856110
1811,bigbluebutton/bigbluebutton,620694982,631117415,Good catch.,2020-05-19T22:28:37Z,290351
1812,bigbluebutton/bigbluebutton,620694982,631417561,Duplicated https://github.com/bigbluebutton/bigbluebutton/issues/7460,2020-05-20T11:31:16Z,1778398
1813,bigbluebutton/bigbluebutton,620694982,631561361,"Pedro you proposed to hide the promotion to guests. In my opinion, the solution should be to make the guest promotion work even if the room requires approval. Am I wrong?",2020-05-20T15:50:17Z,6589813
1814,bigbluebutton/bigbluebutton,620694982,631571819,"By design, `guests` cannot be promoted in bigbluebutton.
I agree with you that it shouldn't be like that but to make such change right now could impact other users that already adopt the guest feature as it is. I hope to see the change you asked (and myself asked https://github.com/bigbluebutton/bigbluebutton/issues/7460) in some future release.",2020-05-20T16:06:51Z,1778398
1815,bigbluebutton/bigbluebutton,620694982,631660347,"Pedro, when the room does not require approval, guest users can be promoted without any problem, that works fine. That's why I understand the right solution is to behave the same way. Are you sure that is ""a feature"" that without approval you can promote them, and with approval you can't? I would say no. In fact that is confusing! Just my humble opinion. ",2020-05-20T18:54:02Z,6589813
1816,bigbluebutton/bigbluebutton,620694982,631673602,"Can you explain a little more about what is a room that does not require approval? What are you considering to be a guest?
This is the server-side control on user's promotion:
https://github.com/bigbluebutton/bigbluebutton/blob/da9489c9dde1e1d0a3da942b95d95def66dd6105/akka-bbb-apps/src/main/scala/org/bigbluebutton/core/apps/users/ChangeUserRoleCmdMsgHdlr.scala#L32-L43
Promotions are only enabled to non-guest users.",2020-05-20T19:18:54Z,1778398
1817,bigbluebutton/bigbluebutton,620694982,631688446,"@pedrobmarin  Apologies if I didn't use the right terminology, I'm not so experienced with the project yet. The behavior I see (and others reported in the google group as well) is when the room was created with the guestPolicy=ALWAYS_ACCEPT , then you can promote any user as moderator. But if you created the room with guestPolicy=ASK_MODERATOR , then the menu option is shown but the user is not promoted. In both cases, the users are VIEWERS. Not sure if the code you are showing is connected to this issue.  ",2020-05-20T19:47:31Z,6589813
1818,bigbluebutton/bigbluebutton,620694982,631692477,"Nothing will change on that behalf. If you could promote someone before, you will still be able to do so.
There is a wild role entitled `guest` that is used in some particular cases. You can identify guest users by checking at the users' list, they will be labled as _Guest_. Those users can't be promoted. The thing we tried to manage on that commit was to simple not show the promote option for something moderators won't be able to execute whatsoever.",2020-05-20T19:55:35Z,1778398
1819,bigbluebutton/bigbluebutton,620694982,631697163,"I can understand that from a technical point of view, but not from a user's perspective. Is there a way we can request the feature to have the same behavior for any kind of user (guest or not)? I see at least 5 guys reporting this as a bug in the google group (https://groups.google.com/forum/#!topic/bigbluebutton-dev/W56HWasv5z8), I guess many more could be interested. Thanks!!! @ffdixon @pedrobmarin  ",2020-05-20T20:05:44Z,6589813
1820,bigbluebutton/bigbluebutton,620694982,631735671,"This is the point where we go full circle as I agreed with you before that's the behaviour I would expect from the guest feature. Since the changes to make it work as we want lies under a design level of the product, it's more complicated to modify and guarantee nobody loses from it. I hope this is something we could add to bbb core in future releases but right now we can only make sure the UI is in sync on what an user is able to do or not.",2020-05-20T21:26:01Z,1778398
1821,bigbluebutton/bigbluebutton,620694982,631741842,"Hi Pedro, no full circle at all. I understood (but not agreed) what is the current behavior of the guest feature. 
I'm just asking if there is a process for the community to request changes or new features, because I see I'm not the only one interested. I would like to follow the right process and not to leave it in the comments of a closed ticket as now, if possible. Thanks again for your help!",2020-05-20T21:40:12Z,6589813
1822,bigbluebutton/bigbluebutton,620694982,631754285,"I was the one who agreed with you and the person who took the time to answer all your questions. All I said was that I was one of the interested people on the change you proposed.
If you try to open a new issue you will probably see there is an option for `Feature request`. That's usually the place people can ask for new things.
Have a great evening. No problem at all!",2020-05-20T22:09:46Z,1778398
1823,libretro/RetroArch,1072125460,1072125460,"
### Yuzu Emulator Core Request

[So, i thinking a new core as Nintendo Switch called Yuzu, so what do you think?]

Note: this  iis an request, if u add it its a pleasure♡♡
",2021-12-06T13:02:30Z,91480651
1824,libretro/RetroArch,1072125460,986945489,"i dont like, to make a new issue open. well, i agree but i would like to see a Duckstation on Nintendo Switch :)
THX",2021-12-06T16:35:36Z,26491758
1825,libretro/RetroArch,1072125460,988603205,"Yeah, It will be a nice addition to Retroarch as it allows us to emulate **Nintendo Switch Games**. 

> ### Yuzu Emulator Core Request
> [So, i thinking a new core as Nintendo Switch called Yuzu, so what do you think?]
> 
> Note: this iis an request, if u add it its a pleasure♡♡

",2021-12-08T08:33:06Z,26346867
1826,libretro/RetroArch,1072125460,998652066,"RETROARCH DEVELOPERS.
NOT ADD ANY CORE IF IS EMULATING CURRENT VIDEO GAME GENERATION.
EMULATING CURRENT GENERATION CREATE PROBLEMS FOR HONEST PROJECTS ... MAINLY MAME PROJECT.",2021-12-21T10:18:13Z,21184987
1827,libretro/RetroArch,1072125460,998719992,"> RETROARCH DEVELOPERS.
> NOT ADD ANY CORE IF IS EMULATING CURRENT VIDEO GAME GENERATION.
> EMULATING CURRENT GENERATION CREATE PROBLEMS FOR HONEST PROJECTS ... MAINLY MAME PROJECT.

I agree with you , even though the addition of this core will be appreciated, since it includes support for current generation Nintendo Switch console it would create problems and legal complications for retroarch. ",2021-12-21T11:58:31Z,26346867
1828,libretro/RetroArch,1072125460,999280045,Retroarch need in controller add option to assign same console controller button to multiple game controller thus creating combo buttons helping NES and other games need press 2 buttons at same time and also allow turbo button for any console button.,2021-12-22T04:34:42Z,21184987
1829,libretro/RetroArch,1072125460,1001823474,"Yuzu is still pretty young, and while it quite possibly could be possible to create a libretro port, it may distract from getting Yuzu stable in the first place. I would recommend supporting Yuzu over at https://yuzu-emu.org , and joining their Patreon, in order to help get it to a solid and stable state.

If anyone wants to have a hack at the libretro port in the mean time, don't let this issue stop you.",2021-12-28T01:32:00Z,25086
1830,libretro/RetroArch,1072125460,1001853520,"EMULATE CURRENT GENERATION IS WRONG !!!
CREATE PROBLEMS FOR OTHERS EMULATOR PROJECTS !
UNHAPPILY HAVE USERS WAITING PLAY CURRENT GENERATION AND WISH UNDERSTAND IF WILL CREATE PROBLEM 
NINTENDO HAVE WAKE UP ABOUT IT ... THEY MAYBE CONTACT POLITICS CREATING LAWS AGAINST ANY EMULATOR PROJECT.",2021-12-28T03:46:43Z,21184987
1831,libretro/RetroArch,1072125460,1285419380,"I was curious about yuzu core and found a guy called Augusto screaming at my face.
I love internet.

by the way Augusto, emulation it's not and it'll never be illegal. you just need to own the games you play.
 ",2022-10-20T12:06:58Z,6727358
1832,libretro/RetroArch,1072125460,1356063691,"From a legal standpoint, it doesn't matter if you're emulating the current generation.  It all depends on whether you're following the rules with regard to copyright, and it doesn't matter what generation is the target hardware.  However, from a rights holding company's POV, it probably does matter to them, since they're looking to make more money off the current market.

Like people have said, it's definitely legal, as they have already tried to shut down emulation before through lawsuits, so it's been fully confirmed by the US courts that emulation is legal.  I doubt they'll test that again since the industry has been against emulation forever, and they haven't really done much about it and all that lobbying politicians hasn't amounted to much as of yet.  I could be wrong, but there's no reason to believe things will change without some solid evidence. 

Please consider including yuzu since it is fairly stable, as far as I can tell.  It would be nice to have it as a RetroArch core.",2022-12-17T06:20:10Z,42723784
1833,libretro/RetroArch,1072125460,1359002425,"Oh heavens ...
From an gamer only waiting play emulate current system is ""correct"" , but from an viewpoint of an professional game developer and emulator coder of mature projects emulate current system is totally wrong even if the law allow to do it.
Not is impossible Nintendo and others companies having contact with politics and changing the laws at point creating problems for other mature projects. Only reading an reply from an mamedev saying to stop wit it is enough to understand about it.
Unhappily an high percentage of PC gamers have done terrible actions demanding emulate and dump current comercial games ... all to avoid buy an console or even an recent released game doing problems from all sides.
I not believe here several gamers will reply with good sense saying to not add cores to current systems.
That's the because Sony and Nintendo add security protection in console OS.

Retroarch devs NOT add cores of current systems ... THAT IS AN OF MORE WORST PROBLEMS BEING DONE AGAINST MATURE PROJECTS ( MAME ).
If Retroarch continue using cores from before PS3 is all right and not any problem.

Now for example ... if is released an PS5 emulator and each PC have access to PS5 discs and that emulator is being extremely downloaded you believe Sony not will try to do the correct action ?
In past was tried stop some emulators (Connectix Virtual Game Station and others) ...
Today not is the same thing.
If Sony , Nintendo and any other company join to say about that problem to politics will happen problems.

Stop trying to do it. Buy the console or wait to next generation before demanding an emulator for current generation.",2022-12-20T08:33:08Z,21184987
1834,libretro/RetroArch,997299420,997299420,"Inspired by the steam forum topic ""Convoluted"" by Chelle, and a prompt by Developer Gadsby to post feedback,
this is a Quality Assurance Write-up Complaint about everything bad, unintuitive, convoluted, and otherwise
in need of serious QA (Quality Assurance) love. Retroarch NEEDS to up their QA to be able to act as a platform
for emulators. This post is meant to be intentionally hyper-critical and nitpicky on first world problems, because the experience is just that annoying. I hope to point out to readers many problems with the readability of Retroarch, and it's common user experience. I also hope if taken seriously, i get a contributor interested enough by my points, to add me and talk to me on discord, so we can work together on bringing retroarch up to a level people expect it to be at, and help retroarch hit console or platform level quality it is so desperately missing.

==================Chapter 0: I Didn't Even Launch It Yet And I Have Complaints=================

Note: If you are only contributor level and not developer level, scroll down and skip to chapter 1.

I'm going to start by looking at retroarchs website and steam page, and boy, before we even launch retroarch
for the first time, there a lot to talk about already. Infact, to even do this write up, or rather to attempt to
talk about it on the issues section on github i was pointed to, it says this is for bug reports only.
Currently, i'm under the assumption i'm already somehow breaking the rules by doing what i was told.
Not a good start, but not the worst. With that out of the way that i'm posting here on request, and not out
of ignorance for the warning github gives new posts, lets move on.

Lets look at the Website and steam page front pages. I kind of want to talk about them both at once.
I'll start with the opening description on both. It's to technical. Retroarch as i see it, is looking to be
a platform, like steam, to access things. Like the playstation XMB, or switch home screen. For a description,
on the site we get ""RetroArch is a frontend for emulators, game engines and media players."" and on steam 
""RetroArch is an open source and cross platform frontend/framework for emulators, game engines, 
video games, media players and other applications""

This actually tells a user very little about what retro arch is, and isn't. Even people emulator savvy won't
understand the rest of what is being thrown at them. a 'frontend' for emulators? lets google frontend. 
""frontend can refer to any hardware that optimizes or protects network traffic."" People aren't playing these
online, or it's not their first thought. They don't need protection from the net when playing mario. Okay,
maybe a smart person will think it has multiple definitions, and google more accurately. lets google Frontend Program
""A Front End developer (dev) works with designers and Back End devs to create a website"" Well, retroarch
isn't a website (Well technically it is but you know what i mean), it's a program itself. Already, using the
term frontend is to much. If you want retroarch to really succeed, to shine, you need to make it marketable and 
understandable. The optics matter, if people don't know what it is, they won't know why they should care
about being interested in it. when people say it's convoluted, you shouldn't need an 30 minutes of research
to even understand your opening sentences.

it also says 'game engines, and media players'. Why? Lets be honest, retroarch runs emulators. Thats what it
does. Why do you need to complicate this? Does it run other things sure, but nobody is seriously wanting 
it as their number 1 music player or picture taker. why can't you speak english? the steam page says emulators,
so someone knew that word, why aren't these two sources on the same page for their advertising? Unfortunately
the steam page then says game engines, video games, media players, and other apps.
What fucking goon is downloading retroarch just to play music. This is not a good use of advertising space.
The human being looking at your steam page can see the music menu in your advertising images. Use your text
space better, seriously!!! Why is this so hard! Also, it says emulators, then game engines, then video games.
pick ONE! these are all synonyms in practicality. People also don't need to know it's open source
in your intro, you can say that later. From what i've later come to understand, the devs want to use the term 'Core'. 
They want this because they believe in what retroarch does aside from emulators. Sure, except...the opening 
advertisement doesn't say 'Core'. 

Here is a good opener. ""RetroArch is a all in one platorm for Emulation that enhances the games you play
and other Cores. From retro era to modern games, you can even play online with friends!"" 

This is a fantastic, easy to digest, explanatory, to the point advertisement that tells people what
retroarch is, what it DOES (Enhances emulator games), and avoid words like ""nintendo"" or ""platstation"" for
copywrite while still getting the ""old to current"" idea across. Make your text readable to people! If they want more
they can hit up a FAQ. Note: Going forward, i will be referring to a reasonable way to read something, or
a problem with text that is to jargin-driven as ""Readability"" as a unbrella term. This will come up A LOT
and will be a serious problem with retroarch in nearly every part of anyones experience. Please remember this.

Lets move on. in the websites next section, it says you can run original game discs, but steam says 
the feature isn't out yet. These are literally giving conflicting information. Seriously? ITS YOUR FRONT PAGE!

Next, lets look at some visuals here. For the website, i want to look at the ""A polished Interface"" Slide.
This makes it look like playstation, cool. Except this isn't what it looks like in retroarch by default.
Make it clear in advertising this is a skin. Also, ironically, the interface is excessively NOT polished, but
i will be getting to that as a major complaint in another chapter. But right now, it's a skin, say it's
a skin. The very idea their not saying its not the default skin here, implies someone knew it was terrible and
tried to hide it. Maybe ask why they needed to hide your real interface? We will get to that later...

Moving back to steam, lets look at the slides/image roll. First is the trailers. All three are really fucking bad.
Trailer 1: its a fun logo (sure, fine) then shows the menu with garbage readability that makes a person
want to flee just looking at it, followed by showing what has GOT to be one of the WORST POSSIBLE game examples.
like, you CANNOT be serious. the website slides show mario, and this trailer has a level ID called zero / 
megaman129. Show us a actual video game. Make the trailer show like 8+ games of different genres. 
Make it CLEAR that retroarch plays VIDEO GAMES, and not, whatever the fuck that is. Bad advertising affects
end user readability to understand what retroarch is and is not. and this is contributing to why you get topics
saying people are confused about retroarch.

Trailer 2: Literally a logo. Why is this here? Remove it.
Trailer 3: This feels like a meme. Seriously, this is cruel, it hurts. It's like someone took what 
makes a good intro video and butchered it to the max. No BGM, no person explaining things, no examples
of games being played, it's super fast (Epilepsy educing even?). So fast you can't even read all the text
without pausing the video. Please make a real trailer. Look at like, any nintendo direct. Do that, but 
for retroarch. Show off and explain lots. I'll help QA it if you need someone. We can improve this.

Then the steam slides, their mostly the exact same menu. Why? Why not Show off various parts of retroarch? 
Actually make people interested. Your telling me you create all of retroarch, and the only part
you want people to know about is 2 screens? wtf? 

Now i can go on, but the pre-retroarch experience is getting comically long at this point.
I'll move onto actual retroarch, but rest assured, i can REALLY go on about whats wrong before even downloading.

In summary
1: Very poor readability
2: Poor advertising
3: Bad use of space
4: inconsistent between steam and website
5: Doesn't actually say what retroarch does in words people can understand let alone google. 


===============Chapter 1: ""A Polished Interface""? Actually It's Really Fucking Garbage======================

Note: I have gotten some responses to points in this chapter from users using custom alternate skins for
RetroArch that change how the menu actually functions and displays information, and not just the visuals.
This chapter is about a first time experience, and as such uses the default skin. 

Before i actually talk about the REAL first impression, i want to point out i downloaded website and steam
both, to check for differences. Guess what? The first impression is diffrent. Inconsistent again? Ugh.
From website, it launched windowed, and you lack the cool cursor, and get the ultra low quality windows
menu that feels like a 2005 program with file/command/windows despite that retroarch has it's own in-app menus.
This doesn't happen on steam, it launches in fullscreen on first boot. The difference here is very important.
Not everyone knows you can Alt+Enter programs to fullscreen them. Infact, this is semi-rare info. 
This is made worst by F11 being the windows default to fullscreen, but boy do i wish complaining ended here.

If a non-steam user tries to fullscreen by hitting F11, retroarch hotkeys this to swapping between the windows
and custom cursor. Now, the custom one looks good, but it locks you into the windows. Surely you can
make your cursor not get locked, come on. But even worse, is when someone wants out and presses F11 again. 
I CANNOT BELIEVE how bad this is. If you are multi-monitor, your cursor is now locked to the monitor
retroach was in itself, untill you move retroarch. Clickin on it, pressing hotkeys ect won't fix this.
I don't mean using the retro cursor, i mean normal windows cursor, you can't leave your moniter. 
what the actual fuck? what a joke! I haven't even talked about retroarch contents directly, and
i can barely type up this QA review because retroarch literally won't let my cursor go click on notepad. >:(

it gets worse. If your fullscreen and press F11, your cursor doesn't change, but your mouse still can't
leave the moniter. The user would have no idea why and think retroarch is just a bad program. How can
you have this many errors and i'm not even talking about retroarch itself yet?????


OKAY, actual retroarch time. You can argue before this, thats a unintended bug, sure. Lets looks at whats NOT a bug.
So, we boot into the main menu that is...lacking readability at best.  ""Load Core""? Why? these things are called emulators, 
call them emulators. ""Load Emulator"" is perfectly reasonable English. It's my understanding the devs want the term
Cores because it also uses something called Cores in addition to emulaitors. Sure, but you want to be
a marketable, understandable program? speak in words people actually understand. it's my understanding the team personally wants 'Load Core / Content' as they want to do more then emulators.  In counter to this, i have 2 suggestions. Either split the menu AND load between Emulators (Load Emulaitor / Game), and other Cores (Load Core / Content), or make the load
button hidden untill a emulaitor or core is ready, then have it show Load Game or Load Content based on what Emu/Core
is loaded! Easy! Also, this menu is bloated.

So, 
1: Split the cores menu, between ""Cores"", and a new one called ""Emulators""
2-1: In addition to Load Content, add a new menu called Load Game right under Emulators.
2-2: Or, make load content/game hidden until a emulator or core is loaded in already.
3: Also change Esc defaulting to exiting the program, to a back button. This is semi-common, and straight up default
in nearly every PC game. If your a platform on a PC for people gaming on a PC, use a PC control scheme. wtf?
there is a quit button if needed, and Alt+F4 or Alt+Enter, we don't need to troll PC users with a comically
bad default control scheme thats normally a menu button on almost every pc video game ever released.

""Bloated"" i'm going to define, as to many things being in one menu, or things being in the wrong location.
Here, we have the main menu showing config, info, and help. But we have a left side bar. whats the 
difference between config and settings? this is already a readability problem, they can be seen as synonyms. 

Now, i'm gonna be going all over unfocused here, because there is so many problems their all interwoven.
The Config file menu, why is this a menu? 

1: Delete the Config menu entry
2: Move the contents of the config menu, to the top of the settings menu. 
3: Why is help a menu with only 1 content? Literally delete this menu.
4: Move Menu Controls and it's icon to the bottom left of retroarch below the white line. This menu is 
first of all stupid and should probably be removed entirely, however...
4-2: Rename Menu Controls to Controller and have this lead directly to the input menu. If you really want,
include the Menu controls in Controller, but really it's common sense, and people can customize this,
so the stated controls in Menu Controls can literally lie to the user, this is dumb.
5-1: on first startup, maybe do a first time setup that asks the user what menu icons they would like.
let them pick from switch, xbox, playstation, ect. Its also kinda bad to display Switch by default, but
use american playstation input scheme, and not having a switches A button actually be A. Readability issues!!!!!
5-2: If that is a copywrite problem, alternatively let people customize the on-screen symbols with letters. In this way,
people can recreate their console of choice setup while working around looking like it clones a competitor, because
it's a customization option of a user. And don't tell me customization options aren't fair game when your straight up
stealing the playstation home menu theme >_>


Now that thats done, we have a nice (enough) looking home. Onward to settings. The entire thing is bloated. 
There is also a lot of 2-menus required trolls. There are so many times you look in one menu and need to 
google to find another menu, it's not even funny. 

1: First up, just like the white line at the bottom of retroarch, i recommend a new break line in the middle
of settings. Things below it will be advanced settings, and title the section Advanced. Alternatively, if
this won't translate well to alternate skins, instead have a menu in here called Advanced at the bottom of settings.
Rename Saving to Saving Games (Readability, this seems like save Retroarch settings when it's here like this!)
2: Throw logging, File Browser, On-Screen Display, AI Service, Latency, into Advanced.
3: Delete the entire power management menu. wtf is this? It's literally empty. LITERALLY, USELESS. WHO????
4: Rename Network to Online, or Online Multiplayer. (Readability!)
5: Rename Input to Controllers

6-1: On the left bar, Rename Import Content to ""Create Playlist"" (Because that is literally what it does)
6-2: Move the playlists menu into the now named ""Create Playlist"" menu, de-bloat settings, so only people 
who care about the playlist menus are forced to see and scroll past this option. 
6-3: Add a button named ""More Information on Playlists"" or maybe ""Playlists Tutorial"" or similar as well
to explain what is a 'Playlist', how to set them up, needing cores/emulators, ect.
6-4: if a Scan to create a playlist in a directory comes up empty, have a pop up explaining why. (Missing cores, ect)

7: I cannot believe you have a menu called configuration here, when i earlier talked about configuration in
the main menu. Why do you have 2 config menus, and a settings menu? HELLO? This is so troll, the
config options in settings/config effect your stuff in main menu/config. Why the fuck are they not together?
Who designed this? *Phew* now that thats done.....
7-1: In this submenu, move 'Use Global Core Options File' to the 'Load Core' Menu we are renaming to 
'Load Emulator'.
7-2: We are also renaming this option ""Use Global Emulator Options"".
Readability is very important! 
7-3: Finally, If (hopefully) Main menu config is moving to settings, either delete this Configuration menu, 
and move its only remaining option (Save Config on Quit) to settings, or move the config stuff from 
main menu, inside this config sub-menu. 

8: AGAIN with the 2 menus needed trolls. Inside Directory we have a lot of playlist stuff, why?
8-1: In playlist menu (that we are moving to the menu in point 6 we are calling ""Create Playlist"" Menu) 
create a new menu called Playlist Directories
8-2: Move everything playlist related from settings/Directory to Music/Playlist Directories. 
This is to de-bloat menus with unnecessary information overflow, and put settings in menus they are actually
used in, like any other emulaitor (PCSX2/RPCS3/Citra/Cemu/Yuzu/ect)


9: Speaking of multiple same named menus, we have achievements, and a users achievements. STOP DOING THIS!!!!
why do you want people to need to google one menu doesn't work without the other menu! THIS IS SO TROLL, STOP IT!
9-1: Rename User to Account Settings & Achievements
9-2: Move the achievements menu into this newly named menu to de-bloat settings, and put this in a reasonable spot.
9-3: The Accessibility menu only has 1 option in it. Stop having troll 1 option menus. Delete this menu
9-4: Rename the Sub-menu ""Accessibility Enable"" To ""Text to Speech""
9-5: Move ""Text to Speech"" to the Account Settings & Achievements, so one user can use it, and
another different user doesn't have to deal with troll text to speech in their ears. 

10: Continuing from 9, we have streaming options in account, and them in recording. WHY.
10-1: Rename ""Recording"" to ""Recording & Live Streams""
10-2: move the the content of the now ""Account Settings & Achievements"" / Account  / Retroachievements to
the Achievements menu that was moved here in part 9. 
10-3: The Youtube/Twitch/Facebook Gaming sub-menus only have 1 content, STOOOP FUCKINGGG DOOOING THIIIIS. >:((((
we are deleteing these sub-menus. 
10-3: Move the menus inside the Youtube/Twitch/Facebook Gaming sub-menu stream key options from here, to the now 
named ""Recording & Live Streams"" menu. 
10-4: in the ""Recording & Live Streams"" menu/Streaming Quality, give one of those grey sub-descriptions.
Something like ""Suitable for 720P Streams"" ""For 1080P Streams"" Ect. 
10-5: Also rename the menu to Stream Quality.

Oh boy, onto input (that we said we are renaming to Controller)
11-1: Move Maximum Users to...somewhere. Wtf even is this menu? If this actually means users, move it to the
relevant online menu. If it means locally, rename this to ""Maximum Controllers"". 
11-2: If the later, Create one of those Advanced Options line breaks again at the bottom, and move this
down there, along with every other option in this menu untill we get to Menu Controls. 
11-3: MENU CONTROLLS? HELLO? Why does main menu get help on menu controlls, BUT WE HAVE A ACTUAL MENU CONTROLS MENU? PUT RELEVANT MENUS IN RELEVANT PLACES THANK YOU.
11-4: Move the Automatic Mouse Grab option to the Video menu in Settings. This is something usually in that
menu on other programs, stop being unintuitive!!! How can you make so many mistakes!!!

Speaking of the Video menu, lets tackle that next.
12-1:in Fullscreen Mode, rename Windowed Fullscreen to ""Borderless Fullscreen"" or just ""Borderless"",
and change the description appropriately.
12-2: Rename Fullscreen Width & Height to ""Custom Width"" (& Height). It's already in the fullscreen sub-menu, 
you don't need to remind them what menu they are in, it's at the top of the screen!
12-3: Move CRTSwitchRes and Output below the Fullscreen and Windowed sub-menus. Order matters! Most important
things go on top!

I can keep going, but i'll stop here. This is a Serious Checklist of Readability improvements across the 
entire settings menu. However, if i reconsider, i may suggest adding Controller to the left bar as a major
menu instead, as it's reasonab;y so important, it should be a major menu, rather then some garbage like Music.


=====================Chater 2: A Smoothe Gameplay Experience? Or A Troubleshooting Nightmare?=====================

Before i get into further details on games, i strongly recommend a new setting somewhere, on by default, that
makes it so when you load a game, and pick a emulaitor (Remember, we are calling these Games and Emulaitors, NOT
Cores and Content) that it associates that game to that emulaitor by default. In the future, you should be
able to Load game -> Pick the game -> it just boots up without needing to pick a emulaitor. 

I...HOPE i don't have to explain how bad it is that steam retroarch is missing a download button. I will skip
over the easy wall of text on this and move forward.

1: For steam users, there should still be a download emulaitor button, but as a submenu. 
1-1: In here, There should be a link to your website with a list of emulaitors to download.
1-2: There should be a setting here, or in the Install Emulaitor menus to set the directory for cores.
This is because this options is only useful & relevant right at this moment, and never again, so it
does not need to be in the directroies menu elsewhere.

2: When in-game, move the Command/Menu button to the top of the list. 
I also feel a menu rename is in order, but can't think of one right now, and want to move on.
3-1: Add save state and load state to the Command menu. Yes i know they are in the F1 menu. Make them accessable.
3-2: In the F1 menu, move State slot below load state

And oh boy, lets get into the F1 menu. In no particular order...
4-0: Make it WAY more obvious to assign a button or combo to open this menu from controller.
probably include it in the quick menu directly at the bottom, even if the option exists somewhere else,
just duplicate it. 
4-00: Also, Escape should open the quick menu by default
4-1: Originally i said Rename Information to Game Information, but i forgot i was still allowing for cores/content menus before.
i now suggest making this menus name be contextual, based on if a emulator, or core is loaded. This way people only using
emulators can have it reasonably say 'Game Information' and those who use cores still get 'Information' so it
doesn't clash with any development team desires.
4-2: Move the Controller menu to right below Close Content
4-3: Also rename that to Close Game
4-4: Adding something to favorites and you can't remove it? WHY? 
Make this menu change to Remove from Favorites if it's a favorite >_>
4-5: Remove the descriptions for Resume, Restart, and Close game. This screen has lots the player wants
to see and not enough screen space even when in fullscreen. 
4-6: Also remove descriptions on save state, load state, Take Screenshot, Cheats, Achievements, and Rewind.
4-7: (Someone gave a good point against this, nevermind)
4-8: Make it so if the player presses cancel in the F1 to go to Main Menu, Then cancel again, it resumes the game.
4-9: move Show Desktop Menu from Main Menu while ingame to somewhere else (Probably settings).

With that out of the way, once ingame, there is no menu option to go back!
Add a button in windowed mode to open back the retroarch menu. If you tell me it's a hotkey, i don't care.
thats not intuitive! Make a button to go back to the retroarch menu!!!


Finally, this SHOULD be obvious, but it's not, so i will say it here.
I assume this is a major feature, but really, let people access that emulaitors settings when using it.
if we can't do that, retroarch is just worse then every emulaitor, because they have settings you NEED to even
run some games. If this option DOES exist, it's EXTREMELY hidden!

Moving on, post gameplay, your recent games don't show up in load content / load game.
Why isn't there a recent games list? And why can't you set a content / game directory in directories? 
If you can, that option should 100% be in the load content / Load Game menu, as this is where
it's actually relevant!!!


=======================Ending==================

I'll stop here, it's been 3 hours of complaints, but retroarch is riddles with terrible quality.
Why would anyone want to use this? 
1: An unfun troubleshooting experience in exchange for playing games in the worst quality possible 
without access to emulators settings. 
2: Some games are literally unplayable in retroarch because you can't access emu settings
3: The menus are bloated, and lie / decieves / give half truths by hiding relevant other options in other menus
for no other reason then to troll you. The overall feeling is Retroarch is giving you the run-around, asking you
to do something in some menu you don't know where it is, and when you find it, wants another setting in
another menu you don't know where it is. 
4: it's a pain to setup even for simple emulators
5: you can't even understand what retroarch is or isn't from inconsistent and contradictory messages,
to near moon-rune levels of advertising.
6: unintuitive menus and lacking default settings mean you need to go over everything before even attempting
to test a single title.
7: Using it via steam is a whole extra nightmare that could be made easy if anyone actually fucking cared.

It's easy to see why people don't recommend recroarch. The real question is if anyone is going to actually
do anything about it, or delete this post / no reply / no one for me to directly work with in a timely fashon /
stick their head in the clouds and ignore why people don't enjoy their product. 

My discord is Dawnbomb#3408, and i'm open to any actual developer contacting me and working hand in hand for
some actual quality assurance before the steam deck launches, but considering how obvious, easy, and
near intensionally-troll the levels of design are here, i expect no contact and noone willing to work to make
retroarch not be complete fucking bullshit bloatware on a PC, and i only scratched the surface of this QA nightmare.



====================Chapter 3: Extra add-ons i thought of later=========================

Note: Some of the things here delve closer to more serious changes to retroarchs codeing.
Still, their good changes.

1: Controller backround input should be in the input (Hopefully named Controller) Menu, not in another menu (User Interface).
Setting up controller input in your controllers, then having if the actual input happens be decided in a separate menu, is another weird location problem.
2: While im at it, Turbo Fire should be a setting INSIDE a controllers port controls. This would allow each controller to have separate turbo settings per player. 
3: Altho highly unusual, some players enjoy playing games with two people one controller mode. Some people even go so far as to speedrun like this. Allowing multiple controllers assign buttons to the same button on the same port of a input would go a long way in support for those people. ",2021-09-15T16:49:31Z,29080681
1835,libretro/RetroArch,997299420,920211783,"I think this rant belongs in the forums or any other social channel of libretro. Not on the github issue tracker.

If you're serious in your offer to help out with 'quality assurance', that would be great. But i think the best approach would be contacting the developers on the libretro forums or discord and discuss your suggestions.

Forum: https://forums.libretro.com/
Discord: https://discord.com/invite/C4amCeV",2021-09-15T17:14:21Z,561623
1836,libretro/RetroArch,997299420,921061432,"Repeated fowl language, capslock cruising, know-it-all attitude, and giving thumbs up to their own posts should be bannable offenses.
",2021-09-16T16:42:32Z,45124675
1837,libretro/RetroArch,997299420,921071710,"I appreciate the time you took to write this up but we don't appreciate your wording.
However, I will let this slip because you called it, but please not again.

While I agree on _some_ parts, in the end its a matter of time and money.
Everyone here does this in their free time, so unless you can come up with 20k in funding at minimum to work on these, I dont see the codebase reaching proper production-grade quality anytime soon.
If i have the choice between a full stomach and catering to anonymous dickheads, well the choice is obvious.
You can reach out to us anytime if you want to get involved and donate your time working on QoL issues, but then use Discord so it can be coordinated.
I can tell you tho, these issues are like the last of our problems with the codebase.. Theres way more fundamental/bigger fishes to fry first.
I recommend you split up your post into several seperate issues, completely reword it and try talking like a normal person.
",2021-09-16T16:57:15Z,13141469
1838,libretro/RetroArch,997299420,921072495,"Also: >near intensionally-troll the levels of design are here

What design _laughs_",2021-09-16T16:58:29Z,13141469
1839,libretro/RetroArch,997299420,921127970,"It doesn't take 20000$ funding to reword some menu options. you can do some of this in like 10 minutes if someone really cared. Stop hiding behind excuses to shut perfectly valid changes down. Another contributor could take up the mantle, all you did was make sure no one looking over open issues would be able to see this topic, and implement any of these meaningful changes. ",2021-09-16T18:09:11Z,29080681
1840,libretro/RetroArch,997299420,921167665,"Like I said: I recommend you split up your post into several seperate issues, completely reword it and try talking like a normal person.
Your post is harder to navigate than retroarch xmb without assets, not to mention it makes my eyeballs itch.

You underestimate the cost it takes for doing everything over what we are already doing because it means time needs to be taken from office hours at our dayjobs, which are expensive. For 20k you get about a month worth of fulltime work for one person.
So why care about low priority issues when theres way bigger issues? Your reading comprehension is lacking.
Or maybe you dont have a idea what production-grade code implies, certainly not just changing a few options. 

In your words: Get your shit together
",2021-09-16T19:06:37Z,13141469
1841,libretro/RetroArch,997299420,921515060,"10 Minutes?! Alright then, you claim to be a dev. Stop moaning and start coding, if you ""really care"". Pull requests are a thing. See how easy it is.",2021-09-17T05:42:15Z,13473205
1842,libretro/RetroArch,997299420,1120434466,"or yknow you could just use some pull requests, or better yet

fork retroarch and do your own thing with it instead of putting an essay on the issue tracker, lol",2022-05-08T15:02:51Z,47698702
1843,libretro/RetroArch,906427489,906427489,"This issues is created in order to collect all remaining issues from vita port in order to fix them. It will be constantly updated

- [X] Broken Video Scaling even when using ""core provided"". #12456 
- [ ] Broken NetPlay.
- [X] Menu Sounds don't work at all. #12457 
- [ ] Check assets.",2021-05-29T09:22:58Z,399894
1844,libretro/RetroArch,906427489,850826583,"Video Scaling is broken in general, even when having aspect ratio as ""core provided"" (not sure if actual menu driver issue though).
NetPlay is just completely broken and does not work at all.
Menu Sounds don't work at all (not that anyone cares, but thought I'd mention it anyway).
Assets have to be downloaded and added manually for certain menu drivers",2021-05-29T12:40:52Z,68133913
1845,libretro/RetroArch,906427489,850864993,"i didn't know if this could be actually added or as it's a request, would be completely separate?",2021-05-29T17:00:11Z,68133913
1846,libretro/RetroArch,906427489,1234740921,@Cthulhu-throwaway I can assume we can check 'Broken netplay' now as resolved for the Vita port?,2022-09-01T20:21:49Z,105389611
1847,libretro/RetroArch,906427489,1234761701,"> @Cthulhu-throwaway I can assume we can check 'Broken netplay' now as resolved for the Vita port?

Over my dead body, you're not!

As has this been throughly tested? If not, it needs to be either removed or replaced with a alternative.

As I'm sick and tired of this being actually overlooked entirely, as the actual developers don't care about this at all.",2022-09-01T20:44:44Z,68133913
1848,libretro/RetroArch,906427489,1234765809,Wow... Not even going to bother.,2022-09-01T20:50:08Z,10137
1849,libretro/RetroArch,906427489,1234767870,"> Wow... Not even going to bother.

That's exactly what I mean, can't be bothered...

Just do whatever! As no-one even cared about this issue in the first place anyway!",2022-09-01T20:52:54Z,68133913
1850,libretro/RetroArch,906427489,1234770547,"@DoctorWarez Warning, do not talk back to our developers like this. This is your final warning. Next time, it will result in a permanent suspension from the entire organization. Hostility against developers is completely unacceptable, you keep your communication to the point and factual and you leave your emotions at the door. If you can't handle that and want to proceed in badgering our developers, there will be consequences. ",2022-09-01T20:55:10Z,105389611
1851,libretro/RetroArch,906427489,1234772944,I really wanna see his face once someone links him to all the work I did for the Vita on networking and netplay.,2022-09-01T20:56:49Z,10137
1852,libretro/RetroArch,906427489,1234774822,"This kind of childish mentality by certain endusers is certainly disrespectful to the core, and I guess we have been too lax on it before in the past which has allowed it to fester. I guess it's hard to dawn on some of these endusers that the majority of them are frankly lazy and consumerist and think others just exist to implement all their features and requests and that developers just magically self-materialize and implement all this stuff on a whim. That is not the case. This is all highly specialized code, only a handful of coders know how to do it, and pissing them off and upsetting them is a sure-fire way of making sure no improvements get made for a specific port at all anymore. This is a volunteer-led project. Everything that gets implemented, gets implemented because a volunteer decided to invest the time and effort into making it work.

Hence I guess why we need to start making some examples out of people so that this bad behavior can be discouraged. ",2022-09-01T20:58:46Z,105389611
1853,libretro/RetroArch,906427489,1234776793,"> I really wanna see his face once someone links him to all the work I did for the Vita on networking and netplay.

Sorry... What? Where's the proof!? As I've clearly missed something here, as I only got a email about this today and I see nothing apart from ""oh, is it alright to close this issue?""",2022-09-01T21:00:46Z,68133913
1854,libretro/RetroArch,906427489,1234779466,"@DoctorWarez Go check out recent nightlies and test with other people with recent nightlies. Oh, and lose the shitty attitude already. You are already on thin ice here, it's best not to continue pushing your luck.

We are fine with making improvements to platform ports but we cannot deal witha busive and disrespectful endusers, sorry. Fix the attitude and things can actually start happening, the only other alternative is either you getting banned and capable devs losing interest in further improving this platform port due to hostility (perceived or otherwise) from endusers like you. The former actually gets stuff done, the latter is just petulant childishness that will result in nothing. You make the choice what it's going to be. No more arguing from this point on.

I remind you again, if we get any more of this attitude from you, it's a permanent suspension from the entire organization. You will NOT be disrespectful to our developers on Github. That is a red line in the sand that nobody can cross. ",2022-09-01T21:03:19Z,105389611
1855,libretro/RetroArch,906427489,1234787383,"Don't even know what else to actually say really...

Apart from I raised this issue time and again, though no-one actually understood what I meant.

Portable ports need local connection or rather ad hoc support, for when there is no internet access available or you just want to pick up and play with a friend wherever you are together. Not to be confused with netplay, which actually requires a actual internet connection. no other devices, just what you have to actual play the actual games on.

How exactly has it only taken until now for someone to actually look at this properly?

Sorry Admin, do what you wish... As I don't even care anymore.",2022-09-01T21:11:13Z,68133913
1856,libretro/RetroArch,906427489,1234790997,"Then your entire objection to 'Fix Broken netplay' makes no sense, since it's not related to your feature request 'local connection/ad hoc support'. You could create a separate issue request for that, but we'd prefer you do not interrupt then on this issue when it's clear you have no interest in the netplay feature as is. We need feedback from people who DO have an interest in it and who can report useful information back to us. We certainly have better things to do than to argue with you.

> How exactly has it only taken until now for someone to actually look at this properly?

Because this is nobody's day job and you should quit being entitled. You are being owed nothing at all, and you have everything to thank our contributors for, and for this project to even exist in the first place. There are about 99 issue pages open and it takes a long time to go through it all and we don't have a surplus of people to go through it all. You are being extremely entitled, demanding and disrespectful right now. Again, lose the attitude. If this keeps on going, we are going to proceed to an organization-wide ban. As it is, we've already given you more of a chance to correct your behavior than many other projects would do, they'd just have banned on sight the moment this turned irate.

Again, last time we are going to request this and after this point you cannot say you did not have it coming, lose the attitude and stay on topic this time, and be constructive and helpful. If you cannot do that, quietly leave this issue thread without making any more snarky remarks, you are wasting our time at this point. Failure to comply will have consequences from this point forward.",2022-09-01T21:15:44Z,105389611
1857,libretro/RetroArch,906427489,1234821372,"> I really wanna see his face once someone links him to all the work I did for the Vita on networking and netplay.

I’m going to actually ask this again… nicely…

Where’s your actual proof of this, please?
No words, just actual physical proof in a way that even the actual end user can actually understand properly. That’s if it’s actually publicly accessible?

also, does it in anyway involve any sort of ad hoc support?
",2022-09-01T21:53:28Z,68133913
1858,libretro/RetroArch,906427489,1234839253,Nobody said anything about or implied ad hoc support. Please stop conflating things. Working netplay does not imply implementing whatever token platform specific network feature you want to see implemented. It just means internet netplay.,2022-09-01T22:12:23Z,105389611
1859,libretro/RetroArch,906427489,1234850373,"@LibretroAdmin just please kindly remove me, from this... whatever, this is.

As I don't understand and no longer want to be any part of it. As I'm not exactly contributing anything at all, that's even useful or essential.

Also, as there's no actual context... I don't even know what's going on anymore.

I also closed the separate issue which was merged, as it's not considered priority by the developers afaik and at least actually never will.",2022-09-01T22:26:55Z,68133913
1860,libretro/RetroArch,906427489,1234901268,"> Also, as there's no actual context... I don't even know what's going on anymore.

Maybe don't waste developer's time then with your childish temper tantrums, and especially don't start insulting them or berating them. We have better things to do than cater to your whims. 

> I also closed the separate issue which was merged, as it's not considered priority by the developers afaik and at least actually never will.

Stop with the childish throwing of tantrums, things will be done as they get done. There is no estimated timeframe for when something gets done, it can take a few days, it can take a few years, but you as the enduser are NEVER entitled to start throwing a tantrum over it, you should only be grateful that an open source project like this is allowed to thrive and further continue. Beyond that, if you do decide to partake on these developer discussions, you should keep your communications to the point and respectful. If not, you will be barred from further discussion. That's it. Behave like a grownup and you won't have any issues. Behave like a child, and you're going to get removed. Now stop creating further noise when you don't know what is going on.

Going to close this issue since you already ruined it. Next time I see you ruining another issue thread like this again, you will be banned. You're getting off this once. Give a snippety attitude again and we're done here.",2022-09-01T23:26:58Z,105389611
1861,libretro/RetroArch,643360992,643360992,"I met some 12yo guy and he played in Megaman, it was very hard for him. Not only megaman, even Duck Tales
And... Out of box your software is totally unusable, especially for kids

These AI, wtf is this? Why even cheap old chinese consoles with shitty hardware can just launch games and play without fucking with settings, but your software can not?

You have tons of shaders, but no shader preview. With paint.net u can preview changes, in your AAA software u cant. Very slow navigation, it does not remember position
Extremely huge interface
My config is this
![image](https://user-images.githubusercontent.com/61594968/85333850-4e563780-b4a8-11ea-97f7-b730b2c5f262.png)
Your is this
![image](https://user-images.githubusercontent.com/61594968/85333927-70e85080-b4a8-11ea-86c7-f9e45bac2f7f.png)
Why you describe wtf does 'audio' means? Can u make it fit in screen

Ozone has no scrollbar, no mouse support, if u try to scroll, actually its equal to up/down button

People dont wait that software does not work with mouse

U cant explain even default configuration, what button u need to press for some effect
![image](https://user-images.githubusercontent.com/61594968/85334229-e0f6d680-b4a8-11ea-8ee7-44cdc1c33a8b.png)
U can draw something like normal games if gamepad is not connected like 'L - ok', 'O - cancel'
How to reset settings with start button u dont descibe too

Why do u draw licence? It takes much space
![image](https://user-images.githubusercontent.com/61594968/85334410-3501bb00-b4a9-11ea-962d-8b9ed949ca50.png)
Why not organize cores like

NES\
Snes\

U just give people huge list of all cores. What difference between then - there is no description. But u can see that lisense is GPL v3, very important information haha

Your software with 'release' has >13000 of files, size is bigger than 600mb and out of box it even does not have any core

Alphabet keys are assigned on emulator functions. If u wanna make keyboard gamepad like
wasd iokl it will conflikt, and no highlight. WTF, why F1 calls menu? Do u know that many people use laptop, they can assign something on F keys by default

And they have to press Fn+F1 to call menu, who wait that?

Your bloatwared AAAAAA software is extremely glitchy, unusable for NORMAL people

Even your buttons numeration
![image](https://user-images.githubusercontent.com/61594968/85335010-48615600-b4aa-11ea-83fc-caddb6418469.png)
Like BY...some keys...AX is wtf

On 1st place u put very strange settings like bind timeout, duty cycle

![image](https://user-images.githubusercontent.com/61594968/85335123-747cd700-b4aa-11ea-8d85-c3affaf220d2.png)

If user wanna change settings, on 1st place u need to have Port1, just tune control

I dont know how after 10 years of development u still have so glitchy and unusable software",2020-06-22T21:06:38Z,61594968
1862,libretro/RetroArch,643360992,647770931,"And ra eats cpu even if emulation is on pause. nestopia or epsxe do not do this, it continues to render and heat laptop even if it in fullscreen and u switch from ra to windows ",2020-06-22T21:10:07Z,61594968
1863,libretro/RetroArch,643360992,647772059,"For normal people on windows u can make small version without these bloatwared things, simple structure of folders, no tons of files in root, some cores are included out of box",2020-06-22T21:12:59Z,61594968
1864,libretro/RetroArch,643360992,647773390,"As for me, to make interface usable, i just reduce scale to 0.6x and disable sublabels
If I need some description, i can press select button, btw font size in this is very small
![image](https://user-images.githubusercontent.com/61594968/85336082-105b1280-b4ac-11ea-9a05-117d092eacb4.png)


",2020-06-22T21:16:13Z,61594968
1865,libretro/RetroArch,643360992,647775337,"For normal view as for me i need some calibration, its possible to make it very simple

From this
![image](https://user-images.githubusercontent.com/61594968/85336237-49938280-b4ac-11ea-9db4-30048a378ff7.png)
To this
![image](https://user-images.githubusercontent.com/61594968/85336254-53b58100-b4ac-11ea-8f56-0c2f357b90fb.png)
YIQ boost, levels correction, some blur (coz CRT TV does not provide clean lines), some noise for texturization

Its not correct to just draw black lines or crt subpixels, coz when u played from tv, it was no huge subpixels from typical distance

These crt shaders have too strong settings and its a parody",2020-06-22T21:20:43Z,61594968
1866,libretro/RetroArch,643360992,647775729,"I appreciate you taking the time to assemble these suggestions and pics. I deleted the pic of your friend, as we don't want any pictures of people (esp anyone 12 or under) for various international liabilities.

While I/we don't agree with all of your suggestions (or with how you've expressed them lol), some of them are valid and helpful, so thanks for that. As long as you want to keep your suggestions in this thread, I won't close/lock it.",2020-06-22T21:21:32Z,2805586
1867,libretro/RetroArch,643360992,647776372,vashe pofig,2020-06-22T21:23:02Z,61594968
1868,libretro/RetroArch,643360992,647777508,"But u should know that for normal people, not crazy linuxoids, especially for kids, your software is nightmare in everything what u did. Any normal guy i thing can tune classic emulator, but retroarch looks like u hate people and kidding them",2020-06-22T21:25:50Z,61594968
1869,libretro/RetroArch,643360992,647780377,your ranting is misplaced. You should know this is not the place of such.,2020-06-22T21:32:33Z,54053706
1870,libretro/RetroArch,643360992,647795277,"Chill out men!! kids under 10 can use retroarch confirmed, aside from that, your attitude it's so bad, you could read retroarch docs to how to make a plesant UI, also you could use kiosk mode to hide advanced settings, you can customize retroarch to your own flavor, but have respect to the devs, also you could make a PR to contribute, instead of just spittings scorpions from your keyboard, this is an open source project, help them, not sink them.",2020-06-22T22:12:52Z,2125887
1871,libretro/RetroArch,643360992,647800184,"Oh okay where is place :)?
Its just my thoughts

People says that u need to be correct, but actually retroarch is not correct of anything. It has some logic, ok, but this logic works only for developers and geeks

Who wait from gaming software that F1 calls menu? Or there is mouse, but actually mouse does not work like normal mouse. Does making scrollbars or mouse support is more difficult that AI discord twitch?

Why am I see that video button changes 'video output settings'
Does user so stupid that he does not know what video means? But this stupid user must know that F1 calls menu

Base mission of emulators is providing people play in games. Just install and play, whats wrong with this logic? May be it needs some delays, but retroarch as i know is crazy about strange ultra super zero latency. Sound could lag on not super PC, but its okay, coz there is super zero latency is activated

For most cases laptop is enough. ps1 emulators worked on pentium 3
psp with 333mhz cpu and 32mb ram has emulators of snes gba, whatever, fullspeed no lags even with some delays buffer size whatever

But retroarch...",2020-06-22T22:26:32Z,61594968
1872,libretro/RetroArch,643360992,647805368,"As non-native english speaker, I recognice that I have taken a lot of time to be able to express myself in a proper, polite way, I think that @noiseshaade is a non-english native, so I recognize his writing as my own of the first years, and I feel it not like a rant, but as an struggle to express his point of view. I found that @hizzlekizzle was very cool and polite to accept it. 

Retroarch is an incredible piece of software, powerful but complex to master. To be constructive, 
I would suggest that maybe all @noiseshaade frustration, and probably many others could be solved with an option to launch a ""first time launch wizard"" that allows less savvy users to get retroarch up and running with a few clicks here and there: Just basically downloading a few cores depending on the system you want to emulate, setting up the gamepad, enable kiosk mode, and you are good to go. With that you cover the bare basics to have it up and running and you can later prettify or tweak the output to your liking. 

I also found nice the shader preview suggestion (and also a notification of ""resource intensive shader"" would be good). I had some struggles finding out , by example, which was the highest level of XBRz shader my HTPC could handle without getting sluggish. Sometimes it got so slow that I had to kill retroarch because it was faster to relaunch it than waiting to reach the shader menu again.
  
Not everyone will agree with me, that's just my two cents.",2020-06-22T22:43:05Z,61637041
1873,libretro/RetroArch,643360992,647810394,"Its poweful... But powerful for who?
What 'zero latency' means?
For example, u can reduce audio buffer size from 64ms to 1ms. Latency is 64x time less, but actually normally u will not see any difference. And it needs as i know extremely powerful hardware

On normal hardware it will be extremely laggy as i know

Its possible to make these 'ultra small latencies' so small that comfortable SNES emulation needs intel i9 10900k 6GHz DDR6 128Gb ram quantum pc neural AAA networks whatever, but its not fun",2020-06-22T22:59:51Z,61594968
1874,libretro/RetroArch,643360992,647822989,"For example, people can buy low-end laptops if they just need some internet
Modern games are AAA shit, overloaded with special effects, its not magic, its not fun, they just eat resources. People or kids do no wanna play in 'modern' games

its nothing wrong if by default retroarch does not eat many resources, if somebody needs these zero latency or what these guys use, he can activate it

retroarch focused on mobile phones, but actually playing in old games from touchscreen is awful

there is some cool things for phones which turn them to psp like console
https://www.youtube.com/watch?v=b3-Ua8ZQc84

but retroarch 'shop' sells t-shirts, iphone cases... https://teespring.com/stores/retroarch ... very very strange",2020-06-22T23:41:41Z,61594968
1875,libretro/RetroArch,643360992,647824307,"@Zephyr-Battiassi Yes, I agree that an optional first-run wizard would be nice. Unfortunately, it's one of many things that are very difficult for us to do within our UI vs with a single-platform native GUI toolkit.

@noiseshaade we don't have anything invested in the teespring merchandise. It's just a free service that sells stuff with your logo/designs for a cut of the proceeds. In contrast, it's very expensive and labor-intensive to produce, market and sell your own electronics hardware. It's something we've wanted to do for a very long time, but it has very high up-front costs (upwards of $20k for the controllers we were looking at a few years ago) and we would have to store the stock, mail out orders (internationally), deal with refunds, etc.",2020-06-22T23:46:30Z,2805586
1876,libretro/RetroArch,643360992,647828723,"> Extremely huge interface

`./configure --disable-menu`

Problem solved!",2020-06-23T00:03:10Z,1697807
1877,libretro/RetroArch,643360992,647831716,"oh for me 0.6x scale and disable sublabels solve interface problems, even no real scrolling is no critical
Instead of this
![image](https://user-images.githubusercontent.com/61594968/85346749-6cca2c00-b4c4-11ea-9c37-5b78b2e7f189.png)

 when gamepad is not connected, it could be something like this
![image](https://user-images.githubusercontent.com/61594968/85346774-7d7aa200-b4c4-11ea-8341-6820bf9917b8.png)

If u just launch emulator on PC, u dont know what this means
![image](https://user-images.githubusercontent.com/61594968/85346827-a8fd8c80-b4c4-11ea-9266-1762bb8ed733.png)
",2020-06-23T00:14:10Z,61594968
1878,libretro/RetroArch,643360992,647836604,"I dont know how retroarch is popular on mobile phones or consoles, may be a lot of people use it, but actually they do not have much choice

If there is regular windows user

For example u wanna just play one old game like megaman

Download nestopia, ntsc filter and set up controller fastly

With retroarch

Download 200mb installer with thousands of files, unpack them. download core, forget about previous user experience, even calling menu

And after it... Same thing, but after tons of manipulations and time 

Even somebody wanna try discord/twich/ai achivements whatever, they have to spend A LOT of time and energy. And mental health",2020-06-23T00:31:20Z,61594968
1879,libretro/RetroArch,643360992,647870047,"> Chill out men!! kids under 10 can use retroarch confirmed, aside from that, your attitude it's so bad, you could read retroarch docs to how to make a plesant UI, also you could use kiosk mode to hide advanced settings, you can customize retroarch to your own flavor, but have respect to the devs, also you could make a PR to contribute, instead of just spittings scorpions from your keyboard, this is an open source project, help them, not sink them.

Hmm. I do not need 'pleasant ui'
Project exist 10 years. Classic emulator like nestopia snes9x was created with very small team, or with only one person
Or, for example, there is zsnes it has its own 'driver' like retroarch does, but its intuitive
![image](https://user-images.githubusercontent.com/61594968/85352867-eff37e00-b4d4-11ea-8f27-e45e43a1a67b.png)
only one 1 file, size is 580kb

It just works. Its not 'ideal', but its very easy to understand.

Ntsc 'shader' exists too
![image](https://user-images.githubusercontent.com/61594968/85353582-95f3b800-b4d6-11ea-854a-7c129d5f45e1.png)

Simple sliders
![image](https://user-images.githubusercontent.com/61594968/85353636-ba4f9480-b4d6-11ea-9f52-2611cbb05f7c.png)
Retroarch has **54** crt shader presets. No preview. No remember position. How does they work... Usually they are very strong or complex, some of them can lag nes on intel hd4000. No sliders or any visualisation
![image](https://user-images.githubusercontent.com/61594968/85353837-30ec9200-b4d7-11ea-914f-5bfb68c2a67c.png)
After 10 years of development with huge team...


For me as 'normal' user retroarch has only 1 advantage, i can calibrate image

I dont know all these ntsc crt things, just some colors changing, noise, blur...
",2020-06-23T02:34:05Z,61594968
1880,libretro/RetroArch,643360992,647870893,"I mean, Retroarch is more complex than single emulators. Retroarch is a flexible front-end to these emulators, not a 1:1 representation of them. It's a bit like that multi-emulator from Byuu.",2020-06-23T02:37:05Z,15256014
1881,libretro/RetroArch,643360992,647874138,"Man

There is a lot of chinese multi emulator handheld consoles google it on youtube
https://www.youtube.com/watch?v=9mSvczcgS9c
https://www.youtube.com/watch?v=TX7igfC8wgs

They emulates ps1, i think that there is no much ram or not great CPU

U can put them in pocket

Just launch game and play it.

Whats wrong with retroarch on PC? ",2020-06-23T02:48:26Z,61594968
1882,libretro/RetroArch,643360992,647877773,"Rk2020 3.5 ips screeen 
It emulates psp well

https://www.youtube.com/watch?v=CsxWApj5xmw

RockChip RK3326 (4cores ARM Cortex-A35 1.3 GHz)

In **2020** year i still have troubles on 2.6Ghz intel i5 to emulate ps1, oh no. its after 10 years of retroarch development...",2020-06-23T03:00:40Z,61594968
1883,libretro/RetroArch,643360992,647884263,"Do it by yourself and stop complaining, pay to new developers to make it like you want, all of your issues have of somekind of hate, stop being pasive-aggresive with your comments, and mainly agressive.",2020-06-23T03:23:08Z,2125887
1884,libretro/RetroArch,643360992,647889145,"I mean... I think its huge team. Huge team which do nothing. They just promise that they fix something... But they cant do this. If they make ready out of box software, there is no growth... No growth means degeneration, people say. They should growth. Bigger size. More icon pack. More borders. More shaders. More system requments. More cores. They can write 100 of cores for every platform. Its 'job'

For example, Microsoft has >151000 of people, Apple >137000

What does these people do actually? Win 10 vs Win 7 has no much difference

In modern world its normal situation that billions of people do nothing. And retroarch team...",2020-06-23T03:41:56Z,61594968
1885,libretro/RetroArch,643360992,647890632,do nothing geez. can we see what you've done aside from rambling? did you pay anything? were you forced to pay if so? stop rambling and get a life.,2020-06-23T03:48:23Z,54053706
1886,libretro/RetroArch,643360992,647890686,"U should know that now we live in the 'end of time'. All these events 😼. There is no school, no job or predictions anymore :)",2020-06-23T03:48:43Z,61594968
1887,libretro/RetroArch,643360992,647900395,"8)
![image](https://user-images.githubusercontent.com/61594968/85360995-b3ca1880-b4e8-11ea-9511-189c695159c8.png)
![image](https://user-images.githubusercontent.com/61594968/85361005-b7f63600-b4e8-11ea-95d7-1ded806d5615.png)
",2020-06-23T04:30:27Z,61594968
1888,libretro/RetroArch,643360992,648145569,">There is a lot of chinese multi emulator handheld consoles google it on youtube

Yes and some of them use RetroArch lol

> huge team

There are quite a few people who contribute to libretro as a whole (cores, cheats, etc.), but for RetroArch itself, there's usually only a handful. I think the most active contributors at one time was like 6 and it's currently ... 2.",2020-06-23T13:25:40Z,2805586
1889,libretro/RetroArch,643360992,648229583,"I agree with you on at least one thing : Ozone should have a proper touch interface, for :

- Scrolling
- Going backward without having to touch the tiny ""Back"" button on the bottom righ hand corner (e.g. using the back button of the phone, make sense.)",2020-06-23T15:15:01Z,19387072
1890,libretro/RetroArch,643360992,648242759,"Oh some guys make core. There is a lot of cores for snes for example
In old days people who made core - they make interface too. Its art work too, by the way. Snes9x has its own interface, zsnes, nestopia

Its nothing wrong if people who make cores make usable interface. Or they can share retroarch build with included core

People actually have their own taste. If somebody likes classic emulators, he never make build with such huge scale for pc and dont assign function on alphabet keys. Or something like F1 calls menu

Its possible to make build for yourself or your friend",2020-06-23T15:35:42Z,61594968
1891,libretro/RetroArch,643360992,648272933,"When it was classic emulators, developers of cores took responsibility how emulators look and works

Its a shame, if u turn compact menu (it takes 22px of height)
![image](https://user-images.githubusercontent.com/61594968/85429199-6384a200-b54c-11ea-9ffe-d1b27ca17a47.png)
To this huge menu (102 px of height with stupid) with no scrollbars 

![image](https://user-images.githubusercontent.com/61594968/85428881-dd685b80-b54b-11ea-8085-1c81dfbfdd84.png)

",2020-06-23T16:26:59Z,61594968
1892,libretro/RetroArch,643360992,648276694,its time for you to stop rambling and use something else. no one is forcing you to keep using the most awful software ever.,2020-06-23T16:33:53Z,54053706
1893,libretro/RetroArch,643360992,648303966,"Yeh, but in my opinion retroarch destroys whole emulation party :). For example, u wanna play in game and enjoy it, but u cant enjoy it, coz of
I spent about 4 month to tune retroarch

For my side it looks like

Play in usual emulator
When boring, launch retroarch
Catch another AAHAH WTF IS THIS
Return to normal emulator

I just know that i can calibrate picture using shaders, for me advantages of ra vs epsxe-like emulators is only framerate counter - it means i can generate dynamic noise. And there is gaussian blur preset, im not shader coder

Then it can be used on nes or any console

I had some practice with shader on videoplayers like potplayer and in retroarch its very glitchy

![image](https://user-images.githubusercontent.com/61594968/85433773-ed376e00-b552-11ea-9371-98a9fd97b136.png)

For players its like 'before scaling' and 'after scaling', it possible to use fullscreen as surface. It allows u hide (apply levels correction) black borders and apply noise for them too

Ra has strange behaviour. For example if u wanna work with screen as surface, u have to use bilinear filter. Imho if u use just blur instead of bilinear filter, it gives better results

For example i wanted to try

Its original
![image](https://user-images.githubusercontent.com/61594968/85434412-07258080-b554-11ea-8dd7-286e23c670b2.png)
Its bilinear
![image](https://user-images.githubusercontent.com/61594968/85434616-523f9380-b554-11ea-838e-5d9ceeb74aa2.png)
Its nearest algo + gaussian blur
![image](https://user-images.githubusercontent.com/61594968/85434653-5e2b5580-b554-11ea-9ad9-6d40daaf10a8.png)

But u cant get 'clean' pixels for nearest algo, it should works like integer upscale roundup(displayres/consoleres), then bilinear or whatever downscale to fit in windows

But its not possible, u have to apply blur after bilinear filter... Okay, it works, but less accurate",2020-06-23T17:25:17Z,61594968
1894,libretro/RetroArch,643360992,648309346,"Shaders is like 'killer feature' of ra, but actually...
No preview, no remember position, tons of strange effects, no visualization (sliders) like reshade

There is a tons of ra features but they do not work well. They spend a lot of time for documentation, porting, web site, youtube preview, whatever. No usability",2020-06-23T17:35:45Z,61594968
1895,libretro/RetroArch,643360992,648318801,"Maybe its user feedback. Or 'its open source, anybody can fix it'
But retroarch has reputation of ugly unusable bloatwared software

They share people 200mb installer for windows with thouthands of files

Usual people do not approve it, if they have fun with classic emulators
For third part developers its no much reason to help retroarch, they continue share this 200mb installer with 190mb debug exe, tons of files

Its 'universal' politics of universal gui with same scale on phones, PC, whatever, universal borders and pngs

Normal people who stay on PC i think know about retroarch, but users and coders who can fix something should hate retroarch",2020-06-23T17:49:37Z,61594968
1896,libretro/RetroArch,643360992,648320894,"Alright, you've moved from constructive feedback to pointless complaining, so I think we're done here.",2020-06-23T17:53:47Z,2805586
1897,libretro/RetroArch,627087857,627087857,"Whats wrong with retroarch? Everything is wrong! Its terrible, hujnya ebanaya karoche!

btw its my pack. No all borders/icons/whatever , simple shader which improves graphic out of box
https://github.com/noiseshaade/RetroArch-vape

No stupid descriptions, interface fits in screen
![image](https://user-images.githubusercontent.com/61594968/83236221-27cf0600-a161-11ea-822d-83149d470924.png)

Out of box, retroarch is extremely complex even for advanced users. Who thought that F1 is great idea for menu? Why not escape like in usual games/emulators?

The only advantage of ra vs standalone emulators is that it support shader and TECHNICALLY it could be better, but on practice its 100mb of shader folder. Whats wrong with syncronization, why snes has sound crackling out of box? U link syncronization with monitors sync rate but it seems like it changes.  Why you cant do like previous old emulators did and somehow link emulation with stable generator? Its not very bad if u sync game with time, not display

Why sound should lag in 2020?

Why you linked sound control with DB and pression +- returns random volume?

For example, just scaled image with bilinear filter looks extremely clean like this
![image](https://user-images.githubusercontent.com/61594968/83236335-5e0c8580-a161-11ea-952e-9696308c42dc.png)

Color and feelings are not natural

Its possible to fix with some YIQ, blur, black/white point correction, noise for texturization, it gives this effect
![image](https://user-images.githubusercontent.com/61594968/83236413-7ed4db00-a161-11ea-9163-7d076ec77254.png)
Mario looks like
![image](https://user-images.githubusercontent.com/61594968/83236448-88f6d980-a161-11ea-8039-190ef1c86d9c.png)
Not like extra clear
![image](https://user-images.githubusercontent.com/61594968/83236780-13d7d400-a162-11ea-9532-d47b1e38261b.png)

Its also problem that LCD provides unnatural, too juicy color. May be this mario looks more natural, i reduced saturation

![image](https://user-images.githubusercontent.com/61594968/83236857-38cc4700-a162-11ea-9715-e1fd050c8f87.png)


U provide users extremely complex shader pack with AAAAAAA effects for tik tok

Why, if games created for playing, not playing in emulators? I promise that you software makes people VERY angry

A lot of hotkeys assigned on alphabet part of keyboard, why? Stock keyboard buttons sucks. Is it bad if u want just play from keyboard?

as for me, i use

WASD for arrows
QE - start/select
IO-/\O
KL-[]X

JU-L1L2
;P-R1R2

Why does its so necessary to load ffmeg library and crash if u remove this? It needs a lot of memory and huge size of emulator part. Are u really sure that so much users want to record the game or stream on twich?

May be there is some simple fork of retroarch for normal people? Why u cant make simple and ready for use, not so terrible?
",2020-05-29T08:16:02Z,61594968
1898,libretro/RetroArch,627087857,635840655,"These CRT shaders how does they works? They are very complex but at least out of box they provide ugly dark scanlines, no noise, no blur, thats all

![image](https://user-images.githubusercontent.com/61594968/83237679-4b934b80-a163-11ea-826d-bbab8fabdbe3.png)

Look and ducktales gradients, do u really wanna say that crt method is 'narural'?

![image](https://user-images.githubusercontent.com/61594968/83237790-767d9f80-a163-11ea-96e2-07a4a0f72afe.png)

",2020-05-29T08:18:41Z,61594968
1899,libretro/RetroArch,627087857,635841408,"Your project exist about 10 years and there is still not usable interface out of box, even no guide which explain how make this shit normal, explain what do u need to restore graphic etc",2020-05-29T08:20:22Z,61594968
1900,libretro/RetroArch,627087857,635845204,"U are so mad about graphic sync and ideal sync, that forgot about sound interpolation

If u link this with some stable realtime generator, it could give some frames offset like 60.4hz vs 59.8hz, but who cares on PC ~_~

As for me, i changed emulation speed on nestopia to 150% and its okay for fun. And if somebody so geeky that wants ideal sync, he can create custom resolution with 59.94hz",2020-05-29T08:28:48Z,61594968
1901,libretro/RetroArch,627087857,636000251,"I am sure you didn't read the contributing guidelines before posting...
As for every open source and free projects, maintained by people on there spare time, you are free to contribute, or use something else.",2020-05-29T14:24:17Z,4692881
1902,libretro/RetroArch,627087857,636031891,"![medecin-famille-masque_23-2148168455](https://user-images.githubusercontent.com/6266038/83276270-a44cfd80-a1d0-11ea-93d2-dc5004f4d889.jpg)
",2020-05-29T15:20:14Z,6266038
1903,libretro/RetroArch,565358712,565358712,"## Description

This greater ensures the `HAVE_MENU_WIDGETS`, `HAVE_RGUI`, `HAVE_MATERIALUI`, `HAVE_OZONE` and `HAVE_XMB` will never be defined when `HAVE_MENU` is not.

## Related Issues

This allows only checking if the specific feature is enabled and not that `HAVE_MENU` is also defined with processor code and some clean up in `Makefile.common`.

## Related Pull Requests

I noticed this was not correctly fixed as I previously thought in commit https://github.com/libretro/RetroArch/commit/2be5ffe4e999757ba89a87b0012d1441ec78af77.",2020-02-14T14:25:27Z,4204285
1904,libretro/RetroArch,565358712,590073199,"@twinaphex  Please stop censoring users, its very unprofessional.",2020-02-23T14:15:40Z,4204285
1905,libretro/RetroArch,565358712,590074184,"> My reason for not immediately merging this is for another reason - I want to make menu widgets independent of the menu code altogether. This includes also the menu animation code. That is why I don't want to immediately merge this PR overnight.

Also since you obviously do not understand what I even was submitting let me point out this PR would of only made that task easier in the future where you would of needed to remove one line instead of dozens as is the current state in the code, but good job throwing out the baby with the bathwater. :)",2020-02-23T14:25:39Z,4204285
1906,libretro/RetroArch,477798655,477798655,*redacted*,2019-08-07T09:02:42Z,17709993
1907,libretro/RetroArch,457082435,457082435,"Sorry to meddle but what is going on?
Why is the whole architecture of the program changing?

I'm far from an expert or even a proficient dev, and not part of the ""team"" but I still have some WIP things going on and these latest changes seem very concerning.

Is there any rationale or long term goals of rolling everything into retroarch.c?

",2019-06-17T18:19:21Z,1721040
1908,libretro/RetroArch,457082435,502805640,Here's an official answer: https://github.com/libretro/RetroArch/pull/8910#issuecomment-502708016,2019-06-17T18:48:42Z,31176526
1909,libretro/RetroArch,457082435,502806729,"Thank you for responding.

In my opinion it's awful but like someone told me recently ""not my circus, not my monkeys""
I'll stick to the current codebase until things stabilize.",2019-06-17T18:51:37Z,1721040
1910,libretro/RetroArch,457082435,502810622,"It's how the original SSNES/RetroArch was like, with most of the code that was split into separate driver files inside one big runloop file. 

This allows for better restructuring of the codebase by getting most global state in one central place, and less passing around of state everywhere.

Anyway, it's an iterative process, and it's done to make the program last and become more maintainable in the long run. We cannot get there with all these separate files in their current form with all sorts of global state all over the place. And as I keep maintaining, the majority of the code outside of retroarch.c and libretro-common should become 80 to 90% tasks, thread-safe and all. Anything global has to go into one centralized place, so that we can make order out of chaos. Not to mention it's more optimal this way as well. The code outside retroarch.c should become a lot more pure, no longer relying so much on global state everywhere, so that we can convert them easier into tasks. For this and other reasons, we need to do this drastic rebuilding. ",2019-06-17T19:03:00Z,1075274
1911,libretro/RetroArch,457082435,502812715,"> We cannot get there with all these separate files in their current form with all sorts of global state all across the place

Of course you can, you use a branch, do it piecemeal until you are done.

Anyway, as I said, I'm no expert.
On the few programming classes I had this was the complete opposite of what I was taught.
Tasks are fine for certain things but are a huge headache when you need things to run in order.

I'm sticking for the 1.7.7 codebase for the stuff I'm implementing.",2019-06-17T19:09:28Z,1721040
1912,libretro/RetroArch,457082435,502814248,"Branches are a non-issue, the refactors are already done piecemeal. Anyway, I'm the leader of this project, and I am making this decision to do it this way. The code more or less stayed the same anyway, it's a simple matter of the driver files being moved into one big file, and more functions turned static.

> Tasks are fine for certain things but are a huge headache when you need things to run in order.

Of course they are a headache, but it eventually has to happen anyway. CPUs are only getting more cores instead of faster clock speeds so allowing for stuff to be put on multiple threads at the same time is a very definite help (for things that can be done out of sequence), the main UI cannot block, many of these tasks on the main thread do block, which is bad. It's bad for Android, iOS, it's bad for the A/V synchronization timer, etc. It's bad for platforms like UWP where synchronous file I/O is very heavily penalized. It's a simple case of dealing with the headache because this is the only way forward.",2019-06-17T19:14:27Z,1075274
1913,libretro/RetroArch,457082435,502816792,"It was also an opportunity of getting rid of all these ctl functions (camera_driver_ctl, location_driver_ctl, audio_driver_ctl, etc). The ones that haven't already been gotten rid of, will be soon. There should be only one such function for all the driver stuff in retroarch.c, and that should be rarch_ctl. It was a bad design decision I made a few years ago to go for that approach, and themaister didn't like it either. So this is a way to correct that mistake.",2019-06-17T19:22:15Z,1075274
1914,libretro/RetroArch,457082435,502818994,"It's just making it even harder for contributors to work in the codebase.
As I said before it's your call. I don't have any stake into this. 

Just my opinion.",2019-06-17T19:29:07Z,1721040
1915,libretro/RetroArch,457082435,502820441," I think keeping it the way it was actually does that. Sure, a near-500Kb file is not ideal and things are a lot bigger than they were in the SSNES days, but I'm sure the total code size can be made far smaller. And all this state passing around was not helping performance either for certain. You might even notice a speedup or two.

Anyway, it's part of a longer-term redesign. It will pan out well.",2019-06-17T19:33:48Z,1075274
1916,libretro/RetroArch,457082435,502834364,"If it's in the name of performance, multithreading and also reducing code, it can't be that bad, all that global state had it coming anyways.

The maintainability might suffer a bit, but speaking as someone who just started contributing a few weeks ago, getting into it requires lots of debugging know-how anyways.
There's no easy ""reading into it"" (if you try starting with configuration.c, you will go bonkers), no ""it's about this, so it's probably there"" (for example: guess where the core init is called... retroarch.c? Nah, command.c), you basically only find stuff by starting with a log message and backtracking (good ol' printf debugging).
So in practical terms, putting more code in 1 place doesn't change much.

What I'm saying is: stay positive, if it turns out bad, it's not like we are now stuck with it forever but it'll give an idea of how to do it even better, and if it doesn't turn out bad, well, great!

Personally I think what's much more upsetting than the actual code changes are all these small commits, almost a whole page of them! Please aphex, squash more! :smile: ",2019-06-17T20:16:09Z,31176526
1917,libretro/RetroArch,457082435,502863117,"Thank you for the upbeat attitude @LazyBumHorse . And yeah you're right, the sourcecode is already not readable as is it for newcomers being splintered across so many files. It can only be improved really.",2019-06-17T21:46:08Z,1075274
1918,libretro/RetroArch,435537996,435537996,"Many many, many users complains about RetroArch not adding files to their playlists because the CRC doesn't match the original dump.

It can be because of a translation patch, an SRAM patch, and so on...

So I suggest a very simple solution :

Just add an option in playlist menu that could be named '_Ignore CRC while scanning roms_' or '_Add unknown roms to playlists_', whatever.

The thing is that it'll replace that line

`""crc32"": XXXXXXXX | crc""`

By

`DETECT`

In order to easily display all roms on playlists, with a label **taken from rom filenames**, so that roms with a good labels would display thumbnails, and roms with another label (like T-eng 100% etc...) would not display cover (it's not a big deal).

RetroArch playlists are painful to use because of those unique CRC32, and it would be wonderful to simplify it in that way !


For now, I'm creating my Playlists with _RetroArch Playlists Manager_ on Windows, to add my SRAM patched GBA roms, as well as all my non-detected translated roms.

Thanks for reading, I really hope to see that option.

### Version

- RetroArch: 1.7.6

### Environment information

- OS: Windows, Android, Switch
",2019-04-21T19:05:55Z,19387072
1919,libretro/RetroArch,435537996,485278274,"Why not just use the playlist manager in the desktop menu? It doesn't use the scanner.

Part of the reason why we validate the checksum is because an equal many number of people have bad dumps that won't work and then blame RA or the core instead.",2019-04-21T19:57:03Z,10137
1920,libretro/RetroArch,435537996,485278843,"@bparker06 What is desktop menu ? Is it the menu that opens with F5 on Windows ?

I mainly manage my Playlists on my Nintendo Switch, I guess there isn't that menu ?

I understand your point, but you can add it as an hidden option that would appear if advanced settings are enable. Advanced settings for advanced users.

Yes there arr bad dumps, but most of players also have goods dumps + patched versions, and it's hard to have those beautiful playlists with all our collection.",2019-04-21T20:06:46Z,19387072
1921,libretro/RetroArch,435537996,485279021,"One of the problems with this is that there's no way to associate database entries/metadata or thumbnails with playlist entries that don't match checksums, I would consider this a bad UX and defeating the point of working towards a ""beautiful playlist"".",2019-04-21T20:09:48Z,10137
1922,libretro/RetroArch,435537996,485283668,"I don't really understood your message, but, why can't RA act like RetroArch Playlists Manager ?

It takes the file name and put it as a displayed title, so that thumbnails can match that name.

e.g. Mega Man Zero (Europe).gba will be Mega Man Zero (Europe), and will read Mega Man Zero (Europe).PNG as a boxart.

Here, it will scan filenames and copy the name as a title in the .lpl file, and refers to file extension and/or current loaded core, to decide which console is concerned and save it as a playlist (e.g. GBA here)",2019-04-21T21:30:11Z,19387072
1923,libretro/RetroArch,435537996,485284120,"It could, but not everyone's roms are already named properly, and they don't want to rename them. Matching based on checksum fixes that issue.

If the roms aren't named properly then thumbnails won't work. And when you scan without the database, even if the name is correct, it won't know which folder to look in for thumbnails.",2019-04-21T21:37:46Z,10137
1924,libretro/RetroArch,435537996,485288676,"> It could, but not everyone's roms are already named properly, and they don't want to rename them. Matching based on checksum fixes that issue.

That's why I suggested to make it an option and not forcing people to rename their roms. It's just adapting to multiple users.

> If the roms aren't named properly then thumbnails won't work.

Yes, but it's a small trade-off for having user friendly playlists IMO...

> And when you scan without the database, even if the name is correct, it won't know which folder to look in for thumbnails.

RA can't create a playlist based on the extension ?
For instance well named .sfc files would be added on 'Nintendo - Super Nintendo Entertainment System.lpl' with DETECT lines.",2019-04-21T23:04:14Z,19387072
1925,libretro/RetroArch,435537996,485292098,"Thumbnails shouldn't work like this anyway.
For the packs or on-demand downloads (https://github.com/libretro/RetroArch/pull/8490) forcing filename is fine I guess.

But in the end the playlist format should be able to reference an arbitrary thumbnail filename.
That way on both GUIs we could have an option to link a thumbnail that is not on the thumbnails folder or isn't part of the packs.

The whole al filenames have to match convention was due to the fact that the playlist couldn't grow or store any more information because the format was hardcoded.

The new one has room to grow.

As for the OP.
I figure we could have an option that reads ""Add to playlist"" then you select a file and it would search the DBs for occurrences with that name, then the user would be able to select the best suited entry. 

Of course that's a lot more complex than it sounds

Bottom line, giving infinite hands anything can be done. But that's not the case on this project.",2019-04-22T00:03:38Z,1721040
1926,libretro/RetroArch,435537996,485292557,"> 
> As for the OP.
> I figure we could have an option that reads ""Add to playlist"" then you select a file and it would search the DBs for occurrences with that name, then the user would be able to select the best suited entry.

Why not, but it'd be a hundred time longer than manually creating playlists on a PC.

I also thought about adding roms to playlists according to their unique header/informations so that an header can be linked to a name on the database. But I guess it'll require to recreate an entire database.",2019-04-22T00:11:59Z,19387072
1927,libretro/RetroArch,435537996,485328570,"Here is a intense (but terrible) newflash:

CRCs are already ignored when scanning many many roms types - for serials. Among other things this means that contributing to the libretro-database romhacks CRCs is completely useless¹ and is most of the reason i'm hesitating to continue after contributing quite a lot and creating a tool to auto update my PRs and hacks.

If anything what i want is the exact opposite of this feature: give a option to ignore the serials and go full crc instead of pretending that a serial is a unique id (hilariously, even if you scan something like a snes rom, you get all of its 'versions' and 'dump variants' - and maybe even hacks don't even remember but it may be worse and they simply not be there). 

This is hardly unique, since console makers didn't really care often about editions or bugfixes bumping serials, though as i mentioned it gets kafkaesque once hacks come into the picture. I made plenty of suggestions to both make a CRC scanner faster (only do it optionally, only do it for (cd) formats with a inbuilt crc like chd, keep a separate hack database and only check when needed, keep the crcs of cue files for the 'canonical' dump sets etc).  The RA project made it clear that the scanner is a afterthought with a rather impressive number of ways it can go wrong and no interest in doing anything to get data the original dump sets didn't provide (filtering mame split sets for only the main 'rom' for instance) so i stopped caring.


¹ in fact i'm about 95% suure that last time i checked, the 'crc' on the playlist files was often bogus now, and a artifact of the old format / code using crcs but the scanner changing to serial as primary key, so you have crcs being 'found' that are completely fictitious.",2019-04-22T05:23:34Z,757900
1928,libretro/RetroArch,435537996,485331035,"Please stop being so negative.

>  The RA project made it clear that the scanner is a afterthought with a rather impressive number of ways it can go wrong and no interest in doing anything to get data the original dump sets didn't provide

Send us PRs to deal with these issues. The communication couldn't have been clearer. No walls of text are going to fix anything here on its own.

You might be mistaken into thinking we have infinite amounts of contributors, manpower and time to do all this - we don't. We depend on the generosity of coders to help the project forward, and we can't exactly rely on programs like Google Summer of Code either.
",2019-04-22T05:41:06Z,1075274
1929,libretro/RetroArch,435537996,485331636,"I opened the issues for about, oh 1 year? Possibly more. You yourself even mentioned 'uh maybe we shouldn't do this guys' on one of the worst examples and then someone marked the bug as 'minor'.

https://github.com/libretro/RetroArch/issues/7455#issuecomment-430884329

So no, i'm not going to do your features for you. I just thought it especially precious that people are so misinformed about this that they think CRCs are the 'problem that needs to be solved' when the scanner is a dumpster fire in many other ways because it abandoned unique keys or because there isn't a option to make the scanner stop trying to support everything under the sun and be bad at everything.

I'm not even opposed to fast scan of serials myself, only not when the 'serial' has a obvious other entry in the same database with a different crc. In that case, yes, i'd love the option to 'if it isn't a unique key, fallback to CRC plz'.",2019-04-22T05:45:13Z,757900
1930,libretro/RetroArch,435537996,485332503,"> I opened the issues for about, oh 1 year? Possibly more. You yourself even mentioned 'uh maybe we shouldn't do this guys' on one of the worst examples and then someone marked the bug as 'minor'.

> #7455 (comment)

>So no, i'm not going to do your features for you.

Stop being abrasive and show some respect. Last warning before I get Github moderators involved in here. Any kind of passive aggressive nonsense you can leave at the door from now on, keep it strictly focused on the factual and behave yourself. Nobody has time for this nonsense and this is supposed to be a drama-free developer zone. If you're not a developer, your ability to contribute to this project is limited to say the least, and walls of text don't amount to any code anybody can do anything with.

You say ""I'm not going to do your features for you"". The way I see it, YOU want us to do all this for you when I have indicated to you already it's not a fact of us wanting to do it or not, it's a matter of not having the necessary manpower for it and resources already being spread thin. Once again, stay polite while you're posting in these threads and don't cause a scene.

Being developers on open source projects is already enough of a burden and thankless job without having to deal with toxic personalities like the one you're portraying right now. Don't make our jobs even harder.",2019-04-22T05:50:55Z,1075274
1931,libretro/RetroArch,435537996,485333749,"Here's the facts:

RA is using serials, which lead to duplicates. It's also ditching certain emulator specific configurations of roms, lke MAME split sets (which could be supported by just taking the crc of the main game archive and pretend the others are not important). So the project decided to 'canonize' some particular sets, sometimes, and fuzzify most times.

This is still not enough to both fuzzily 'identify' certain games (games are missing), so people are asking for even more fuzzy filenames. I disagree with this because if the 'game is missing' even with serial, there is something else seriously wrong (what? Probably the scanner is dying on a untested hill somewhere on a set. Misidentifying sets of a console for another is my guess, that whole mess of scanner looking at a cd dump 'magic bytes' to find out what type of console it's supposed to be, which is not very resistant to different types of fileformat).

It's also *misidentifying* game hacks, because naturally they don't bother changing serials.

Finally, RA after getting the first entry of a game list (or the last don't recall) puts its (possibly bogus) CRC on the playlist (not certain, but probably because there are features and code that needs a crc).

Any future feature that depends on crc for uniqueness (say, web data, autoconfigs, auto tweaks, netplay) is built on quicksand.

",2019-04-22T05:58:33Z,757900
1932,libretro/RetroArch,435537996,485333751,"> I'm not even opposed to fast scan of serials myself, only not when the 'serial' has a obvious other entry in the same database with a different crc. In that case, yes, i'd love the option to 'if it isn't a unique key, fallback to CRC plz'.

My response to this is still going to be the same - make a Proof of Concept PR, if it's good, we can merge it. Our resources are already spread way too thin, and you are seeing armies of developers that are simply not there. This is the realities of an open source project - change happens at home, and if you want to see who is going to be leading that change, look in the mirror for the guy who could likely do it.",2019-04-22T05:58:33Z,1075274
1933,libretro/RetroArch,435537996,485334994,"Alternatively, start a bounty and add some money to it so these issues can be incentivized so we can find a developer who would like to take this on.

I wish it were different and I could find a developer overnight who would take this on, but that is not the current-day reality, it was not last year, and it is not the case this year either.",2019-04-22T06:05:31Z,1075274
1934,libretro/RetroArch,435537996,485353302,"I'm terribly sorry @andiandi13, but your issue was hijacked by two people in particular who really should have known better and who only seemed to participate in this thread to start drama and air grievances, and apparently caused them to piggyback off each other. To prevent this from spiralling out of control, the post in particular was removed and this thread locked. Please open a new issue and I'll make sure this does not happen again and your issue will be properly heard.

To @grant2258 and @i30817, don't do this again.",2019-04-22T07:42:37Z,1075274
1935,libretro/RetroArch,435537996,486021213,"Just adding a link to the new issue for reference.

https://github.com/libretro/RetroArch/issues/8624",2019-04-24T00:23:19Z,4204285
1936,libretro/RetroArch,430055555,430055555,"## Description

With vertically oriented games, Retroarch switches the emulator-provided width and height. For example, if the width/height is supposed to be 320x240, RA will switch this to 240x320. This only occurs with vertically oriented games. I tested this in both MAME and FBA using the Windows 64 bit version of RA. 

### Expected behavior

The game to be displayed using the correct resolution (width/height). 

### Actual behavior

The height and width is switched when a vertically oriented game is loaded. 

### Steps to reproduce the bug

1. load any vertically oriented game in either FBA or MAME using Windows 64 bit version of RA (example: DonPachi)
2. under video settings, enable integer scale. 
3. set ""custom aspect ratio width/height"" to 1x.
4. look at ""custom aspect ratio width/height"" to see that it says 240 for width and 320 for height. This is the reverse of what it should be; it should be 240 for height and 320 for width. 

### Bisect Results

I first noticed this a few days ago. 

### Version/Commit
You can find this information under Information/System Information

- RetroArch: 1.7.6
Git version: 9750719074

### Environment information

- OS: Windows 10 
- Compiler: NA

![image](https://user-images.githubusercontent.com/12267058/55672818-c2cd9800-585c-11e9-8ba2-0fd5024aee7c.png)

![image](https://user-images.githubusercontent.com/12267058/55672822-d7aa2b80-585c-11e9-8747-b31df5c07da0.png)

",2019-04-06T17:14:16Z,12267058
1937,libretro/RetroArch,430055555,480553748,some arcades have the monitor rotated 90 degrees. The  can the emulator can either rotate the image to the displays orientation or display it as intended and you rotated your display 90 degrees   ,2019-04-07T02:40:04Z,10137
1938,libretro/RetroArch,430055555,480587874,"I understand that, but the emulator-provided resolution should not be changed from the correct resolution, regardless of orientation. If the game is 320x240 it should be output as 320x240, not 240x320. The user can flip it or rotate it or resize it or whatever, but the output resolution from the emulator should remain the same (eg, 320x240 should be output as 320x240 before the user does stuff to it). Otherwise you get scaling artifacts unless you just happen to adjust the y axis to a multiple of both 240 and 320 (eg, 960). And if you’re playing a game that uses a weird resolution like 19XX (384x240), then it causes big problems with overscan or underscan when using integer scaling. As of right now, video aspect ratio for vertical games must be configured manually (custom ratio width/height, non-integer) along with custom ratio x/y position, because the resolution/ratio for vertical games isn’t being detected correctly. ",2019-04-07T12:56:06Z,12267058
1939,libretro/RetroArch,430055555,480592063,"the thing is the topic say the width and height are switched they simply arent it just a rotated monitor. It draws the image on the monitor as 4:3 then you flip the monitor physically this changes it to 3:4 on real hardware as far as the hardware is concerned its a normal 4:3 device but your rendering the image 90 degrees rotated.

In mame2003 or plus just use tate mode this stop the rotation and you need to flip your monitor like a real one. look at sf2 this is on a 4:3 screen look at the resolution of the game it use crt timing when rendering to look the way it does in a crt if you used the literal resolution for sf2 the gfx would be overly stretched. 

",2019-04-07T13:48:23Z,10137
1940,libretro/RetroArch,430055555,480592631,"it looks like your talking about capcom games watch this to understand whats going on and it is indeed 4:3

https://www.youtube.com/watch?v=LHfPA4n0TRo&feature=youtu.be",2019-04-07T13:55:27Z,10137
1941,libretro/RetroArch,430055555,480595131,"I’m not referring exclusively to capcom games. You can see this behavior in the example screenshots I posted above, which are from Donpachi. 

I understand very well that certain games are meant to be displayed vertically. This doesn’t change anything that I’ve said. 

With a vertically-oriented 320x240 game, width should say 320 and height should say 240, and the displayed image should be sideways. On a 4:3 CRT monitor the image would be 320x240 and sideways. You then rotate the monitor 90 degrees. This doesn’t change the image to 240x320. It is still 320x240 but now it is rotated 90 degrees so it is no longer sideways.

Altering the internal resolution of the game is incorrect, and will always result in scaling artifacts unless you happen to use a resolution that is a multiple of both 320 and 240. 

",2019-04-07T14:27:09Z,12267058
1942,libretro/RetroArch,430055555,480595698,If you display the arcade pcb to a CRT monitor in normal orientation it will be a 320x240 image that is sideways. You rotate the CRT 90 degrees. The resolution of the game remains unchanged. ,2019-04-07T14:34:06Z,12267058
1943,libretro/RetroArch,430055555,480596063,yes resolution of you display remains unchanged. I dont have a crt to test whats going on there I would assume you should be using core provided information and tate mode on for mame2003 and plus dont know if the others support tate mode,2019-04-07T14:39:08Z,10137
1944,libretro/RetroArch,430055555,480596550,You just aren’t getting what I’m saying. I’m going to have to post some screenshots later to help illustrate what’s going on currently and what should be going on instead. ,2019-04-07T14:44:58Z,12267058
1945,libretro/RetroArch,430055555,480596681,can you make them screenshot in mame2003 and 2003+ with tate mode on so i can see what going on make sure your using core provided for aspect ratio as well in them screenshots will help a lot in seeing whats going on,2019-04-07T14:46:37Z,10137
1946,libretro/RetroArch,430055555,480611617,"First of all, just look at the first two screenshots I posted above. See where it says ""custom aspect ratio width"" and ""custom aspect ratio height""  Both of these screenshots were taken with video rotation disabled. 

Width/height should be the reverse of what is shown in the above screenshots. ",2019-04-07T17:19:06Z,12267058
1947,libretro/RetroArch,430055555,480611698,"Here is what it SHOULD look like with video rotation disabled. This is what is NOT happening, currently. I created this in GIMP by manually switching the height/width in GIMP. 

![image](https://user-images.githubusercontent.com/12267058/55687288-89faf500-5928-11e9-9508-00ba5fc81b1e.png)
",2019-04-07T17:20:09Z,12267058
1948,libretro/RetroArch,430055555,480612022,"Here is what currently happens. Both of these are incorrect. 

video rotation disabled: 

![image](https://user-images.githubusercontent.com/12267058/55687236-135df780-5928-11e9-9070-10774a5cf9fe.png)

video rotation enabled: 

![image](https://user-images.githubusercontent.com/12267058/55687245-240e6d80-5928-11e9-8063-120409cd94f9.png)
",2019-04-07T17:24:10Z,12267058
1949,libretro/RetroArch,430055555,480612380,"Next, here are some tests conducted by HunterK. See how video height/width are switched from what it is in my screenshots in the very first post? That's what it should be. 
![image](https://user-images.githubusercontent.com/12267058/55687268-51f3b200-5928-11e9-91f6-379ac47df734.png)

![image](https://user-images.githubusercontent.com/12267058/55687272-5d46dd80-5928-11e9-8edd-af60b66eafa8.png)

",2019-04-07T17:29:14Z,12267058
1950,libretro/RetroArch,430055555,480612418,what emulator is this from here is screenshots from with ?,2019-04-07T17:29:50Z,10137
1951,libretro/RetroArch,430055555,480612765,"Next, I made this shot by manually resizing the resolution to 1600x1200, and used a scanlines shader. Notice how the scanlines and mask are perfectly scaled with no scaling artifacts present. This could only be the case if the game's resolution is indeed 320x240. With the currently reported resolution of 240x320, you get scaling artifacts. 

![image](https://user-images.githubusercontent.com/12267058/55687343-04c41000-5929-11e9-8e50-27049b2a3079.png)
",2019-04-07T17:33:54Z,12267058
1952,libretro/RetroArch,430055555,480612817,"In my initial post, I said that the behavior is present in both MAME and FBA in the Windows 64 bit version of RA. As you can see in the tests conducted by HunterK, this behavior is not present in the Linux version of RA.",2019-04-07T17:34:30Z,12267058
1953,libretro/RetroArch,430055555,480612964,"this is mame2003 with tate mode on and off using core provided aspect ratio I cant speak for the other emulators

![fshark-190407-183154](https://user-images.githubusercontent.com/6128601/55687360-0f4fcb00-5964-11e9-872f-05b77581644a.png)
![fshark-190407-183227](https://user-images.githubusercontent.com/6128601/55687361-0f4fcb00-5964-11e9-8bbc-2be81a66a576.png)



",2019-04-07T17:36:34Z,10137
1954,libretro/RetroArch,430055555,480613279,have you even tried running mame2003 with these settings it what it shoud do tate mode skips the rotating and renders like its should.,2019-04-07T17:40:25Z,10137
1955,libretro/RetroArch,430055555,480613300,"How are we supposed to know what the core reported width and height are from the screenshots you just posted? Again, the issue is that the emulator reported height and width are not correct. 
",2019-04-07T17:40:37Z,12267058
1956,libretro/RetroArch,430055555,480613632,"Tate mode ON solves the issue in MAME 2003. 

Tate mode off:
![image](https://user-images.githubusercontent.com/12267058/55687436-31c4f280-592a-11e9-8287-09ddfcff8861.png)

Tate mode on:
![image](https://user-images.githubusercontent.com/12267058/55687485-ae57d100-592a-11e9-8068-62d3bcc17dec.png)

",2019-04-07T17:45:10Z,12267058
1957,libretro/RetroArch,430055555,480613938,i think mame current has rotation options in the tab menu dont know if they will work as expected though ,2019-04-07T17:49:51Z,10137
1958,libretro/RetroArch,430055555,480613968,"The problem is, tate mode ON in MAME 2003 should be the default for all emulators including FBA. This is what the image looks like when you connect the actual hardware to a CRT that isn't rotated. There shouldn't be any automatic switching for vertical-oriented games. Switching height/width (Tate mode off) should be something that the user has to select because it's altering the original output of the emulated hardware. Anything done to alter the original output of the emulated hardware in this way should be something that the user has to manually select. 

Still no idea how HunterK was able to get the image to to display correctly using FBA, and I'm still unable to get the other version of MAME to display the image correctly. ",2019-04-07T17:50:17Z,12267058
1959,libretro/RetroArch,430055555,480614499,"sure that can be true im sure most people dont have there displays rotated unless they have a barcade of some sort setup for vertical games only. 

Its  a personal opinion but feel free to post an issue on it is easy enough to change the defaults through issues in the emulators in question. I dont really feel strongly about this its one for the people in charge of the repos it switchable in mame2003 and plus anyway in the options so the default wont really matter it can be changed to suit your display",2019-04-07T17:57:09Z,10137
1960,libretro/RetroArch,430055555,480614871,"I'm not just being pedantic, here. The width/height being switched isn't helpful and causes numerous potential issues. With the current behavior of height/width being automatically switched with vertical games, you still have to rotate the image if you want correct aspect ratio and orientation or to avoid a huge amount of letterboxing, and with height/width being switched like this, it results in scaling artifacts unless you happen to choose a resolution that is a multiple of both 320 and 240. 

Furthermore, there is still no way that I know of to get any emulator besides MAME 2003 to work correctly. 

The current rationale of ""people don't want to rotate their displays"" just isn't sufficient to justify this. ",2019-04-07T18:02:04Z,12267058
1961,libretro/RetroArch,430055555,480616075,"first of all i completely agree with you all options should be clear to change and i feel fba does cover this option well its just you have to restart for it to take effect/

ill need to look into mame current at a later time. I will tell you how to to fix fba got quick menu options turn vertical mode on and restart the emulator it should work as expected.",2019-04-07T18:16:18Z,10137
1962,libretro/RetroArch,430055555,480618244,"In FBA, enabling vertical mode in the quick menu options and restarting the emulator does not fix the problem. I'm still getting this: 

![image](https://user-images.githubusercontent.com/12267058/55688174-e95e0280-5932-11e9-867a-751931535c68.png)

",2019-04-07T18:43:54Z,12267058
1963,libretro/RetroArch,430055555,480618472,"Another problem related to this is the sheer number of knobs and dials related to video rotation. There are options related to video rotation in all of these places: 

main menu ->quick menu -> options
settings -> video
settings -> core 

If I'm an average user, how am I supposed to know what to do with all of this? Even as an experienced user, this is quite confusing. ",2019-04-07T18:46:46Z,12267058
1964,libretro/RetroArch,430055555,480619132,"SUCCESS. 

Actually, in FBA I have to turn vertical mode OFF under quick menu -> options. 

![image](https://user-images.githubusercontent.com/12267058/55688303-40b0a280-5934-11e9-9b0b-b53909c16ef2.png)

So, yes, there is a lot going on here. Each of the cores seems to be handling this in a different way and there are far too many knobs and dials related to this. Disabling vertical mode makes it work in FBA, while enabling tate mode in MAME 2003 makes it work there... how does that make any sense?! As things currently stand, the options for this are an incoherent mess. 

Even you thought that enabling vertical mode would be the same as enabling tate mode, which makes sense, intuitively. There just seems to be no rhyme or reason to the way these settings currently work. ",2019-04-07T18:55:27Z,12267058
1965,libretro/RetroArch,430055555,480620589,"I'm still getting the same problem in FBA 2012. There is no option in quick menu -> options for vertical mode. Under settings -> core there is an option for ""enable rotation."" I'm getting the same results with this turned either ON or OFF. 

settings -> core -> allow rotation ON:
![image](https://user-images.githubusercontent.com/12267058/55688480-342d4980-5936-11e9-8518-0d0c7cb97ac2.png)

settings -> core -> allow rotation OFF:
![image](https://user-images.githubusercontent.com/12267058/55688505-7060aa00-5936-11e9-9d88-ce8daffcca83.png)

",2019-04-07T19:14:18Z,12267058
1966,libretro/RetroArch,430055555,480622042,"as far as I can tell, the problem is the frontend based rotation not taking into account the aspect ratio change.
I don't think we need so many knobs in the cores to do this, it should be one option in the frontend and that's all.

I'm not even sure why there is a ROTATION ENV callback in the API",2019-04-07T19:33:15Z,1721040
1967,libretro/RetroArch,430055555,480628910,"there is a reason for it to be there clearly this isint a front end issue. It cant guess if you have rotated you monitor 90 degrees physically you need to deal with this as a core option if that is the case. 

",2019-04-07T20:55:59Z,10137
1968,libretro/RetroArch,430055555,480628956,"Agree w/fr500. 

In my opinion, a vertically-oriented game should look like this by default: 

![image](https://user-images.githubusercontent.com/12267058/55689631-1961d180-5944-11e9-8bf0-d554f236ae08.png)

The user can then rotate the image using the advanced user option under settings -> video -> rotation

<strike>You could also add an option under video settings (""automatically rotate vertical games 90 degrees"" or something) to automatically set settings -> video -> rotation -> 90 degrees whenever a vertical oriented game is detected. But, I'm getting ahead of myself.</strike> EDIT, okay yeah this wouldn't work. But I still think that the core shouldn't be doing stuff to alter the output resolution (at least not by default) and that the user should do this with the frontend options for rotation and aspect ratio, or by manually selecting a core option to switch width/height for non-rotated displays. ",2019-04-07T20:56:32Z,12267058
1969,libretro/RetroArch,430055555,480629221,"> 
> 
> there is a reason for it to be there clearly this isint a front end issue. It cant guess if you have rotated you monitor 90 degrees physically you need to deal with this as a core option if that is the case.

The resolution/aspect ratio output by the emulator should remain unchanged by the core- see above. A vertical game that is 320x240 should be output as 320x240. The frontend options for video rotation and aspect ratio are more than sufficient and solve the problems related to the way things are currently handled, which are, to wit:

1. it's incorrect according to what the original hardware does
2. it's counter-intuitive
3. it results in scaling artifacts with integer scaling

If people want to play vertical games rotated 90 degrees on their monitors, then they should use the frontend for that. The emulator/core shouldn't alter the actual output. ",2019-04-07T21:00:02Z,12267058
1970,libretro/RetroArch,430055555,480630056,and how is the front end supposed to guess that this particular arcade is rotated or not mame2003 covers this i dont see how you can get the front end to be psychic,2019-04-07T21:11:27Z,10137
1971,libretro/RetroArch,430055555,480630788,"I'm saying that the front end options are sufficient and that the cores are currently doing something wrong/there's too much going on with the cores. Just make the cores output the original resolution, unaltered, and let the user use the frontend options for rotation and aspect ratio. That would solve the problem(s). 

In standalone FBA, you get this by default, which is as it should be: 

![image](https://user-images.githubusercontent.com/12267058/55690013-449aef80-5949-11e9-872d-ea9cd88d04ce.png)


With ""Video -> rotate vertically aligned games -> enabled"" you get this, which is again as it should be:

![image](https://user-images.githubusercontent.com/12267058/55689969-ab6bd900-5948-11e9-8f7b-fcebec130e3c.png)

",2019-04-07T21:21:25Z,12267058
1972,libretro/RetroArch,430055555,480631219,ok with you so far you failing to mention the users monitors physical orientation in all this,2019-04-07T21:27:02Z,10137
1973,libretro/RetroArch,430055555,480631512,"I don't see why the monitor orientation is relevant to what resolution the core is outputting. Altering the height/width of the resolution so that the image is correctly oriented on a non-rotated monitor is altering the output resolution, and thus incorrect. Furthermore, in some cases you still have to rotate the image using the frontend options to get a properly oriented image even though the height/width are being switched; automatically switching the height/width doesn't solve the problem that it is intended to solve and is introducing more problems. You can see this in the above example with MAME 2003; TATE mode on or off still results in a sideways image so that the user either has to rotate it using a front end option or physically rotate their display. The core should just output the unaltered resolution. Altering the resolution, aspect ratio or video rotation is all stuff that should happen on the frontend or through a user option that has to be selected. 

At the very least, it would be preferable to stop enabling automatic rotation by default and have this be something that the user has to enable through a core option. The default should be the original unaltered image (see the example of standalone FBA, above). The user shouldn't have to dig through multiple menus of confusingly-named settings and apply a lot of trial and error in order to get the emulator to display the original unaltered image.",2019-04-07T21:30:48Z,12267058
1974,libretro/RetroArch,430055555,480632602,"Also, as we can see, there's just a lot of weird stuff going on. TATE mode ON in MAME results in the same behavior as vertical mode OFF in FBA, and FBA 2012 won't display a correct image no matter what settings are applied, and I haven't even tested all the emulators yet. I understand the lack of standardization when it comes to core options, but this just seems somewhat ridiculous. ",2019-04-07T21:46:08Z,12267058
1975,libretro/RetroArch,430055555,480632982,"im trying to understand where you are hitting this from the cores arent scaling the resolution RA scaless the resolution to your display.

",2019-04-07T21:52:06Z,10137
1976,libretro/RetroArch,430055555,480633397,To what are you referring with this statement? ,2019-04-07T21:58:10Z,12267058
1977,libretro/RetroArch,430055555,480633538,"> and how is the front end supposed to guess that this particular arcade is rotated or not mame2003 covers this i dont see how you can get the front end to be psychic

If the front end can't detect whether a game is vertical or not, then it's the cores that are automatically switching the height/width for vertical games. ",2019-04-07T22:00:17Z,12267058
1978,libretro/RetroArch,430055555,480634732,"yes they do people that dont rotate there monitors dont want to play there vertical games sideways or rotate there monitor by default.  Tate mode wont rotated vertical games at all on mame2003 that what you requested. You have t do your part now and rotate your monitor physically to match the arcade. 
 ",2019-04-07T22:18:57Z,10137
1979,libretro/RetroArch,430055555,480635529,"This whole discussion is pointless.

> and how is the front end supposed to guess that this particular arcade is rotated or not mame2003 covers this i dont see how you can get the front end to be psychic

There is not need to guess anything, this is software development, there is an API, an implementation, and a frontend.

There is an API for this:
https://github.com/libretro/RetroArch/blob/master/libretro-common/include/libretro.h#L486

So the players are:
- core
- api
- frontend

Who knows the content needs rotation? The core does.
The environment callback is a set, which means it's telling the frontend to ""do something""

So what should happen could go two ways

a. The core tells the frontend: ""hey video is rotated, adjust aspect ratio from what I reported accordingly""
b. The core tells the frontend: ""hey this content requires rotation, do whatever you need to do so it works properly""

That's all. This is a frontend problem, but before anything can be do about it what we need is clarification from the API side so we can adjust both the frontend and the cores to do whatever needs to be done.
That's all.

",2019-04-07T22:31:04Z,1721040
1980,libretro/RetroArch,430055555,480640219,"there is no front end issue some arcades have different rotations not just 90 degrees. The core needs to work this out the user will need to be more specific from what he says he wants vertical games to display like this by default  which is perfectly valid if you have a rotated monitor.

![fshark-190407-183227](https://user-images.githubusercontent.com/6128601/55691157-04605f00-5993-11e9-8e9b-f51fa4f66dde.png)

this is how it displays when you dont have a rotated monitor the user seems to think this is wrong
![fshark-190407-183154](https://user-images.githubusercontent.com/6128601/55691184-54d7bc80-5993-11e9-8fe8-57f884e15aa1.png)
",2019-04-07T23:15:00Z,10137
1981,libretro/RetroArch,430055555,480641025,"Ok, you win, have fun arguing forever instead of proposing a solution.",2019-04-07T23:21:34Z,1721040
1982,libretro/RetroArch,430055555,480641314,"Ok you have a 4:3 monitor it draws as 4:3 like the fist picture you rotate it the monitor in the arcade 90 degrees what aspect ratio is it now for us to display on out normal monitor sitting on the tv stand? ill leave you two to it. 

Despite what the op said before  fba mainline are set this way by default i just downloaded it.  Its default  rotate vertical games  mame mainline and all our lr coresdo this by default afaik. However not all lr cores have the option not to rotate.",2019-04-07T23:23:50Z,10137
1983,libretro/RetroArch,430055555,480643111,"> This whole discussion is pointless.
> 
> > and how is the front end supposed to guess that this particular arcade is rotated or not mame2003 covers this i dont see how you can get the front end to be psychic
> 
> There is not need to guess anything, this is software development, there is an API, an implementation, and a frontend.
> 
> There is an API for this:
> https://github.com/libretro/RetroArch/blob/master/libretro-common/include/libretro.h#L486
> 
> So the players are:
> 
>     * core
> 
>     * api
> 
>     * frontend
> 
> 
> Who knows the content needs rotation? The core does.
> The environment callback is a set, which means it's telling the frontend to ""do something""
> 
> So what should happen could go two ways
> 
> a. The core tells the frontend: ""hey video is rotated, adjust aspect ratio from what I reported accordingly""
> b. The core tells the frontend: ""hey this content requires rotation, do whatever you need to do so it works properly""
> 
> That's all. This is a frontend problem, but before anything can be do about it what we need is clarification from the API side so we can adjust both the frontend and the cores to do whatever needs to be done.
> That's all.

I don't understand all of this, but it sounds very reasonable. 

Yes, it does seem grant2258 and I have been going in circles with this. I think I've provided enough info on the problem as it currently stands to work towards a solution, but since I'm not a programmer, I've probably done all I can by this point. ",2019-04-07T23:42:17Z,12267058
1984,libretro/RetroArch,430055555,480646624,"i agree with that  @CharlesBukowski  im sure @fr500 has some idea looking forward to seeing what he is going to do since he thinks there is an issue and what exactly is wrong as i cant see any issue at all with mame2003 or fba on libretro it both have the ability rotate or not rotate vertical games. It does it the same way as mainline fba and mame. 

So i wll digress im at a bit of a loss what you two seem to think the issue is is you want to maintain a 4:3 ratio you need to rotate the Monitor or physically force the aspect ratio to 4:3 when rotating and put up with the streched gfx
",2019-04-08T00:23:50Z,10137
1985,libretro/RetroArch,430055555,480651030,"The issue is that the behavior isn't consistent across cores and it's confusing as hell what is actually going on. 

There is the separate issue of what the default behavior should be for vertical games, which you seem to be caught up on. Just having this stuff standardized across cores would be a huge improvement over the current situation, regardless of what the default behavior is set to. 

As of right now, MAME and FBA do the exact opposite thing with video_allow_rotate = true/false. And that's just two cores. Who knows what's going on with other cores; I haven't even had a chance to test them yet. 

TATE on and video_allow_rotate = true in MAME 2003 produces the same results as vertical mode on and video_allow_rotate = false in FBA. How does this make any sense? 

posting this for reference, read from this post down. 

https://forums.libretro.com/t/switch-video-width-height-when-video-is-rotated/21674/35?u=nesguy",2019-04-08T01:11:15Z,12267058
1986,libretro/RetroArch,430055555,480652403,"

> 
> 
> i agree with that @CharlesBukowski im sure @fr500 has some idea looking forward to seeing what he is going to do since he thinks there is an issue and what exactly is wrong as i cant see any issue at all with mame2003 or fba on libretro it both have the ability rotate or not rotate vertical games. It does it the same way as mainline fba and mame.
> 
> So i wll digress im at a bit of a loss what you two seem to think the issue is is you want to maintain a 4:3 ratio you need to rotate the Monitor or physically force the aspect ratio to 4:3 when rotating and put up with the streched gfx



1. automatically switching the width/height for vertical oriented games is altering the output resolution, which is incorrect. The emulator should just output the resolution completely unaltered. Altering what is output by the emulator should always be done by the frontend, or through an option that the user has to manually select, but maybe that's just my opinion. The default should just be whatever the emulator spits out before you start doing stuff to it. 

2. rotating the game is something that can be easily handled in the frontend without all of the confusion that currently exists 

3. 2 is something that can even be done automatically if desired by the user, through the frontend as explained by fr500.

4. the current method of automatically switching the height/width with vertical games will always result in scaling artifacts no matter what you set the aspect ratio to (unless it's a multiple of both 240 and 320). How can this be considered correct? 

5. the current method of automatically switching the height/width of vertical games isn't even making things easier for the user in all cases, because video_allow_rotate = true/false isn't doing the same thing in all cores. The current method is just adding more confusion. 

My brain is tired. Hopefully the thread I linked to above sheds some additional light on the problem. ",2019-04-08T01:21:59Z,12267058
1987,libretro/RetroArch,430055555,480656907,"IMO Standalone FBA does things right.

 See: https://github.com/libretro/RetroArch/issues/8551#issuecomment-480630788

By default it's sideways and 320x240, and there's a clearly labeled and easily accessed option to rotate video under the video menu. 

There should be a way to make RA work the same with all cores using one of the methods @fr500 described (it may need either a or b depending on what the core is doing). You could then add an easily accessed user option to automatically rotate vertical games. (or an option to disable automatic rotation if rotation is the default behavior)

Or we could just endlessly debate what the default behavior should be when launching RA for the first time. ",2019-04-08T01:55:23Z,12267058
1988,libretro/RetroArch,430055555,480660954,"Ok here is the screenshots with you information the core geometry is reporting the right resolution. I explained before above that the information you need. Fba by default comes with setup like rotated i installed it and checked. It does not display vertical none rotated by default. Just giving you information in one place here not debating at all

rotated
![Untitled](https://user-images.githubusercontent.com/6128601/55694766-85c4eb00-59ad-11e9-8938-fbe965c7f255.png)

tate mode
![Untitled1](https://user-images.githubusercontent.com/6128601/55695277-cd4c7680-59af-11e9-81de-deb700b117b2.png)

fba default install no setting changed
![fba default](https://user-images.githubusercontent.com/6128601/55695612-426c7b80-59b1-11e9-8938-b8717439adfc.png)

Ill add one more this is screen shot with fba pay attention to the width and height

![res](https://user-images.githubusercontent.com/6128601/55737000-526d7500-5a1c-11e9-9815-dc3a78be4ad0.png)

",2019-04-08T02:22:40Z,10137
1989,libretro/RetroArch,430055555,480911698,"> Ok here is the screenshots with you information the core geometry is reporting the right resolution. I explained before above that the information you need. Fba by default comes with setup like rotated i installed it and checked. It does not display vertical none rotated by default. Just giving you information in one place here not debating at all

I'm trying to figure out the point you're trying to make. If you're still arguing over what the default behavior should be when launching RA for the first time, I'm done with that conversation. 

",2019-04-08T16:49:46Z,12267058
1990,libretro/RetroArch,430055555,480912425,you rotate 320 x 240  image 90 degrees is becomes 240 x 320 thats as simple as you can describe it,2019-04-08T16:51:51Z,10137
1991,libretro/RetroArch,430055555,480913207,"> 
> 
> you rotate 320 x 240 image 90 degrees is becomes 240 x 320 thats as simple as you can describe it

Who is disputing that? What is the point of this? 

",2019-04-08T16:54:09Z,12267058
1992,libretro/RetroArch,430055555,480914297,"```
With vertically oriented games, Retroarch switches the emulator-provided width and height. For example, if the width/height is supposed to be 320x240, RA will switch this to 240x320. This only occurs with vertically oriented games. I tested this in both MAME and FBA using the Windows 64 bit version of RA.
```",2019-04-08T16:57:17Z,10137
1993,libretro/RetroArch,430055555,480914973,"Yes, and how do you think what you just said contradicts this statement? 

I think there is a language barrier here that is preventing meaningful discussion. ",2019-04-08T16:59:05Z,12267058
1994,libretro/RetroArch,430055555,480915661,that was you that it in your first post   your rotate an images the 90 degrees THE x /y swap,2019-04-08T17:00:54Z,10137
1995,libretro/RetroArch,430055555,480916855,"Furthermore, the conversation has progressed quite a bit beyond the initial post. If you had been keeping up with the conversation you'd realize that the real issue is a lack of consistency between cores when it comes to the options related to video rotation. 

I am 100% done debating with you what the default behavior related to video rotation should be. 

It doesn't matter what the default behavior is. There are pros and cons to having rotation on or off by default and ultimately it's just an arbitrary decision that has to be made. What matters now is getting consistent behavior across cores and removing unnecessary knobs/dials that just lead to more confusion. @fr500 already outlined the solution and what he needs to implement it. ",2019-04-08T17:04:05Z,12267058
1996,libretro/RetroArch,430055555,480917580,"> 
> 
> that was you that it in your first post your rotate an images the 90 degrees THE x /y swap

I literally can't even tell what you're trying to say, here. 

I'm sorry, I really don't want to offend but the language barrier is just too much to deal with. I feel like you just consistently fail to understand the point being made and respond with irrelevant information most of the time and now I'm just wasting a lot of time trying to clarify things to you. I appreciate that you're trying to help, but we haven't been getting anywhere for a while. ",2019-04-08T17:06:14Z,12267058
1997,libretro/RetroArch,430055555,480921992,"i literally cant even figure your problem out the games will show sideways if they arent rotated. mame fba and lr cores do this. So i guess i leave it at this not wasting more time on you back tracking it not a good default leaving vertical games sideways. Something you claimed fba done

https://github.com/libretro/RetroArch/issues/8551#issuecomment-480630788",2019-04-08T17:14:56Z,10137
1998,libretro/RetroArch,430055555,480942598,"> 
> 
> i literally cant even figure your problem out the games will show sideways if they arent rotated. mame fba and lr cores do this. 


did you even bother following the steps I listed to reproduce the bug? Or bother looking at the discussion at the libretro forums? There is most definitely something weird currently going on with video rotation and height/width being switched. It's not a problem present in all cores. Rotation behavior is not consistent across cores, for the nth time. That's probably the main problem. 


>  So i guess i leave it at this not wasting more time on you back tracking it not a good default leaving vertical games sideways. Something you claimed fba done

FFS, the screenshots I provided prove it! Facepalm. Here, to make things even clearer:

![image](https://user-images.githubusercontent.com/12267058/55746327-a7969000-59f6-11e9-90c8-ed577e14f713.png)

![image](https://user-images.githubusercontent.com/12267058/55746399-cf85f380-59f6-11e9-8be5-e09bd8da31f1.png)
",2019-04-08T18:09:02Z,12267058
1999,libretro/RetroArch,430055555,480948593,"you rotated the image  what do you expect to happen? you screen shots prove nothing accept the video is rotated and not rotated.  fba and mame2003 create the same images

",2019-04-08T18:25:06Z,10137
2000,libretro/RetroArch,430055555,480948833,"> 
> 
> you rotated the image what do you expect to happen? you creen shots prove nothing accept the video is rotated fba and mame2003 create the same images

That's exactly what I expect to happen. Do you see that you're just not following what I'm saying very well? ",2019-04-08T18:25:45Z,12267058
2001,libretro/RetroArch,430055555,480949068,"> 
> 
> > you rotated the image what do you expect to happen? you creen shots prove nothing accept the video is rotated fba and mame2003 create the same images
> 
> That's exactly what I expect to happen. Do you see that you're just not following what I'm saying very well?

No they don't, and I've provided numerous examples demonstrating this. ",2019-04-08T18:26:25Z,12267058
2002,libretro/RetroArch,430055555,480949376,"I guess you dont get it when you rotate the video it becomes 3:4 proof is in the screen shot from fba rotated. it 240 x 230 as well not wasting any more time repeating this
 
![res](https://user-images.githubusercontent.com/6128601/55747495-4beaf780-5a34-11e9-9f22-acf658589866.png)
",2019-04-08T18:27:10Z,10137
2003,libretro/RetroArch,430055555,480950665,"> 
> 
> I guess you dont get it when you rotate the bideo it becomes 4:3 proof is in the screen shot from fba rotated. it 240 x 230 as well not wasting any more time repeating this
> 
> ![res](https://user-images.githubusercontent.com/6128601/55747495-4beaf780-5a34-11e9-9f22-acf658589866.png)

Read this thread, from this post down to the end. Others clearly recognize that there is an issue. 
https://forums.libretro.com/t/switch-video-width-height-when-video-is-rotated/21674/21?u=nesguy

Please just go away. You aren't getting it and I've reached the limit of my patience with this. 

",2019-04-08T18:30:37Z,12267058
2004,libretro/RetroArch,430055555,480951120,is fba wrong as well. Ive lost my patience with this as well. ,2019-04-08T18:31:52Z,10137
2005,libretro/RetroArch,430055555,480958875,"> 
> 
> is fba wrong as well. Ive lost my patience with this as well.

From the thread I just linked to, above:

> BarbuDreadMonSenior Member
> 30m
> 
> @Nesguy My mistake about width/height being switched for vertical games in fbalpha. While there is no official statement about this, i also think the core should always report resolution before rotation, i changed this behavior in a commit today. Now that it’s settled i would recommend allowing core rotation again and enabling vertical mode, it’s now upscaling properly for me, and there doesn’t seem to be issues with shaders either. One last thing about fbalpha : if you want proper aspect ratio, use core-provided aspect ratio + DAR in core options for fbalpha (actually i’m thinking about removing this core option and always forcing DAR), aspect ratio is per-game and hardcoded in the emulator for each one.


So, yeah. It's not that you're wrong, it's just that you never quite understood what the problem was in the first place. Thankfully, the issue with FBA was recognized and corrected by BarbuDreadMon.",2019-04-08T18:51:23Z,12267058
2006,libretro/RetroArch,430055555,480966333,"sending the wrong geometry form the core to fix it i would say thats out of spec and a patch. im sure mark will patch mame2003 and + up to send wrong info so it works in mame2003 and plus. Its trivial to do never the less i wont be changing is as its out of spec to whats really happening. 

no matter what your little numbers say the resolution is 240 x 320 when rotated :). I wold suggest you use fba for now because every other core will have it the same way most likely. Ps current mame rotates fine its in the tab menu video options of course its wrong as well cause you said so
",2019-04-08T19:09:39Z,10137
2007,libretro/RetroArch,430055555,481212456,"i can explain the reasoning behind TATE mode core option in mame2003:

the emulator sends two bits of information to the retroarch API; the games width and height, and how much it should be rotated:

so, for a typical arcade game, that would be:
**width:** 320
**height:** 240
**rotation:** 0 (none)

now, for a typical arcade vertical game (which were rendered sideways, and then the CRT was rotated when put in the cabinet), that information would be be:
**width:** 240
**height:** 320
**rotation:** 1/3 (90/270 degrees)

the issue is, retroarch uses the sent height and width regardless of whether it has been rotated or not. so, the emulator has to send the width and height of the game on the presumption that the rotation has happened. so, if you set `video_allow_rotate = false` you will get an unwanted result:

`video_allow_rotate = true`:
![centiped-190409-121359](https://user-images.githubusercontent.com/13054748/55795957-ce20fd80-5ac0-11e9-8bbc-10d01a5cf1e0.png)

 `video_allow_rotate = false`:
![centiped-190409-120210](https://user-images.githubusercontent.com/13054748/55795769-71bdde00-5ac0-11e9-868c-f08684fff394.png)

so in this case, mame2003 is sending the core:
**width:** 240
**height:** 320
**rotation:** 1/3 (90/270 degrees)
but `video_allow_rotate = false` is saying ""I don't care about how much the emulator wants to rotate it"", so you are ending up with the game not rotated (fine), but the screen dimension are now wrong.

TATE mode was added so that it doesn't presume rotation. Ie, it instead would send:
**width:** 320
**height:** 240
**rotation:** 1/3 (90/270 degrees)
(the rotation is still sent - the presumption is that anyone with a TATE setup would also set `video_allow_rotate = false`)
![centiped-190409-122201](https://user-images.githubusercontent.com/13054748/55796687-7daa9f80-5ac2-11e9-9abb-dae5908ac2e9.png)

(PS, screenshots from behaviour of mame2003 from a year or so ago, and retroarch 1.3.6 - this is what i originally designed TATE mode on - someone else rewrote TATE mode in 2003 so it instead does the rotations within the core itself - personally i think it should all be handled in the front end)

so hopefully that shows the issue that TATE mode was trying to solve. IMO there's some better solutions:

1) the API lets the core find out what video_allow_rotate is set to. that way, it can send the appropriate height/width for the situation.
2) rotation of 1 (90) or 3 (270deg) also flips the height/width. this would be my preference.

i haven't really thought about the ramifications of these! :)
",2019-04-09T11:27:28Z,13054748
2008,libretro/RetroArch,430055555,481215666,"there is also some interplay with aspect_ratio that I've forgotten the details about.

EDIT: hmm actually it think TATE mode only changed the core provided aspect ratio. but anyway you can see the problem i had to solve.",2019-04-09T11:39:10Z,13054748
2009,libretro/RetroArch,430055555,481292393,"the front end is still doing the rotating in mame2003 the fix is trivial your just dont swap change the x/y (like you should be doing in the core geometry).    rotate anything manually the geometory x/y in the menu doesnt change. 


https://github.com/libretro/mame2003-plus-libretro/compare/master...grant2258:out_of_spec?expand=1

",2019-04-09T15:07:53Z,10137
2010,libretro/RetroArch,430055555,481441626,"> no matter what your little numbers say the resolution is 240 x 320 when rotated :). 

No one ever disputed that the correct resolution is 240x320 when rotated. What's being disputed is whether or not all the cores are reporting the correct resolution when the image is rotated. You really are failing to grasp what's going on, here. 

Numerous people have since recognized the issue I'm describing and the issue in FBA was both recognized and fixed. The fact that you're holding firm on this even in spite of this indicates either a willful stubbornness on your part to admit that you're wrong, or just a complete misunderstanding on your part. 

There are numerous ambiguities and equivocations occurring in this conversation which make it very difficult to have a meaningful discussion with you. In fact, on more than one occasion you seem to think that I'm arguing for the exact opposite of the point I'm trying to make.

The question of what the default rotation behavior should be for vertical games is almost trivial. It literally only matters the first time RA is launched, after which the user can set the behavior in the frontend. The MAIN issue is getting consistent behavior across all cores, regardless of what the default behavior should be. 

Now, if we're discussing the SEPARATE issue of what the default behavior should be or what is technically ""correct,"" I've already said what I'm going to say about that. You can either respond directly to the points I've made or continue ignoring them, either one is fine by me. Both standalone FBA and now also the FBA core do exactly what that they should be doing: by enabling rotation in the core options and enabling vertical mode in the quick menu core options, you get the correct image, which looks like this: 

![image](https://user-images.githubusercontent.com/12267058/55835445-6b882b80-5ad9-11e9-95e5-767e36d2622c.png)


",2019-04-09T21:10:28Z,12267058
2011,libretro/RetroArch,430055555,481445394,@CharlesBukowski clearly in not debating with you and your screenshots anymore. you clearly dont understand geometry when a rotate happens. Im not wasting more time on it with you. No offence i posted a fix details for mame2003 and plus to send this bad geometry clearly you dont understand this at a level beyond screenshots.,2019-04-09T21:22:34Z,10137
2012,libretro/RetroArch,430055555,481451616,"As for tate mode in mame2003 and  plus is a ccw 90 degree rotate (it was renamed to what it is now from that) so if the arcade is 90 or 270 it will rotate the same way if the user has there screen rotated in a barcade/ servo/ stand setup . If you want a no rotate option youll need to post a github issue to get it added. 


intended use for this rotation is this https://www.youtube.com/watch?v=dShq_reM-84 if you have the ability to do it. So a no rotate would need to have a good reason",2019-04-09T21:43:58Z,10137
2013,libretro/RetroArch,430055555,481483688,"to sum it up its to do with core rotating an image with ra the cores info updates the rotated geom. RA doesnt like it or update geometry even when you do a manual rotate itself.  So you have to return the original orientation even when you rotate through RA with the core. 
",2019-04-10T00:02:56Z,10137
2014,libretro/RetroArch,430055555,481484714,all mame cores will need updates or ra should report rotated geometry.  ,2019-04-10T00:08:27Z,10137
2015,libretro/RetroArch,430055555,481500003,"@grant2258

You are completely hopeless. Several other devs agreed that there’s an issue, FBA dev already agreed that there is an issue and fixed it In FBA. To deny that there is an issue at this point is pretty much insane. I’m done talking to you. ",2019-04-10T01:38:18Z,12267058
2016,libretro/RetroArch,430055555,481501898,"@fr500, yeah I don’t know why this is even still a discussion. I’m at a loss. 

I use custom aspect ratios with RA because I overscan some games that scale to 1120 on the y axis. Switching it to core provided didn’t change what was going on with the reported width/height and rotation, as far as I could tell. 

Anyway, the issue in FBA was fixed yesterday so I think this issue can be closed; rotation behavior appears to be consistent, at least from the user’s perspective. The issue of how to best handle rotation is really a separate topic that probably warrants its own issue. ",2019-04-10T01:49:10Z,12267058
2017,libretro/RetroArch,430055555,481504574,@CharlesBukowski ive already done the fix for 2003 and plus linked it and described the issue you are either stupid or argumentative or clueless about whats going on i think the latter :),2019-04-10T02:03:58Z,10137
2018,libretro/RetroArch,430055555,481512478,"@grant2258 Let's please keep this respectful at the very least. That last line was unnecessary and he shouldn't have been insulted like that.

People in this thread recommended that I close this thread so that is what I will do.",2019-04-10T02:48:25Z,1075274
2019,libretro/RetroArch,285132250,285132250,"## Description

RetroArch builds debug builds by default after commit https://github.com/libretro/RetroArch/commit/ec4b0f90896d9cca2b9eaa0df0e9127b3ca5445d
This is very bad and breaks ./configure && make which would explicitly need `DEBUG=0`.

## Related Issues

Debug support should not be enabled if `DEBUG` is undefined.

## Related Pull Requests

https://github.com/libretro/RetroArch/commit/ec4b0f90896d9cca2b9eaa0df0e9127b3ca5445d

## Reviewers

@twinaphex, @alcaro, @bparker06
",2017-12-29T18:00:37Z,4204285
2020,libretro/RetroArch,285132250,354483782,"Release builds are impossible to debug properly, therefore they're the broken ones. Debug builds work fine, they're a bit slower but are not broken by any plausible definition.

Fix your buildbots instead. They're probably already setting a dozen variables, it's easy to add another one.",2017-12-29T18:37:19Z,5085186
2021,libretro/RetroArch,285132250,354484814,"No, I'm not going to fix every distro package that follows correct behavior of not setting `DEBUG` for release builds.

You should just make this script.
```
#!/bin/sh

./configure && make DEBUG=1
```",2017-12-29T18:45:34Z,4204285
2022,libretro/RetroArch,285132250,354485178,"Can you please untag my PR with that ridiculous bs? Changes will not be made because this is the correct behavior and you just broke it, seriously why can't you just accept that you are wrong?",2017-12-29T18:48:30Z,4204285
2023,libretro/RetroArch,285132250,354485606,"For windows we even ship debug builds on every nightly.
I guess adding DEBUG=0 is feasible but I don't think it should be the default behavior",2017-12-29T18:51:47Z,1721040
2024,libretro/RetroArch,285132250,354485615,"I will admit I'm wrong when (if) I'm convinced I'm wrong. Right now, I believe you're just as wrong as you believe I am.

Who says not setting debug is correct? Especially considering we defaulted to debug=yes a while ago, this is a regression (though probably an ancient one).

Either you change your build scripts once and ignore it, or I have to remember RetroArch being _**SPECIAL**_ every single time I want to do anything. I don't see any advantages whatsoever to your approach.",2017-12-29T18:51:52Z,5085186
2025,libretro/RetroArch,285132250,354485640,"For the record, this probably breaks 95% of build scripts including the buildbot. Pretty much any build script that didn't set `DEBUG=0` explicitly.",2017-12-29T18:52:00Z,4204285
2026,libretro/RetroArch,285132250,354485786,"Debug behavior should be opt in, not opt out. This the standard pretty much everywhere for a damn good reason. The fact this has to be spelled out for you is not only insulting, its incredibly inane.",2017-12-29T18:53:16Z,4204285
2027,libretro/RetroArch,285132250,354486066,"If there is a ""damn good reason"" to default to undebuggable programs, post said reason and I'll consider it. Repeating variants of ""because I said so"" or ""because this person I'm conveniently not naming said so"" is not a valid argument.",2017-12-29T18:55:27Z,5085186
2028,libretro/RetroArch,285132250,354486747,"I'm merging this, it's the right thing to do. If you have a problem with it, take it up with @twinaphex privately, NOT HERE.",2017-12-29T19:00:44Z,10137
2029,Python-Markdown/markdown,1227174892,1227174892,"This adds a way for the attr_list extension to lift attribute values set on list items to their parent list element (i.e. `<li>` attribute to `<ul>` or `<ol>`). This allows list-wide attributes, such as list start number or class to be set by setting them on list items with the appropriate syntax.

Contains additions to the docs describing it, as well extra test input (plus all existing unit tests still pass)

Should be able to solve #227, #668 and many of the other requests for a similar feature, including my own quest to get this feature.",2022-05-05T21:04:50Z,1408287
2030,Python-Markdown/markdown,1227174892,1119061217,"I'm not commenting on whether this feature should or should not be included, but how would you specify both a class on an `<li>` and on `<ol>` or  `<ul>` if you must do it on the same line?",2022-05-05T21:26:08Z,1055125
2031,Python-Markdown/markdown,1227174892,1119066814,"> I'm not commenting on whether this feature should or should not be included, but how would you specify both a class on an `<li>` and on `<ol>` or `<ul>` if you must do it on the same line?

As mentioned in the doc update this isn't possible with this patch. I initially started out with a `parent:` prefix (e.g. `parent:attr=value`) to be able to more precisely specify which attributes should be lifted to the parent, but decided against it as it needed quite more code changes. Plus I wonder how many times you would really want to be able to set attributes on both list items and the list itself. For the latter you only need to use an attribute on a single list item, while all the other list items can still have their own attributes.",2022-05-05T21:32:45Z,1408287
2032,Python-Markdown/markdown,1227174892,1119113841,An alternative could be to have a single ^ as a marker in the attribute list to indicate that all attributes following it should be set on the parent. That would allow both list and table attributes with minimal code changes.,2022-05-05T22:56:50Z,1408287
2033,Python-Markdown/markdown,1227174892,1119329307,I updated the pull request to use `^` as a marker to signal when attributes should be lifted. Solves the issue of not being to define both list element attributes and parent attributes in the same attribute definition,2022-05-06T07:20:29Z,1408287
2034,Python-Markdown/markdown,1227174892,1119577942,"As I have stated on every request we have received for this, we will not be implementing this feature. Of course, you are free to provide this functionality in a third party extension.",2022-05-06T12:44:39Z,78846
2035,Python-Markdown/markdown,1227174892,1119584619,"> As I have stated on every request we have received for this, we will not be implementing this feature. Of course, you are free to provide this functionality in a third party extension.

I have seen a number of similar comments about not wanting to implement it, but I'm really struggling to understand why? This feature is currently not available, yet is obviously valuable to many, given all the requests over the many *years* it has shown up as an issue here. The way I implement it here does not seem to interfere with any existing feature, as far as I can tell. And if the special `^` marker isn't used at all then there's no difference in behaviour, so it should be fully backwards compatible. It adds value, not only to me, yet it is coldly being rejected for unclear reasons other than ""we will not be implementing this feature"". 

Please help me understand, because this all does not make any sense...",2022-05-06T12:52:28Z,1408287
2036,Python-Markdown/markdown,1227174892,1119607463,"This has been discussed and explained many times over. You even linked to some of those discussions. In short, I am not interested in supporting a syntax that is does not make sense to me. It should be no surprise that it was rejected. Of course, you are free to disagree with me on this. If this is important to you, then you can provide and support your own third-party extension which provides the feature.",2022-05-06T13:16:07Z,78846
2037,Python-Markdown/markdown,1227174892,1119633891,"> In short, I am not interested in supporting a syntax that is does not make sense to me.

From me as an outside-viewer I can see this response as essentially ""I don't like this, so I don't want it"" response.
You put your own bias over something that the majority of people may like to have for once. I saw this problem in MkDocs and see it here again. You seem to have a general issue with accepting things that you don't like, but that the community could benefit from.",2022-05-06T13:38:45Z,11576465
2038,Python-Markdown/markdown,1227174892,1119641198,"I don't accept things that I'm not willing to support long term. Long after the person who proposes it is gone, I'm left defending it and fixing issues with it. That's (one of the reasons) why we provide a public extension API. Anyone can provide their own implementation which works how they want. Then they can support it without any additional burden on me. I am not limiting anyone at all except for myself.",2022-05-06T13:46:26Z,78846
2039,Python-Markdown/markdown,1011467346,1011467346,"
I'm trying to package your module as an rpm package. So I'm using the typical build, install and test cycle used on building packages from non-root account.
- ""setup.py build""
- ""setup.py install --root </install/prefix>""
- ""pytest with PYTHONPATH pointing to sitearch and sitelib inside </install/prefix>

On testting usimg pytest it fails with:
```console
+ /usr/bin/pytest -ra tests
=========================================================================== test session starts ============================================================================
platform linux -- Python 3.8.12, pytest-6.2.5, py-1.10.0, pluggy-0.13.1
benchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
Using --randomly-seed=1062042675
rootdir: /home/tkloczko/rpmbuild/BUILD/markdown-3.3.4
plugins: forked-1.3.0, shutil-1.7.0, virtualenv-1.7.0, expect-1.1.0, flake8-1.0.7, timeout-1.4.2, betamax-0.8.1, freezegun-0.4.2, aspectlib-1.5.2, toolbox-0.5, rerunfailures-9.1.1, requests-mock-1.9.3, cov-2.12.1, flaky-3.7.0, benchmark-3.4.1, xdist-2.3.0, pylama-7.7.1, datadir-1.3.1, regressions-2.2.0, xprocess-0.18.1, black-0.3.12, asyncio-0.15.1, subtests-0.5.0, isort-2.0.0, hypothesis-6.14.6, mock-3.6.1, profiling-1.7.0, randomly-3.8.0, nose2pytest-1.0.8, pyfakefs-4.5.1, tornado-0.8.1, twisted-1.13.3, aiohttp-0.3.0, localserver-0.5.0, anyio-3.3.1, trio-0.7.0, cases-3.6.4, yagot-0.5.0, Faker-8.14.0
collected 1043 items / 1 error / 1042 selected

================================================================================== ERRORS ==================================================================================
_____________________________________________________ ERROR collecting tests/test_syntax/extensions/test_md_in_html.py _____________________________________________________
/usr/lib/python3.8/site-packages/_pytest/runner.py:311: in from_call
    result: Optional[TResult] = func()
/usr/lib/python3.8/site-packages/_pytest/runner.py:341: in <lambda>
    call = CallInfo.from_call(lambda: list(collector.collect()), ""collect"")
/usr/lib/python3.8/site-packages/_pytest/python.py:766: in collect
    self.warn(
/usr/lib/python3.8/site-packages/_pytest/nodes.py:230: in warn
    warnings.warn_explicit(
E   pytest.PytestCollectionWarning: cannot collect test class 'TestSuite' because it has a __init__ constructor (from: tests/test_syntax/extensions/test_md_in_html.py)
========================================================================= short test summary info ==========================================================================
ERROR tests/test_syntax/extensions/test_md_in_html.py::TestSuite - pytest.PytestCollectionWarning: cannot collect test class 'TestSuite' because it has a __init__ constr...
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
============================================================================= 1 error in 0.99s =============================================================================
pytest-xprocess reminder::Be sure to terminate the started process by running 'pytest --xkill' if you have not explicitly done so in your fixture with 'xprocess.getinfo(<process_name>).terminate()'.
```
Runnimg pytest with `--import-mode=importli` it fails with:
```console
+ /usr/bin/pytest -ra --import-mode=importlib tests
=========================================================================== test session starts ============================================================================
platform linux -- Python 3.8.12, pytest-6.2.5, py-1.10.0, pluggy-0.13.1
benchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
Using --randomly-seed=3873927089
rootdir: /home/tkloczko/rpmbuild/BUILD/markdown-3.3.4
plugins: forked-1.3.0, shutil-1.7.0, virtualenv-1.7.0, expect-1.1.0, flake8-1.0.7, timeout-1.4.2, betamax-0.8.1, freezegun-0.4.2, aspectlib-1.5.2, toolbox-0.5, rerunfailures-9.1.1, requests-mock-1.9.3, cov-2.12.1, flaky-3.7.0, benchmark-3.4.1, xdist-2.3.0, pylama-7.7.1, datadir-1.3.1, regressions-2.2.0, xprocess-0.18.1, black-0.3.12, asyncio-0.15.1, subtests-0.5.0, isort-2.0.0, hypothesis-6.14.6, mock-3.6.1, profiling-1.7.0, randomly-3.8.0, nose2pytest-1.0.8, pyfakefs-4.5.1, tornado-0.8.1, twisted-1.13.3, aiohttp-0.3.0, localserver-0.5.0, anyio-3.3.1, trio-0.7.0, cases-3.6.4, yagot-0.5.0, Faker-8.14.0
collected 764 items / 1 error / 763 selected

================================================================================== ERRORS ==================================================================================
_____________________________________________________ ERROR collecting tests/test_syntax/extensions/test_md_in_html.py _____________________________________________________
ImportError while importing test module '/home/tkloczko/rpmbuild/BUILD/markdown-3.3.4/tests/test_syntax/extensions/test_md_in_html.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
tests/test_syntax/extensions/test_md_in_html.py:25: in <module>
    from ..blocks.test_html_blocks import TestHTMLBlocks
E   ImportError: attempted relative import with no known parent package
========================================================================= short test summary info ==========================================================================
ERROR tests/test_syntax/extensions/test_md_in_html.py
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
============================================================================= 1 error in 0.87s =============================================================================
pytest-xprocess reminder::Be sure to terminate the started process by running 'pytest --xkill' if you have not explicitly done so in your fixture with 'xprocess.getinfo(<process_name>).terminate()'.
```
tox is failing as well
```conosle
+ /usr/bin/tox --skip-missing-interpreters
.package create: /home/tkloczko/rpmbuild/BUILD/markdown-3.3.4/.tox/.package
ERROR: invocation failed (exit code 1), logfile: /home/tkloczko/rpmbuild/BUILD/markdown-3.3.4/.tox/.package/log/.package-0.log
================================================================================ log start =================================================================================
RuntimeError: failed to build image wheel because:
Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/virtualenv/seed/embed/via_app_data/via_app_data.py"", line 51, in _install
    key = Path(installer_class.__name__) / wheel.path.stem
AttributeError: 'NoneType' object has no attribute 'path'


================================================================================= log end ==================================================================================
ERROR: InvocationError for command /usr/bin/python3 -m virtualenv --no-download --python /usr/bin/python3 .package (exited with code 1)
ERROR: FAIL could not package project - v = InvocationError('/usr/bin/python3 -m virtualenv --no-download --python /usr/bin/python3 .package', 1)
```",2021-09-29T21:36:10Z,31284574
2040,Python-Markdown/markdown,1011467346,930563883,Additional Q: How to build module documentation as man page?,2021-09-29T21:37:40Z,31284574
2041,Python-Markdown/markdown,1011467346,930575863,You seem to be running the tests with pytest which is not supported. Please read development documentation: https://python-markdown.github.io/contributing/#development-environment.,2021-09-29T21:59:58Z,1055125
2042,Python-Markdown/markdown,1011467346,959193796,"I'm closing this as there has been no follow-up. Note that...

1. As previously stated, we do not support pytest so any reports about it not working will be ignored.
2. We are not able to reproduce the report that tox is not running correctly. Presumably, we would need more information about the environment to replicate the issue.

> Additional Q: How to build module documentation as man page?

I'm not sure if you are referring to the CLI or the `docs/` directory. For the CLI, we are using `optparse` from the Python Standard Library. There should be various existing third-party tools which will convert the output of `optparse`'s `--help` to man page(s). I would suggest exploring those options. We currently use MkDocs to render the content of the `docs/` directory to HTML. Last I knew, no-one has made a MkDocs plugin which outputs man pages and I could care less about doing so.
",2021-11-03T14:14:01Z,78846
2043,Python-Markdown/markdown,1011467346,999940954,"> You seem to be running the tests with pytest which is not supported

I want only to add that running test suite in fixed env has some value however it is some other value to run test suite in opened set of modules if pytest is uses because power of that framework comes from extension which can alter testing process without single touch tested tree.
I'm not asking to switch CI to pytest however I think that alter test framework to be able tun pytest in some minimal env may have some added value.
Looks like it is issue with only with tests/test_syntax/extensions/test_md_in_html.py.
As long as that is added to ignore list pytest Work™️.
```console
+ PYTHONPATH=/home/tkloczko/rpmbuild/BUILDROOT/python-markdown-3.3.6-2.fc35.x86_64/usr/lib64/python3.8/site-packages:/home/tkloczko/rpmbuild/BUILDROOT/python-markdown-3.3.6-2.fc35.x86_64/usr/lib/python3.8/site-packages
+ /usr/bin/pytest -ra --ignore tests/test_syntax/extensions/test_md_in_html.py
=========================================================================== test session starts ============================================================================
platform linux -- Python 3.8.12, pytest-6.2.5, py-1.11.0, pluggy-1.0.0
rootdir: /home/tkloczko/rpmbuild/BUILD/markdown-3.3.6
plugins: datadir-1.3.1, regressions-2.2.0, mock-3.6.1
collected 772 items

tests/test_apis.py ......................................................................................                                                            [ 11%]
tests/test_extensions.py .....................................                                                                                                       [ 15%]
tests/test_legacy.py ................................................................................sssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 34%]
sssss..s.......................                                                                                                                                      [ 38%]
tests/test_meta.py ..                                                                                                                                                [ 38%]
tests/test_syntax/blocks/test_blockquotes.py .                                                                                                                       [ 38%]
tests/test_syntax/blocks/test_code_blocks.py .....                                                                                                                   [ 39%]
tests/test_syntax/blocks/test_headers.py ss.........................................................................                                                 [ 49%]
tests/test_syntax/blocks/test_hr.py ...............................................                                                                                  [ 55%]
tests/test_syntax/blocks/test_html_blocks.py ..............................................................................................................          [ 69%]
tests/test_syntax/blocks/test_paragraphs.py .................                                                                                                        [ 71%]
tests/test_syntax/extensions/test_abbr.py .............                                                                                                              [ 73%]
tests/test_syntax/extensions/test_admonition.py ......                                                                                                               [ 74%]
tests/test_syntax/extensions/test_attr_list.py ..                                                                                                                    [ 74%]
tests/test_syntax/extensions/test_code_hilite.py sssssssssssssssssssssssssssssssssss                                                                                 [ 79%]
tests/test_syntax/extensions/test_def_list.py ......                                                                                                                 [ 79%]
tests/test_syntax/extensions/test_fenced_code.py ..................ssssssssssssss                                                                                    [ 83%]
tests/test_syntax/extensions/test_footnotes.py ..........                                                                                                            [ 85%]
tests/test_syntax/extensions/test_legacy_attrs.py .                                                                                                                  [ 85%]
tests/test_syntax/extensions/test_legacy_em.py ......                                                                                                                [ 86%]
tests/test_syntax/extensions/test_tables.py .                                                                                                                        [ 86%]
tests/test_syntax/extensions/test_toc.py .........................                                                                                                   [ 89%]
tests/test_syntax/inline/test_autolinks.py .....                                                                                                                     [ 90%]
tests/test_syntax/inline/test_emphasis.py .................                                                                                                          [ 92%]
tests/test_syntax/inline/test_entities.py ....                                                                                                                       [ 92%]
tests/test_syntax/inline/test_images.py ......................                                                                                                       [ 95%]
tests/test_syntax/inline/test_links.py ................................                                                                                              [ 99%]
tests/test_syntax/inline/test_raw_html.py .                                                                                                                          [100%]

========================================================================= short test summary info ==========================================================================
SKIPPED [19] markdown/test_tools.py:185: Excluded
SKIPPED [50] markdown/test_tools.py:144: Tidylib not available.
SKIPPED [1] tests/test_syntax/blocks/test_headers.py:112: This is broken in Python-Markdown
SKIPPED [1] tests/test_syntax/blocks/test_headers.py:132: This is broken in Python-Markdown
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:116: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:55: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:69: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:85: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:150: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:141: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:174: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:283: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:215: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:318: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:301: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:187: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:200: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:159: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:269: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:228: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:256: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:243: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:99: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:335: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:134: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:359: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:556: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:581: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:604: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:479: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:543: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:530: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:398: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:418: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:459: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:435: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:377: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:630: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_code_hilite.py:517: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:537: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:414: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:573: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:450: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:609: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:485: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:683: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:735: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:708: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:760: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:506: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:665: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:644: Pygments== is required
SKIPPED [1] tests/test_syntax/extensions/test_fenced_code.py:384: Pygments== is required
===================================================================== 652 passed, 120 skipped in 3.52s =====================================================================
```
In other words to make whole tree pytest ready it does not look that is that much to do ..
",2021-12-22T23:34:16Z,31284574
2044,Python-Markdown/markdown,1011467346,999944718,"I can understand the desire to use a specific testing framework, but that choice is chosen by the maintainer and must be respected.

If there was a specific advantage for us to move over to pytest, we could have that conversation, but I realize, as you mentioned, that is not the conversation we are currently having.

If we were to add py.test support just so others could use it, then we have to maintain it and ensure it doesn't break or we are going to get issues to ""fix pytest"" on occasions when it breaks inadvertently. Since this project is not using any features from pytest, it makes no sense for us to go out of our way, even if it is only slightly out of the way, to accommodate pytest.

I have nothing against pytest and use it in a number of my projects, but this project's maintainer has chosen not to use it. I can see no upside for us to fix pytest support as we have no intention of monitoring ensuring that it does not break.",2021-12-22T23:47:37Z,1055125
2045,Python-Markdown/markdown,1011467346,999965027,"> If we were to add py.test support just so others could use it, then we have to maintain it and ensure it doesn't break or we are going to get issues to ""fix pytest"" on occasions when it breaks inadvertently. Since this project is not using any features from pytest, it makes no sense for us to go out of our way, even if it is only slightly out of the way, to accommodate pytest.

Adding correctly working pytest support is VERY important.
That importance is not only about exact tested module. Just please add to testing picture fact that in exact test suite is adding yet another workload to all modules involved in testing. In other words it is about health of whole ecosystem (in this case python ecosystem).
Additionally .. I 'm finishing now some packages build automation to be able to run kind of set f scratch build if build systems are not bus. In case of python it address some random set of additional pytest extension and perform build with logging results .. only. In such cases if for example if lets say will be added `pytest-randomly` it would be possible to prove that random reshuffling units may cause that test suite fail it will mean that internally test suite have some dependencies between units and even if all test suite executed without `randomly` extension running exact unit by `pytest <some/file.py>::<some_unit>` will definitely fail. Sometimes test suites are running quite long time or on implementing new unit such way of running isolated units may be useful.
I want only to say that as long that `markdown` test suite will be not working I would not be able to do that or in this exact case I would be doing that only with part of the test suite.

Form that point of view I'm asking to to fix test suite or something like that.
As I have now `markdown` in set of my distro packages I'll be to perform such new way of testing your module.
In other words I want only to encourage to prepare some base line id you want to have such testing completely for free (that automation will be only flagging some found issue without single second spend by you :) )
If you don't care about that is 100%/perfectly fine :)
If you care .. just please have look closer on the code which you know because cost of that fix may be really low :)
I'm specialist on packaging area so form me fixing that seems is a bit to high :P (I've stumped on python modules only ~10 months ago so I'm still learning however after finishing +700 modules as rpm/Solaris IPS packages I see some patters which I can exploit giving back to the community results of my work).
Cheers :)",2021-12-23T00:48:53Z,31284574
2046,Python-Markdown/markdown,1011467346,1000000284,"> I want only to say that as long that markdown test suite will be not working I would not be able to do that or in this exact case I would be doing that only with part of the test suite.

I understand you have a vision and you have a desire to collect testing data, but I'm going to be very blunt about this, if your whole system depends on every python package you use to support your specific testing framework, you are building a very fragile system. Cornering other projects into using one specific testing framework is not a good idea, and it takes away the freedom of that project to maintain their project in a way that they find most suitable to them.

What if some other framework comes out that a project finds more enticing than pytest? Are they not allowed to take advantage of that testing framework as to not break your pytest dependent system?

I'm not seeing an advantage for us to switch to pytest, and I definitely can't see a reason to set up and test an additional framework that will test the exact same tests.

Again, I have nothing against pytest, and I do use it myself. There are features pytest has that make testing easier in some cases, but this project doesn't currently have a scenario that we **need** anything beyond what is already offered by Python's unittest module.",2021-12-23T02:38:12Z,1055125
2047,Python-Markdown/markdown,1011467346,1000007082,"> if your whole system depends on every python package you use to support your specific testing framework

Indeed .. issue is that I want to know before those areas of sensitivity before my client will come to me and will tell me ""here it is something wrong"" :)
In other words it is kind of proactive approach.
What will happen next is completely independent an probably would require some assessment about what to do with that.
Running few POCs I found some interesting results. What could be done next with those results is matter of running that framework on larger scale to be able identify some patterns and maybe design some other automation on top of that.

Going back to the ticket subject. Really please have look one more time on that case because it may be something very easy to fix for you  .. only this and nothing more :P
Please ..",2021-12-23T03:00:29Z,31284574
2048,Python-Markdown/markdown,1011467346,1001694480,We support and use `unittest` in the Python Standard Library. We will not be adding support for any other testing framework. The extra work would provide no benefit to the project itself as @facelessuser has explained multiple times. Please stop wasting our time with this.,2021-12-27T18:37:27Z,78846
2049,Python-Markdown/markdown,978528716,978528716,"Regarding the [earlier pull request](https://github.com/Python-Markdown/markdown/pull/1177) I propose a _custom parser interface_ for better integration with third party projects using e.g. _lxml_.

### Purpose:
Help developers of third party projects to easily change the internal parser (etree) if their custom extensions do have need for extended functionality of e.g. _lxml_.

### Usage:
```python
import markdown
import lxml.etree
markdown.etree.set_parser(lxml.etree)
html = markdown.markdown(""# Hello extended XPath World"")
```

### Criticism:
Compatibility issues of third party parsers do not have to be targeted by Python-Markdown development team, as projects like _lxml.etree_ claim to be compatible with _xml.etree.ElementTree_

### Licensing:
There should be no legal conflicts (BSD/MIT) in case of lxml. :-D

**I hope you'll accept my idea to make my life a bit easier beyond python-markdown update cycles.**

Cheers
Dom",2021-08-24T22:17:41Z,6597386
2050,Python-Markdown/markdown,978528716,905494623,"My initial kneejerk reaction when seeing `markdown.etree.set_parser(lxml.etree)` was was 'but we aren't using etree as a parser so `set_parser` is the wrong name for the function.' I think that that gets to my underlying objection to adding any support for lxml. We are a Markdown parser. 

What harm is there in allowing third party extensions to inject a separate parser? Well, then we officially support it with all its edge cases. Now ever time some user runs into some obscure edge case with lxml, we need to deal with it. No thanks. I'm not interesting in the extra burden that the long term support will require.

Of course, this doesn't prevent you from accomplishing your end result. Just return a string from your parser which you then insert as a raw string into the Markdown output.",2021-08-25T13:19:03Z,78846
2051,Python-Markdown/markdown,978528716,905660102,"I can't follow your argumentation because an interface for optional parsers doesn't oblige the maintainers to support any of those optional injections. It's the third party's obligation to be xml.etree.ElementTree compliant.

I don't see the point of forcing extension developers to rely on the built-in xml parser.

I won't inject lxml-trees into markdown results. it's cumbersome and excludes extended xpath operations on markdown-html result. let's fork.

",2021-08-25T16:04:04Z,6597386
2052,Python-Markdown/markdown,978528716,905728391,"All of the basic built-in Markdown elements are built as internal plugins. They rely on etree. If we allow custom parsers, how do we avoid the inevitable questions of ""I implemented my own parser now all of the basic Markdown things are broken?"". Or ""Everything isn't broken but with lxml this edge case is broken for Admontions, can we fix that so it isn't broken?"".

This is what we don't want to deal with. I don't really know what you are doing with lxml and at what stage in the process, but I imagine there are ways to do what you need to without lxml. I realize that lxml is a preferred option for you, but I can envision all sorts of issues if we went this route.

And this isn't really a completely abstracted approach as there may be non-etree like parsers out there that someone would want to use. This really would only work for etree-like parsers, particularly lxml. What if I wanted to use html5lib? I don't think this would work. You'd have to completely abstract the interface. But again, you'd still have quirks maybe.

This is the bare minimum of abstraction needed to get lxml (and probably only lxml) shoehorned in, but not enough to abstract this for all custom parsers and it may introduce all sorts of quirks for anyone interfacing with the built-in Markdown parsing when using their own custom parser.",2021-08-25T17:24:08Z,1055125
2053,Python-Markdown/markdown,978528716,907356606,"Again, it is not the Python-Markdown's responsibility to guarantee every third party's parser compability. I know that there are some differences between lxml and ElementTree, but they can be solved without touching the Python-Markdown's source code by overriding affected methods from my own project's side for example.

> I don't think this would work.

Did you ever try to implement an abstract interface for html5lib? Programming is not a belief system.

>  it may introduce all sorts of quirks for anyone interfacing with the built-in Markdown parsing

Again, python is no voodoo magic. I use lxml and Python-Markdown and it works great on a great variety of Markdown source. Why would I commit code improvements then? Anyway I'll solve the issue with local patches. I see no point in supporting a closed source ElementTree cult without reasonable argumentation.

> inevitable questions of ""I implemented my own parser now all of the basic Markdown things are broken?""

Seriously? Common I'll just fork it and add a Python-Markdown2 archlinux package...

My decision fell for Python-Markdown because of it's expandability with further tree processing. I quickly realized that my objectives need lxml functionality. There's no other reason to use it when markdown CLI tool is times faster. 
",2021-08-27T17:23:26Z,6597386
2054,Python-Markdown/markdown,978528716,907393050,"> Did you ever try to implement an abstract interface for html5lib? Programming is not a belief system.

I am involved in the [BeautifulSoup](https://launchpad.net/beautifulsoup) project as I provide the [CSS selector](https://github.com/facelessuser/soupsieve) library for that project. That project actually does create an abstract interface (for the purpose of extracting the content) around the built-in HTML lib, lxml, and html5lib, so I do know a little about this, and I don't appreciate the condescending tone. Let's try and keep this civil.

> Again, it is not the Python-Markdown's responsibility to guarantee every third party's parser compatibility.

I understand that, but it may introduce issues into existing pieces we do maintain. But it also may not. The point is that the maintainer has selected an HTML parser, and is not currently interested in opening the door for more ""potential"" maintenance burden. It may not offer any issues currently, but it could in the future, and once we start supporting this, we are committed to handling related workarounds etc. that may be required in the future.

> Again, python is no voodoo magic. I use lxml and Python-Markdown and it works great on a great variety of Markdown source. Why would I commit code improvements then? Anyway I'll solve the issue with local patches. I see no point in supporting a closed source ElementTree cult without reasonable argumentation.

You are free to do so as you feel is necessary as that is the benefit of open source, but just because something is open source does not mean they are obligated to accept every suggested feature or fix.

",2021-08-27T18:29:19Z,1055125
2055,fabricjs/fabric.js,1352488799,1352488799,"As you can see, the image shows a very fancy way to distort/transform text, there should be a way to do something like that.

(They are using Fabric 5.2.1)

![image](https://user-images.githubusercontent.com/6233059/186951321-c65775dc-3330-4292-9cbc-7b916f489623.png)
",2022-08-26T16:36:42Z,6233059
2056,fabricjs/fabric.js,1352488799,1228932938,what is the url ?,2022-08-26T20:45:16Z,5160844
2057,fabricjs/fabric.js,1352488799,1229093793,"> what is the url ?

https://app.kittl.com/",2022-08-27T01:38:52Z,6233059
2058,fabricjs/fabric.js,1352488799,1229127392,"this is not an issue and is quite useless
should be? ",2022-08-27T05:28:46Z,34343793
2059,fabricjs/fabric.js,1352488799,1229191522,"USELESS? Really?
This is one of the most desired features.
Text on path was just a kick off. Text warping should be the main goal.",2022-08-27T13:18:19Z,6233059
2060,fabricjs/fabric.js,1352488799,1229192711,"your description is useless
so little effort in writing a decent ticket

> Text warping should be the main goal.

Should be? 
This is open source.
Get to work and PR.
",2022-08-27T13:26:53Z,34343793
2061,fabricjs/fabric.js,1352488799,1229192886,"https://github.com/fabricjs/fabric.js/blob/master/CONTRIBUTING.md

I will back you if you invest true grit",2022-08-27T13:28:09Z,34343793
2062,fabricjs/fabric.js,1352488799,1229195469,"You have violated the code of conduct
Refrain of doing so in the future",2022-08-27T13:46:03Z,34343793
2063,fabricjs/fabric.js,1352488799,1229397031,"@EdnaldoNeimeg if you think a feature is needed or missing present it in the correct way. Don't assume we have time to interpret every new issue and understand if it is a feature request, a thank you post, a bug, or a start of a discussion.

Said so useless was probably refered to the issue and not the feature, since it has no indication of how the feature work, only a screenshot of a generic ui, and the use of should that seems to imply the feature is expected.

We are people not answering bots and we need a bit more of polite interaction to work best.
",2022-08-28T07:19:01Z,1194048
2064,rust-lang/rust,1458781974,1458781974,"I don't see anything utterly offensive in `PROBLEMATIC_CONSTS`, why not let programmers have fun and talk about boobs in their code?",2022-11-22T00:12:17Z,44733677
2065,rust-lang/rust,1458781974,1322888499,What you are asking for is already the case. Tidy only applies to this repo: https://github.com/rust-lang/rust/blob/28a53cdb4695b71cb9ee39959df88542056479cd/src/tools/tidy/src/style.rs#L1,2022-11-22T01:39:33Z,12105168
2066,rust-lang/rust,1458781974,1322892673,"This decision has already made, we don't revisit old decisions without new information.

I'm going to lock this since it has a tendency to bring out trolls.",2022-11-22T01:44:00Z,23638587
2067,rust-lang/rust,904754673,904754673,"I don't see an issue for this.

The behavior of binary_search_by changed in https://github.com/rust-lang/rust/pull/74024 such that the result of the function after the patch is not the same as before.

While not directly responsible for the downtime, Polkadot ran into this problem while trying to recover from a network failure this week: https://polkadot.network/a-polkadot-postmortem-24-05-2021/",2021-05-28T05:45:46Z,147214
2068,rust-lang/rust,904754673,850162891,"How is this a regression? The documentation for `binary_search_by` *clearly* states

>  If there are multiple matches, then any one of the matches could be returned. ",2021-05-28T06:07:01Z,63051
2069,rust-lang/rust,904754673,850235297,"It being valid for a binary search to return any element is a sufficiently well-known property of the underlying algorithm that [even Wikipedia mentions it](https://en.wikipedia.org/wiki/Binary_search_algorithm#Duplicate_elements).

And from there we can find that C's [bsearch](https://pubs.opengroup.org/onlinepubs/9699919799/functions/bsearch.html) does not specify which element should be returned.
[java.util.Array.binarySearch](https://docs.oracle.com/en/java/javase/16/docs/api/java.base/java/util/Arrays.html#binarySearch(java.lang.Object%5B%5D,java.lang.Object)) also does not specify.
C#'s [List<T>.BinarySearch](https://docs.microsoft.com/en-us/dotnet/api/system.collections.generic.list-1.binarysearch?view=net-5.0) also does not specify.

Indeed, the cases where binary search methods do specify which value they would return are cases where the underlying algorithm is not required to be a binary search or when it returns the qualifying range. As [already mentioned in the PR](https://github.com/rust-lang/rust/pull/74024#issuecomment-771891494), if people really want to introduce determinism they can capture it with usage of [`partition_point`](https://doc.rust-lang.org/std/primitive.slice.html#method.partition_point).",2021-05-28T08:08:28Z,46493976
2070,rust-lang/rust,904754673,850326366,"> The behavior of `binary_search_by` changed in #74024 such that the result of the function after the patch is not the same as before.

We make behavioral changes all the time, I don't understand why *anyone* is bringing ""determinism"" into all of this.

If you want *the exact same behavior*, you have to use *the exact same* standard library version.
We *never* guaranteed ""cross-version reproducibility"", except wherever documented as such (or in situations where only one behavior can be observed).

I don't want to unilaterally close this for now, but I am removing `C-bug`.",2021-05-28T10:38:51Z,77424
2071,rust-lang/rust,904754673,850360023,This specific behavior has been documented since 2018. Closing since this is not a bug.,2021-05-28T11:43:44Z,1786438
2072,rust-lang/rust,904754673,850552425,"Wow, the hostility in this thread is stunning.",2021-05-28T17:04:52Z,147214
2073,rust-lang/rust,904754673,850557097,"> Wow, the hostility in this thread is stunning.

Polkadot did lie by referring to this situation as a ""compiler bug"" in [their tweet](https://twitter.com/Polkadot/status/1396936062266716166) on the subject. To be honest, it might be a good idea to preemptively lock this thread, before it gets any more attention.",2021-05-28T17:13:33Z,26806
2074,rust-lang/rust,904754673,850741826,"My two cents:

I do indeed think that it's fair to change the details of which of many equal elements gets returned in a binary search, presuming that we have left this explicitly document as not specified. (That said, I think it'd potentially be useful to specify).

However, I agree with @brson that the general tenor of this thread felt hostile. We want to *encourage* people to file bugs when they see something that surprises them, even if they're not sure whether it's a bug or not.  A more encouraging response might have been something like, ""Thanks for filing the issue! However, even though the behavior changed, I don't think this is a bug. Binary search is documented as returning an arbitrary element when there are multiple equal elements, as is the case here. Therefore, I'm going to close as behaving as expected. Let me know if you think I've got this wrong.""",2021-05-29T01:06:40Z,155238
2075,rust-lang/rust,670691047,670691047,"
https://github.com/rust-lang/rust/issues/33674<< Still happening in 2020!!!",2020-08-01T09:15:23Z,3456691
2076,rust-lang/rust,670691047,667530128,"No reason to open a duplicate. You will need to provide more info if you want to see this fixed. 

I also suggest toning down your language a bit, most people working on Rust do this in their free time, and it isn't exactly motivating to have people complain like that.",2020-08-01T13:13:14Z,1786438
2077,rust-lang/rust,670691047,667668829,"""most people working on Rust do this in their free time""
And again that crap: ""I'm doing it for free so don't expect any quality from it.""
I don't care if you fix it or not. I tell you what. Leave it and don't fix it. It is not my soft... I don't care...",2020-08-02T12:35:41Z,3456691
2078,rust-lang/rust,639133998,639133998,"There is currently a strong debate about whether the standard branch should be given a name that has less negative implications than ""master"". The starting point was apparently this [thread on the Git Mailing List](https://lore.kernel.org/git/CAOAHyQwyXC1Z3v7BZAC+Bq6JBaM7FvBenA-1fcqeDV==apdWDg@mail.gmail.com/), which, as far as I know, brought the topic up for the first time. It is criticized that the term ""master"" might have a negative connotation for people who were directly or indirectly affected by slavery. 

I think that especially in the light of the fact that the Rust Community is known for its openness towards all ethics and minorities, serious consideration should be given to renaming the master branch correspondingly. The GitHub CEO himself has already expressed his support for the idea (see [this tweet](https://twitter.com/natfriedman/status/1271253144442253312?s=19)), and the official GitHub cli repository has also already been changed accordingly.
Currently, it seems that the following two are the most common alternatives to the term ""master"":
- trunk
- main

GitHub has chosen the term trunk for the cli repository. Whether the name of the master branch of this repository should be changed accordingly should be discussed. I'm open to any constructive opinions on this topic.",2020-06-15T20:34:00Z,10137
2079,rust-lang/rust,639133998,644378043,"I vote for `trunk`, `main` is very confusing as a german speaking hu**man**.",2020-06-15T20:42:12Z,66971165
2080,rust-lang/rust,639133998,644381762,Or just don't change anything because this is a dumb idea and noone has complained before because it's not related to racism in any way and the 2 accounts above are just alts.,2020-06-15T20:49:21Z,17956143
2081,rust-lang/rust,639133998,644382130,"What is an alt? Do you mean the german word for `old`? You're name doesn't seem very old, it seems like a minecraft name.",2020-06-15T20:50:10Z,66971165
2082,rust-lang/rust,639133998,644385243,Alt is alternate account or new account and yes i do play minecraft,2020-06-15T20:57:06Z,17956143
2083,rust-lang/rust,639133998,644385528,"oh good to hear, great game",2020-06-15T20:57:47Z,66971165
2084,rust-lang/rust,639133998,644396957,"Hi, This user has been deleted. Beyond that, major changes need to go through the RFC process or the compiler team's major change process. As such, I'm going to give this issue a close, as this isn't the right way to pursue this change. Thanks.",2020-06-15T21:21:19Z,27786
2085,rust-lang/rust,60472468,60472468,"Code:

```
$ cat typetest.rs 
struct Foo<Elem, List:AsSlice<Elem>> {
    elems: List,
}

fn main() {
}
```

Compile:

```
$ rustc typetest.rs 
typetest.rs:1:12: 1:16 error: parameter `Elem` is never used
typetest.rs:1:16: 1:16 help: consider removing `Elem` or using a marker such as `core::marker::PhantomData`
error: aborting due to previous error
```

I'm pretty new to rust, so forgive me if I've missed something. But I can't see a way to say ""my struct contains a type T2, which implements `SomeGenericTrait<T>`"" without my struct also being parameterized over T, as I'm doing here. But rust thinks that I'm not using T.

I tried adding:

```
dummy: Elem,
```

to my struct def, which makes it compile. So it doesn't look like this is just one error message obscuring another, rust does really think this is the only thing wrong with my code.

---

```
$ rustc --version --verbose
rustc 1.0.0-dev (2fc8b1e7c 2015-03-07) (built 2015-03-08)
binary: rustc
commit-hash: 2fc8b1e7c4ca741e59b144c331d69bf189759452
commit-date: 2015-03-07
build-date: 2015-03-08
host: x86_64-unknown-linux-gnu
release: 1.0.0-dev
```
",2015-03-10T09:36:32Z,14172
2086,rust-lang/rust,60472468,78093628,"This is actually intended behavior due to a [recently merged RFC](https://github.com/rust-lang/rfcs/blob/master/text/0738-variance.md). The idea here is that the compiler needs to know what constraints it can put on the type parameter `Elem`, and usage of the [`PhantomData`](http://doc.rust-lang.org/std/marker/struct.PhantomData.html) type will instruct the compiler how it can do so.

You can check out the [examples](http://doc.rust-lang.org/std/marker/struct.PhantomData.html#examples) for `PhantomData` to see some usage, and in this case you'll probably want to do something along the lines of:
1. Refactor the struct so unused type parameters aren't needed. This can often be done with associated types, something along the lines of:
   
   ``` rust
   trait A { type B; }
   struct C<T: A> {
       inner: A::B,
   }
   ```
2. Use `PhantomData<Elem>` to indicate that the compiler should just consider the type parameter used, considering `Foo<A, B>` as containing something of type `A` as well as `B`.

You probably don't want to store `dummy: Elem` as it will require you to store an extra element and could have other memory implications as well.
",2015-03-10T16:38:32Z,64996
2087,rust-lang/rust,60472468,78457964,"Thanks for the helpful explanation, @alexcrichton. You also conveniently educated me on associated types, which I was wondering about but didn't know what to search for (I was wondering ""why is `Iterator` not generic""). It turns out this is just what I needed.
",2015-03-12T10:47:24Z,14172
2088,rust-lang/rust,60472468,325308253,"I just stumbled upon this limitation in a context where `PhantomData` is of no help: generic enums. Consider the following piece of code:

```
enum Foo<A, B>
    where A: Bar<B> + Sized
{
    X(A),
    Y,
    Z
}

trait Bar<B> {
   fn bar(&self, &B);
}
```

Edit: I'm aware of several workarounds. These include adding `PhantomData` to `X`, but this pollutes the enum (in the sense that construction of a value by the user becomes iffy).",2017-08-28T09:37:03Z,6747556
2089,rust-lang/rust,60472468,342592975,"Hi, I had a similar problem regarding to unused type parameter.
Here is the simplified code, I need to control the visibility for quicksort's sort function. 
```rust
struct Quicksort<T>;  // <---- unused parameter warning
trait Sorter<T> {
  fn sort(&self,ar<T>){...}
  fn join(...);
  fn split(...);
}
impl<T> Quicksort<T> {
  pub fn sort(&self,ar<T>){
    Sorter::sort(self, ar<T>) ;
  }
}
impl<T> Sorter<T> for Quicksort<T>{
 fn join(...) {...}
 fn split(...) {...}
}
```

Any help to resolve the problem? because struct Quicksort does not have any fields. The type parameter is solely for the sort function from trait.
Otherwise if I removed `impl<T> Quicksort<T>`, then I don't need the T for Quicksort at all. 


",2017-11-07T19:23:09Z,12042284
2090,rust-lang/rust,60472468,399987553,"For those who will look here for answers:
""In most cases you do not need additional type parameters at all"".
That's it. Just remove them.


Taking @Piping example for illustration.
Let's get rid of type parameter for `Quicksort`.
```rust
struct Quicksort;
```

Now `rustc` obviously won't complain about unused parameters since there are none.
But at some point we actually need that type-parameter.

Continuing. Type parameter can be introduced at function level.
```rust
impl Quicksort {
  pub fn sort<T>(&self,ar<T>){
    Sorter::sort(self, ar<T>) ;
  }
}
```

Now when it comes to implementing a trait things goes smoothly again since type parameter is already exists in `Sorter` trait.
```rust
impl<T> Sorter<T> for Quicksort {
 fn join(...) {...}
 fn split(...) {...}
}
```



There are few places where you actually need artificial type parameter.
1. When type conceptually contains some generic type.
2. Type must implement a trait with associated type.
```rust
trait Trait {
  type X;
}
```

Now either the type in question will be able to implement the `Trait` with some predefined `X`.
```rust
struct Foo;

impl Trait for Foo {
  type X = u32;
}
```

Or it has to have this artificial type parameter after all.
```rust

struct Foo<T>(PhantomData<T>);

impl<T> Trait for Foo<T> {
  type X = T;
}
```",2018-06-25T15:13:57Z,4456841
2091,rust-lang/rust,60472468,609420134,"@neithernut 

In case you're still interested, I had a similar problem and I solved it by creating an additional variant that stores the `PhantomData` and a never type `Empty`:

```rust
pub enum Empty {}
pub enum Foo<A, B> {
    X(A),
    // ...
    Void(Empty, PhantomData<B>),
}
```

[playground example](https://play.rust-lang.org/?version=stable&mode=debug&edition=2018&gist=c297fb0f024a70630ce2ffa1e6810b3e)

The good news is that it's zero-cost and does not pollute the variants you do use.

The bad news is that, until [never type exhaustive matching](http://smallcultfollowing.com/babysteps/blog/2018/08/13/never-patterns-exhaustive-matching-and-uninhabited-types-oh-my/) comes to Rust, this approach will pollute the code when you match on your enum (although you could macro your way around it):

```rust
match foo {
    Foo::A(_) => (),
    // ...
    Foo::Void(empty, _) => match empty {},
    //               ^
    // phantom data
}
```",2020-04-05T13:52:39Z,7377723
2092,rust-lang/rust,60472468,723657165,"@alexcrichton 
I also had some issues with this and generic enums. It would be very useful if the compiler could recognize that the type parameter is only used in a trait bound for another type parameter. I don't understand the reason why this would not work now.. Can't the compiler derive the constraints from the trait the type parameter is passed to?",2020-11-08T19:52:37Z,20745737
2093,rust-lang/rust,60472468,731195418,"@mankinskin I agree. PhantomData in this instance just pollutes the code. It is an implementation detail that should never be exposed in a public API (which is what happens with both @AdrienChampion 's and @neithernut 's code. It's also exceedingly irritiating, adding code complexity, complicating construction (so making it harder to just pass generic enum members as closures w/o writing extra code, etc. It's a code smell in Rust itself.",2020-11-20T14:16:04Z,1307097
2094,rust-lang/rust,60472468,732503909,"Looking back to @alexcrichton's response and the RFC, I think the question is not so much why unused type parameters cause an error, but why the compiler considers type parameters unused in examples like the OP's.

To me, it seems like `Elem` has one usage (in `List: AsSlice<Elem>`), albeit not in the body of the struct. It seems like that should constitute a usage for the purpose of variance inference, since it implies a variance bound: `Elem` must be invariant since `AsSlice`'s parameter is invariant.

In Scala for example, the equivalent would be `class Foo[E, S <: Seq[E]](val seq: S)`. Scala doesn't infer variance, but it ""knows"" that `E` must be covariant or invariant. If we try to make it contravariant (with `-E`), we get `error: contravariant type E occurs in covariant position`.

If getting the compiler to recognize these variance bounds would be complicated, perhaps lifetime and non-lifetime parameters could be handled differently? As the RFC notes, only lifetimes have subtype relationships, so it seems like non-lifetime parameters might as well be ""inherently"" invariant, and not subject to inference. Code like `struct Thing<'a, 'b: 'a>(&'b u32);` would continue to be rejected, but I don't think there's any ""legitimate"" reason to write lifetime parameters like that.",2020-11-24T00:25:10Z,828366
2095,rust-lang/rust,60472468,782851250,"I think perhaps we should reopen this one. The associated RFC doesn't seem to justify a compiler error in code like:

```rust
pub struct Foo<A> {
    a: A,
}

pub struct Bar<A, F>
where
    F: AsRef<Foo<A>>,
{
    foo: F,
}
```",2021-02-21T12:37:39Z,487050
2096,rust-lang/rust,60472468,784448160,"Changes of this magnitude would require an RFC these days, and I would encourage folks to either open an RFC or a thread on users or internals rather than continuing to comment on an issue that was closed almost six years ago.",2021-02-23T19:19:24Z,27786
2097,rust-lang/rust,60472468,784475770,"But can't this issue be reopened? This is kind of the first place you end up at when googling about this, and the discussion is basically already here. I don't think there is a lot of designing left to do, it just needs to be implemented in the right way.

I can try to write up an RFC some time, but it is not high on my priority list right now and I am completely unexperienced with rustc. Opening up this issue for now would at least raise attention to it and helps resolving this faster. ",2021-02-23T20:05:23Z,20745737
2098,rust-lang/rust,60472468,784478530,"Large scale changes to the language, of which this is one, require an RFC. I am not on the lang team, but this isn't actually a bug: this is a request for a change to how the language works. A bug being open for it is not the procedure by which this would be changed.",2021-02-23T20:10:19Z,27786
2099,rust-lang/rust,60472468,785757920,"@mankinskin I agree completely, and I feel your frustration; I share it, and it's got worse as rust has become more widely used. Rust's core team also have a very narrow view of what a bug is - one that with my software craftsmanship hat on I couldn't disagree with more strongly.

Rust's development has been captured by the bureaucratically minded; the same mindset that has infested wikipedia, and parodied beautifully in [Brazil](https://www.youtube.com/watch?v=KZ-SdU53MnY).

Personally, I find it rude; it shows disinterest in the views of those outside of the main development, and the idea that anyone can have a good idea.",2021-02-25T09:37:01Z,1307097
2100,rust-lang/rust,60472468,785774295,"@raphaelcohn I know what you mean and we should always be wary of overdoing that, it might stagnate innovation, but I also think there is value in organization and strict review. Rust's promise is to be a secure language, that means it must takes steps carefully, which makes it more difficult to innovate. I agree that there eventually needs to be an RFC for this but I don't really see the point of restricting discussions about feature changes in these github issues. I suppose they use this as a bug tracker, and new features are not supposed to be tracked here.

In general, I think there are a lot of improvements that could be made to the entire open source workflow nowadays. The tools don't really integrate well, basically just using hyperlinks. For open source work there ought to be issue dependencies, resource budgets, integrated documentations and tests, voting on forks, funding, user permissions, all of that. That would make open source a lot more ""open"" and transparent. It has to come eventually, but there still needs some work to be done.",2021-02-25T10:02:35Z,20745737
2101,rust-lang/rust,60472468,785971235,"Moderation note: what @steveklabnik said is correct. The kind of unconstructive broad criticism that has taken place in the past few comments is not appropriate here. Since this discussion doesn't seem to be headed anywhere good, and the next steps if one were to make progress have been clarified, I'm going to lock this thread.",2021-02-25T15:07:28Z,456674
2102,sinonjs/sinon,663559628,663559628,"Saw there are places where the term master is used. Consider replacing these.

Didn't do an exhaustive search of other terms that could be offensive, but see https://twitter.com/TwitterEng/status/1278733305190342656

also see:
https://stackoverflow.com/questions/8762601/how-do-i-rename-my-git-master-branch-to-release",2020-07-22T08:01:49Z,5059100
2103,sinonjs/sinon,663559628,662360185,"Thank you for raising this issue.

In our opinion, in the context of software repositories, ""master"" takes on the meaning of [""an original version of something from which copies can be made""](https://dictionary.cambridge.org/dictionary/english/master).

We are choosing to follow git conventions. When those conventions change, we will make appropriate changes to the Sinon repositories.",2020-07-22T09:55:11Z,20321
2104,sinonjs/sinon,657523246,657523246,"Saw there are places where the terms blacklist/white list are used.  Consider replacing these.  

Didn't do an exhaustive search of other terms that could be offensive, but see https://twitter.com/TwitterEng/status/1278733305190342656 


",2020-07-15T17:22:42Z,1899907
2105,sinonjs/sinon,657523246,659210799,"I agree, words matter.

How about contributing a pull request to make the changes?",2020-07-16T07:20:25Z,20321
2106,sinonjs/sinon,657523246,660974031,"There is not one place where the term blacklist was used in this project code! Only once in a comment (how to change the comment???). Also a quick search for the term ""sanity"", ""dummy"" and so on did not result in any code line affected. 

Yeah sure... rename your ""master"" branch to ""primary"" or so... 

The lazyness from OP stinks as it is easier to . Close this thread, as this is not stopping supposed racism but some person feeling better by yakshaving instead of actually solving issues.",2020-07-20T11:36:25Z,5059100
2107,sinonjs/sinon,657523246,661074648,"https://github.com/sinonjs/nise/pull/167/commits/bbfc55a9a5e6061cb8234cd65b7983a7713813c1  

The yak is mostly shaved now.  But your vitriol is misplaced; much like my original suggestion may have been ignorantly misplaced not knowing how the code was divided in sub-projects. There may be other subprojects that could be improved.",2020-07-20T14:28:11Z,1899907
2108,sinonjs/sinon,657523246,661328348,"Nice, so you can close this Issue as there was no issue.",2020-07-20T20:55:30Z,5059100
2109,sinonjs/sinon,657523246,661767318,@Uzlopak Your comments are not constructive and the tone is not very respectful. Please read our [Contributor Covenant Code of Conduct](https://github.com/sinonjs/sinon/blob/master/CODE_OF_CONDUCT.md).,2020-07-21T10:12:35Z,332271
2110,sinonjs/sinon,657523246,661813033,"@mantoni 
Consider my post as freedom of speech under Article 10 of the European Convention on Human Rights, which overrides your CoC (Stichwort: mittelbare Drittwirkung von Grundrechten). 

If you disagree:
When will you rename the master-branch to primary-branch?",2020-07-21T12:00:27Z,5059100
2111,sinonjs/sinon,657523246,661813844,"This is how you can rename master to anything you want, which ""improves language to respect less privileged populations""

https://stackoverflow.com/questions/8762601/how-do-i-rename-my-git-master-branch-to-release",2020-07-21T12:02:18Z,5059100
2112,sinonjs/sinon,657523246,661814265,@rgeerts Maybe renaming the master branch is something you should also tell everyone in their projects?,2020-07-21T12:03:18Z,5059100
2113,sinonjs/sinon,657523246,661926005,@rgeerts thank you for your contribution 💯 ,2020-07-21T15:20:33Z,20321
2114,sinonjs/sinon,657523246,662306251,I also thank rgeerts for his extensive work on this project.,2020-07-22T08:00:33Z,5059100
2115,mysqljs/mysql,305129827,305129827,"This patch accommodates some breaking changes introduced with MySQL 8.

Closes #1959 

In a nutshell, the `caching_sha2_password` plugin (which is used by default since MySQL 8.0.4) hashes the password using SHA-256 and, after a first successful authentication attempt, saves it in a cache. That first attempt needs to be done under one of two conditions. The client either uses SSL and sends the password as clear text or it encrypts the password using the RSA public key generated by the server. On any subsequent attempt, until the password is somehow removed from the cache or the server shuts down, these rules no longer apply.

The handshake process remains unchanged when connecting to any server with version lower or equal to `8.0.3`. Whereas for `8.0.4` or above, the process is now the following:

- the client sends a `ClientAuthenticationPacket` with a scramble computed using a SHA-256 hash
- if the password is not cached, the server sends back a `PerformFullAuthenticationPacket`
- if the client uses SSL, the password is sent to the server (as clear text) via a `ClearTextPasswordPacket` to which the server replies with a `OkPacket`
- otherwise it uses the server authentication public key compute the scramble, sending a `AuthSwitchResponsePacket` to which the server replies with a `OkPacket`
- if the client does not know the server public key (is not provided by the user), it requests it from the server, which sends it back using a `AuthMoreDataPacket`
- after a first successful authentication attempt, and until the password is cached, the server will reply to the initial `ClientAuthenticationPacket` with a `FastAuthSuccessPacket` (which basically just signals that an `OkPacket` will follow)

If the account is created using the `mysql_native_password` authentication plugin, the client will just fall back to the ""traditional"" process during the handshake, keeping compatibility, by default for any previously supported server version.

MySQL 8.0.2 disables the `local_infile` server variable by default, which breaks a couple of integration tests. The tests were updated to enable the feature by themselves (something that does not have any effect on older server versions and allows the tests to pass with newer versions).

Additionally, one of the integration tests was updated to avoid failing after the first run (using any server version) since it tried to create a table that already existed from the previous runs.",2018-03-14T12:00:25Z,413255
2116,mysqljs/mysql,305129827,373085142,"This is awesome, thank you soo much! The only real issues I'm seeing with the PR to continue to work on is the following:

- [ ] Get support working on Node.js 0.10 and below, or we can decide if this PR should be shelved for the next major release as breaking change to drop those Node.js versions. Being a widely-used base driver we try to support as many Node.js and MySQL versions as possible at once to make it easy to use for everyone.
- [ ] There may not be enough coverage of the new code added, as the coverage with the PR is decreasing. There are probably some areas that can use tests.
- [ ] MySQL 8 image needs to be stood up on the CI server to test against. This is done in the `.travis.yml` file similar to our current MySQL / MariaDB matrix.",2018-03-14T16:26:02Z,67512
2117,mysqljs/mysql,305129827,373139995,"To address your points.

Regarding the first one, I've missed those tests (sorry), but as far as I can tell there is no sane way to use RSA public key encryption on `node <=v0.12.0`. There are some packages on npm that try to add support through native addons, but I think that might not be a good idea here, since it's pure JavaScript. So the only easy way to work around that is to require users on those older `node` versions to use SSL when connecting to new MySQL versions. If you are OK with that, we can go that route, otherwise, going in the next major version is probably the best possibility (I guess that's mostly your call).

Missed the coverage as well, I'll try to fix that ASAP.

Again, missed the CI config. Should be fine, at least on Travis. However, besides that, I suggest we also create a test account to run the tests (besides `root`) to make sure the handshake works fine for both empty and non-empty passwords, which lead to different behaviors. Any issue with that? Other than that, I guess on windows we are out of luck anyway since appveyor only seems to support MySQL 5.7, correct?",2018-03-14T19:06:39Z,413255
2118,mysqljs/mysql,305129827,373241016,"> Regarding the first one, I've missed those tests (sorry), but as far as I can tell there is no sane way to use RSA public key encryption on node <=v0.12.0. There are some packages on npm that try to add support through native addons, but I think that might not be a good idea here, since it's pure JavaScript. So the only easy way to work around that is to require users on those older node versions to use SSL when connecting to new MySQL versions. If you are OK with that, we can go that route, otherwise, going in the next major version is probably the best possibility (I guess that's mostly your call).

Ah, very sad :( Did you do a through search on this (it sounds like it)? Because I can always help if you want to focus on the other parts, but just asking before I duplicate the search for no reason. If we really have to drop supported versions, we can, and it wouldn't particularly delay this, just wanted to make sure it was actually required to do so before saying we should :)

> Again, missed the CI config. Should be fine, at least on Travis. However, besides that, I suggest we also create a test account to run the tests (besides root) to make sure the handshake works fine for both empty and non-empty passwords, which lead to different behaviors. Any issue with that? Other than that, I guess on windows we are out of luck anyway since appveyor only seems to support MySQL 5.7, correct?

Yea, so in Travis CI it just uses a Docker image. You can make multiple non-root accounts too; the current setup was just what was needed for testing, so if MySQL 8 requires more elaborate setup, that sounds fine to me. I would ignore AppVeyor.",2018-03-15T02:36:28Z,67512
2119,mysqljs/mysql,305129827,373422784,"I'm using `crypto.publicEncrypt()` which was added in `v0.11.14`, and it uses native bindings which I assume were also added around that time. There are some libraries on npm (e.g. [ursa](https://github.com/JoshKaufman/ursa) and [dcrypt](https://github.com/dekz/dcrypt)) that claim support for earlier node versions, however they seem to implement their own native bindings, which I guess would, sort of, defeat the purpose of this module (or maybe not, you tell me).

There's this [node-rsa](https://github.com/rzcoder/node-rsa) thingy which apparently is pure JS, but I couldn't make it work and it's pretty slow. I've also seen other [approaches](https://github.com/ragnar-johannsson/rsautl) like delegating work to openssl via a child process, but that's probably not a good idea either, since it introduces a huge moving part. There's also this [forge](https://github.com/digitalbazaar/forge) kit which brings the entire browser kitchen sink with it and I also haven't managed to figure out how it worked.

In any case, most of these things, when they work, it's usually on node `>=v0.10`, and I guess this module is supposed to support at least `v0.6`. So, no good news from my side. However, if you can do your own research, that would, of course, be great. I'm not a cryptography expert, but from what I can tell, the reason I have not found anything good is because this RSA thingy in pure JavaScript is still not really feasible (`number` precision, primes and what not).

My suggestion here is to fail with a client error (the `HANDSHAKE_SECURE_TRANSPORT_REQUIRED` introduced by this PR) when `process.env.version` tells us that we are below `v0.11.14`, which basically would be the default behavior using `{ ssl: false, secureAuth: false }`. It's a pretty naive solution but at least it's not really a breaking change, because existing implementations will keep working as expected with MySQL 5.7 and, let's be honest, people wo don't upgrade from those ancient node versions will, almost for sure, not upgrade MySQL to series 8 as well.

However, if you still feel you are better off bumping the major version, that's fine.
",2018-03-15T15:44:57Z,413255
2120,mysqljs/mysql,305129827,378665937,@dougwilson what's your preference about the issue I mentioned? Do you want me to add the workaround for older node.js versions or do you think it's better to have a major version bump (in that case I guess all we need is to remove the CI setup for older versions)?,2018-04-04T16:40:00Z,413255
2121,mysqljs/mysql,305129827,383069686,"Sorry I didn't reply. Your comment on a new issue reminded me of this. Yea that works: if the server requires an auth that the client can't do we can error out. We should feature detect that, though, not sniff the version string.",2018-04-20T11:38:25Z,67512
2122,mysqljs/mysql,305129827,383171533,"Ok. Just to be sure, are you talking about sniffing the Node.js version string, MySQL version string, or both? Because we have two different issues right now. 

Supporting MySQL 8.0.4 (previous DMR release) which expects the password to be encrypted with the server public key using `RSA_PKCS1_PADDING`, whereas since 8.0.11 (the new GA release), the encryption should use `RSA_PKCS1_OAEP_PADDING`. We can always retry the handshake, but the only way to avoid that is by ""sniffing"" the MySQL server version.

Regarding the Node.js encryption APIs, I guess we can always resort to duck typing to check if they are available (or not) instead of sniffing the version. So, no problem here.

Let me know if there is something else that bothers you about these or other issues. I'll try to push this forward ASAP. Maybe during the weekend.",2018-04-20T17:45:20Z,413255
2123,mysqljs/mysql,305129827,383174051,"Why do you have to sniff the server version? Shouldn't the server say which one it expects directly? I'm just asking because I think version numbers break down when we start talking about compatible things like MariaDB, right? They may share some version numbers.

I have done Node.js version sniffing before and when it broke the Node.js collaborators just said I should never have been version sniffing their versions and told me feature detect only. So if that is the official direction from there I would go with that.",2018-04-20T17:54:37Z,67512
2124,mysqljs/mysql,305129827,383208410,"The case where we might need to sniff the server version is to distinguish MySQL 8.0.10 (pre GA, first one introduced chaching_sha1_passwors) from MySQL 8.0.11 (GA). since 8.0.11 was released yesterday one also could not support 8.0.10, as nobody should be using that RC anyways.

jojannes

On April 20, 2018 1:38:35 PM GMT+02:00, Douglas Wilson <notifications@github.com> wrote:
>Sorry I didn't reply. Your comment on a new issue reminded me of this.
>Yea that works: if the server requires an auth that the client can't do
>we can error out. We should feature detect that, though, not sniff the
>version string.
",2018-04-20T20:09:24Z,44364
2125,mysqljs/mysql,305129827,383210478,"Right. However, in this case we would be sort of ""whitelisting"" MySQL 8.0.4 specifically. I don't think MariaDB has this new authentication plugin (enabled by default or not), but I'm not entirely sure.

In any case, I believe the padding mode was probably a mistake, which was ""fixed"" for GA. So we can just default to `RSA_PKCS1_OAEP_PADDING` (the new 8.0.11 GA version) and optionally do a handshake retry for the specific case where `RSA_PKCS1_PADDING` is needed (MySQL 8.0.4 RC only). Or we can just simply ignore the RC version entirely, like @johannes suggested, by always using `RSA_PKCS1_OAEP_PADDING`.

This is currently my only open question. Other than that, not doing Node.js version sniffing makes perfect sense, so that is settled.",2018-04-20T20:17:07Z,413255
2126,mysqljs/mysql,305129827,383212072,"By the way, just to be clear, there was a straight jump from 8.0.4 (last RC) to 8.0.11 (GA).

https://mysqlrelease.com/2018/03/mysql-8-0-it-goes-to-11/",2018-04-20T20:23:25Z,413255
2127,mysqljs/mysql,305129827,384088602,I like the idea to either (a) just ignore the pre GA padding or (b) retry with the old padding because that would not involve the version sniffing :+1:,2018-04-24T21:38:53Z,67512
2128,mysqljs/mysql,305129827,387411600,"@dougwilson I've kept the option for providing a custom key padding, to allow users to exceptionally connect to a MySQL `8.0.4 RC` server. In any case, the default value is always `RSA_PKCS1_OAEP_PADDING` which means it will work, by omission, with the latest `8.0.11 GA` version.

I couldn't find a sane way to add an additional Travis CI test matrix for users with non-empty passwords though. I was constantly bumping into an authentication error (I believe it might be related to the `mysql` docker setup and environment variables mixup), but maybe you can give it a try.

Let me know if there is something else you would like to see in this PR. Otherwise, feel free to merge it! :wink: ",2018-05-08T13:58:15Z,413255
2129,mysqljs/mysql,305129827,388878819,"This is awesome, thank you. I've been reviewing it, and sorry I didn't reply that I was in progress. I did find at least one regression just in existing behavior so far, and pushed a test to cover this to `master` so we have it covered for the future as well. I noticed the change for the `ChangeUser` sequence no longer passes any of the options down to the `Sequence` constructor, which differs not from every other sequence. Not sure what the purpose of that change was, but it at least prevents the `timeout` option from getting passed down.

I'll look at what the fix is and push it up to your PR branch here 👍 ",2018-05-14T16:27:25Z,67512
2130,mysqljs/mysql,305129827,390856212,"The merge process last so long, and the pr is still not merged. So it there any alternative way to solve the problem of ""ER_NOT_SUPPORTED_AUTH_MODE: Client does not support authentication 
authentication ""? I am nodejs startup and just want to connect to mysql 8.0 for demo.",2018-05-22T04:04:53Z,2305537
2131,mysqljs/mysql,305129827,390900841,"@fan123199 everything should still work if you use the `mysql_native_password` plugin.

```sql
ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'YourRootPassword';
-- or
CREATE USER 'foo'@'%' IDENTIFIED WITH mysql_native_password BY 'bar';
```
",2018-05-22T08:07:30Z,413255
2132,mysqljs/mysql,305129827,391585418,"In addition to what @ruiquelhas said, I also had to:
```
mysql> FLUSH PRIVILEGES;
Query OK, 0 rows affected (0.00 sec)
```
And then restart the server.
If you are using multiple MySQL users, ensure that you update all of them that are connecting from remote (ie, not localhost). In my case I also had to use `'%'` instead of `'localhost'`.",2018-05-24T04:37:07Z,6442613
2133,mysqljs/mysql,305129827,400600651,Any updates on this? Unable to authenticate with MySQL 8,2018-06-27T09:09:45Z,17955210
2134,mysqljs/mysql,305129827,405859181,"@ruiquelhas Sorry junior dev here but have some questions.

Im getting the:
`Error: ER_NOT_SUPPORTED_AUTH_MODE: Client does not support authentication protocol requested by server; consider upgrading MySQL client
    at Handshake.Sequence._packetToError`

i tried the following solution you gave to somene else:

`ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'YourRootPassword';`
-- or
`CREATE USER 'foo'@'%' IDENTIFIED WITH mysql_native_password BY 'bar';`

And that fixes the issue for me but I have some questions.

Questions:
Is this a good fix that can be deploy to production or is this just a workaround for getting things working locally?

I am also assuming that what is happened is that **8.0.11 MySQL Community Server** is just not compatible with the npm mysql library right out the box at the moment. To make it compatible right out the box You have submitted this bug fix but it hasn't been merged to master yet, and that is the reason it still doesn't work for me right out the box even tho i have the latest version of the mysql dependency.

Is my assumption correct?

If I am correct about my assumption, is there a practical way to implement the bug fix to my codebase before it is merged into master? and if there is, is that something that would be advised, or good practice?

thank you so much.

",2018-07-18T08:51:24Z,13215891
2135,mysqljs/mysql,305129827,405867267,"@Osuriel

> Is this a good fix that can be deploy to production or is this just a workaround for getting things working locally?

The new default authentication plugin is more secure because it's using SHA256 instead of SHA1 to hash the password, but it's not available for MySQL versions below 8.0, which means most existing MySQL production instances are not using it as well.

> I am also assuming that what is happened is that 8.0.11 MySQL Community Server is just not compatible with the npm mysql library right out the box at the moment. To make it compatible right out the box You have submitted this bug fix but it hasn't been merged to master yet, and that is the reason it still doesn't work for me right out the box even tho i have the latest version of the mysql dependency.
>
> Is my assumption correct?

Correct.

> If I am correct about my assumption, is there a practical way to implement the bug fix to my codebase before it is merged into master? and if there is, is that something that would be advised, or good practice?

I don't know if it is a good practice or not, it mostly depends on your internal policies and processes (and how much you would be ""trusting"" the fork in this case), but you can install and link npm packages from a git repo even using a specific branch or commit.

```bash
$ npm install ruiquelhas/mysql#feature/caching-sha2-password
```",2018-07-18T09:19:10Z,413255
2136,mysqljs/mysql,305129827,405867698,Any update? Still unable to connecto to MySQL 8,2018-07-18T09:20:36Z,10175883
2137,mysqljs/mysql,305129827,405922316,@ruiquelhas thank you so much! You just helped me become a better developer hannibwas yesterday! 💪🏽🔥,2018-07-18T13:02:08Z,13215891
2138,mysqljs/mysql,305129827,424126892,@dougwilson Is this still planned to be merged? It obviously keeps accumulating conflicts while it is not :(,2018-09-24T21:12:52Z,1847934
2139,mysqljs/mysql,305129827,424128320,"This PR regresses behavior and cannot be merged until fixed as I noted above. I volunteered to fix the PR but have not gotten to it. Anyone is welcome to fix the regression in the PR, though, and will merge.",2018-09-24T21:17:39Z,67512
2140,mysqljs/mysql,305129827,424131454,"@dougwilson Thanks for the update! Not sure I'll get to it in the nearby future personally, though (but it's a possibility).",2018-09-24T21:28:06Z,1847934
2141,mysqljs/mysql,305129827,424138608,It's np. I did get it mostly rebased and fixed locally but didn't quite finish. I'll bring this close to the top of my current todo list.,2018-09-24T21:54:46Z,67512
2142,mysqljs/mysql,305129827,424319430,"> I'll look at what the fix is and push it up to your PR branch here

@dougwilson I was under the impression you would take over from there, and just assumed something else was blocking this. In any case, I'll gladly help to push this forward. Please, let me know what I can do.",2018-09-25T12:17:46Z,413255
2143,mysqljs/mysql,305129827,424321907,You are correct. I volunteered to fix the PR. I did get it mostly rebased and fixed locally but didn't quite finish. I'll bring this close to the top of my current todo list.,2018-09-25T12:25:32Z,67512
2144,mysqljs/mysql,305129827,424323464,"Alright, perfect. Let me know if there is something I can help with.",2018-09-25T12:30:10Z,413255
2145,mysqljs/mysql,305129827,427799522,I think you should keep track of this issue :) still on 8.0.,2018-10-08T11:27:31Z,15044488
2146,mysqljs/mysql,305129827,441585562,"Hi all,
Thanks for your effort!
But please merge this PR. It's been around 8 months that we have this compatibility issue with mysql 8.
Thanks 🎉",2018-11-26T10:11:26Z,13666448
2147,mysqljs/mysql,27120442,27120442,"Hi, im running node-mysql latest on node-latest.
Somebody using the acunetix vulnerability scanner has triggered this error:
UNKNOWN COLUMN '$acunetix' IN WHERE CLAUSE.
The query: SELECT id, email FROM accounts WHERE username = ?

How is this possible? Its very dangerous to our application, please respond quickly.
",2014-02-07T08:42:11Z,1161017
2148,mysqljs/mysql,27120442,34424093,"The problem seems to be about params that are not strings. Although I'll continue to sanitize all my user inputs (to avoid username impersonation attacks like `admіn` posing as `admin`), I'd expect the query engine to convert any param to a string if it should have been one in the first place. If it already was, `String(param)` should be of low cost.
",2014-02-07T10:21:03Z,3442602
2149,mysqljs/mysql,27120442,34425569,"@thekiur @mk-pmb can you post code samples?
",2014-02-07T10:44:21Z,173025
2150,mysqljs/mysql,27120442,34425679,"We can confirm that the problem is caused by passing objects to the query call.
The objects come from the express bodyParser middleware.
We were simply passing req.body.username as the parameter for that query.
The acunetic vulnerability tester injected an object there.
We are not sure on the severity of this issue, but its unexpected to say atleast.
As we experienced, this can crash a live application in production mode if you dont expect any db errors.

There is no code to show: its as simple as passing a req.body.something to the .query call of node-mysql when using express with the bodyparser middleware. Running the vulnerability scanner against https://gist.github.com/ssafejava/9a2d77704712a8769322 causes the exception to be thrown.
",2014-02-07T10:46:16Z,1161017
2151,mysqljs/mysql,27120442,34441093,"This is not an issue with escaping with this library; this library is properly escaping all values and column names. The security issue is just with the way you are combining express and this library, such that you were expecting to get a string from express, so you were only expecting the `?` to expand according to string rules.

`req.body` properties can be anything with `bodyParser` and as such you need to at least verify what you are using is a string before passing to your query.
",2014-02-07T14:35:23Z,67512
2152,mysqljs/mysql,27120442,34467169,"I consider prepared statements as intended to mitigate lack of input validation in the params in general. Therefor, limiting it to the case where input has already been validated as being a string, in my opinion misses the point.
Yours, MK
",2014-02-07T16:44:34Z,3442602
2153,mysqljs/mysql,27120442,34470964,"These are not prepared statements, they are done client-side and have various rules for how `?` is replaced depending on the data type, which is documented. If you want to be sure you are using the string-based `?` replacement though the API, you have to give the API a string. If you don't want to validate at all, you can use the `String()` function:
`conn.query('SELECT * FROM user WHERE username = ?', [String(req.body.username)]')`

The _purpose_ if it doing stuff different for objects is to help people who want to easily use `SET`:
`conn.query('UPDATE user SET ? WHERE id = ?', [{username: 'bob', password: '1234'}, 43])`

Please see the ""Different value types are escaped differently, here is how:"" section in https://github.com/felixge/node-mysql#escaping-query-values
",2014-02-07T16:58:32Z,67512
2154,mysqljs/mysql,27120442,34478411,"I see. Looks like an unlucky case of embrace and extend. I wish you had opted for something like `??` in that case. Probably too late to change the interface?

Edit: Not really embrace and extend, as you wrote they aren't prepared statements. Rather just a pitfall for people who learn from tutorials and conclude topical similarity from visual similarity.
Edit 2: I see, `??` is already used for column names.
",2014-02-07T17:36:36Z,3442602
2155,mysqljs/mysql,27120442,38685690,"I can't see how it's the type system's fault when programmers assume that a mechanism that looks like prepared statements will defuse any data they pass in. Let's at least blame it at the programmers for trusting visual similarity instead of reading the manual thoroughly.
",2014-03-26T13:56:29Z,3442602
2156,mysqljs/mysql,27120442,38686940,"@mk-pmb sure, though this module only has a small Readme, which has all the `?` stuff explained (https://github.com/felixge/node-mysql#escaping-query-values), so it's not even some weird hidden feature. Unfortunately if people on the Internet are writing tutorials about this module and giving incomplete or wrong information, it's hard for us to even try to police that.
",2014-03-26T14:07:14Z,67512
2157,mysqljs/mysql,27120442,43112213,"@mk-pmb it's the programmers role to understand the libraries he/she is using at least to the extend they are documented before including them in any production environment. If the library isn't fully documented, that's on the creator, but since this is an open-source world you can't really blame somebody for dedicating their time towards creating something for free.

Inferring functionality from syntax is useful, but think rationally: if the `?` operator accepts strings, would it only accepts strings? What if it accepted other data types? Jumping to blind assumptions about a library is a recipe for disaster, and good security protocols still mandate data validation.

Libraries and languages that make it easier to start developing are extremely useful, but I fear it gives a novice developer a misplaced sense of confidence. It's easy to build a small application, and when it ""just works"" assume nothing could possibly go wrong.
",2014-05-14T17:35:59Z,1348991
2158,mysqljs/mysql,27120442,43314833,"> Jumping to blind assumptions about a library is a recipe for disaster, and good security protocols still mandate data validation.

I agree with that. And still, lots of people do it. So for all software that I manage, I'll try and have it be compatible with everyday flawed humans, in hopes to lessen the risk and impact of errors in software based on mine, written by fallible humans.

BOfH would ship a GNU/Linux distro where the default shell acts fully like bash, just that on every line starting with an uppercase letter, the meaning of `&&` and `||` is swapped. Might even document it properly. You'd read the manual and probably wouldn't use it. However, if the next day a toy drone crashes into your car because it's pilot didn't read the manual as thoroughly as you did, your expectations of how humans should act had much less impact than how they really do act. And I'd still partially blame that BOfH.

Update: Thanks for making it opt-in.
",2014-05-16T09:49:01Z,3442602
2159,mysqljs/mysql,27120442,43332022,"Please, this issue doesn't need any more comments. It is still open as a tracking issue for me. There are coming changes that will affects this module and even things like `express` which will make any kind of ""shoot yourself in the foot"" operations opt-in. As an example, for this module `?` really should strictly only result in a single entry in the SQL (i.e. numbers, strings, null, etc.). Anything over that should be opt-in (on the connection-level or one-off on the query level to reduce accidental exposure.

These are changes that are coming I listed, not speculation. Please just know that this issue is taken seriously.
",2014-05-16T13:41:27Z,67512
2160,mysqljs/mysql,27120442,69188777,"Are there any circumstances where this would lead to an injection attack?

As far as I can work out so far this appears to only ever result in syntax errors.
",2015-01-08T14:49:47Z,1228777
2161,mysqljs/mysql,27120442,69234364,"@SystemParadox: I don't think so. The report seems to be badly explained and seems to be related to constructing SQL based on user input without any check.

Good usage:

``` js
db.query(""SELECT * FROM users WHERE id = ?"", [ +req.params.id ], next);
```

No harm on that, casting forces it to be a number. Even if it wasn't a number and the `+` was omitted, it's just fine (or else you would have problems when UPDATing columns with binary data - there's tests for that).

The problem here seems to be with something more like:

``` js
// BAD! BAD!
db.query(""SELECT * FROM "" + req.params.table + "" WHERE ...."", next);
```
",2015-01-08T19:29:02Z,157494
2162,mysqljs/mysql,27120442,69249939,"@SystemParadox yeah I just took a look at the formatting and escaping code. I don't see any way that passing unvalidated data to be interpolated into the query could result in an injection vulnerability. Without validation you can easily get a syntax error.
",2015-01-08T21:11:46Z,1348991
2163,mysqljs/mysql,6209234,6209234,"I noticed that Prepared Statements seem to be emulated client-side by escaping certain characters.

Any plans to fully support service-side Prepared Statements? This can be done via the binary protocol, but there's a slower SQL-based approach available for non-binary clients:
http://dev.mysql.com/doc/refman/5.5/en/sql-syntax-prepared-statements.html
",2012-08-14T06:26:39Z,479816
2164,mysqljs/mysql,6209234,7718792,"Yes, prepared statements are on my todo list. I don't need them myself, so unfortunately they kind of linger at the bottom of the list unless somebody wants to sponsor some of my time to work on the feature.

That being said, the SQL based approach looks interesting as a stop-gap solution for the short term.
",2012-08-14T07:54:35Z,15000
2165,mysqljs/mysql,6209234,7722530,"The only downside with the SQL-based approach is that you probably still end up needing to do client-side escaping. Still it does offer a little bit more structure, so it might still buy some protection. Depending on how you do it, it might also simplify the escaping part.

Unless I'm mistaken, you are already implementing the actual protocol at the lower levels of your driver. I wonder how much more you need at that level to finish?
http://dev.mysql.com/doc/internals/en/command-packet-details.html
",2012-08-14T10:58:58Z,479816
2166,mysqljs/mysql,6209234,7722736,"Prepared statements use a range of additional packets that are currently not implemented by my driver:
- http://dev.mysql.com/doc/internals/en/prepared-statement-initialization-packet.html
- http://dev.mysql.com/doc/internals/en/parameter-packet.html
- http://dev.mysql.com/doc/internals/en/long-data-packet.html
- http://dev.mysql.com/doc/internals/en/execute-packet.html

I have not yet analyzed how much work it would be to implement them, but my gut feeling is ~5 days of work.
",2012-08-14T11:10:09Z,15000
2167,mysqljs/mysql,6209234,7744377,"How does https://github.com/sidorares/nodejs-mysql-native handle this? Any reason not to just borrow parts of the way it's done over there?

I'm still somewhat struggling with the number of different MySQL drivers for Node.JS. I think Node makes it way too fun to write network protocol code. :P Maybe in a year or so the community will have coalesced around one or two really solid libraries.
",2012-08-14T23:57:06Z,479816
2168,mysqljs/mysql,6209234,7750467,"> How does https://github.com/sidorares/nodejs-mysql-native handle this?

It seems to implement the parts of the protocol that are required for prepared statements.

> Any reason not to just borrow parts of the way it's done over there?

Yes, I didn't have the time to work on this yet. I'm also not in the business of copying code unless it's up to my personal coding standards. So even with good inspiration like this, it will still take me some time.

> Maybe in a year or so the community will have coalesced around one or two really solid libraries.

This library is solid. It just does not implement all features.
",2012-08-15T08:02:28Z,15000
2169,mysqljs/mysql,6209234,10515691,"Couldn't we just prepare and execute statements using SQL instead of raw packets?
",2012-11-19T14:42:02Z,157494
2170,mysqljs/mysql,6209234,10531343,"@dresende the SQL method still winds up tampering with values to make them safe (escaping quotes, etc), whereas the protocol method explicitly separates query from values so tampering is not necessary. To be fair, as long as its impossible to smuggle a query in as a value, the driver is plenty secure enough. I suppose I'm just being a nitpicky ex-PHP developer who wants everything to be conceptually elegant. :P
",2012-11-19T21:14:37Z,479816
2171,mysqljs/mysql,6209234,10616045,"I just asked because I don't know the protocol deeply, but besides that it seems easy to implement based on the current module structure. I just don't have the knowledge about the protocol and documentation is scarce..
",2012-11-21T21:59:03Z,157494
2172,mysqljs/mysql,6209234,12895955,"I also need this feature from a security point of view. Your implementation of parameter escaping in JS only is naive (no harm intended - I've written things like that myself in the past), and I'm pretty sure it's open to any semi sophisticated SQL injection bot (e.g. using UTF-8 escape strings). MYSQL Prepared statements uses MYSQL own escaping subsystem, which has been refined for years, and is maintained by security experts. 

Support for prepared statements and parameter binding is a must if you want your library to be used in corporate world.
",2013-01-30T15:52:35Z,99944
2173,mysqljs/mysql,6209234,12896712,"> I also need this feature from a security point of view. Your implementation of parameter escaping in JS only is naive (no harm intended - I've written things like that myself in the past), and I'm pretty sure it's open to any semi sophisticated SQL injection bot (e.g. using UTF-8 escape strings).

If you care about security, please don't spread FUD.

> Support for prepared statements and parameter binding is a must if you want your library to be used in corporate world

I see that you're a prolific open source contributor yourself, so you should know that this isn't exactly the most successful strategy for requesting features. Of course this is an important feature and needs implementing, but unless somebody steps up to the challenge, it won't happen anytime soon. 
",2013-01-30T16:06:13Z,15000
2174,mysqljs/mysql,6209234,12898649,"Again, no harm intended. I don't know your security background, and both your past answers in this issue and the current implementation of parameter escaping made me think that you may not care about this as much as it deserves to. If I was wrong, sorry to have offended you.

So I understand that you care about this issue but won't implement it yourself, is that correct? I am surprised you don't put this one on top of your list, but maybe you're a much better expert in security that me and I overestimate the risks.
",2013-01-30T16:38:50Z,99944
2175,mysqljs/mysql,6209234,12899963,"> and the current implementation of parameter escaping made me think that you may not care about this as much as it deserves to.

This is what I mean by FUD. Please point out an actual attack vector. If it exists, it will be fixed.

> So I understand that you care about this issue but won't implement it yourself, is that correct? I am surprised you don't put this one on top of your list, but maybe you're a much better expert in security that me and I overestimate the risks.

As I said before, I don't have any time to hack on this module right now / don't need this feature myself. This says nothing about the importance of the feature, it's just my current situation. If somebody was to sponsor the development costs, I could make room in my schedule, otherwise this feature won't land until somebody else contributes a patch.

> but maybe you're a much better expert in security that me and I overestimate the risks.

I'm not a security expert. But I implemented the same escaping approach outlined by the MySQL documentation, which is also used by libmysql as well as mysqlnd. Afaik, it is correct and secure.

Additionally, multiple statement execution is disabled by default in this module, this limits the impact of any possible exploit could have.
",2013-01-30T17:00:51Z,15000
2176,mysqljs/mysql,6209234,13956847,"PHP has had tons of different vectors of SQL injection attacks before everyone started using prepared statements by default. Escaping is better left for MySQL's engine, rather than trying to reinvent the wheel on the client-side (in this case, the client is the app making the query). 

For example, if I remember correctly, in PHP one attack vector involved the escaping mechanism trying to escape strings for a certain encoding, and the connection using a slightly different encoding, causing some characters to not be escaped properly for what MySQL was expecting. 

Prepared statements are also important for queries that are re-used a lot. There are significant performance gains to he bad by using the same prepared statement multiple times rather than the fully query every time.
",2013-02-22T17:26:10Z,939776
2177,mysqljs/mysql,6209234,13957583,"Some literature on the encoding thing:
http://security.stackexchange.com/questions/9908/multibyte-character-exploits-php-mysql
http://stackoverflow.com/questions/5741187/sql-injection-that-gets-around-mysql-real-escape-string/12118602#12118602

I'm not a accusing the library of being bad / insecure, but PHP battled with SQL injection problems for many years. The only ""real"" solution they came up with after all these years is using prepared statements.
",2013-02-22T17:41:05Z,939776
2178,mysqljs/mysql,6209234,13958777,"So there is event more issues then security with this, which regardless should obviously be _the_ most important reason for implementing this.  I've done my own personal benchmarks vs mysql-native, which has prepared statements, and this library and it's over twice as performant, that's a pretty big jump.  I'm not sure if this will convince you more that this might be a feature you need, honestly I think the whole security aspect should do it alone IMO.

I'm sympathetic to the idea of if you want this feature, submit a patch.  But I have a few issues with the attitude taken here:

1) This isn't some minor feature. A lot of people, myself included, would consider prepared statements essential to any driver considering itself to be mature and stable.  As of now, from what I can see this is the main driver the node community seems to be coalescing around.  To not have prepared statements be a priority I think is a real problem because of that.

2) You haven't really abandoned this project, from what I can see.  There is active development going on, there is a 2.0 alpha 7 version.  From the commit logs it doesn't seem you're doing it yourself now, so I would be interested to see the developers who are actually actively working on this chime in.  Stemming from the point above I see it as a real concern that a 2.0 version of mysql driver is being developed and prepared statements just aren't ""that important"" to be included.

3) If you care above the Node js community then you should care about this issue.  If you care about security or radically increasing the performance of your code base then you should care about this issue.  If you don't care about any of the above I'm a little concerned for the Node.js community in general, if this is the attitude essential libraries and their maintainers are going to take.

Yes, I'm free to not use this code, I'm free to not use Node.js in general.  But if you care about actually improving the community and improving the quality of code out there on node js then the attitude shouldn't be either pay me or write it yourself.  In the end this was your project and you decided to contribute back and still have developers actively working on it.  And I think if you really want to keep contributing to the community you should reevaluate your attitude to this sort of issue.
",2013-02-22T17:58:34Z,136434
2179,mysqljs/mysql,6209234,13960461,"I think everyone is convinced but none of us is payed to do it. We have other projects (some give as money, others give us joy..), so we can't be 100% on this. If anyone wants to step up and do something, just do it and make a pull request. It doesn't need to be perfect or even complete, we can improve it then. Feel free to be part of that community and actually improve the quality of this code.
",2013-02-22T18:14:21Z,157494
2180,mysqljs/mysql,6209234,13961154,"Why are you closing this issue? You don't think not supporting prepared statements is an issue? People can still contribute to this issue and discuss it. If you close it, someone else will eventually open a duplicate of it.
",2013-02-22T18:20:14Z,939776
2181,mysqljs/mysql,6209234,13966568,"Supporting prepared statements is not an issue. Having someone do it is an issue. Since when someone does something about it and does a pull request, a new issue is opened, this is not an issue anymore.
",2013-02-22T19:27:24Z,157494
2182,mysqljs/mysql,6209234,13968329,"I don't think anyone here is demanding that you drop everything and implement prepared statements.  But I do feel like the lack of prepared statements is a bigger issue than you guys are making it out to be, and closing the ticket on github seems like an attempt to sweep this under the proverbial rug instead leaving it open to encourage a transparent and constructive discussion, inducing collaboration / contributing with code. Just my 2 cents.
",2013-02-22T20:01:50Z,939776
2183,mysqljs/mysql,6209234,14546257,"@dresende I'm re-opening this. The open github issues should list all reasonable suggestions for improvements, this is one of them.

@efuquen please go away. People arguing that me not implementing this feature in my spare time with virtually no benefits for myself is the same as not caring about the community is incredibly hurtful. I've poured my heart and soul into this library, and yes, I didn't get to implement every feature yet, and yes, this is unfortunately a very important one, but seriously, it's people like you that make me regret giving away so much of my work for free sometimes
",2013-03-07T07:25:57Z,15000
2184,mysqljs/mysql,6209234,14886974,"go away?  I've made what I feel are valid arguments.  I'm sorry I hurt your feelings but I was only being direct and I certainly wasn't rude and I don't feel like that means I should ""go away"" or that I'm somehow a bad person like you've implied in your comment.  You've made a library that right now is the defacto mysql library in the nodejs world, there are no other maintained alternatives.  There is a ""2.0"" version being developed missing a critical feature that was brushed aside, I tried to highlight this with new arguments because previous ones haven't worked.  Anyway there doesn't seem much more to be gained by commenting anymore so I'll leave it at that.
",2013-03-14T05:11:19Z,136434
2185,mysqljs/mysql,6209234,14900114,"@efuquen :
You seem not to understand the issue. **Everyone agrees that prepared statements are important, no need to argue on that.** Instead, if it's so important to you please step forward. I don't have the knowledge to do it. If I did, it would be already in an alpha version, and I don't have time to search for it. Do you? Can you please explain me how to do it? I'll happily implement it if you don't want to contribute and code some lines and just give me a technical explanation. I'll wait for a comment explaining or pointing to a clear explanation page and I'll do it.

This applies to **anybody really wanting this feature**. I want it to, it's just not my top priority and I don't have time for research.
",2013-03-14T12:47:23Z,157494
2186,mysqljs/mysql,6209234,15888762,"Everybody who wants/needs prepared statements, here is your chance: @pyramation contacted me to make this happen, setup a crowd funding campaign, and made an incredibly generous initial donation: https://www.crowdtilt.com/campaigns/prepared-statements-for-nodemysql/description

If the funding goal is met, I've guaranteed to handle the implementation. However, if somebody from the community would like to lead this effort, I'd be happy to mentor / share the funds. I'm mostly thinking about @dresende who's been the driving force of goodness behind node-mysql for several months now and would really deserve to be paid for some open source hacking. So if anybody is interested, let me know - otherwise I'll write the code.
",2013-04-04T09:56:57Z,15000
2187,mysqljs/mysql,6209234,15910111,"I'm in :) :+1: 
",2013-04-04T17:02:38Z,157494
2188,mysqljs/mysql,6209234,15923538,"I will try to help as much as I can. What's the game plan? :D
",2013-04-04T20:59:49Z,939776
2189,mysqljs/mysql,6209234,15938895,"Great to see @dresende is in!

@cblage one easy thing people can do is share this link where people who are interested will find it: https://www.crowdtilt.com/campaigns/prepared-statements-for-nodemysql/description
",2013-04-05T04:51:34Z,545047
2190,mysqljs/mysql,6209234,16154902,"@pyramation I'll share :) I'm also willing to help out with code. I'm not interested in getting any money for this, but I have a real interest in helping out. Prepared statements would make life easier for me at work :)

I don't have enough time to write it all / send a patch in, but I'm volunteering to help :) 
",2013-04-10T04:29:41Z,939776
2191,mysqljs/mysql,6209234,26011053,"I was wondering if this was being worked on.  If not I would like to take the time to contribute.  Please let me know. Thanks.
",2013-10-09T21:35:46Z,315908
2192,mysqljs/mysql,6209234,26035603,"@smitt04 that would be awesome. I did some initial work here: https://github.com/felixge/node-mysql/tree/prepare but never completed it.
",2013-10-10T08:07:26Z,15000
2193,mysqljs/mysql,6209234,26058203,"Ok cool, I will check out your other branch. I have some stuff to finish up first but hopefully i can get started on it with in the next couple of weeks.
",2013-10-10T14:28:48Z,315908
2194,mysqljs/mysql,6209234,26067906,"@smitt04 you can take a look at this module: https://github.com/sidorares/node-mysql2

It's backwards compatible with node-mysql, but they have prepared statements support since day 1. 
",2013-10-10T16:16:40Z,939776
2195,mysqljs/mysql,6209234,26216141,"(from discussion started in #616)
Question for everyone interested in prepared statements support: what API would fit best your use case?

Prior art:
1) initial implementation in node-mysql:

``` js
var statement = connection.prepare('SELECT * FROM '+table+' WHERE id = ?');
statement.execute([1], function(err, results, fields) {
  console.log(arguments);
});
```

2) node-mysql2:

``` js
connection.execute('SELECT * FROM '+table+' WHERE id = ?', [1], function(err, results, fields) {
  console.log(arguments);
});
```

3) node-mariasql (named placeholders resolved client side, mysql protocol only supports unnamed):

``` js
var pq = c.prepare('SELECT * FROM users WHERE id = :id AND name = :name');

c.query(pq({ id: 1337, name: 'Frylock' }))
 .on('result', function(res) {
   res.on('row', function(row) {
     console.log('Result row: ' + inspect(row));
   })
   .on('error', function(err) {
     console.log('Result error: ' + inspect(err));
   })
   .on('end', function(info) {
     console.log('Result finished successfully');
   });
 })
 .on('end', function() {
   console.log('Done with all results');
 });
```
",2013-10-13T11:37:00Z,173025
2196,mysqljs/mysql,6209234,26216188,"2 is closest to the core api. 1 seems natural. 3 doesn't belong.
",2013-10-13T11:40:30Z,369698
2197,mysqljs/mysql,6209234,26216209,"Named parameters would be nice (and I think MySQL supports it), but yeah 3 is really weird looking. I vote for 1 and/or 2.
",2013-10-13T11:42:03Z,479816
2198,mysqljs/mysql,6209234,26216256,"@jokeyrhyme no, unfortunately it does not support named parameters on the wire level. If you see them in client library then it translates to unnamed automatically. Id like to do the same at some point
",2013-10-13T11:45:54Z,173025
2199,mysqljs/mysql,6209234,26216298,"note that you likely to have `prepare` call somewhere in separate part of the code from where you `execute` it.
",2013-10-13T11:49:11Z,173025
2200,mysqljs/mysql,6209234,26216325,"Then 1 seems to be the most logical, if you expose an `unprepare()`; it affords a good amount of control.
",2013-10-13T11:50:37Z,369698
2201,mysqljs/mysql,6209234,26216366,"@avoidwork what if prepare fails? Should error just wait for `execute` call and propagate to its callback?
",2013-10-13T11:53:34Z,173025
2202,mysqljs/mysql,6209234,26216415,"oops, wrong example with node-mariasql. Its a ""prepared query"", e.i client side query template which gives you sql with interpolated parameters.
",2013-10-13T11:57:10Z,173025
2203,mysqljs/mysql,6209234,26216460,"I think `prepare()` should follow the core api, have a callback solely for handling an error. If prepared statements are long lived, it'd be nice to have them emit errors if something goes wrong after instantiation.
",2013-10-13T12:00:26Z,369698
2204,mysqljs/mysql,6209234,26216786,"I'd advocate following api, but there is still a lot of time before solidifying it:

``` js
   conn.prepare(string, callback(err, id, parameters, results) {});
   conn.execute(number, params?, callback?); // use statement id
   conn.execute(string, params, callback); // prepare if necessary, save result in internal hash
   conn.unprepare(id, callback); // release statement
   conn.unprepare(string, callback); // lookup statement id from hash
```
",2013-10-13T12:21:57Z,173025
2205,mysqljs/mysql,6209234,26217218,"With your 3rd use case, would all statements get prepared? And would the internal hash store all of them during the life of the connection?
",2013-10-13T12:49:39Z,25966
2206,mysqljs/mysql,6209234,26217527,"@reconbot all that called with `execute`. During the life of connection or until `unprepare` call.
",2013-10-13T13:08:26Z,173025
2207,mysqljs/mysql,6209234,26217988,"What if I have 1000 queries to insert. Isn't it better to prepare once and execute 1000? In this case the first example looks better.
",2013-10-13T13:41:24Z,157494
2208,mysqljs/mysql,6209234,26218102,"if you do `execute('insert into ... (?, ?, ?)', [x, y, z])` 1000 times, then first call will result in prepare + execute, and last 999 reuse prepared statement.
",2013-10-13T13:47:26Z,173025
2209,mysqljs/mysql,6209234,26219250,"@reconbot probably misunderstood you, see example above. Statements automatically prepared only if not already done so
",2013-10-13T14:47:36Z,173025
2210,mysqljs/mysql,6209234,26295340,"This LGTM, can't wait to have a final 2.0.0 with this :)
",2013-10-14T22:46:57Z,157494
2211,mysqljs/mysql,6209234,26295737,"I think it would be nice to have the api open, so you can prepare and execute separately, but at the same time also have a single api call that does prepare and execute in one statement, for those simple queries you dont want the extra code to use
",2013-10-14T22:55:15Z,315908
2212,mysqljs/mysql,6209234,26296365,"Slight variation of initial proposal:

``` js
conn.prepare(sql, function(err, statement) ); 
conn.execute(sql, params, callback); // prepare or use cached statement internally
statement.execute(params, callback);
statement.close(); // unprepare statement
conn.unprepare(sql); // find cached and unprepare
```
",2013-10-14T23:07:23Z,173025
2213,mysqljs/mysql,6209234,26296905,"I like the new API that @sidorares just listed, though `statement.close()` seems a little weird. `statement.destroy()` sounds better, i.m.o.
",2013-10-14T23:18:01Z,67512
2214,mysqljs/mysql,6209234,26297017,"@dougwilson it's called ""close"" in wire protocol documentation: http://dev.mysql.com/doc/internals/en/com-stmt-close.html
",2013-10-14T23:20:32Z,173025
2215,mysqljs/mysql,6209234,26297950,"@sidorares Ah, OK. That's fine, then. For some reason `.destroy()` just sounds more node-y to me :)
",2013-10-14T23:39:56Z,67512
2216,mysqljs/mysql,6209234,26323884,"imo, if you have a `destroy()` you need `create()` or the semantics break down. the revised api is looking good!
",2013-10-15T10:33:15Z,369698
2217,mysqljs/mysql,6209234,26340639,"I think the api @sidorares last suggested is simple and efficient. :+1: 

However, I think there should be an option to disable prepared statements caching when using ""conn.execute"". 
",2013-10-15T14:47:15Z,939776
2218,mysqljs/mysql,6209234,26340991,"I agree, I would predict lots of memory leak complaints otherwise.
On Oct 15, 2013 10:47 AM, ""Carlos Brito Lage"" notifications@github.com
wrote:

> I think the api @sidorares https://github.com/sidorares last suggested
> is simple and efficient. [image: :+1:]
> 
> However, I think there should be an option to disable prepared statements
> caching when using ""conn.execute"".
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/felixge/node-mysql/issues/274#issuecomment-26340639
> .
",2013-10-15T14:50:57Z,25966
2219,mysqljs/mysql,6209234,26343472,"@cblage @reconbot even then, MySQL will keep using memory on the server-side per connection to keep these queries prepared. Not caching them will make the queries take much longer, as there preparing the query on the server is not free. Perhaps just adding a method to the `connection` object to be able to say ""unprepare all all the queries in the cache"" to essentially be able to ""flush"" the cache. Or maybe the cache should be a configurable size and keep them with `mru`.
",2013-10-15T15:17:11Z,67512
2220,mysqljs/mysql,6209234,26344703,"I didn't realize that the server would keep the query too. I'm not sure
what level of control we want there but the MRU scheme is a neat idea.

---

Francis Gulotta
wizard@roborooter.com

On Tue, Oct 15, 2013 at 11:17 AM, Douglas Christopher Wilson <
notifications@github.com> wrote:

> @cblage https://github.com/cblage @reconbothttps://github.com/reconboteven then, MySQL will keep using memory on the server-side per connection
> to keep these queries prepared. Not caching them will make the queries take
> much longer, as there preparing the query on the server is not free.
> Perhaps just adding a method to the connection object to be able to say
> ""unprepare all all the queries in the cache"" to essentially be able to
> ""flush"" the cache. Or maybe the cache should be a configurable size and
> keep them with mru.
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/felixge/node-mysql/issues/274#issuecomment-26343472
> .
",2013-10-15T15:29:53Z,25966
2221,mysqljs/mysql,6209234,26346381,"That makes sense @dougwilson. I'm curious though, why would a MRU cache be better than an LRU cache for this situation? Regardless of MRU vs LRU, there should be a setting like ""maxCachedStatements"" that is defined when instantiating the connection.
",2013-10-15T15:47:15Z,939776
2222,mysqljs/mysql,6209234,26347740,"If ""maxCacheStatements"" is set to 0, however, ""conn.execute"" should immediately call ""statement.close()"" after fetching the result.
",2013-10-15T16:02:44Z,939776
2223,mysqljs/mysql,6209234,26348240,"`MRU` vs `LRU` just is different if you are talking about ejection or retention :) I meant it as the user can set a `maxCachedStatements` and keep them in a list where the most recently used would be retained when a new statement is cached over the max. And yes, @cblage this would nicely mean setting the max to `0` would immediately `.close()` the statement after the query :)
",2013-10-15T16:08:16Z,67512
2224,mysqljs/mysql,6209234,26353742,"What? LRU caches discard the least recently used items first, MRU do the opposite (they discard the most recently used ones first). It's about prioritization of what is retained vs what is ejected. I believe that for the case of caching prepared statements, an LRU cache would probably generate a higher cache hit ratio.
",2013-10-15T17:15:02Z,939776
2225,mysqljs/mysql,6209234,26382342,"During nearly 4 years of prepared statements support in mysql-native and later, mysql2 all issues related to leaking memory on client or server due to growing number of prepared statements were of similar pattern:

``` js
    for (var i=0; i < 10000000; ++i)
      conn.execute('select foo from bar where id=' + i); // ...
```

which obviously results in max_prepared_stmt_count error and not the way prepared statements are intended to be used.

I can't imagine application where you would need more than few hundred of prepared statements. On the client hash of 100 ints adds very little memory footprint. Not sure how much memory it takes for mysql server to store PS, but they are all disposed after connection is closed. 
",2013-10-15T23:45:37Z,173025
2226,mysqljs/mysql,6209234,26912252,"I also think the revised api by @sidorares is the best. I've a couple of projects that could benefit from prepared statements.
",2013-10-23T15:00:59Z,151715
2227,mysqljs/mysql,6209234,30501352,"Any SQL driver that doesn't have REAL prepared statements is useless and not worth consideration. Even node-mysql2 is useless: It doesn't have a separate prepare and execute. What is the point of a prepared statement that you use once? With all due respect, this seems to be a case of ""it's too difficult to support prepared statements, so let's pretend that we don't need them"".
",2013-12-13T10:56:31Z,3521553
2228,mysqljs/mysql,6209234,30502518,"> Any SQL driver that doesn't have REAL prepared statements is useless and not worth consideration. Even node-mysql2 is useless: It doesn't have a separate prepare and execute. What is the point of a prepared statement that you use once? With all due respect, this seems to be a case of ""it's too difficult to support prepared statements, so let's pretend that we don't need them"".

Your comment is really discouraging to people like me who have donated large amount of time to this community. Please read: http://felixge.de/2013/03/07/open-source-and-responsibility.html . You're welcome to contribute.
",2013-12-13T11:18:07Z,15000
2229,mysqljs/mysql,6209234,30503391,"@doug65536 its prepared once and can be used many times. If it's useless for you - please don't waste your time writing comment. And 30000+ downloads a week for node-mysql is a good sign that it's actually useful for many people.
",2013-12-13T11:35:09Z,173025
2230,mysqljs/mysql,6209234,31402172,"Sorry, I was completely wrong about node-mysql2. It does indeed prepare once. The API led me to believe that it didn't have a separate prepare and execute, but I looked at the code and found that it does have a cache that reuses the prepared statement if you use the same query text.

I was a bit nasty when I posted that comment. I was frustrated. Sorry about that.
",2013-12-31T17:03:25Z,3521553
2231,mysqljs/mysql,6209234,38177221,"Would love to see this functionality added, any chance of a status update?
",2014-03-20T15:00:22Z,735558
2232,mysqljs/mysql,6209234,53922211,"About prepare statement, I think it is not just about security, it also greatly improves performace. Because we can reuse queries with different corresponding values instead of recreating new queries.

@felixge I really surprise that you didn't receive a penny from this project. This is so sad. Your work is amazing. I strongly believe, that someday, `node-mysql` will be much more valuable. I even think that this module can be a nodejs built-in package. It's so real that more and more projects are depending on `node-mysql`. I am one of them. Thanks for your great job.
",2014-08-29T19:50:03Z,7781091
2233,mysqljs/mysql,6209234,53947373,"@tresdin there is no huge speed improvement with prepared statements - cost of parsing SQL is usually not that big compared to actual data access and IO. Of course this depends on your data and type of queries. Latency wise it's same sequence ""request - wait for data - request etc"" as with text based protocol (aka non-prepared statements). If you have real life examples and data sets I'd be happy to see your benchmark results
",2014-08-30T03:29:56Z,173025
2234,mysqljs/mysql,6209234,53952882,"@sidorares My point is just based on this article. 
http://dev.mysql.com/doc/refman/5.1/en/c-api-prepared-statements.html

According to this mysql dev docs, using prepare statment gains performance benefits because:
- Query is parsed only once.
- Prepared execution also can provide a reduction of network traffic because for each execution of the prepared statement, it is necessary only to send the data for the parameters.
- Prepare statement uses a binary protocol that makes data transfer between client and server more efficient.

But you're right. It also heavily depends on specific situation.
",2014-08-30T08:49:09Z,7781091
2235,mysqljs/mysql,6209234,53953060,"My point is based on actual benchmarks using node-mysql2 library. You are right as well, but usually one can get much better performance for example by going from single connection to a pool than by switching from text protocol to binary
",2014-08-30T08:58:21Z,173025
2236,mysqljs/mysql,6209234,65886321,"Hi all! I'd like to give node-mysql prepared statements some attention during christmas break. I changed my node-mysql2 api to conform my proposal discussed here, if anyone can look at https://github.com/sidorares/node-mysql2/pull/133 pr or try 'standalone-prepare' branch that would be helpful. I'd like to see some feedback on api in node-mysql2 before introducing it to node-mysql - because of number of users here it would be harder to fix. I'm going to release mysql2 with new api in a couple of days and start working on prepared statements for node-mysql late December (unless @dougwilson  have this already implemented :) )
",2014-12-06T05:25:44Z,173025
2237,mysqljs/mysql,6209234,65886364,"> unless @dougwilson have this already implemented :)

I don't :) I saw you working on your branch and if there is an implementation here, that would be amazing!
",2014-12-06T05:28:14Z,67512
2238,mysqljs/mysql,6209234,65886967,"there is very little to re-use except for tests - the internals are too different, but I'm really keen to implement the same here
",2014-12-06T05:58:34Z,173025
2239,mysqljs/mysql,6209234,65931121,"@sidorares wow, this sounds great. I'll have a look once you got a branch for node-mysql going.
",2014-12-07T09:02:09Z,15000
2240,mysqljs/mysql,6209234,247429392,"Any progress on this?

I'm more interested in the speed benefit of using prepared statements with big data and analysis than the alleged security benefits. Preparing an insert query once and executing millions of times will surely benefit more than node-mysql batching 100 non-prepared statements at a time.

Just curious. I'm very happy with `node-mysql` (I mean this module, called `mysql`, but that's ambiguous). I understand very much the issues of volunteering spare time to contribute and getting payed in joy, and was just wondering if any progress was made on this in the past year.

@felixge I feel bad about how some people in the community have difficulty expressing their appreciation for the work you have done. Know that it is appreciated. :+1: 
",2016-09-15T19:36:28Z,1702193
2241,mysqljs/mysql,6209234,247477445,"> I'm more interested in the speed benefit of using prepared statements with big data and analysis than the alleged security benefits. Preparing an insert query once and executing millions of times will surely benefit more than node-mysql batching 100 non-prepared statements at a time.

this is probably not the case. ( mostly because of serial nature of protocol ) 10000x 100batched non-prepared is going to be way faster than 1m prepared. Even more faster way is to use `LOAD INFILE` (basically whole 1m in one batch with very little overhead)
",2016-09-15T22:54:23Z,173025
2242,mysqljs/mysql,6209234,247527332,"@Redsandro thanks for the kind words. I can't comment on progress, I haven't written any node.js stuff in several years now.
",2016-09-16T06:25:31Z,15000
2243,mysqljs/mysql,6209234,247528126,"@Redsandro unfortunately I didn't kept my promise and not invested my time into porting prepared statements to here (yet). I might have a look again and see how much work left
",2016-09-16T06:32:02Z,173025
2244,mysqljs/mysql,6209234,247776181,"@sidorares 

> this is probably not the case. ( mostly because of serial nature of protocol ) 10000x 100batched non-prepared is going to be way faster than 1m prepared.

Really? Hmm, in that case I'm a tad bit disappointed by the speed, but perhaps the server is to blame. I'm talking about _""Nested arrays are turned into grouped lists (for bulk inserts)""_. I assumed the translating would add some overhead.

> Even more faster way is to use LOAD INFILE (basically whole 1m in one batch with very little overhead)

Unfortunately in my particular case I can't (or don't want to) do this, because I can only generate a 100-ish records at a time, depending on input data. Right now it's very resilient. It just waits if the table is locked (in case something else does a big query), data is in the table directly and isn't delayed by a day writing an intermediate `INFILE` first, and it picks up where it left off.

I have experimented with node not waiting for the `INSERT` to call back, but then the queue can fill up and choke the script.

> I might have a look again and see how much work left

I applaud this! :+1: 
Although as you pointed out, I might not benefit from this in my particular use case. But I'm looking forward to experimenting with this.
",2016-09-17T14:32:28Z,1702193
2245,mysqljs/mysql,6209234,354755765,"@sidorares we're using mysql quite extensively and we're having quite broad set of integration tests for our applications. If we could help somehow, even with testing non-production ready prepared statements implementation, happy to help. Looking forward and thanks for the effort!",2018-01-02T12:21:09Z,624797
2246,mysqljs/mysql,6209234,354756753,@Turneliusz unfortunately not very much spare time with day job and 6 kids. At the moment the only way to bump priority is to hire me for a few days to work full time on this,2018-01-02T12:28:00Z,173025
2247,mysqljs/mysql,6209234,453524154,"> @sidorares we're using mysql quite extensively and we're having quite broad set of integration tests for our applications. If we could help somehow, even with testing non-production ready prepared statements implementation, happy to help. Looking forward and thanks for the effort!

or just use mysql2? I'm testing and it seems to be fully compatible",2019-01-11T13:56:22Z,1990816
2248,mysqljs/mysql,6209234,461435270,"> 
> 
> @Turneliusz unfortunately not very much spare time with day job and 6 kids. At the moment the only way to bump priority is to hire me for a few days to work full time on this

I too feel that this is an important, performance and security-related, feature...

How much do you think that would cost?
Not that I think I could afford ya on my own, but... that's what communities are for...",2019-02-07T14:14:16Z,26166377
2249,mysqljs/mysql,6209234,461645608,"> How much do you think that would cost?

@lflfm the crowdit campaign 6 years ago had a goal around $3k if I'm not mistaken. If somebody want to reach me out with similar figure I'm happy to put all my other commercial work on hold and we'll have prepared statements in mysqljs/mysql within a week",2019-02-08T00:20:08Z,173025
2250,mysqljs/mysql,6209234,465936953,"I see, sounds like a fair figure; I couldn't get the sponsorship that I was hoping for unfortunately; but I'll try another source further down the road.",2019-02-21T09:58:40Z,26166377
2251,mysqljs/mysql,6209234,465939020,"Why not use 
`const mysql    = require('mysql2');` 
",2019-02-21T10:04:40Z,1990816
2252,mysqljs/mysql,6209234,465942138,"> 
> 
> Why not use
> `const mysql = require('mysql2');`

I don't remember why now, but the results of my analysis not too long ago was that this library (mysql) was the better choice, rather than mysql2... I think it was a combination of these factors:
- smaller/fewer dependencies
- reliability and/or stability (or something...)
- I remember there was something with clusters as well...
In any case, dependency analysis is a big issue for me (most people take this for granted, I see any dependency as big risk and potential for technical debt); and I remember that this was one of the main points that pushed me toward mysql instead of mysql2.",2019-02-21T10:14:10Z,26166377
2253,mysqljs/mysql,6209234,465959494,"@lflfm most critical dependencies in mysql2 are for perf reasons or represent functionality that is missing in mysqljs/mysql - denque ( fast queue used instead of array ) generate-function ( mysql2 generates optimised JS code at runtime ), iconv-lite ( mysqljs/mysql at the moment does not have good support for charsets ), lru-cache for prepared statements and parsers cache. Rest of deps is same",2019-02-21T11:08:47Z,173025
2254,mysqljs/mysql,6209234,465964328,And I guess you would need to add these deps as well if you were to add the prepared statements on mysql. cheers,2019-02-21T11:24:51Z,1990816
2255,mysqljs/mysql,6209234,465968342,"@sathio not really, nothing specific to prepared statements ( lru-cache maybe if we want same api with implicitly cached statements )",2019-02-21T11:39:13Z,173025
2256,mysqljs/mysql,6209234,481392194,Does mysql support prepared statements at this point or should I move to mysql2? Thanks,2019-04-09T18:59:15Z,22104001
2257,mysqljs/mysql,6209234,531543918,"> Does mysql support prepared statements at this point or should I move to mysql2? Thanks

mysql2 doesn't support prepared statements either... so...",2019-09-15T07:58:11Z,1825272
2258,mysqljs/mysql,6209234,531544075,"> mysql2 doesn't support prepared statements either... so...

@dnutels In what way?",2019-09-15T08:00:46Z,173025
2259,mysqljs/mysql,6209234,531798402,"[__Using Prepared Statements__](https://github.com/sidorares/node-mysql2#using-prepared-statements)
> With MySQL2 you also get the prepared statements. With prepared statements MySQL doesn't have to prepare plan for same query everytime, this results in better performance.",2019-09-16T14:18:39Z,1702193
2260,nim-lang/Nim,534583789,534583789,"Bad C code is generated with the following Nim code:

### Example
```nim
import locks

var l: Lock
echo l
```

### Current Output
```
CC: ltest.nim
Error: execution of an external compiler program 'gcc -c  -w  -I/home/tay/.choosenim/toolchains/nim-1.0.4/lib -I/home/tay -o /home/tay/.cache/nim/ltest_d/stdlib_dollars.nim.c.o /home/tay/.cache/nim/ltest_d/stdlib_dollars.nim.c' failed with exit code: 1

/home/tay/.cache/nim/ltest_d/stdlib_dollars.nim.c: In function ‘dollar___3Dq1hcGH7a2iNlnh4hnPbg’:
/home/tay/.cache/nim/ltest_d/stdlib_dollars.nim.c:219:50: error: ‘pthread_mutex_t’ {aka ‘union <anonymous>’} has no member named ‘abi’
  219 |  addQuoted__N56rmvBL9a3xqnKq09cKdb3A((&result), x.abi);
      |                                                  ^
```

### Expected Output
Compiler either compiles properly or errors before C code gen

### Possible Solution

* It looks like there might be an issue with the implementation of `$` for the Lock type

### Additional Information

* Fedora 31: `Linux localhost.localdomain 5.3.11-300.fc31.x86_64 #1 SMP Tue Nov 12 19:08:07 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux`

```
$ nim -v
Nim Compiler Version 1.0.4 [Linux: amd64]
Compiled at 2019-11-27
Copyright (c) 2006-2019 by Andreas Rumpf

git hash: c8998c498f5e2a0874846eb31309e1d1630faca6
active boot switches: -d:release
```
",2019-12-08T18:53:54Z,10137
2261,nim-lang/Nim,534583789,616117085,Why close? The issue is still there,2020-04-19T12:04:15Z,21169548
2262,nim-lang/Nim,534583789,616764562,"It's been several months and there was no feedback, I figured an issue wasn't necessary.",2020-04-20T19:34:40Z,10137
2263,nim-lang/Nim,534583789,616768581,"@3n-k1 that's not how it works in open-source in general - it just means that nobody worked on the issue yet, but as long as the bug is still there it should be open",2020-04-20T19:42:41Z,21169548
2264,nim-lang/Nim,534583789,616902966,"I understand how FOSS works. Now, I would prefer if you didn't mention me again as I would prefer not to receive notifications for this.",2020-04-21T01:53:41Z,10137
2265,nim-lang/Nim,534583789,652191027,"It's been over half a year, and I get a bad taste in my mouth every time I see this. I'm closing this, and I will be blocking anyone who responds or re-opens; I really don't care if that makes me an ass.

Y'all have a minimal example and all the information necessary for this, feel free to open a ***new*** issue with a copy-paste of the initial issue I posted here.

This is me officially leaving the Nim community. So long, and thanks for all the fish.",2020-07-01T04:59:56Z,10137
2266,nim-lang/Nim,534583789,652205691,"> It's been over half a year, and I get a bad taste in my mouth every time I see this. I'm closing this, and I will be blocking anyone who responds or re-opens; I really don't care if that makes me an ass.
> 
> Y'all have a minimal example and all the information necessary for this, feel free to open a ***new*** issue with a copy-paste of the initial issue I posted here.
> 
> This is me officially leaving the Nim community. So long, and thanks for all the fish.

I'm okay with getting blocked by you - but really, ""leaving"" the community because of INCORRECT Nim code not compiling (even if it compiles it'll crash at best since you didn't initialize the lock, and for printing you can just try to use repr or cast to a pointer and repr) is not okay. Also this is open source, not some software you paid for, so you maybe should've tried fixing it yourself before? :)",2020-07-01T05:47:33Z,21169548
2267,nim-lang/Nim,534583789,652207779,Just brigading so I can get my promised block. I certainly would not like you to use anything I contribute on.,2020-07-01T05:54:08Z,11792483
2268,nim-lang/Nim,534583789,652456636,"@Yardanico I'm leaving the community because Araq has treated me like shit numerous times, but thanks for playing.",2020-07-01T14:35:44Z,10137
2269,nim-lang/Nim,534583789,652457283,The fact you need to bitch and moan about someone not wanting to be treated like a fucking piece of garbage just confirms this community is fucking garbage.,2020-07-01T14:36:50Z,10137
2270,nim-lang/Nim,534583789,652462641,"@beef331 Even if I *did* want to use any of your trash, it would be a bit difficult since you apparently can't bring yourself to write more than two sentences in a readme, assuming you write anything at all.

Nice website by the way, I really enjoy the gif of your superman64 clone playing as the site's background.

You wanna get shitty? Let's fuckin' go, kid. At least I actually contributed to Nim before Araq treated me like shit, unlike you.

Araq, be careful who you treat like garbage, you may lose one of your shitty niche language's biggest evangelists.",2020-07-01T14:45:52Z,10137
2271,nim-lang/Nim,534583789,652463713,"Am I ranting now? Sure the fuck am. I thought you'd all be able to be fucking adults about this, but I guess fucking not.",2020-07-01T14:47:40Z,10137
2272,nim-lang/Nim,534583789,652466479,"@Yardanico I tried to be as polite as possible when closing this, and you attempt to dogpile and bitch? Real fucking mature.
![image](https://user-images.githubusercontent.com/51172302/86258541-ecbd5980-bb88-11ea-8e54-477aaba9f3ef.png)
",2020-07-01T14:52:26Z,10137
2273,nim-lang/Nim,534583789,652466902,"just gonna document why I'm leaving. You're all petulant children.
![image](https://user-images.githubusercontent.com/51172302/86258654-0ced1880-bb89-11ea-8517-36188fecf092.png)
",2020-07-01T14:53:11Z,10137
2274,nim-lang/Nim,534583789,652467689,"Please do google ""inurl:irclogs.nim-lang.org simula"". Better yet, search ""from:simula#3097"" in the discord server. My last interaction is exactly the bullshit I'm talking about. Glad to see it hasn't ended.
![image](https://user-images.githubusercontent.com/51172302/86258722-22fad900-bb89-11ea-8ab6-a109557aad3d.png)
",2020-07-01T14:54:33Z,10137
2275,scikit-learn/scikit-learn,1210960989,1210960989,"### Describe the bug

ndcg_score doesn't work. you're a bunch of useless spaghetti coders, just go to italian restaurant if you like pasta, don't waste other people's time producing tons of govnokod.

### Steps/Code to Reproduce

import ndcg_score

### Expected Results

work

### Actual Results

no work

### Versions

```shell
all
```
",2022-04-21T12:24:38Z,39010098
2276,scikit-learn/scikit-learn,1210960989,1105276616,"I'll just close this issue.

@irodionzaytsev if you want your issue to ever be considered, first please be respectful in your discussions and then try to provide a reproducible example showing the wrong behavior.",2022-04-21T14:17:51Z,34657725
2277,scikit-learn/scikit-learn,1072818585,1072818585,"### Describe the bug

https://stackoverflow.com/questions/70253946/sgd-breaks-down-when-encountering-unseen-values

### Steps/Code to Reproduce

-

### Expected Results

-

### Actual Results

-

### Versions

-",2021-12-07T01:40:53Z,95664312
2278,scikit-learn/scikit-learn,1072818585,987682838,"Is this specific to `SGDClassifier`? Or can you reproduce the problem with other classifiers with different solvers such as `LogisticRegression`?

Can you please craft a minimal reproducer with a synthetic dataset so that we can reproduce the problem without any ambiguity? Please also include the expected and observed results in your report.",2021-12-07T08:26:34Z,89061
2279,scikit-learn/scikit-learn,1072818585,987947665,"Hello, I would like to work on this.  As ogrisel mentioned, can you provide the synthetic dateset to reproduce the problem  ?",2021-12-07T13:54:57Z,13694836
2280,scikit-learn/scikit-learn,1072818585,988073948,"Hello @ogrisel , @Nivi09 - thank you for your answers.

I do not have any synthetic data, I use a real dataset that I cannot really share.

But if you pick any dataset from the Internet that has categorical variables or even better text then you can easily test what I describe.

It does not have to be only eg with categorical; it can be 9 numerical features and 1 categorical feature.
Especially if the categorical feature has some importance (although the same may happen if it does not; I have not tested that) and in your test set one observation has one unseen value for the categorical feature then the SGD results returns essentially random predictions (or at least not directly related).

It may be that scikitLearn implementation is fine but just SGD itself has this weakness in general?
I think though that RandomForestClassifier does not have the same problem in these cases.

I have not tested much the LogisticRegression.
even like that:
logistic_regression = LogisticRegression(penalty='elasticnet', tol=10, C=1.0, class_weight='balanced', random_state=0, solver='saga', max_iter=20, n_jobs=os.cpu_count()-1, l1_ratio=0.7)
it takes too much time to converge",2021-12-07T16:16:00Z,95664312
2281,scikit-learn/scikit-learn,1072818585,988476317,"Hi @GeorgeZan , let me rephrase the issue that you have described and please correct me if I have not understood it correctly.
The task is a classifier algorithm, let's say a data set with 9 numerical predictors and target label as class. For example, let's say you have dataset of len 1000 and your target label contains 5 classes - C1, C2, C3, C4, C5 and there is only one observation for class C5. When you divide it for train and test, your train and test split algorithm has divided the classes C1, C2, C3, C4 divided in some proportion in both train and test, but because observation of class C5 is only one it gets allotted to test set. Now, when you train SGD classifier on train set and then test it on test set, the classifier breaks down for class C5 . Is my understanding correct ? ",2021-12-08T03:57:42Z,13694836
2282,scikit-learn/scikit-learn,1072818585,988627284,"> The task is a classifier algorithm, let's say a data set with 9 numerical predictors and target label as class. For example, let's say you have dataset of len 1000 and your target label contains 5 classes - C1, C2, C3, C4, C5 and there is only one observation for class C5. When you divide it for train and test, your train and test split algorithm has divided the classes C1, C2, C3, C4 divided in some proportion in both train and test, but because observation of class C5 is only one it gets allotted to test set. Now, when you train SGD classifier on train set and then test it on test set, the classifier breaks down for class C5 . Is my understanding correct ?

In general, any classifier can never predict a class that was not part of the training set. I am not sure what ""break down"" means in this context.

> But if you pick any dataset from the Internet that has categorical variables or even better text then you can easily test what I describe.

Can you please craft a standalone reproducible code snipet example that generates its own toy dataset so that we can just execute ? I am afraid the English language is ambiguous while Python is not.

Please also report the result you observe and the output you would have expected.",2021-12-08T09:08:18Z,89061
2283,scikit-learn/scikit-learn,1072818585,988810453,"Hi @ogrisel, GeorgeZan has mentioned that - SGD results returns essentially random predictions (or at least not directly related).
I think this is what  break down means. Please correct me if my understanding is not correct @GeorgeZan ? 

And as ogrisel mentioned, my understanding is also the same that no classifier can predict a class that was not a part of training set. 

I read GeorgeZan comment again, so here is my second understanding : 
   input set X has 10 independent variables in which one variable is categorical. Along with that you have categorical dependent variable. Your test fails when there is a class in categorical I.V. which has not been seen in train data set while training and that's where you get random prediction. 

@GeorgeZan, please let me know, out of two examples, which one is correct.
",2021-12-08T13:22:39Z,13694836
2284,scikit-learn/scikit-learn,1072818585,988841478,"@Nivi09, yes indeed.
""Hi @ogrisel, GeorgeZan has mentioned that - SGD results returns essentially random predictions (or at least not directly related).
I think this is what break down means.""

To give a hypothetical example, let's say I have a model with 9 numerical features and a categorical feature and let's say there are 3 classes in my problem (A, B, C class).
Let's also assume that I have a test set instance X that belongs to the A class.
Specifically, it belongs to class A regardless of the value of the categorical feature.
At best, the categorical feature can help the model to predict class A for X just by inference but it is not a defining feature of X.
i)
If I keep the categorical feature in the model and the test set instance X provided has a known/seen value at the categorical feature then the model returns the following prediction:
A: 0.7
C: 0.2
B: 0.1

iI)
If I drop the categorical feature from the model then the model returns the following prediction:
A: 0.55
C: 0.35
B: 0.1

iii)
If I keep the categorical feature in the model and the test set instance provided has a unknown/unseen value at the categorical feature then the model returns the following prediction:
B: 0.5
A: 0.3
C: 0.2

.........................
Obviously the impact of the categorical features in all these cases depends on how important is the feature for the model; in the example above it is assumed that the categorical feature has quite significant weight.

This is also why in (ii) the top score drops by quite a lot in comparison with (i) given that it is only one feature.

However, and this is what I mean that the model ""breaks down"", in (iii) it returns essentially random predictions.
It does not make sense to me that if you just receive an unseen value your prediction changes so so much.
I would expect (iii) to be more like (ii).

The even more interesting and problematic thing is that the same thing happens as mentioned at my original post if you have eg 100k tf-idf sparse features and just one categorical at the same model and if you put an unseen value at the categorical at the test set then SGD ""breaks down"" as in (iii).

In the dataset that I experimented the categorical feature is quite significant (eg dropping it reduced the overall accuracy of the model by 5 percentage points; eg accuracy drops from 80% to 75%) but I would not exclude based on how unstable SGD is that the same thing would happen even if the feature is not that significant.
.........................
@ogrisel I think now it must be quite clear.

As I said, you can pick any dataset on the Internet which has also categoricals or you can even create a very basic one on Excel and I think you will observe what I mean.",2021-12-08T14:04:06Z,95664312
2285,scikit-learn/scikit-learn,1072818585,989208269,"Hello @GeorgeZan , for the case 3 as you have mentioned:

If I keep the categorical feature in the model and the test set instance provided has a unknown/unseen value at the categorical feature then the model returns the following prediction:
B: 0.5
A: 0.3
C: 0.2

So, to which class do you expect this example to belong to ? Because I'm confused here when you mentioned - - ' (iii) it returns essentially random predictions.
It does not make sense to me that if you just receive an unseen value your prediction changes so so much.
I would expect (iii) to be more like (ii).' ",2021-12-08T21:25:21Z,13694836
2286,scikit-learn/scikit-learn,1072818585,989984238,"@Nivi09 ah sorry, just edited my post to add ""Let's also assume that I have a test set instance that belongs to the A class."".
Although it is relative obvious what I mean.

My point is that if you have a known value at the categorical feature or a if you entirely drop the categorical feature then the model returns the right prediction (although with slightly different ""confidence"").

But when you put an unseen value at the categorical feature then the model returns random results although given (ii) essentially the categorical feature does not play much role in the prediction so (iii) should have been returning something more like (ii).",2021-12-09T15:58:00Z,95664312
2287,scikit-learn/scikit-learn,1072818585,996847749,"I don't think that there is anything wrong but I would not be able to be sure until we have an example showing the wrong behaviour and the expected behaviour.

Since `SGDClassifier` is a linear model, when it comes to categorical data, the type of encoding is rather important. So did you check the coefficients of your model? Did you use an `OneHotEncoder` for encoding your categories? Basically, the value of the ""unseen categories"" will have an impact depending on the coefficient of the associated variable.",2021-12-17T16:18:27Z,7454015
2288,scikit-learn/scikit-learn,1072818585,998023134,"@glemaitre as I mention at my post, I put the categorical value in the text and then apply tf-idf on it.
In this sense you could say that I do one hot encoding on it (?).

I have not really used the categorical variable separately but when I did was with label encoder.
Even though not ideal because ""LabelEncoder can turn [dog,cat,dog,mouse,cat] into [1,2,1,3,2], but then the imposed ordinality means that the average of dog and mouse is cat."", still it should not break down so much with unseen values.

PS
""I don't think that there is anything wrong ""
Wrong or not, the question is why this happens like that with SGD and unseen values.",2021-12-20T15:24:33Z,95664312
2289,scikit-learn/scikit-learn,1072818585,998026500,"> Even though not ideal because ""LabelEncoder can turn [dog,cat,dog,mouse,cat] into [1,2,1,3,2], but then the imposed ordinality means that the average of dog and mouse is cat."", still it should not break down so much with unseen values.

This encoding is thus ordinal. Now if your unseen value is mapped to 100 then it will induce 100 x coef on the output and thus it would go sideways.",2021-12-20T15:27:58Z,7454015
2290,scikit-learn/scikit-learn,1072818585,998037887,"@glemaitre First of all, as I said (not sure if you read carefully) I primarily use the TF-IDF and the problem occurs there.
Any thoughts on these or we entirely skipped my main question haha?

Secondly, not sure why it would go sideways like that ""Now if your unseen value is mapped to 100 then it will induce 100 x coef on the output and thus it would go sideways.""

Let's say that I do not have an unseen value but a seen one and specifically the one mapped to 99 which would induce a 99x coef then why this would not go equally sideways and the 100 would?",2021-12-20T15:40:47Z,95664312
2291,scikit-learn/scikit-learn,1072818585,998064725,"> Let's say that I do not have an unseen value but a seen one and specifically the one mapped to 99 which would induce a 99x coef then why this would not go equally sideways and the 100 would?

The thing that goes sideways here would be to use some ordered pattern for a feature that does not have any and where an increment/decrement of the feature value induces an increment/decrement of `coef` on the target. So my example was just to mention that if your mapped unseen values are mapped to larger values then it would even have a larger impact on the target.

> not sure if you read carefully

I think I did but I don't think that you did. As mentioned by @ogrisel or myself, it would be much easier to understand the problem and if there is an issue by having a concrete Python piece of code. Up to now, I am still not sure what is the problem that you are trying to express. You are referring to unseen values. However, an unseen term not part of the `TfidfVectorizer` dictionary will not be used. So more generally because you were referring to categories, I tried to understand what encoding you are actually using because a linear model it will have a different impact on the decision.

So please provide a Python code snippet that we can run and discuss concretely about the issue that you observe.",2021-12-20T16:10:21Z,7454015
2292,scikit-learn/scikit-learn,1072818585,998079164,"@glemaitre 
""I think I did but I don't think that you did. ""
I have answered this my boy since day 1 so you have not read again:
https://github.com/scikit-learn/scikit-learn/issues/21906#issuecomment-988073948
""I do not have any synthetic data, I use a real dataset that I cannot really share.
But if you pick any dataset from the Internet that has categorical variables or even better text then you can easily test what I describe.""
So do not try to be too smart now.

But anyway if nobody can see what could be the problem without a dataset then I may try to construct something.
 
""So my example was just to mention that if your mapped unseen values are mapped to larger values then it would even have a larger impact on the target.""
yeah but then the unseen values would experience the ""same"" ""jump"" as the 99 class and they do not.
The behaviour with unseen values is totally different and anyway as I said I do not even mostly encode the categorical but use it in the TF-IDF.",2021-12-20T16:24:56Z,95664312
2293,scikit-learn/scikit-learn,1072818585,998090723,"@GeorgeZan I don't think we can suggest better here. It does not seem to be a software issue we can fix. I invite you to share a code snippet we can just run so we can maybe suggest some practical tricks to make good use of SGDClassifier.

feel free to reopen this issue with a code snippet.",2021-12-20T16:38:44Z,161052
2294,scikit-learn/scikit-learn,1072818585,998103219,"> I have answered this my boy since day 1
   So do not try to be too smart now.

@GeorgeZan please try to stay respectful in the discussions. I know that trying to explain something and be understood can be hard to achieve in online discussions but people are just trying to get a clear vision of the problem you describe and these kind of passive-agressive comments do not help. Please respect the [code of conduct](https://github.com/scikit-learn/scikit-learn/blob/main/CODE_OF_CONDUCT.md).
",2021-12-20T16:55:18Z,34657725
2295,scikit-learn/scikit-learn,1072818585,998130920,"@jeremiedbb I agree about being respectful but then comments like that which preceded mine are equally problematic (""I think I did but I don't think that you did. "").",2021-12-20T17:29:11Z,95664312
2296,scikit-learn/scikit-learn,1072818585,998140040,"That was preceded by:

> First of all, as I said (not sure if you read carefully)

Be aware that reading this thread took me probably over an hour from the time you opened this issue. So firing such comments when trying to help tend to tease me somehow.",2021-12-20T17:43:14Z,7454015
2297,scikit-learn/scikit-learn,1072818585,998152700,"I don't this discussion is going in the right direction :)
I'm locking the conversation now. @GeorgeZan feel free to open a new issue if you manage to come up with a reproducible example with clear expected and not expected results.",2021-12-20T18:03:28Z,34657725
2298,scikit-learn/scikit-learn,47049745,47049745,"Hi all,

I am very new in scikit-learn. 

My questions is: how to download my own dataset (csv file).

I will be highly appreciated any answers.

Thanks.
Martin
",2014-10-28T16:41:25Z,9435135
2299,scikit-learn/scikit-learn,47049745,60845063,"The documentation of sklearn is really very useful and should answer your question: http://scikit-learn.org (basically you have to put your data in numpy arrays)
",2014-10-28T22:45:14Z,869592
2300,scikit-learn/scikit-learn,47049745,60850809,"This is something that could have a bit more documentation than is in there
currently. You might find Pandas useful.

On 29 October 2014 09:45, Alexander Fabisch notifications@github.com
wrote:

>  The documentation of sklearn is really very useful and should answer
> your question: http://scikit-learn.org (basically you have to put your
> data in numpy arrays)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60845063
> .
",2014-10-28T23:37:16Z,78827
2301,scikit-learn/scikit-learn,47049745,60882069,"@jnothman Should we reopen this issue and add a new section in the documentation? For example in this section: http://scikit-learn.org/stable/tutorial/basic/tutorial.html (""Loading your own data"").
",2014-10-29T07:07:50Z,869592
2302,scikit-learn/scikit-learn,47049745,60883212,"see #2801

On 29 October 2014 18:07, Alexander Fabisch notifications@github.com
wrote:

>  @jnothman https://github.com/jnothman Should we reopen this issue and
> add a new section in the documentation? For example in this section:
> http://scikit-learn.org/stable/tutorial/basic/tutorial.html (""Loading
> your own data"").
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60882069
> .
",2014-10-29T07:25:02Z,78827
2303,scikit-learn/scikit-learn,47049745,60899671,"My own dataset means the dataset that I have collected by my self, not the
standard dataset that all machine learning have in their depositories (e.g.
iris or diabetes).

I have a simple csv file and I on my desktop and I want to load it inside
scikit-learn. That will allow me to use scikit-learn properly and introduce
it to my colleges to serve our community.

I need a very simple and easy way to do so.

I will be highly appreciated any useful advice.

On 29 October 2014 15:25, jnothman notifications@github.com wrote:

> see #2801
> 
> On 29 October 2014 18:07, Alexander Fabisch notifications@github.com
> wrote:
> 
> > @jnothman https://github.com/jnothman Should we reopen this issue and
> > add a new section in the documentation? For example in this section:
> > http://scikit-learn.org/stable/tutorial/basic/tutorial.html (""Loading
> > your own data"").
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60882069>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60883212
> .
",2014-10-29T10:15:50Z,9435135
2304,scikit-learn/scikit-learn,47049745,60906075,"See http://pandas.pydata.org/pandas-docs/stable/io.html

On 29 October 2014 21:15, MartinLion notifications@github.com wrote:

>  My own dataset means the dataset that I have collected by my self, not
> the
> standard dataset that all machine learning have in their depositories
> (e.g.
> iris or diabetes).
> 
> I have a simple csv file and I on my desktop and I want to load it inside
> scikit-learn. That will allow me to use scikit-learn properly and
> introduce
> it to my colleges to serve our community.
> 
> I need a very simple and easy way to do so.
> 
> I will be highly appreciated any useful advice.
> 
> On 29 October 2014 15:25, jnothman notifications@github.com wrote:
> 
> > see #2801
> > 
> > On 29 October 2014 18:07, Alexander Fabisch notifications@github.com
> > wrote:
> > 
> > > @jnothman https://github.com/jnothman Should we reopen this issue
> > > and
> > > add a new section in the documentation? For example in this section:
> > > http://scikit-learn.org/stable/tutorial/basic/tutorial.html (""Loading
> > > your own data"").
> > > 
> > > —
> > > Reply to this email directly or view it on GitHub
> > > <
> > 
> > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60882069>
> > 
> > > .
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60883212>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60899671
> .
",2014-10-29T11:12:22Z,78827
2305,scikit-learn/scikit-learn,47049745,60913843,"Thanks for the link. I checked it out, but the process looks complicated.
Perhaps if there is a short youtube video explains the process much easier,
otherwise I do not know what to do to solve this matter.

On 29 October 2014 19:12, jnothman notifications@github.com wrote:

> See http://pandas.pydata.org/pandas-docs/stable/io.html
> 
> On 29 October 2014 21:15, MartinLion notifications@github.com wrote:
> 
> > My own dataset means the dataset that I have collected by my self, not
> > the
> > standard dataset that all machine learning have in their depositories
> > (e.g.
> > iris or diabetes).
> > 
> > I have a simple csv file and I on my desktop and I want to load it
> > inside
> > scikit-learn. That will allow me to use scikit-learn properly and
> > introduce
> > it to my colleges to serve our community.
> > 
> > I need a very simple and easy way to do so.
> > 
> > I will be highly appreciated any useful advice.
> > 
> > On 29 October 2014 15:25, jnothman notifications@github.com wrote:
> > 
> > > see #2801
> > > 
> > > On 29 October 2014 18:07, Alexander Fabisch notifications@github.com
> > > 
> > > wrote:
> > > 
> > > > @jnothman https://github.com/jnothman Should we reopen this issue
> > > > and
> > > > add a new section in the documentation? For example in this section:
> > > > http://scikit-learn.org/stable/tutorial/basic/tutorial.html
> > > > (""Loading
> > > > your own data"").
> > > > 
> > > > —
> > > > Reply to this email directly or view it on GitHub
> > > > <
> > 
> > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60882069>
> > 
> > > > .
> > > 
> > > —
> > > Reply to this email directly or view it on GitHub
> > > <
> > 
> > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60883212>
> > 
> > > .
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60899671>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60906075
> .
",2014-10-29T12:19:24Z,9435135
2306,scikit-learn/scikit-learn,47049745,60917437,"It probably looks something like:

import pandas as pd
data = pd.read_csv(open('myfile.csv'))
target = data[target_column_name]
del data[target_column_name]

# Then fit a scikit-learn estimator

SVC().fit(data, target)

On 29 October 2014 23:19, MartinLion notifications@github.com wrote:

>  Thanks for the link. I checked it out, but the process looks complicated.
> Perhaps if there is a short youtube video explains the process much
> easier,
> otherwise I do not know what to do to solve this matter.
> 
> On 29 October 2014 19:12, jnothman notifications@github.com wrote:
> 
> > See http://pandas.pydata.org/pandas-docs/stable/io.html
> > 
> > On 29 October 2014 21:15, MartinLion notifications@github.com wrote:
> > 
> > > My own dataset means the dataset that I have collected by my self, not
> > > the
> > > standard dataset that all machine learning have in their depositories
> > > (e.g.
> > > iris or diabetes).
> > > 
> > > I have a simple csv file and I on my desktop and I want to load it
> > > inside
> > > scikit-learn. That will allow me to use scikit-learn properly and
> > > introduce
> > > it to my colleges to serve our community.
> > > 
> > > I need a very simple and easy way to do so.
> > > 
> > > I will be highly appreciated any useful advice.
> > > 
> > > On 29 October 2014 15:25, jnothman notifications@github.com wrote:
> > > 
> > > > see #2801
> > > > 
> > > > On 29 October 2014 18:07, Alexander Fabisch <
> > > > notifications@github.com>
> > > > 
> > > > wrote:
> > > > 
> > > > > @jnothman https://github.com/jnothman Should we reopen this
> > > > > issue
> > > > > and
> > > > > add a new section in the documentation? For example in this
> > > > > section:
> > > > > http://scikit-learn.org/stable/tutorial/basic/tutorial.html
> > > > > (""Loading
> > > > > your own data"").
> > > > > 
> > > > > —
> > > > > Reply to this email directly or view it on GitHub
> > > > > <
> > 
> > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60882069>
> > 
> > > > > .
> > > > 
> > > > —
> > > > Reply to this email directly or view it on GitHub
> > > > <
> > 
> > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60883212>
> > 
> > > > .
> > > 
> > > —
> > > Reply to this email directly or view it on GitHub
> > > <
> > 
> > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60899671>
> > 
> > > .
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60906075>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-60913843
> .
",2014-10-29T12:45:42Z,78827
2307,scikit-learn/scikit-learn,47049745,60924818,"You could also have a look at `np.genfromtxt` . Might be useful.
",2014-10-29T13:40:34Z,1867024
2308,scikit-learn/scikit-learn,47049745,60929308,"Hi  jnothman, 
Thank you so much for your help, I really appreciate your  cooperation.

I tried applying your code. Thus, once I interned (import pandas as pd). Directly I had the following message in red color:

import pandas as pd
 No module named 'dateutil'
Traceback (most recent call last):
  File ""<pyshell#0>"", line 1, in <module>
    import pandas as pd
  File ""C:\Python34\lib\site-packages\pandas__init__.py"", line 7, in <module>
    from . import hashtable, tslib, lib
  File ""pandas\tslib.pyx"", line 37, in init pandas.tslib (pandas\tslib.c:76813)
ImportError: No module named 'dateutil'

What should I do? 
Thanks a lot
",2014-10-29T14:08:48Z,9435135
2309,scikit-learn/scikit-learn,47049745,60930453,"It just means you do not have the dateutil module installed. You can install it by doing 

```
sudo apt-get install python-dateutil
```

hth
",2014-10-29T14:15:04Z,1867024
2310,scikit-learn/scikit-learn,47049745,60930829,"You can have a look at this for more details, http://stackoverflow.com/questions/20853474/importerror-no-module-named-dateutil-parser
",2014-10-29T14:17:06Z,1867024
2311,scikit-learn/scikit-learn,47049745,60949693,"Thanks MechCoder for your contribution. 

I tried ""sudo apt-get install python-dateutil"", but it is not clear to me at what stage should indicate  this code?
Do you think that there is an easy way to load my (excel or csv) file suing any simple ways such as open folder (regular way). There is another matter also which how to determine the class label that I want to predict form my dataset using scikit-learn. But anyway this step supposed to be after loading the file itself. Not easy process at all.

Is there any youtube tutorial about loading dataset (not iris which is everywhere or other famous. stuff). Video is easy than links  
",2014-10-29T15:55:28Z,9435135
2312,scikit-learn/scikit-learn,47049745,133601605,"HI all,
I wrote the following code:

import pandas as pd
data= pd.read_csv(open(home/maxinet/Desktop/1.csv))

and i got this error:
File ""<ipython-input-10-dd0ba70fe93f>"", line 2
    data= pd.read_csv(open(home/maxinet/Desktop/1.csv))
                                                                              ^
SyntaxError: invalid syntax

could you plz guide me.
",2015-08-22T00:33:11Z,358737
2313,scikit-learn/scikit-learn,47049745,133643708,"We could tell you what the problem is but I think in this case you will learn more from it if you find it on your own. You should read the error message carefully. It is a Python syntax error.

```
  File """", line 2
    data= pd.read_csv(open(home/maxinet/Desktop/1.csv))
                                                    ^
SyntaxError: invalid syntax
```
",2015-08-22T06:11:03Z,869592
2314,scikit-learn/scikit-learn,47049745,133650026,"On 22 Aug 2015 08:33, ""samira afzal"" notifications@github.com wrote:

> HI all,
> I wrote the following code:
> 
> import pandas as pd
> data= pd.read_csv(open(home/maxinet/Desktop/1.csv))
> 
> and i got this error:
> File """", line 2
> data= pd.read_csv(open(home/maxinet/Desktop/1.csv))
> ^
> SyntaxError: invalid syntax
> 
> could you plz guide me.

I recommend you finding another tool where you can work with easily without
headache yourself with this particular one. This is what I did myself.

Good luck
Martin

> —
> Reply to this email directly or view it on GitHub.
",2015-08-22T08:03:55Z,9435135
2315,scikit-learn/scikit-learn,47049745,133676255,"To be clear, these previous posters are saying that being somewhat
comfortable with the Python language is a prerequisite to using
scikit-learn. You have missed some quotes around a string. This shows great
unfamiliarity with Python (and a characteristic of most programming
languages), and scikit-learn is probably not the best place to start.

On 22 August 2015 at 18:04, MartinLion notifications@github.com wrote:

> On 22 Aug 2015 08:33, ""samira afzal"" notifications@github.com wrote:
> 
> > HI all,
> > I wrote the following code:
> > 
> > import pandas as pd
> > data= pd.read_csv(open(home/maxinet/Desktop/1.csv))
> > 
> > and i got this error:
> > File """", line 2
> > data= pd.read_csv(open(home/maxinet/Desktop/1.csv))
> > ^
> > SyntaxError: invalid syntax
> > 
> > could you plz guide me.
> 
> I recommend you finding another tool where you can work with easily without
> headache yourself with this particular one. This is what I did myself.
> 
> Good luck
> Martin
> 
> > —
> > Reply to this email directly or view it on GitHub.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-133650026
> .
",2015-08-22T10:33:01Z,78827
2316,scikit-learn/scikit-learn,47049745,160072705,"Just want to support @MartinLion --- I am a scikit-learn newbie and have just have spent a frustrating time going thought the docs, and I can't find anywhere how to read my own data (and not a prepared toy dataset), and what the python format of data is.  
",2015-11-27T08:45:51Z,5497303
2317,scikit-learn/scikit-learn,47049745,160106956,"Kindly refer - 
- [How do I load my data to work with scikit-learn?](http://stackoverflow.com/q/21492726/3109769)
- [How to load data from CSV file?](http://stackoverflow.com/q/11023411/3109769)
",2015-11-27T10:25:25Z,9487348
2318,scikit-learn/scikit-learn,47049745,160115306,">   • How do I load my data to work with scikit-learn?
>   • How to load data from CSV file?

We should add these in the FAQ.
",2015-11-27T11:05:12Z,208217
2319,scikit-learn/scikit-learn,47049745,160152881,"should we instead add as a section in the tutorial below/above [""Loading an example dataset""](http://scikit-learn.org/stable/tutorial/basic/tutorial.html#loading-an-example-dataset)?
",2015-11-27T14:43:55Z,9487348
2320,scikit-learn/scikit-learn,47049745,160153200,"Also could you tag this ""Question"", ""Documentation"" and reopen it?
",2015-11-27T14:46:10Z,9487348
2321,scikit-learn/scikit-learn,47049745,160153930,"> should we instead add as a section in the tutorial here?

We should reference it. But I don't see this as tutorial material because
it is outside the scope of scikit-learn. We can only give pointers

That's an answer that the users really don't want to hear, because there
point of view is that they have a lump of data and they want it inside
scikit-learn. The answer is: this is not a problem that scikit-learn
solves, go see pandas if you have CSV, scikit-image if you have images,
database connectors (SQLAlchemy?) if you work on databases...

I guess that we should have a sentence like this in the tutorial, where
you reference, with pointers.

As a side note, the kind of errors hit by the users on the thread of this
issue (lack of basic knowledge of Python for instance) tells me that we
cannot solve their problem. They need to go to entry-level tutorials on
Python, and get a bigger picture. Maybe we should make sure that we give
pointers to these in the right spots, eg early on in the tutorial.
",2015-11-27T14:50:54Z,208217
2322,scikit-learn/scikit-learn,47049745,160186970,"> > should we instead add as a section in the tutorial here?
> 
> We should reference it. But I don't see this as tutorial material because
> it is outside the scope of scikit-learn. We can only give pointers
> 
> That's an answer that the users really don't want to hear, because there
> point of view is that they have a lump of data and they want it inside
> scikit-learn. The answer is: this is not a problem that scikit-learn
> solves, go see pandas if you have CSV, scikit-image if you have images,
> database connectors (SQLAlchemy?) if you work on databases...
> 
> I guess that we should have a sentence like this in the tutorial, where
> you reference, with pointers.
> 
> As a side note, the kind of errors hit by the users on the thread of this
> issue (lack of basic knowledge of Python for instance) tells me that we
> cannot solve their problem. They need to go to entry-level tutorials on
> Python, and get a bigger picture. Maybe we should make sure that we give
> pointers to these in the right spots, eg early on in the tutorial.

Well, take it easy!!!

I don't know whether you are one of scikit-learn staff or not, but I need
to say that your way of talking is harming both scikit-learn staff and
users (us), due to the two reasons:

First reason, criticizing people (like what you did) and assuming that they
are novice in Python so they don't know how to work with scikit-learn,
means you or the staff are trying to blind their eyes to the truth that
scikit-learn staff are not able to create a clear tutorial to allow loading
the real data, at least. In addition, pretending that the tutorial of
scikit-learn is perfect in spite all the questions regarding loading the
real data (not the toy data as it is too easy to be imported comparing to
the real data) is something needs to be reconsidered, and this means that
scikit-learn staff don't care about the name of scikit-learn at all.

Second, we can understand from your unsuitable way of talking that you
already forgot that  scikit-learn is a product, and we as users are
customers, so either you or the staff of scikit-learn should respect all of
us and thank us for any comment or bug fixing. This is the professional way
of behavior. So I recommend you to think of your words before saying them. If
you are knowing the way of loading the real data and you'd like to help,
don't only say go see pandas, better you answer people's question nicely
rather than hurting them with your words, but if you're simply not able to
do that, so keep quite.

On the other hand, regarding the question ""should we instead add as a
section in the tutorial here?"", I would like to say ""_YES_"", you or
scikit-learn staff should add a section in the official tutorial about how
to load your own data either CSV, or ARFF or text or whatever, as users are
interested to load their own data, this is very critical issue should be
considered in the tutorial (not to be ignored). *If you rely on the user,
then what is your work? *

Nevertheless, for those who are still struggling with scikit-learn, I would
like to say, this is not the end of the world, and as I mentioned
previously, find another to tool make your life much easier. For this
reason, and in order to save your time, I would like to recommend some
tools to assist you in data mining procedures. For instance,  Waikato
Environment for Knowledge Analysis (WEKA),
http://www.cs.waikato.ac.nz/ml/weka/, last version is  WEKA 3-7-13, is a
collection of machine learning algorithms for data mining tasks. WEKA
allows you to use its schemes either from GUI or writing Java code, so its
very easy for non-programmers. Additional to WEKA, R is also an excellent
tool for data mining stuff, you can also perform tasks of R from WEKA or
vice versa. However, if you have a patience to design a prediction process
manually (drag/drop), RapidMiner is a great tool for this propose where you
can design a very nice flow to achieve your target.

Thanks David van Leeuwen for your support.

Good luck in your analysis.

Cheers,
Martin

> —
> Reply to this email directly or view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-160153930
> .
",2015-11-27T18:37:41Z,9435135
2323,scikit-learn/scikit-learn,47049745,160201906,"Hey Martin,

Kindly don't be offended.

He did not criticize :) He, being one of the top contributors to scikit-learn has to make tough decisions as to what will go into our codebase and what will not, as a more verbose documentation or tutorial might not be preferable for a lot of people. Gael has in fact contributed a lot of user guides himself to scikit learn to help users.

The reason why he was opposing that addition to the tutorial was that there are multitude of ways in which users have their data stored and such a user guide on how to get the input data from all of them (a text file/csv file/database/zipped archive), is indeed out of scope for scikit learn, which is a machine learning library.

The most important thing to note here is that **it is very clearly explained by documentations of libraries which handle data, like numpy or pandas.**

It is expected from the user that he or she knows this! Since it seems to not be very clear, he suggests that we add a FAQ, pointing the user to such userguides, which are more elaborate than we could possibly get :)

It may appear that our tutorial could be a bit more elaborate on how the inputs are obtained. But the thing, in general, with userguides is that, it could _always_ be a little bit more elaborate, which makes us set a hard limit on how detailed our userguides can get, to help contain the userguide in a maintainable format :) If you think from that perspective, you yourself would understand our situation.

As this issue is open someone will indeed send a PR soon adding a nice FAQ entry and an example, maybe, which could help clarify your (or any other new user's) doubts on input formats.

Cheers!
",2015-11-27T20:48:25Z,9487348
2324,scikit-learn/scikit-learn,47049745,160203689,"Hello @MartinLion ,

we understand your eagerness to solve your problem, and your frustration when it is not solved.

However, you seem quite misinformed about what is scikit-learn, how it works, and how the project is developed. Therefore, I would like to make some points clear for you. As you can see from
http://scikit-learn.org/stable/about.html, scikit-learn is a community effort that is developed by a team of volunteers, mostly on their free time. Gaël is one of the creators of the project and its current leader:
scikit-learn would certainly not be the same without his contribution (the same for other volunteers), and he certainly did not deserve your dismissive words.

What I would like to emphasize is that there is no such thing as a scikit-learn ""product"", or scikit-learn ""staff"" (only a handful of people have worked full time on the project). You mention ""we as users are
customers"", but how much are you paying for using scikit-learn? Despite the important development cost, users get scikit-learn for free (and of course that's how it's intended to be). In fact, the development of the project relies on a fragile alchemy: users' needs being a top priority for developers, and users reporting bugs and concerns in the most positive way. The kind of ""ranting"" that you wrote can be very discouraging for developers, who contribute their free time and their expertise just because they believe that scikit-learn is a useful tool for the community. Some prominent developers stopped contributing to open-source software precisely because of such ""customer-like attitude"" of a few people underlining only shortcomings, and dismissing the huge development efforts. Please try to see the bright side as well: you received advice and comments from a lot of people, I'm sure that there was something for you to learn out of it, even if it did not solve your problem. 

Also, although users' needs are indeed a top priority of scikit-learn (it has an amazing documentation, of which most scientific Python packages can be jealous!), each software addresses a well-targeted niche of users, and it is just normal that scikit-learn cannot fit all users. For example, it is preferable to use scikit-learn with already a good knowledge of Scientific Python. So, I'm really glad that you found a
package that suited your needs better, but please also acknowledge the time and good will that people gave away when answering you.

So, folks, let's all show some good will and keep a constructive dialog.
That's how the project we love will keep on rocking!
",2015-11-27T21:09:03Z,263366
2325,scikit-learn/scikit-learn,47049745,160226153,"> For this reason, and in order to save your time, I would like to recommend some tools to assist you in data mining procedures. For instance, Waikato Environment for Knowledge Analysis (WEKA), http://www.cs.waikato.ac.nz/ml/weka/, last version is WEKA 3-7-13, is a collection of machine learning algorithms for data mining tasks. WEKA allows you to use its schemes either from GUI or writing Java code, so its very easy for non-programmers. Additional to WEKA, R is also an excellent tool for data mining stuff, you can also perform tasks of R from WEKA or vice versa. However, if you have a patience to design a prediction process manually (drag/drop), RapidMiner is a great tool for this propose where you can design a very nice flow to achieve your target.

Maybe we should make clear that scikit-learn is a Python **library**. It does not have the same scope as WEKA or RapidMiner. It fits perfectly into the [scientific Python ecosystem](http://www.scipy-lectures.org/) but you should be willing to write code if you want to use it.
",2015-11-27T23:13:01Z,869592
2326,scikit-learn/scikit-learn,47049745,160267097,"Perhaps I should elaborate on my original frustration, to give you some context. 

I've been programming in Python almost exclusively for a year now (I am a late convert), and am fairly familiar with the ecosystem---I've done lot's of webservice related things, but also manipulation of resources related to automatic speech recognition.  I do my scientific work in [Julia](http://julialang.org/) since a couple of years, and before that, in R, octave, c++/c (some 30 years in total).  The Julia ecosystem is quite dynamic, and it is all very exciting, but Python just has this very large ecosystem and very clean coding, which makes it very attractive to use for little side experiments.  This time I had to do some topic classification of (single sentence) text documents. 

Now there is an abundant choice of language technology tools in Python, and I believe that via [lda](https://pythonhosted.org/lda/index.html) I got to scikit-learn.  Great tutorials, lovely datasets and all, but I found it very difficult to find out how to organize my own data so that I could load this in.  Just now, I browsed through the user guide again to find the docs for ""load_files"", but I could't find an entry.  So a google search for ""sklearn.datasets.load_files"" got me there just now, and I happened to remember the particular module path from more painstaking searches yesterday (it is mentioned somewhere in a tutorial).  For me, the essential information would have been: ""Organize your data one document per file, one directory per class""---more or less what's under the documentation for  [load_files](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html).   This all makes perfect sense, but I come from a community where usual formats are ""one datapoint per line"", often with the class label on that line.  But having said all this, I am pretty impressed how the Python (text) community has standardized data representation, from what I've seen so far.  But perhaps because of the widely used standard data representation, this aspect has naturally less attention in documentation. 

As a final note, whenever I try to teach students how to use some scientific tool set or another, I have to spend quite some time on ""how to import your data"".  Nobody likes to do it, it can be a lot of effort for what you potentially use only once, and is therefore always a difficult threshold. 
",2015-11-28T09:32:38Z,5497303
2327,scikit-learn/scikit-learn,47049745,160270773,"@davidavdav 

I agree that loading data is a difficult and important thing. However, it is a domain-specific problem. You have a particular type of data. I have another. My data is medical images of brain activity. I can tell you how I organize my data and load them. I can even tell you that we have written a whole package about this, with its own documentation. But that will probably not help you.

What you want is something that tells you how to organize and load _your_ data. Now, it may be that your data is something fairly classic, that many people have; for instance tabular data most often stored in CSV files. In which case there is a need for a package doing this data loading. I don't believe that it should be in scikit-learn. It needs to be in a package that is specialized for this data. For instance, we are not going to put image processing in scikit-learn. For tabular data, the dedicated package is pandas; as I mentioned in my reply we need to point to it. We, the scikit-learn team, want to make plugin pandas into scikit-learn easier. But it is not as easy as it may seem and it takes time (one of our core devs is prototyping something).

I realize rereading your post that your data is most likely text documents. So my two examples of data (medical images and tabular data) were both wrong :$. Maybe the documentation on processing text with scikit-learn could indeed be improved and touch a bit on data organization. I don't know, I very seldom process text. But if you want to do add a few words on this, you are most welcomed to do a pull request. Anyhow, this illustrate my point about the diversity of the data: this whole thread is mostly about loading CSV files, as can be seen from earlier comments (before the thread exploded into a rant). The important thing is not the ""CSV"", which is the container, but the data model that underlies a CSV file. This data model is that of columns of data with different nature. It's a very different data model than processing text documents.

And finally, you are unhappy that teaching people ""how to import your data"" is time consuming. I don't think that there is an easy fix for this, even in a specific domain. The reason being that data meaning (ie data semantics) is still very much an open area. It's intrinsically hard to describe what the data means and how it's organized. You can try a simple experience: grab a dataset from someone you don't know, about an experiment you don't know, and try understanding it. Not even loading it, just understanding it. I am sure that it will take time. What takes a human time tends to be very difficult for a computer.
",2015-11-28T09:58:25Z,208217
2328,scikit-learn/scikit-learn,47049745,246521003,"Hm I don't think we added pointers to the FAQ yet. It's certainly a FAQ.
",2016-09-12T22:52:26Z,449558
2329,scikit-learn/scikit-learn,47049745,250000102,"I wrote a short tutorial on how to get the dataset from a text format to a pandas DataFrame for use by sklearn here http://cis399-he.tumblr.com/post/151024047044/load-your-data-to-scikit-learn
",2016-09-27T21:11:30Z,11644691
2330,scikit-learn/scikit-learn,47049745,253608052,"fixed in #7516
",2016-10-13T19:05:06Z,449558
2331,scikit-learn/scikit-learn,47049745,257356922,"Well you could add some information about common errors that happen when loading own data. For example I have bumped on `Unknown label type: 'continuous-multioutput'` and not a hint, anywhere, what that could mean. I've tried passing:
- a numpy array created from csv reader
- a numpy array created from pandas
- regular array parsed line by line
- pandas dataframe
- a numpy array created from csv reader converted to regular array
- etc...

I'd recommend adding section summarizing assumptions about data, for example it was sheer luck that I found information that data sets may not contain NaN's Nulls etc. Also, what should be the parameters of a numpy array and how should pandas dataframe look like.
",2016-10-31T17:15:27Z,5961927
2332,scikit-learn/scikit-learn,47049745,257483426,"Perhaps that error message could be clearer, but I think passing regression targets to a classifier (as in your #7801) is a usage error nothing to do with loading your own dataset.
",2016-11-01T03:56:52Z,78827
2333,scikit-learn/scikit-learn,47049745,261758848,"@MartinLion @MartinLion  
Thank you to brought up this issue.
I am facing your problem now. I want to upload my IRIS like dataset. Which is 500 rows and 16 columns.
and
Thank you @chenhe95 ,
I have seen your tutorial titled ""Can I go to the bathroom"" , that is a real work , real help to users like us.

And Thank you all developers,
You worked free (or almost free) to deliver this library to people. I appreciate that. 

@MartinLion has spirit of learning, I praise that.
Actually stubbornness is a virtue in academic.
",2016-11-20T04:47:16Z,18692233
2334,scikit-learn/scikit-learn,47049745,261759656,"@tursunwali,

Thanks for the kind words.

Please let me know if you still have any issues with your data or learning
process.

Regards,
Martin

On 20 November 2016 at 12:47, tursunwali notifications@github.com wrote:

> @MartinLion https://github.com/MartinLion @MartinLion
> https://github.com/MartinLion
> 
> Thank you to brought up this issue.
> I am facing your problem now. I want to upload my IRIS like dataset. Which
> is 500 rows and 16 columns.
> and
> Thank you @chenhe95 https://github.com/chenhe95 ,
> I have seen your tutorial titled ""Can I go to the bathroom"" , that is a
> real work , real help to users like us.
> 
> And Thank you all developers,
> You worked free (or almost free) to deliver this library to people. I
> appreciate that.
> 
> @MartinLion https://github.com/MartinLion has spirit of learning, I
> praise that.
> Actually stubbornness is a virtue in academic.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/scikit-learn/scikit-learn/issues/3808#issuecomment-261758848,
> or mute the thread
> https://github.com/notifications/unsubscribe-auth/AI_3_7E0UU8Os42HXUC5n2eLVYW6hqe6ks5q_9DngaJpZM4Cz90f
> .
",2016-11-20T05:15:00Z,9435135
2335,pandas-dev/pandas,908088395,908088395,"I post this as a ""Question"" because I am quite new to `pandas`. So maybe I miss some understandings and the ""problem"" described by me is by design and you have good reasons for that.
I use pandas 1.2.4, with Python 3.9.4 on Windows 10 64 bit.

As a user I would expect that pandas check the number of fields per row when importing via csv file. But IMHO it does not in all cases.

# Example 1

Here is a csv file __without header__ and but a set `names=` attribute with three fields. So pandas should be able to know how many fields/columns should be in the CSV file. The second row contains 4 instead of 3 fields.

```
import pandas
import io

csv_without_header = io.StringIO(
'A;B;C\n'
'D;E;X;Y\n'
'F;G;H'
)

df = pandas.read_csv(csv_without_header, encoding='utf-8', sep=';',
                     header=None,
                     names=['First', 'Second', 'Third'])
```

Pandas import this without warnrings or errors. The 4th field in the 2nd row is simply ignored.

# Example 2

I added a header line into the csv file with again three fields.
So pandas should be able to know how many fields/columns should be in the CSV file.
And again the second row contains 4 instead of 3 fields.

```
csv_with_header = io.StringIO(
'First;Second;Third\n'
'A;B;C\n'
'D;E;X;Y\n'
'F;G;H'
)

df = pandas.read_csv(csv_with_header, encoding='utf-8', sep=';')
```

Here an error occurs as I expect.
`pandas.errors.ParserError: Error tokenizing data. C error: Expected 3 fields in line 3, saw 4` 

# Example 3

There are less then 3 fields in the 2nd row. Again here is no warning or error. The missing field is set with `NaN`. And here it does not matter if you give the number of (expected) fields via header line in the CSV or via `names=` attribute.

```
csv_with_header = io.StringIO(
'First;Second;Third\n'
'A;B;C\n'
'D;Y\n'
'F;G;H'
)

csv_without_header = io.StringIO(
'A;B;C\n'
'D;Y\n'
'F;G;H'
)

df_a = pandas.read_csv(csv_with_header, encoding='utf-8', sep=';')
df_b = pandas.read_csv(csv_without_header, encoding='utf-8', sep=';', names=['First', 'Second', 'Third'])
```

Want I want is to import CSV files and be informed if there are to many or less then the expected number of fields in any row.",2021-06-01T09:18:51Z,11861496
2336,pandas-dev/pandas,908088395,852051470,"To me this behaviour is quite consistent with what I would expect a CSV import to do. Having a value outside of the defined columns just doesn't make any sense, so the `ParserError` is appropriate here. However, having a `NaN` or missing value is logically sound in my opinion and happens more often than not. 

For example, if the contents of your CSV file were from some input, where you have to enter three fields, but the CSV contains a row with four columns, this means you have a bigger problem. If you have a row with only two columns and one `NaN` that just means that one input was left blank and you need to handle the data appropriately. Since this is a case-by-case basis (e.g. do you still consider this a valid datum, do you ignore, do you populate it with default values?), but the other case is almost always a problem with the file, I think this behaviour makes sense. 

In any way, checking for empty values after importing is always a good step, using - in your third example - `df_a.isnull()` or `df_a.isnull().sum`. ",2021-06-01T11:32:30Z,22656329
2337,pandas-dev/pandas,908088395,852386847,"This is a duplicate, please search the issue tracker,",2021-06-01T19:24:46Z,61934744
2338,pandas-dev/pandas,908088395,853073653,"It is impolite to assume that each user opening an issue is stupid and lazy.
Of course I search the issue tracker. I assume I used the wrong keywords.

But it is also impolite to point to other Issues without giving the links.
It is a feature of [HyperText](https://de.wikipedia.org/wiki/Hypertext) to link your words to content. ;)

If you want contributers to pandas you should work on your attitude.

And as a ""pandas member"" you should know how to use GitHubs Issue tracker.
if there is a duplicate Issue. Link to the origin Issue and close the duplicate.

Anything else is waste of ressources.",2021-06-02T14:25:15Z,11861496
2339,pandas-dev/pandas,908088395,853127909,@Codeberg-AsGithubAlternative-buhtz we have 3500 issue and *very* limited reviewers / volunteers. Please help us out here. ,2021-06-02T15:32:07Z,953992
2340,pandas-dev/pandas,908088395,853305183,"Hi @Codeberg-AsGithubAlternative-buhtz, 
1 is a bug and is related to #40333/#22144 and may be fixed by #40629
3 is not a bug. It is mentioned in the user guide. https://pandas.pydata.org/docs/user_guide/io.html#handling-bad-lines. 
Can you close the issue if I have answered your questions.
Thanks.",2021-06-02T18:53:27Z,47963215
2341,pandas-dev/pandas,908088395,853419669,"Thanks for the Issues and PR. I will monitor them.

For 3: Yes the (IMHO inconsistent) behavior is described in docs. But it does not make it right. There should be no difference between the handling of to few and to many columns. But it is just my opinion.
But because of this situation I can never trust pandas while importing CSV files. Before using pandas I have to do some pedantic checks by my own.",2021-06-02T22:17:51Z,11861496
2342,pandas-dev/pandas,908088395,853652854,"> Anything else is waste of ressources.

Given the sheer volume and quality of (voluntary!) work produced (see https://github.com/pandas-dev/pandas/commits?author=phofl for a start, and that's excluding reviews + issue triage), is demanding that they search the issue tracker for you really the best use of their resources?

> you should know how to use GitHubs Issue tracker

Such comments are unwelcome - you've been warned",2021-06-03T07:38:43Z,33491632
2343,pandas-dev/pandas,786108116,786108116,"#### Is your feature request related to a problem?

I seriously get the impression that Pandas is actively being sabotaged. As a case in point, `DataFrame.lookup` was deprecated in v1.2. The problems with this are:

1. The [doc for `DataFrame.lookup`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.lookup.html) says to see itself for an example. Well, there is no example in the doc page. The only example I'm aware of is [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#looking-up-values-by-index-column-labels) instead which is a different page.
2. The [changelog](https://pandas.pydata.org/docs/whatsnew/v1.2.0.html#deprecations) for this deprecation points to [GH18682](https://github.com/pandas-dev/pandas/pull/18682) which looks to be wholly irrelevant to this change. A lot has gone wrong here.
3. There is significant user code that already uses `lookup`. Why break it? If the current implementation of `lookup` is suboptimal, shouldn't it be optimized instead?
4. It is extremely complicated to use `melt` when `lookup` works quite simply. For example, compare [this simple answer using `lookup`](https://stackoverflow.com/a/45487394/) with [this complicated answer using `melt`](https://stackoverflow.com/a/65722008/).

Does nobody review changes, docs, and release notes anymore prior to the release? It looks this way.

#### Describe the solution you'd like

1. Optimize `lookup` if attainable using `melt` or otherwise.
1. Undeprecate `lookup`.",2021-01-14T15:59:31Z,566650
2344,pandas-dev/pandas,786108116,760303544,"> Does nobody review changes, docs, and release notes anymore prior to the release? It looks this way.

this is not a friendly way to ask for things.

`DataFrame.lookup` has seen NO love at all since its initial implemention. It is a duplicative and unmaintained function. This is also polluting the namespace and is not in any way integrated to all of the other indexers. We do not need N ways to do the same thing.",2021-01-14T16:21:55Z,953992
2345,pandas-dev/pandas,786108116,760304833,https://github.com/pandas-dev/pandas/pull/35224 is the PR and https://github.com/pandas-dev/pandas/issues/18262 is the issue,2021-01-14T16:23:58Z,953992
2346,pandas-dev/pandas,786108116,760305261,"@impredicative you are welcome to contribute patches, e.g. doc fixes or other things.",2021-01-14T16:24:38Z,953992
2347,pandas-dev/pandas,786108116,767160515,"@impredicative out of curiosity, what is exactly difficult about the [new proposed method](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#looking-up-values-by-index-column-labels), which is also significantly faster since `DataFrame.lookup` was a for-loop on the background. 

I agree that we can fix the documentation in the deprecated message and in the whatsnew though.

@jreback Would you be open to un-deprecate `DataFrame.lookup` and implement the `melt` + `loc` method on the background? I don't say it was an extremely popular method, but on SO I saw it being used sometimes.",2021-01-25T22:41:29Z,34067903
2348,pandas-dev/pandas,786108116,767163699,"@erfannariman What's difficult, for example, is how complicated [this answer using `melt`](https://stackoverflow.com/a/65722008/) is relative to how simple [this answer using `lookup`](https://stackoverflow.com/a/45487394/) is. The answer using `melt` requires three complicated method calls relative to the answer using `lookup` which is a single simple method call. In summary, just as you're saying, `DataFrame.lookup` could've been reimplemented using `melt`, thereby satisfying everyone.",2021-01-25T22:49:31Z,566650
2349,pandas-dev/pandas,786108116,767165019,"> @erfannariman What's difficult, for example, is how complicated [this answer using `melt`](https://stackoverflow.com/a/65722008/) is relative to how simple [this answer using `lookup`](https://stackoverflow.com/a/45487394/) is. The answer using `melt` requires three complicated method calls relative to the answer using `lookup` which is a single simple method call. In summary, just as you're saying, `DataFrame.lookup` could've been reimplemented using `melt`, thereby satisfying everyone.

I don't disagree with your point, although I don't think the proposed method is persé difficult, but I might be biased. I would not be against un-deprecating `DataFrame.lookup`. I will await @jreback response.",2021-01-25T22:53:08Z,34067903
2350,pandas-dev/pandas,786108116,767166129,"Pandas is used because it is believed to have a wealth of friendly methods. If this belief erodes, so will its user base.",2021-01-25T22:55:54Z,566650
2351,pandas-dev/pandas,786108116,767188415,"@impredicative there is almost no usage of lookup AFAICT. very very few issues / tests / SO entries. If you have a valid, real work very common then am happy to show a doc example / recipe. Having a method must be a high bar.",2021-01-25T23:55:03Z,953992
2352,pandas-dev/pandas,786108116,767191957,"@jreback A google.com search for `""df.lookup"" site:stackoverflow.com` lists 462 hits. That's more than for `df.bfill` (208) and `df.ffill` (375). Will you now be deprecating these two methods as well? What is your cutoff?",2021-01-26T00:05:02Z,566650
2353,pandas-dev/pandas,786108116,767200211,"@impredicative you are not comparing apples to apples here. try with indexing operators, we *already* have `.loc`, `.iloc`, `.at`, and `.iat`, not to mention partial slicing and of course `[]`. which one shall we cut to keep `.lookup`????

I in fact have an issue to remove `.iat` and `.at`. We simply do not need this much choice.",2021-01-26T00:28:55Z,953992
2354,pandas-dev/pandas,786108116,767627283,"@jreback I don't use `.iat` or `.at`.

In general, I would ask the question: ""_Is the alternative more simple or more difficult to use?_"" If a method simplifies code, making it less verbose and prevents users from reimplementing common patterns, then it's valuable, even if you personally think it's a duplicate, proportional to how well it accomplishes this goal.",2021-01-26T15:37:55Z,566650
2355,pandas-dev/pandas,786108116,768318988,"> @jreback I don't use `.iat` or `.at`.
> 
> In general, I would ask the question: ""_Is the alternative more simple or more difficult to use?_"" If a method simplifies code, making it less verbose and prevents users from reimplementing common patterns, then it's valuable, even if you personally think it's a duplicate, proportional to how well it accomplishes this goal.

this is balanced against an already huge api. There *are* alternatives from this. If you have  use case where this is especially awkward / burdensome I would encourage you to open an issue with a reproducible example and show why a new method makes sense.",2021-01-27T14:23:34Z,953992
2356,pandas-dev/pandas,786108116,772976219,"@erfannariman
> out of curiosity, what is exactly difficult about the [new proposed method](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#looking-up-values-by-index-column-labels), which is also significantly faster since `DataFrame.lookup` was a for-loop on the background.

I learned about the deprecation of `lookup` when answering [this SO question](https://stackoverflow.com/q/65892265/4238408). And [this](https://github.com/pandas-dev/pandas/pull/35224#discussion_r564046976) is my thought on the proposed alternative for `lookup`. Of course, if current `lookup` is a `for` loop, this method would be faster for small dataset. But for bigger dataset, things would change due to potentially larger memory allocation, especially when column names are long strings and data to be looked up are homogeneous.",2021-02-04T02:25:16Z,3301546
2357,pandas-dev/pandas,786108116,773144423,"> @erfannariman
> 
> > out of curiosity, what is exactly difficult about the [new proposed method](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#looking-up-values-by-index-column-labels), which is also significantly faster since `DataFrame.lookup` was a for-loop on the background.
> 
> I learned about the deprecation of `lookup` when answering [this SO question](https://stackoverflow.com/q/65892265/4238408). And [this](https://github.com/pandas-dev/pandas/pull/35224#discussion_r564046976) is my thought on the proposed alternative for `lookup`. Of course, if current `lookup` is a `for` loop, this method would be faster for small dataset. But for bigger dataset, things would change due to potentially larger memory allocation, especially when column names are long strings and data to be looked up are homogeneous.

Sure I would be happy to make a PR with a new proposed more efficient method and there we can discuss into more details and the core devs can share their thoughts as well. What would be your suggestion, the answer you gave on SO? Could you maybe share a reproducible example in a new ticket and mention this issue? @quanghm ",2021-02-04T08:58:21Z,34067903
2358,pandas-dev/pandas,786108116,773374707,"@erfannariman 

> out of curiosity, what is exactly difficult about the new proposed method, which is also significantly faster since DataFrame.lookup was a for-loop on the background.

I ran a quick test and the results said the reverse:

```
np.random.seed(43)
# also tested for n = 1000, 10_000, 100_000
n=1_000_000
cols = list('abcdef')
df = pd.DataFrame(np.random.randint(0, 10, size=(n,len(cols))), columns=cols)
df['col'] = np.random.choice(cols, n)

# the proposed method
%%timeit -n 10
melt = df.melt('col')
melt = melt.loc[lambda x: x['col']==x['variable'], 'value']
melt = melt.reset_index(drop=True)
# 623 ms ± 6.86 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

# What I would do without `lookup`
%%timeit -n 10
cols, idx = np.unique(df['col'], return_inverse=True)
df.reindex(cols,axis=1).to_numpy()[np.arange(df.shape[0]), idx]
# 430 ms ± 4.73 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

# Current lookup on 1.1.4
%%timeit -n 10
df.lookup(df.index, df['col'])
# 321 ms ± 1.55 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

The proposed method always comes last by a big margin in terms of speed. This only gets worse if the column names get longer.



",2021-02-04T15:02:23Z,3301546
2359,pandas-dev/pandas,786108116,773377986,"> lambda x: x['col']==x['variable']

Using lambdas is a non-starter. It shouldn't even be suggested unless it is the last option on earth.

> cols, idx = np.unique(df['col'], return_inverse=True)

`np.unique` obviously will support only a very limited number of Pandas datatypes, so it's probably a non-starter too. Numerical programming using Numpy is different from more generic dataframe programming using Pandas.",2021-02-04T15:07:06Z,566650
2360,pandas-dev/pandas,786108116,773387055,"@erfannariman 

Regarding the lambda function, that was originally an attempt to chain the three lines to detect possible speed improvement. Answer was not.

Using `.loc[melt['col']==melt['variable'],...]` wouldn't improve/change anything.

`np.unique` is used on the column names, so most likely will be strings. In general case, we can use instead, `pd.Series.unique()` and `pd.Series.factorize()` for that single line. But as far as I know, those two also returns `np.array`. 

Also, can you name some Pandas datatypes that wouldn't work with `np.unique`, especially those one would use as column names?

All those aside, the message here is that the proposed method is **slower** than current `lookup` method by factor of `2`.",2021-02-04T15:19:32Z,3301546
2361,pandas-dev/pandas,786108116,773407648,"Thanks for the extensive example and speedtests @quanghm . I did a quick check and seems like `lookup` wasn't a for loop actually, only for columns with mixed types. So the in terms of speed it looks okay. Only thing is that I remember it being quite slow for certain cases, but those must have been mixed type columns in my cases I think. See here for source code https://github.com/pandas-dev/pandas/blob/b5958ee1999e9aead1938c0bba2b674378807b3d/pandas/core/frame.py#L3848-L3861

For your approach, I agree that it's more efficient both in speed and memory allocation, but I remember writing multiple methods to replace `lookup` and I decided to go with melt since it was most ""readable"", without making it too complex for the documentation.

So I think it all boils down to @jreback and the argument that `DataFrame.lookup` has not been used as much, which is being discussed by @impredicative 
",2021-02-04T15:48:34Z,34067903
2362,pandas-dev/pandas,786108116,773420703,"@erfannariman Thanks. The code for `lookup` indeed reflects what I imagine, that is resolving to numpy advance indexing.

However, in the case that `col` is a column in the data (e.g. in my test or in the proposed solution) it's almost certainly that `self._is_mixed_type == True`. So you are right that it is just a for loop as of now. Still, it's faster than the proposed `melt` method.

On another note, this is certainly new for me

> `np.unique` obviously will support only a very limited number of Pandas datatypes

can you provide some details?
",2021-02-04T16:06:51Z,3301546
2363,pandas-dev/pandas,786108116,773477244,"> `np.unique` obviously will support only a very limited number of Pandas datatypes, so it's probably a non-starter too. Numerical programming using Numpy is different from more generic dataframe programming using Pandas.

I confirmed that `pd.factorize()` works and is much faster than `np.unique`, at least in my test above:

``` language=Python
%%timeit -n 10
idx, cols = pd.factorize(df['col'])
df.reindex(cols,axis=1).to_numpy()[np.arange(df.shape[0]), idx]
# 48.4 ms ± 716 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

",2021-02-04T17:27:05Z,3301546
2364,pandas-dev/pandas,786108116,778665287," I think it is premature to deprecate `lookup` function. At the same time, the documentation for `melt` function is too superficial. For example when I read the documentation ([1](https://pandas.pydata.org/docs/reference/api/pandas.melt.html), [2](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html?highlight=melt#looking-up-values-by-index-column-labels)) I do not understand how I need to port my code from `lookup` to `melt`.

I do simple things. Here is a simple example:
```python
>>> index = pd.Series(['2020-01-04', '2020-01-03'], index=['A', 'B'])
>>> df = pd.DataFrame({'A': [1,2,3], 'B': [4,5,6]}, index=['2020-01-03', '2020-01-04', '2020-01-05'])

>>> index
A    2020-01-04
B    2020-01-03
dtype: object

>>> df
            A  B
2020-01-03  1  4
2020-01-04  2  5
2020-01-05  3  6

>>> values = df.lookup(index.values, index.index)
>>> values
array([2, 4], dtype=int64)

>>> pd.Series(values, index=index.index)
A    2
B    4
dtype: int64
```

What should I do to use `melt` instead of `lookup`? And why can't I use high-level lookup API like before? Maybe I just stupid, but I do not want to think about it and dive into ornate concepts, I just want to implement the algorithm in high-level clean code. I want to keep my code simple and the code should clearly express my **intention**.

The code example from SO is tricky.
```
df['new_col'] = df.melt(id_vars='names', value_vars=['a', 'b'], ignore_index=False).query('names == variable').loc[df.index, 'value']
```

Ask yourself what **intention** it expresses. This is some kind of esoteric gibberish without reference to the subject area. The code is too low-level and verbose compared to the simple and concise `lookup(rows, cols)`.",2021-02-13T19:19:57Z,1299189
2365,pandas-dev/pandas,786108116,778936909,"> @impredicative you are not comparing apples to apples here. try with indexing operators, we _already_ have `.loc`, `.iloc`, `.at`, and `.iat`, not to mention partial slicing and of course `[]`. which one shall we cut to keep `.lookup`????

@jreback I think it's you that not comparing apples to apples here: **none** of those APIs you listed offer `lookup` functionality (otherwise, you wouldn't have used `melt` as an alternative). `lookup` is a direct translation of `arr[list_1, list_2]` from Numpy to Pandas. The Numpy API is there and it looks like it will stay for a while, why remove Pandas' equivalent.

The example that @espdev listed in his comment above is a more common case for the use of `lookup`. And from that example, I agree that it's really is unnatural to think `melt` at all, let alone using it.",2021-02-15T04:54:36Z,3301546
2366,pandas-dev/pandas,786108116,780707864,"First off, I can confirm I can reproduce the performance difference from https://github.com/pandas-dev/pandas/issues/39171#issuecomment-773374707, even without the lambda function (this is all on the v1.1.4 tag):

```ipython
In [58]: %%timeit -n 10
    ...: df.melt('col', ignore_index=False).query('col==variable')['value'].rein
    ...: dex(df.index).to_numpy()
    ...: 
    ...: 
671 ms ± 17.5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

In [59]: %%timeit -n 10
    ...: df.lookup(df.index, df['col'])
    ...: 
    ...: 

290 ms ± 2.79 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

Second, is there a case when the solution in https://github.com/pandas-dev/pandas/issues/39171#issuecomment-773477244 would not work well enough? If not, should the user guide just be updated to use that and we can move on from this issue?

Third - @impredicative please consider your tone",2021-02-17T17:10:24Z,33491632
2367,pandas-dev/pandas,786108116,780711627,"> Third - @impredicative please consider your tone

My tone is justifiable. I have had enough of Pandas being unprofessionally developed. Panda works only for simple manipulations of small dataframes, and scales extremely poorly or not at all. I have spent weeks and weeks trying to work around its various issues, and the best thing for me to do right now is to move to a real package like Dask, PySpark, etc. I'm sure that many other users here feel similarly.

Fourth - @MarcoGorelli please consider how Pandas is actually developed.",2021-02-17T17:15:53Z,566650
2368,pandas-dev/pandas,786108116,780712894,"If anyone wants to take this forward with a respectful tone (e.g. constructive comments like https://github.com/pandas-dev/pandas/issues/39171#issuecomment-773374707 ), then feel free to open a new issue - closing and locking as this is not the way to have a productive discussion and several comments have been off-topic",2021-02-17T17:17:51Z,33491632
2369,pandas-dev/pandas,786108116,780797556,thanks @MarcoGorelli ,2021-02-17T19:31:00Z,953992
2370,pandas-dev/pandas,628465400,628465400,"What's up with the release notes still missing for 1.0.4? It has been four days since the release. This is unprofessional.

https://github.com/pandas-dev/pandas/releases/tag/v1.0.4 links to https://pandas.pydata.org/docs/whatsnew/v1.0.4.html but the whatsnew page has been missing. Shouldn't it be made available no later than two hours after a release? People look for it and can't find it. A release shouldn't be published if its notes are not ready to go.

@simonjayhawkins you seem responsible.",2020-06-01T14:04:55Z,566650
2371,pandas-dev/pandas,628465400,636881444,"@impredicative check your tone please: things like ""This is unprofessional"" and ""you seem responsible"" are not kind.

See https://github.com/pandas-dev/pandas/issues/33300#issuecomment-635932128.",2020-06-01T14:07:14Z,1312546
2372,pandas-dev/pandas,628465400,636883137,try https://pandas.pydata.org/pandas-docs/version/1.0.4/whatsnew/v1.0.4.html in the meantime.,2020-06-01T14:10:28Z,13159005
2373,pandas-dev/pandas,628465400,636918517,"@TomAugspurger I am stating the facts, and you don't like the facts. Do you know what else is not kind? People wasting their time clicking on a link that doesn't work, waiting days, and it still doesn't work. It's unkind and it's a waste of time. This is a routine problem with Pandas releases. Also, why did you close this issue considering it's not resolved yet? Who is your manager? I want to report your dysfunction to them.",2020-06-01T15:18:50Z,566650
2374,pandas-dev/pandas,628465400,636920749,"> @TomAugspurger I am stating the facts, and you don't like the facts. Do you know what else is not kind? People wasting their time clicking on a link that doesn't work, waiting days, and it still doesn't work. It's a waste of time. This is a routine problem with Pandas releases. Also, why did you close this issue considering it's not resolved yet? Who is your manager? I want to report your dysfunction to them.

@impredicative 

pandas is an open source project, which is ALL VOLUNTEER. If you want to help great. Otherwise, please stop comments that are not helpful at all.",2020-06-01T15:23:05Z,953992
2375,diaspora/diaspora,369840190,369840190,"Hello,

what can I do if my templates are broken or something else in the asset pipe is missing?

I override some CSS things, imported it in the scss files and then i was figuring out how to get those assets in the system, i did following a few times:

RAILS_ENV=production bundle exec rails assets:clobber
RAILS_ENV=production bundle exec rails assets:clean
RAILS_ENV=production bundle exec rails assets:precompile

Then I also changed the logo on my pod (diaspora/app/assets/branding/logos)
But they all worked yesterday.

First thing which come up was that the branding/logos/asterisk_white_mobile.png was not found, but it was there, i changed something on templates and could not remember, so i put the original back in.

`[2018-10-13T15:28:15] INFO  PID-32105 TID-19394480 ActionController::Base: Processing by HomeController#show as HTML
[2018-10-13T15:28:15] INFO  PID-32105 TID-19394480 ActionController::Base: Completed 500 Internal Server Error in 89ms (ActiveRecord: 4.5ms)
[2018-10-13T15:28:15] FATAL PID-32105 TID-19394480 Rails:   
[2018-10-13T15:28:15] FATAL PID-32105 TID-19394480 Rails: ActionView::Template::Error (The asset ""branding/logos/asterisk_white_mobile.png"" is not present in the asset pipeline.):
[2018-10-13T15:28:15] FATAL PID-32105 TID-19394480 Rails:     14:                   .hidden-xs
    15:                     = link_to AppConfig.settings.pod_name, root_path, class: ""navbar-brand""
    16:                   .visible-xs-block.header-title
    17:                     = link_to(image_tag(""**branding/logos/asterisk_white_mobile.png**"", class: ""img-responsive""),
    18:                         root_path, class: ""navbar-brand"")
    19:                 = render ""layouts/header_not_connected""
[2018-10-13T15:28:15] FATAL PID-32105 TID-19394480 Rails:   
[2018-10-13T15:28:15] FATAL PID-32105 TID-19394480 Rails: app/views/layouts/_header.html.haml:17:in `_app_views_layouts__header_html_haml__3542251370305246389_89831620'
app/views/layouts/with_header.html.haml:2:in `block in _app_views_layouts_with_header_html_haml__4381373114498847661_97260960'
app/views/layouts/with_header.html.haml:1:in `_app_views_layouts_with_header_html_haml__4381373114498847661_97260960'
app/views/layouts/with_header_with_footer.html.haml:4:in `_app_views_layouts_with_header_with_footer_html_haml__1720277477106993151_92204540'
app/controllers/home_controller.rb:25:in `show'`

**Then it worked somehow I think and the shit was moving forward:**

`[2018-10-13T15:30:23] FATAL PID-32105 TID-19394480 Rails:   
[2018-10-13T15:30:23] FATAL PID-32105 TID-19394480 Rails: app/views/layouts/_header.html.haml:17:in `_app_views_layouts__header_html_haml__3542251370305246389_85561160'
app/views/layouts/with_header.html.haml:2:in `block in _app_views_layouts_with_header_html_haml__4381373114498847661_87208320'
app/views/layouts/with_header.html.haml:1:in `_app_views_layouts_with_header_html_haml__4381373114498847661_87208320'
app/views/layouts/with_header_with_footer.html.haml:4:in `_app_views_layouts_with_header_with_footer_html_haml__1720277477106993151_92204540'
app/controllers/home_controller.rb:25:in `show'
[2018-10-13T15:30:27] INFO  PID-32105 TID-19394480 Rails: Started GET ""/notifications?per_page=10&page=1&_=1539433227331"" for 2400:cb00:75:1024::a29e:ca94 at 2018-10-13 15:30:27 +0200
[2018-10-13T15:30:27] INFO  PID-32105 TID-19394480 ActionController::Base: Processing by NotificationsController#index as JSON
[2018-10-13T15:30:27] INFO  PID-32105 TID-19394480 ActionController::Base:   Parameters: {""per_page""=>""10"", ""page""=>""1"", ""_""=>""1539433227331""}
[2018-10-13T15:30:27] INFO  PID-32105 TID-19394480 ActionController::Base: Completed 500 Internal Server Error in 59ms (ActiveRecord: 8.7ms)
[2018-10-13T15:30:27] FATAL PID-32105 TID-19394480 Rails:   
[2018-10-13T15:30:27] FATAL PID-32105 TID-19394480 Rails: ActionView::Template::Error (The asset ""user/default.png"" is not present in the asset pipeline.):
[2018-10-13T15:30:27] FATAL PID-32105 TID-19394480 Rails:      9:         .aspect-membership-dropdown.placeholder{data: {person_id: note.target.id}}
    10: 
    11:   .media-object.pull-left
    12:     = person_image_link note.actors.first, :size => :thumb_small, :class => 'hovercardable'
    13:   .media-body
    14:     = notification_message_for(note)
    15:     %div
[2018-10-13T15:30:27] FATAL PID-32105 TID-19394480 Rails:   
[2018-10-13T15:30:27] FATAL PID-32105 TID-19394480 Rails: app/models/profile.rb:70:in `image_url'`

**Then the mobile.js was missing in the asset pipeline**

`[2018-10-13T15:31:43] INFO  PID-32105 TID-19394480 ActionController::Base: Completed 500 Internal Server Error in 476ms (ActiveRecord: 53.9ms)
[2018-10-13T15:31:43] FATAL PID-32105 TID-19394480 Rails:   
[2018-10-13T15:31:43] FATAL PID-32105 TID-19394480 Rails: ActionView::Template::Error (The asset ""mobile/mobile.js"" is not present in the asset pipeline.):
[2018-10-13T15:31:43] FATAL PID-32105 TID-19394480 Rails:      6: %html{lang: I18n.locale.to_s, dir: (rtl? ? ""rtl"" : ""ltr"")}
     7:   %head{prefix: og_prefix}
     8:     - content_for :javascript do
     9:       = javascript_include_tag ""mobile/mobile""
    10:       = load_javascript_locales
    11: 
    12:     = render ""head""
[2018-10-13T15:31:43] FATAL PID-32105 TID-19394480 Rails:   
[2018-10-13T15:31:43] FATAL PID-32105 TID-19394480 Rails: app/views/layouts/application.mobile.haml:9:in `block in _app_views_layouts_application_mobile_haml___4598889698582784532_59880980'
app/views/layouts/application.mobile.haml:8:in `_app_views_layouts_application_mobile_haml___4598889698582784532_59880980'
config/initializers/devise.rb:9:in `to_mobile'
app/controllers/tags_controller.rb:43:in `show'`

I tried to change the _header.mobile.haml and _header.haml to something like that and also tried to insert skip_pipeline, but syntax was wrong or it not worked, but it not worked.

`      = link_to(image_tag(""branding/logos/asterisk.png"", class: ""img-responsive"",  skip_pipeline: true),
`

What's the point?

Regards 001101",2018-10-13T20:53:02Z,8197408
2376,diaspora/diaspora,369840190,429575888,"Hello,

GitHub is not a support forum; it's an issue tracker. For support issues, please post in the [podmin support section](https://discourse.diasporafoundation.org/c/support/podmin-support) of our Discourse forum.",2018-10-13T21:15:40Z,2530168
2377,diaspora/diaspora,369840190,429576018,@goobertron You are wrong,2018-10-13T21:17:38Z,8197408
2378,diaspora/diaspora,369840190,429593929,"No, he is not. And since you already posted there, I will close this issue.",2018-10-14T03:40:11Z,344777
2379,diaspora/diaspora,369840190,431108382,"He is and I posted after he told me, postfactum century here we are!!

The main reason why Github is the first choice, cause no noob in the forum has a clue about the code these days, got one answer, which repeats what I already wrote, great support!

https://discourse.diasporafoundation.org/t/templates-failed-did-no-changes/1951",2018-10-18T18:10:12Z,8197408
2380,diaspora/diaspora,369840190,431117605,"Before you contribute, comment, or somehow interact with this project any further, I'd like you to read [our community guidelines](https://diasporafoundation.org/community_guidelines), as well as our [Code of Conduct](https://github.com/diaspora/diaspora/blob/develop/CODE_OF_CONDUCT.md). If you disagree with those rules and do not want to follow them, please stop interacting with this project.

When members of the project team, clearly indicated by a ""member"" badge in the header of all GitHub comments, ask you in a friendly tone do do something, then they do this because our project has rules, and everyone needs to follow them. In this case, GitHub is there to track software issues and track actionable feature requests. Support requests of any kind are handled on Discourse, as this allows more people to join. If you don't like this, well, too bad. This decision has been made by the majority of our community, and we for sure will not change our work just because you feel like it.",2018-10-18T18:39:09Z,344777
2381,arduino/Arduino,1317881905,1317881905,"VERY predictable output from random(), after being seeded with an incrementing counter.

Attached sketch shows how I'm getting four streams of almost perfectly sequential numbers from an algorithm that uses multiple calls to random() after using an incrementing counter as a seed via randomSeed(). It should not be possible to get anything orderly from that, especially over such an immediately short period.

The sketch (below) produces mac-addresses as my original script does, then has four columns: The first three columns show sequential numbers from the high-byte of the last octet, the fourth column shows sequential numbers from the low-byte of the last octet.

Right from the start, with seed 0xb53fdfdf:
```

// Random Seed:    0xb53fdfdf
     e2:fa:74:b3:a5:9c            9
     0e:9c:eb:97:7f:4c        4        c
     3a:3e:62:7a:59:fc    f         
     6a:e0:d9:5e:34:ac            a
     96:82:50:41:0e:5c        5     
     c2:24:c7:25:e8:0c    0         
     f2:c6:3e:08:c3:bc            b
     1e:68:b5:ec:9d:6c        6     
     4a:0a:2c:d0:77:1c    1         
     7a:ac:a3:b3:52:cc            c
     a6:4e:1a:97:2c:7c        7     
     d2:f0:91:7a:06:2c    2         
     02:92:08:5e:e1:dc            d
     2e:34:7f:42:bb:8d        8     
     5a:d6:f6:25:95:3d    3         
     8a:78:6d:09:70:ed            e
     b6:1a:e4:ec:4a:9d        9        d
     e2:bc:5a:d0:24:4d    4         
     12:5e:d1:b3:ff:fd            f
     3e:00:48:97:d9:ad        a     
     6a:a2:bf:7b:b3:5d    5         
     9a:44:36:5e:8e:0d            0
     c6:e6:ad:42:68:bd        b     
     f2:88:24:25:42:6d    6         
     22:2a:9b:09:1d:1d            1
     4e:cc:12:ec:f7:cd        c     
     7a:6e:89:d0:d1:7d    7         
     aa:10:00:b4:ac:2d            2
     d6:b2:77:97:86:de        d     
     02:54:ee:7b:60:8e    8         
     32:f6:65:5e:3b:3e            3
     5e:98:dc:42:15:ee        e        e
     8a:3a:53:25:f0:9e    9         
     b6:dc:ca:09:ca:4e            4
     e6:7e:41:ed:a4:fe        f     
     12:20:b8:d0:7f:ae    a         
     3e:c2:2e:b4:59:5e            5
     6e:64:a5:97:33:0e        0     
     9a:06:1c:7b:0e:be    b         
     c6:a8:93:5e:e8:6e            6
     f6:4a:0a:42:c2:1e        1     
     22:ec:81:26:9d:ce    c         
     4e:8e:f8:09:77:7f            7
     7e:30:6f:ed:51:2f        2     
     aa:d2:e6:d0:2c:df    d         
     d6:74:5d:b4:06:8f            8
     06:16:d4:97:e0:3f        3        f
     32:b8:4b:7b:bb:ef    e         
     5e:5a:c2:5f:95:9f            9
     8e:fc:39:42:6f:4f        4     
     ba:9e:b0:26:4a:ff    f         

```
That's seeded with 0x3ffe883c, and that output is near the beginning.

The first three columns to the right of the MAC addresses all show the high-byte of the last octet:
* The 1st column shows 0xf - 0xf, sequentially
* The 2nd column shows 0x4 - 0x4, sequentially
* The 3rd column shows 0x9 - 0x9, sequentially

The 4th column counts the low-byte of the last octet over a longer period, but the whole pattern repeats almost perfectly, and seemingly indefinitely, regardless of how it's seeded. Not the behaviour I expect from a PRNG!

Just looking at the ""random"" MAC addresses I was wanting, it's obvious that the low-byte of the last octet is repeating for 14-15 times (14.7, on average) and then incrementing. Again, should be 100% impossible to get this from any self-respecting PRNG being seeded by a counter.

nb the sketch has a delay() towards the bottom, so it scrolls up on a console at human-readable speed. For data-collection, just comment out the delay.

nb2 the random seed can be set manually.



sketch:
```
/*
  ===========================================

  buggy ""random()"" function, as observed here:
  https://github.com/atom-smasher/esp8266_beaconSpam

  this sketch starts with a random seed ""randomMacSeed""

  that seeds the prng via ""randomSeed()"" and an incrementing counter
  
  calls to ""random()"" are producing some very predictable outputs!

  this sketch shows the mac-addresses as they would be generated,
  and derived from those mac-addresses are four columns, all counting
  incrementally, with very few errors

  expected results: using an incrementing counter to seed the prng, i expect
  to NOT have predictable output like this

  ===========================================
*/


const uint64_t randomMacSeed = os_random();     // random seed on startup
//const uint64_t randomMacSeed = 0x1234abcd ;   // fixed seed; make it your own


uint32_t i = 0;
uint8_t  macAddr[5];


void mayhemMac(uint32_t ssidNum) {
  randomSeed(uint32_t((randomMacSeed) + (ssidNum)));
  macAddr[0] = uint8_t(random(0x0, 0x100)) & 0xfe | 0x02 ;
  macAddr[1] = uint8_t(random(0x0, 0x100));
  macAddr[2] = uint8_t(random(0x0, 0x100));
  macAddr[3] = uint8_t(random(0x0, 0x100));
  macAddr[4] = uint8_t(random(0x0, 0x100));
  macAddr[5] = uint8_t(random(0x0, 0x100));
}


void setup() {

  // start serial
  Serial.begin(115200);
  //Initialize serial and wait for port to open:
  while (!Serial) {
    delay(10);
  }
  // wait for serial port to connect
  delay(300);
  Serial.println();

  Serial.printf(""\n// Random Seed:    0x%x\n"",  uint64_t(randomMacSeed));

}

void loop() {

    for (i = 0; ; i++) {
      yield(); // needed for extra-large lists
      mayhemMac(i);
      Serial.printf(""     %02x:%02x:%02x:%02x:%02x:%02x"",
        macAddr[0], macAddr[1], macAddr[2], macAddr[3], macAddr[4], macAddr[5]);
        yield();

        if (0 == i % 3) {
          Serial.printf(""            %x"", macAddr[5] >> 0x4);
        }
        if (1 == i % 3) {
          Serial.printf(""        %x     "", macAddr[5] >> 0x4);
        }
        if (2 == i % 3) {
          Serial.printf(""    %x         "", macAddr[5] >> 0x4);
        }
        if (1 == i % 15) {
          Serial.printf(""   %x"", macAddr[5] & 0x0f );
        }

       Serial.println(); 
       delay(200);

       }
    }

```

Edited to correct how the seed is displayed, and share output with a verified seed.",2022-07-26T08:25:44Z,8199959
2382,arduino/Arduino,1317881905,1651978280,"I think they would have to incorporate a battery, I just read Arduino has a 58-day limit in buffer overflow with the millis().  I always wonder how that goes. I imagine three internal tickers, one starting at zero, the other at the negative most number, and they both go forward equal same increment rate, and one starts at the positive most spectrum, and goes backwards at half the rate, all three then are part of calculation to issue a millis(), and because millis is never more than a second, you need then to get the calendar clocking segments like second and weeks, whereas when the timer reaching overflow threat approaches, it begins in reverse and the prior reversing one picks up in rate, and the last most slows in rate, like washing or something like that never has a second, but you probably also then need to incorporate dates as a bar association reset of approaching is when?  In rate of millis() used to possible catch the buffer overflow and it's a big Y2K mess!  But I say that because I think these timer states would be flash saved when booted down, or at a last state type of mode, then booting up the random seeding is partial to the system is unique in back powered up.  IDK, though.  Tuff one.",2023-07-26T14:54:48Z,110750650
2383,arduino/Arduino,1317881905,1668789080,"I just read Arduino has a 58-day limit in buffer overflow with the millis().  I always wonder how that goes. I imagine three internal tickers, one starting at zero, the other at the negative most number, and they both go forward equal same increment rage, and one starts at the positive most spectrum, and goes backwards at half the rate, all three then are part of calculation to issue a millis(), and because millis is never more than a second, you need then to get the calendar clocking segments like second and weeks, whereas when the timer reaching overflow threat approaches, it begins in reverse and the pri9or reversing one picks up in rate.  Something like that never has a second however, and then pending you can wait out how many years to possible catch the buffer overflow and it's a big Y2K mess!  But I say that because I think these timer states would be flash saved when booted down, or at a last state type of mode, then booting up the random seeding is partial to the system is unique in back powered up.  IDK, though.  Tuff one.  You could ignore real time and develop in the calendar intervals, second, minute, day, week, month and year.  Just say all Arduino start at a date when powered and kept from the user interaction.

Actually, there is a better way.  Query the US court system for the jury selection method, and you'll be going over it in review.",2023-08-08T01:46:08Z,110750650
2384,arduino/Arduino,372256539,372256539,"This so good called ""Software"" named Arduino IDE is literally the worst Software i will ever encounter in my entire LIFE !

Its so [expletive deleted] buggy that it wont even ever [expletive deleted] WORK ON ANY Computer like my HomePC, in schools, so everywhere !

If you try to connect one [expletive deleted] Thing via the Right Com port, rates and so on it never EVER will start to upload the code to it even if its just the start code with setup and loop function.

So i ask myself why is ist possible that i try every Combination of any Setting i can in my lifespan and it will NEVER Recognize even A [expletive deleted] VOLTAGE thats applied to the Usb Port.

So if its possible to throm my eq outta the window and buy hole new pc and This nasty IDE works then i might change this comment to a word less rude if its not banned until then by some guys saying,

uhhhh youre just to bad at IT in a whole or dont understand anything and this other basic replies to a form of  
criticism i dont mind cause i know that this IDE wont Run for what so ever unknown broken Transistor in my CPU or so ever. so bye hope you dont have problems with that [expletive deleted] at all.

",2018-10-20T21:56:14Z,44328300
2385,arduino/Arduino,372256539,431635297,"This issue tracker is only to be used to report bugs or feature requests. You haven't provided any information that indicates a bug so I'm going to close this. This topic is more appropriate for the Arduino Forum. I'm sure we'll be able to help you with your problem over there.

Please do this:
1. Take some deep breaths and calm down. You won't get a good response on the forum with this sort of attitude. If you just wanted to vent, then mission accomplished. But if you're interested in getting a solution to your problem then you'll need to provide precise details.
1. Read http://forum.arduino.cc/index.php?topic=148850
2. If you haven't already done so, create a new thread in the appropriate section of the forum (http://forum.arduino.cc/), following all the rules. Be sure to post your code, using code tags (`</>` button on the toolbar).",2018-10-21T03:12:22Z,8572152
2386,arduino/Arduino,312953330,312953330,"With my aging eyes that orange color is increasingly hard to read against a black background, and there is no way to change it.  

Yes I can copy and paste it into another editor, but that shouldn't be necessary just to see a simple message.

This is bad for accessibility and hard on your users.  Please change to something more readable ",2018-04-10T14:38:39Z,645631
2387,arduino/Arduino,312953330,384956581,"I agree, e.g. light green (lime) would be much more easy to recognize!",2018-04-27T12:33:31Z,5814771
2388,arduino/Arduino,312953330,386092501,@jockm any thoughts on the colors used in the last screenshot of #3383 would be welcome. the theme works with 1.9.0-beta (just not the highlights),2018-05-02T19:26:35Z,1318077
2389,arduino/Arduino,312953330,386096296,"I like the highlights, I think the comment color is fine, if a little subtle.  I think the more traditional soft green would be better.  

However the error color is just as bad as it was before

---

Anything with a heavy component of red or blue is harder to read.  The human eye has the fewest cones (the elements of our eyes that see color) for red, followed by blue, and the most for green.  

So that orange — no matter the shade — is taxing to read, even for people with good eyesight.  Factor in people with vision disabilities, or even just people getting older; and any shade of orange is going to feel downright hostel

I mean the the app is short on accessibility as it is, every bit helps.

There is exactly nothing wrong with making the error text white.  It is completely clear that it is the error text from context; but most text is black on white, or white on black for a reason.",2018-05-02T19:40:08Z,645631
2390,arduino/Arduino,312953330,386207196,"@jockm thank you for the input. I [changed the colors](https://raw.githubusercontent.com/lmihalkovic/darkduino/master/sample.png)
",2018-05-03T07:03:18Z,1318077
2391,arduino/Arduino,312953330,386295248,"Here is the thing: the error text is still orange.  Aside from tradition there is no good reason for that, and clearly tradition doesn't matter considering the changes you made on the code area.

So please just make the error text white on black, or black on white.  The color coding on the file:line part is fine.",2018-05-03T13:28:06Z,645631
2392,arduino/Arduino,312953330,387025800,"Hello, thank you for your input. I have attached a picture with the change in error color from orange to white. If this is what you are asking for, I can make a pull request and see if it gets accepted. I can also show you how to change the color if you are running on command line/ terminal in case it does not get accepted. 
<img width=""500"" alt=""ffffff"" src=""https://user-images.githubusercontent.com/35776802/39697890-93a799c0-51c0-11e8-900d-a7c349fd1f90.png"">
",2018-05-07T10:34:19Z,10137
2393,arduino/Arduino,312953330,387040687,"I'm very much against changing error text color to white. It's important for errors and warnings to be visually distinct from the rest of the output. I'd hope there could be a compromise that can improve accessibility for people with visual disabilities without greatly worsening the user experience for everyone else.

As Alogitt said, it's quite easy for anyone to customize the Arduino IDE's theme to their individual preference. I have attempted to document all the theme settings, as well as provide a sketch for testing themes here:
https://github.com/per1234/ThemeTest/blob/master/README.md

Seeing as it doesn't satisfy @jockm, I wonder if there is actually a need for https://github.com/arduino/Arduino/pull/7551? Is there anyone who has trouble seeing the current red color who prefers the orange color used in that PR? I have no strong preference between red and orange.",2018-05-07T11:50:05Z,8572152
2394,arduino/Arduino,312953330,387046213,"@per1234 Personally, I believe the bright orange color is a bit easier to see against the black background than the original color while still conveying the warning that it is supposed to. I don't have a strong preference; just trying to help out.",2018-05-07T12:15:18Z,10137
2395,arduino/Arduino,312953330,387069446,"I think the white is perfect.

@per1234 The error text is 100% visually distinct in that screenshot.  It is below the orange bar, and has a different background color than the code.  It is distinct.

Additionally I concur that there is no need for #7551, if the goal of it is better readability.",2018-05-07T13:45:17Z,645631
2396,arduino/Arduino,312953330,387115710,"I disagree about orange or red - both is  invisible to me!
Please make it WHITE, YELLOW or LIGHT-GREEN (LIME) !

I agree with jockm (""The error text is 100% visually distinct in that screenshot. It is below the orange bar, and has a different background color than the code. It is distinct."")

Perhaps you can provide a setting in the preferences window about the text color.

IMO a pull request would not help, it must always be in the main branch for std Arduino downloads for all future releases.

",2018-05-07T16:07:43Z,5814771
2397,arduino/Arduino,312953330,387129455,I will switch to white in my own theme.. and leave the highlights the way they are now forgot the colors but they are ... high contrast),2018-05-07T16:52:54Z,1318077
2398,arduino/Arduino,312953330,387284925,"There's an objective way to measure where certain color changes are acceptable such as **[this tool](https://webaim.org/resources/contrastchecker/)**, which checks for W3C's [WCAG 2.0](https://www.w3.org/TR/WCAG20/) compliance in the contrast between a background color and a text color.

In the case of PR #7551, the brighter orange (`#FF8300`) proposed passes in all cases, and the darker orange (`#E34C00`) doesn't pass the WCAG AAA for normal texts case.

Arduino users have grown to instantly spot orange as error messages, and I think it's appropriate to keep it so as to not break UX and, IMO, it's is the best color for representing errors in a black background after red.

Just please don't make errors green or white, one would be REALLY counter-intuitive and the other, while easy to see once found, blends with all the other output printed in the log, which IIRC is also white.",2018-05-08T05:01:33Z,4926869
2399,arduino/Arduino,312953330,387288532,"@feikname That only checks color contrast, there are other issues with vision and color that just contrast.  Error messages are second only to the source in terms of the need for readability.  Can we err on the side of an abundance of caution here?

But I am going to argue with you that that white is somehow counter intuitive for error messages, or that the orange is deeply tied to your branding.  You are going to have to present some data to justify that argument.  I don't think most people would notice the change, except for finding the text easier to read.

Please refer to: http://www.ldau.org/ld-facts/guidelines-for-font-size-color/ ",2018-05-08T05:25:59Z,645631
2400,arduino/Arduino,312953330,387324962,">@per1234 The error text is 100% visually distinct in that screenshot. It is below the orange bar, and has a different background color than the code. It is distinct.

@jockm I wasn't talking about the code. I was talking about standard output in the console. It seems you are missing the fact that the console may display more than only warnings and errors. Standard output has always been printed in white text in the console. Warnings and error messages have always been printed in red. The difference in colors allows us to quickly notice the critical parts of the output. If you make standard output and error/warning text both white that means I need to read the entire output to find a warning or error message.
![clipboard02](https://user-images.githubusercontent.com/8572152/39745667-c35cfd08-525c-11e8-9c7b-aff661acc4e0.png)
>I am going to argue with you that that white is somehow counter intuitive for error messages

For years white text in the console has meant standard output so making error/warning text white is very much counterintuitive.

>I don't think most people would notice the change

What we'll be less likely to notice is helpful warning messages.",2018-05-08T08:24:24Z,8572152
2401,arduino/Arduino,312953330,387332308,"I strongly disagree with per1234.
Red or even orange on a notebook in a bright environment (e.g., outside) is not readable to my eyes, not even your screenshot above, and even in a darker environment it's hard!

Make the error messages WHITE 
- or at least YELLOW or LIGHT-GREEN (LIME) !
- or provide a setting in the preferences window about the text color so it can be easily customized!",2018-05-08T08:51:39Z,5814771
2402,arduino/Arduino,312953330,387344941,"@tofrnr you just repeated your previous comment. Please only comment if you have something new to contribute to the discussion.

Remember that this behavior of yours already caused another discussion to be locked:
https://github.com/arduino/Arduino/issues/5033#issuecomment-373317213",2018-05-08T09:37:16Z,8572152
2403,arduino/Arduino,312953330,387408018,"@per1234 No I knew exactly what you meant, and I am not against making error messages visually distinct.  I am against doing it with colors that can be difficult to see.

I am a 51 years old guy, with dyslexia and just the barest signs of macular degeneration and I cannot read the text in your screenshot without great difficulty. Make it more than a line or two and the only way I can cope is to copy the text and paste it into a text editor. I know I am not the only one from my own conversations.

That is a broken UX

By all means, make errors visually distinct.  You could use bold or italics, you could make the words ""Error"", ""Warning"", etc a different color (which I mentioned as a possibility previously), or you could use other forms or text decorations.  You have options other than simply changing the color.",2018-05-08T13:47:41Z,645631
2404,arduino/Arduino,312953330,387584485,Seems like making error texts **bold** and modifying the error color to the brighter orange could be a good compromise.,2018-05-09T00:37:20Z,4926869
2405,arduino/Arduino,312953330,387585496,"@feikname Bold might be a step in the right direction, but you still have the problems I outlined with red heavy colors.  

If we must have red/orange, why not put a ""ERROR:"" prefix to the message and make that red/orange, and then let the actual message be one of the two most readable colors we have (i.e. white).

Because in the end, if given the choice between branding or usability (in this case readability), I think we should pick the latter ever time",2018-05-09T00:44:02Z,645631
2406,arduino/Arduino,312953330,387639527,"> If we must have red/orange, why not put a ""ERROR:"" prefix to the message and make that red/orange, and then let the actual message be one of the two most readable colors we have (i.e. white).

I do not think this is a good idea. The text that is currently printed in a distinct color, is the text that was printed on the ""stderr"" output of subprocesses. Typically, this contains errors, but this is not necessarily so. For the compiler, it is also used for warnings. Avrdude even prints all of its verbose output to stderr. Prefixing each line with ""ERROR"" or something similar would probably only confuse users. More correct would be to prefix each line with ""stdout"" or ""stderr"", but that would still confuse novice users who have no clue what these terms mean.

The current approach of using a different color seems to bypass this problem by not explicitly stating what the colored text means, it just draws more attention to it.

As for using a different color to improve readability: That seems like a viable option to me. Of course, using a red or red-ish color for errors (and some other stuff, as pointed out above) seems common, which is probably why it is done now. Using e.g. green would also draw attention to the text, but I *think* users would not expect green text to contain error messages, so it might be better to consider other colors. @tofrnr also suggested a yellow color, which could work IMHO.

As for making things bold: I'm not sure if that would stand out sufficiently, but that might just need an experiment perhaps?",2018-05-09T06:56:33Z,194491
2407,arduino/Arduino,312953330,387744257,"> Prefixing each line with ""ERROR"" or something similar would probably only confuse users. More correct would be to prefix each line with ""stdout"" or ""stderr"", but that would still confuse novice users who have no clue what these terms mean.

What I was implying was that there would be some parsing of the text to inject ""Error:"" before error messages, and maybe ""Warning:"" before warning, etc.  I am sorry if I wasn't more clear.

> The current approach of using a different color seems to bypass this problem by not explicitly stating what the colored text means, it just draws more attention to it.

And yet I can point to people who are having trouble reading the error text, and have already pointed to two sources that say color is bad when the goal is readability — and error messages are already hard to read if in white.

We have no data on how many people care about colors vis-a-vis branding, have problems with them, or who do have problems with them.  Nor is there a good feedback mechanism for the average user to use.

Don't we want to make the Arduino IDE as easy to use for the greatest number of people as possible?  This includes making it as accessible as possible.  So until and unless there is a theme editor that is directly part of the Arduino IDE along with a set of themes; what is the harm in making errors _more_ readable?",2018-05-09T13:47:53Z,645631
2408,arduino/Arduino,312953330,387749315,"> What I was implying was that there would be some parsing of the text to inject ""Error:"" before error messages, and maybe ""Warning:"" before warning, etc. I am sorry if I wasn't more clear.

The problem there is that it is hard to reliably decide whether something is an error, warning, etc. Hardcoding regexes might work for (certain versions of) gcc, but third-party cores can also use different compilers and tools. Making these regexes configurable per core/board would be the correct approach to this, but also a *lot* more work.



> > The current approach of using a different color seems to bypass this problem by not explicitly stating what the colored text means, it just draws more attention to it.

> And yet I can point to people who are having trouble reading the error text, and have already pointed to two sources that say color is bad when the goal is readability — and error messages are already hard to read if in white.

I was not trying to say that the current approach is free of any problems, I'm just saying that using color to distinguish between stdout and stderr does not have the problem of labeling all stderr output explictly as ""error"".",2018-05-09T14:03:13Z,194491
2409,arduino/Arduino,312953330,387750607,">  I'm just saying that using color to distinguish between stdout and stderr does not have the problem of labeling all stderr output explictly as ""error"".

Then it could be something other than ""error"".  I mean it would be wrong to use ""stderr"", but it could be anything.  As an off of the top of my head suggestion ""Important:""?

You put that in some color, and then put the text to be read in white",2018-05-09T14:07:07Z,645631
2410,arduino/Arduino,312953330,427275339,"@jockm i totally agree with you that the color for errors of orange/red whatever  should be a more distinct color for those with difficulty in reading.
So i have put together some information on changing some parameters in theme.txt that will make the text in the IDE more easily read.

![errormessageinarduinoide](https://user-images.githubusercontent.com/2259579/46522320-cc823d00-c8c5-11e8-8330-0142ec204f54.png)

[changing_error_color.txt](https://github.com/arduino/Arduino/files/2449326/changing_error_color.txt)

 other information https://github.com/per1234/ThemeTest",2018-10-05T07:43:12Z,2259579
2411,arduino/Arduino,312953330,427296732,"# Using Zip files for themes


## Hexcode for colors
http://www.javascripter.net/faq/rgbtohex.htm
https://www.rapidtables.com/web/color/green-color.html

## File locations:

* on mac:
Arduino.app/Contents/Resources/Java/lib/theme/theme.txt 

* on Win7: 
C:\Program Files\Arduino\lib\theme\theme.txt

* on Linux: 
.../arduino-1.6.3/lib/theme/theme.txt

* on Windows 10: when installed as an App, C:\Program Files\WindowsApps\ArduinoLLC.ArduinoIDE_1.8.10.0_x86__mdqgnx93n4wtt\lib\theme\theme.txt, however it does not seem possible to edit it - it can be opened in notepad++ but a save fails.

## Procedure 

Using zip application create a zip file containing the original theme.txt found in the location of your operating system.
call it theme.txt.original.zip

Now modify the theme.txt file for colors, font sizes, etc
using zip application create a zip file containing the edited theme.txt found in the location of your operating system.
call it theme.txt.custom.zip

In the Arduino IDE goto the menu Arduino-->preferences under the option theme select theme.txt.custom.zip from the drop down list and restart Arduino IDE and the custom theme will take effect.

Copy the theme.txt.custom.zip file to a safe location so it will be available when there is an upgrade or reinstallation.

## Some suggested changes

changed console.font to 18
changed console.error.color to #7CFC00 - lawngreen

	# GUI - CONSOLE
	console.font = Monospaced,plain,11
	console.font.macosx = Monaco,plain,18
	console.color = #000000
	console.output.color = #eeeeee
	# original orange #E34C00
	# lime green #BFFF00 
	# lawngreen	#7CFC00
	console.error.color = #7CFC00

changed editor.comment2 to #434F54,bold - dark grey and bold

	# TEXT - COMMENTS
	editor.comment1.style = #434F54,bold
	editor.comment2.style = #434F54,bold

Increased linestatus.font to 14
increased linestatus.height to 30

	# LINE STATUS - editor line number status bar at the bottom of the screen
	linestatus.font	   = SansSerif,plain,14
	linestatus.height  = 30



you could also copy the attached file to the theme directory
[theme.txt_visually_improved pacav.zip](https://github.com/arduino/Arduino/files/2449671/theme.txt_visually_improved.pacav.zip)
",2018-10-05T09:06:50Z,2259579
2412,arduino/Arduino,312953330,427304856,"@pacav69 in Arduino IDE 1.8.6 and newer, themes can be placed in the `theme` subfolder of the sketchbook folder. You can find the location of the sketchbook folder at **File > Preferences > Sketchbook location**. This way, custom themes will persist through updates to a new IDE version. This should also make it possible to use custom themes with the Windows App Store version of the Arduino IDE:
https://github.com/arduino/Arduino/pull/7115

You can put multiple themes stored in .zip files in that folder and select which one you want from the **File > Preferences > Theme** menu. Note that the theme files must be directly under the .zip file, not in a subfolder of the .zip file:
https://github.com/arduino/Arduino/pull/7124",2018-10-05T09:37:56Z,8572152
2413,arduino/Arduino,312953330,809698831,"OK it has been two years now and as I slowly develop cataracts I can read everything in the arduino IDE and yet I cannot read the error color.  The idea that there is sufficient contrast or that it is accessible is just wrong.  Nor should I have to load a theme just to be able to see errors.

If you insist on keeping the error color there needs to be an option in the preferences dialog to *easily* change the color for those who find it unreadable, painful, etc ",2021-03-29T20:41:01Z,645631
2414,arduino/Arduino,312953330,1179623434,"I second most of the remarks above. Errors are unreadable, and this is really bad when you are with a laptop in a bright location. I always have to copy to notepad to see them.
It is not possible that in all these years you have not fixed this that is really the ONE SINGLE thing that is NOT OK in the standard interface.

It is really BAD that there is not a simple setting to change the error color (say to white - there is already the ORANGE bar when there are errors, maybe you may put one red square char at line start if you want to permanently mark in some ways the error, no matter the color the end user chooses).

I tried by the way to change theme.txt including
console.error.color = #ffdddd
but errors keep to be that £%£$ng orange!!",2022-07-09T23:58:07Z,32710073
2415,arduino/Arduino,312953330,1179624546,"It has been more than 4 years since I first brought this up and nothing has been done.  It is disheartening that the app remains unfriendly to people with visual processing issues for what seems like the sake of tradition
",2022-07-10T00:07:40Z,645631
2416,arduino/Arduino,312953330,1179625517,"and the change can be optional, I think this the only bad issue I've found till now.
Ok it's not a interface as sophisticated as Delphi, but it can work.
By the way I have finally been able to fix it the hard way: it looks that this damm Win10 blocks theme.txt modification even if you are admin and modify permissions (don't ask me why), so you copy theme.txt to another folder, edit it, delete the original, copy it back.
For me the file was in 
C:\Program Files (x86)\Arduino\lib\theme
(arduino installed today normally from the download)",2022-07-10T00:14:35Z,32710073
2417,arduino/Arduino,312953330,1179626662,"@aldoaldoaldo Agreed, if there was simply a ""visually impaired"", or ""high contrast"", etc option so you could *easily* and *simply* get to a mode for people who have a hard time with the existing color screen it would make this issue go away",2022-07-10T00:21:25Z,645631
2418,arduino/Arduino,312953330,1179865893,"Thanks all for your care and input about this important subject. I think the best way forward is to focus on the accessibility of the actively developed Arduino IDE 2.x project.

The discussion here got into implementation details which are specific to the Arduino IDE 1.x code base, and not relevant to Arduino IDE 2.x, which is a complete rewrite in a different programming language. So it will be best for you to create a new issue in the Arduino IDE 2.x's repository:

https://github.com/arduino/arduino-ide/issues",2022-07-11T01:37:55Z,8572152
2419,arduino/Arduino,312953330,1179867364,"@per1234  And how many more years must we wait for accessibility to be a
consideration in 2.0?  My problem is that this has been sat on long enough for you to say lets sit on it longer.  Does 2.0 have a release date? How long will 1.x be supported after 2.0 is out?  Why wait any longer for the simple option of a high contrast/colorless option for people with visual disabilities?

I do not say this lightly: it genuinely feels like you do not care, not in an active hostile way, but in the soft neglectful way that someone who doesn't actually understand the problem and thinks it is fine asking people underserved to wait even longer
",2022-07-11T01:40:07Z,645631
2420,arduino/Arduino,312953330,1179871071,"@jockm this sort of negative uncompromising attitude is counterproductive. If you only want to vent, do it somewhere else. If you actually want to work together to make this free open source project better, you need to find a different approach.",2022-07-11T01:45:32Z,8572152
2421,arduino/Arduino,312953330,1179872294,@per1234 and that is the kind of ablest rhetoric that is used when you don't understand the issue.  Do better,2022-07-11T01:47:19Z,645631
2422,rabbitmq/rabbitmq-server,1522369018,1522369018,"In the past there have been some issues and pull requests to introduce a feature to allow dead-lettering messages from temporary and exclusive queues. See for example #29, #271 and #1299. This is for example useful when a client crashes that uses a exclusive queue, so that messages in the exclusive queue are not lost but sent to a different queue instead.

Although the RabbitMQ team seemed back then positive towards the underlying principle of this feature and acknowledged the usefulness of it, the pull request was rejected because of some concerns about the possible performance penalty to publish all messages back. Also in later discussions this was brought up again as reason to not implement this, and users were advised to build a more complex topology with per-message ttls and/or other tools to achieve similar solutions to avoid message loss.

We often use per-client queues because it allows for a scalable software design. But we often indeed find ourselves writing crontab scripts to scan and clean up abandoned queues, or we use complex algorithms with per-message TTL's and messages that are being pumped around to prevent message loss. An out-of-the-box RabbitMQ feature to create a temporary exclusive queue from which messages cannot be lost would be so much more helpful.

Is this still a feature that you have on your backlog?
",2023-01-06T10:50:20Z,3725068
2423,rabbitmq/rabbitmq-server,1522369018,1373574769,"There is nothing specific about dead lettering of exclusive queues.

The behavior you are referring to has a simple explanation: dead lettering is not performed when a queue is deleted. There is a good reason for this, discussed numerous times over the years: dead lettering of millions of messages can put significant stress on the cluster when the user least expects it to, so at best this feature must be used with a length limit.

But specifically with exclusive queues, you are explicitly opting in for having your queues removed when the client disconnects for any reason. It makes no sense to ask for a feature that would help you preserve data when you opt in for the most transient queue type there is.

Just use a durable queue.",2023-01-06T12:41:21Z,1090
2424,rabbitmq/rabbitmq-server,1522369018,1373580579,"Someone has suggested that if your goal is to have a single consumer on a queue with redundancy, there are [Single Active Consumer](https://rabbitmq.com/consumers.html#single-active-consumer) and exclusive consumer flag on `basic.consume`. The former is a much more convenient option to achieve that.",2023-01-06T12:48:50Z,1090
2425,rabbitmq/rabbitmq-server,1522369018,1374502252,"Hello @michaelklishin, I don't think that the Single Active Consumer technology is a solution for this problem. Nor is ""just use a durable queue"".

I was writing a use case with auto-scalers and Kubernetes to explain why this feature is necessary and why the number of queues can scale, and that new clients and queues might be created on the fly and that old queues might become abandoned, and that there are cases when people want to inspect or preserve the contents of such abandoned queues, etc. But to be brutally honest here: I am pretty sure that you understand the background of this feature request, and that you don't need this explanation and that you are aware that users use TTL tricks and/or REST API workarounds to deal with abandoned queues or with lost client connections as alternative for dynamic, auto-delete and/or per-client queues. I also think that it is not that hard for you and your team to come up with a better ""out-of-the-box"" solution for this, _for example_ by supporting dead-lettering - but if you have an alternative/better way to create reliable dynamic, auto-delete or per-client queues, that would be just as welcome.

The ""it doesn't scale with large queues"" argument sounds a bit odd to me. That why we're programmers, aren't we, to solve such things? This is already an issue if you add a policy with a max-queue-length and dead-letter-exchange to a large queue, so it sooner or later requires attention anyway.

Point is that we miss this ability to create dynamic reliable queues in RabbitMQ.",2023-01-07T14:40:55Z,3725068
2426,rabbitmq/rabbitmq-server,1522369018,1374872422,"The discussion in #29 does a good job enumerating the issues with such a feature.

I can guarantee that if this feature were implemented users would immediately expect thousands of messages from thousands of transient queues to magically dead-letter with no performance impact.",2023-01-08T16:16:00Z,514926
2427,rabbitmq/rabbitmq-server,1522369018,1374879604,"Of course people want this feature to perform!

But honestly, is that a show-stopper here? From a programming perspective I would assume that the data is already in an internal data structure, and that a couple of pointers and references are updated to link the data to a different queue. This feature can indeed have a massively fast implementation, because everything stays _within_ RabbitMQ. From a user perspective I would indeed be surprised if this was giving performance issues.

But then again, as a user the implementation is not my concern. Nor is this 'dead-letter-exchange' feature per sé. The real point is that I would like to have an elegant way to create scalable reliable queues, and that we can inspect or reschedule messages that end up in abandoned queues. If you have a tip on how to do this, please share it with me. My current approach with crontabs and REST API calls, or by pumping TTL'ed messages around feels conceptually wrong.",2023-01-08T16:53:10Z,3725068
2428,rabbitmq/rabbitmq-server,1522369018,1374951134,">  From a programming perspective I would assume that the data is already in an internal data structure, and that a couple of pointers and references are updated to link the data to a different queue.

No, that's not how queues are modeled nor how the Erlang VM works. There is a _chance_ that now that v2 classic queues exist this problem may be more reliably solved, because v2 queues always keep their messages on-disk, but that's just me speculating.

> The real point is that I would like to have an elegant way to create scalable reliable queues

Like @michaelklishin said, use durable queues. Yes there is no out-of-the-box solution for transient queues that don't lose messages and you will have to be creative.

AFAIK we just haven't gotten enough requests for this feature (none from _paying_ customers) to implement it.

cc @edbyford @dcarwin-pivotal @MirahImage ",2023-01-08T23:00:55Z,514926
2429,rabbitmq/rabbitmq-server,1522369018,1375362891,"Whilst it is true that many of the people maintaining RabbitMQ are programmers they are also product maintainers which means we try very hard _not_ to dump effort into solving every problem users run into with code. In fact I'd say not taking on  too many features is the main responsibility of a maintainer of any product.

Despite @EmielBruijntjes claims that the problem is well understood I am not so sure it is so I am going to try to re-iterate as far as I understand it:

* Some systems need to dynamically create queues representing a ""session"" of some bounded length of time. In this case the queue itself is a physical representation of the client session and should exist for as long as the client session exists.
* At some point the session is finished or abandoned and the session state (the queue) needs to be cleaned up (deleted).
* If at this point there are unconsumed messages in the queue this should be noted and messages may need to be archived off for off-line inspection.

Does the above sound reasonable? I shall assume it does.

A couple of observations from the above:

* Exclusive and/or transient queues are not suitable for this kind of session queues. Their lifetimes are linked to specific broker events which are not the same as a client session termination event. E.g. a connection can be lost due to networks issues. All transient queues are deleted if a RabbitMQ node crashes or is restarted for example. 

Hence queues and messages for this use case _need to be durable_.

* The broker cannot possibly deduce when the client session is finished. The broker is a generic piece of infrastructure and doesn't contain application specific logic so this decision needs to be an application concern. I.e. when a client application is finished with the session they should delete the session queue.
* Abandoned queues (where the client application either failed to complete it's work or terminated before it could clean up) can be cleaned up by setting (reasonably long) queue TTLs on the session queues. I know of a few systems that do exactly this. This is a crude mechanism but for a certain subset of client session types it may be good enough.
* Queue TTLs do not solve the archiving feature.


Going back to adding additional features that use dead lettering I could imagine a feature that allowed any messages left in the queue when the Queue TTL timer fires to be dead lettered. If the target queue was a stream I'm sure it could absorb the load even if there were many messages on the queue at the time. I would _only_ allow this at point of Queue TTL expiry, _not_ when exclusive or transient queues are deleted or any explicit deletion request is issued. 
",2023-01-09T10:02:05Z,1180564
2430,rabbitmq/rabbitmq-server,1522369018,1375526454,"Hi @kjnilsson, 

Yes, your initial summary is essentially correct. One point of feedback though: this is not only for archiving and off-line inspection, but also for _automatic_ processing. Ideally abandoned messages can be picked up via the standard AMQP protocol.

It is good to know what you wrote about transient queues and crashing RabbitMQ nodes, because we indeed do not want to lose messages in such cases. However, as a user we of course do not _explicitly_ create single-node queues. The flags that we pass are called ""durable"", ""exclusive"" or ""auto-delete"" - the fact that such queues live on a single node or not is, from our user perspective, an implementation detail that we do not explicitly ask for or care about when we use the flags. In fact:

- We set up a RabbitMQ cluster because we need a reliable queuing platform.
- We create durable queues when we do not want to send a ""declare-queue"" frame when we make a connection to RabbitMQ.
- We create an exclusive queue when we will be the only user of the queue.
- We create an auto-delete queue when the queue can be removed when all consumers are done with it.
- We set up dead-lettering because we don't want to lose messages.

It is then, I think, up to RabbitMQ to choose the most suitable storage implementation. For some queues this could mean that they indeed only live on a single node, but other queues (for example when there is some form of dead-lettering) might need a distributed implementation.

But anyway, if it makes the implementation on your side easier, I would already be helped if dead-lettering was enabled for queue TTL's. And thanks for your contribution, also to @lukebakken, I appreciate your feedback and I realize that sometimes things just don't have priority or that you lack the time to pick up every feature request.",2023-01-09T12:05:17Z,3725068
2431,rabbitmq/rabbitmq-server,1522369018,1375554660,"Unfortunately as operators you do have to think about the durability and availability of the different queue types and options. Arguably it is too complex which I'm sure you'll be pleased to know we've deprecated transient queues (which are linked to the lifetime of a the node they're hosted on) so that that choice will from RabbitMQ 4.0 no longer be one users have to make. Also we've deprecated mirrored queues so there is no complex replication config to get right when using classic queues. 
When using quorum queues replication and thus HA is automatically set up but you do need to choose to use this queue type at declaration. 

Dead lettering isn't a good approach for not losing messages. The best option for not losing data is using a quorum queue with publisher confirms. All messages are implicitly durable when using a quorum queue.

Exclusive queues are not deprecated as they do have a lifetime bounds that you can program against. That said they are not a good option either for messages that cannot be lost.

Dead lettering before queue TTL queue deletion is a possible feature but it does add a lot of complexity. For example if some target dead letter queues aren't confirming the messages that are dead lettered should the queue still be deleted? and if so after how long? There are decisions there that ultimately would need to be surfaced to the users, either as config options or documentation.

Considering it I feel perhaps it would be better to allow applications to easier detect potentially abandoned queues so that they could do the right thing for the application rather than making the broker more complex.",2023-01-09T12:28:01Z,1180564
2432,rabbitmq/rabbitmq-server,1522369018,1376255508,"Do you mean an AMQP protocol extension that allows applications to read out queues/exchanges/etc? Like a 'list-queues' frame or so? That could indeed be helpful and is a step forward compared to using the REST API to detect abandoned queues.

Alternatively it could be an idea to have a built-in special ""amq.rabbitmq.meta"" exchange to which all sorts of meta-messages are published, like when a queue is declared, a consumer starts or a consumer stops, et cetera. That would allow applications to detect when a queue is abandoned by subscribing to the appropriate meta-events. That would also allow external applications to implement an _external_ dead-lettering mechanism.

Both options are helpful. But it still feels like a workaround for something that should imho be offered out of the box. Ideally, I would like to ask RabbitMQ ""Hey, can you create some queues for me, and promise me to not lose my messages when something bad happens?"" Because ""not-losing-data"" is for, I think, many users an important reason why they use RabbitMQ.",2023-01-09T20:14:26Z,3725068
2433,rabbitmq/rabbitmq-server,1522369018,1376312821,"Such a meta exchange already exists: https://www.rabbitmq.com/event-exchange.html

As for your last comment, we do our best to not lose messages. It's just that you explicitly declare ""autodelete and temporary queues"" so you turn some mechanisms off.",2023-01-09T21:05:13Z,9566114
2434,rabbitmq/rabbitmq-server,1522369018,1376315594,"> Hey, can you create some queues for me, and promise me to not lose my messages when something bad happens?

Here's what you need to do:
* Use a cluster of 3 or 5 nodes
* Declare a quorum queue
* Ensure you use publisher confirms correctly

That's it.",2023-01-09T21:07:11Z,514926
2435,rabbitmq/rabbitmq-server,1522369018,1376867551,"Passive-aggressive is the culture here, isn't it? Deliberately misinterpreting. That is not good. It is a bit tiresome to always run into these type of responses.

Anyway, this topic was about all sorts of dynamic queues (like with ttl, auto-delete, exclusive). For use-cases where one needs to create and delete queues on the fly, and where you still don't want to lose data when something bad happens. For example for auto-scaled Kubernetes pods. And many other use cases.

I am not a Erlang/RabbitMQ programmer. I don't know how queues are implemented. Don't expect me to get into a discussion about how queues work under the hood and why or why not this is or is not possible. As a programmer with a C++ background however, it seems to be a normal requirement that can be implement without too much of a performance penalty.

Many people have jumped into this conversation already, so I think that I have indeed addressed something. But at the same time I do not have the impression that the willingness exists to fix this. Maybe for business reasons or maybe because of personal pride? I don't know. For the time being we, and other RabbitMQ users, will have to proceed with implementing dirty tricks and workarounds to achieve this.",2023-01-10T08:02:46Z,3725068
2436,rabbitmq/rabbitmq-server,1522369018,1376873659,"> Such a meta exchange already exists: https://www.rabbitmq.com/event-exchange.html
> 

Thanks @mkuratczyk, I didn't know this.",2023-01-10T08:07:53Z,3725068
2437,rabbitmq/rabbitmq-server,1522369018,1376983663,"@EmielBruijntjes you keep asking the same question (not to lose messages) expecting a different answer and I understand it is frustrating when you don't get what you are after. @lukebakken 's reply is correct that _is_ how you use RabbitMQ if you don't want to lose messages. (You could also use a stream which has similar properties but is non-destructive) In fact Quorum Queues have _very_ good data safety properties as well as good availability so is a good option for many use cases.

So let's start with that and then see what else you might need for your specific use case (e.g. some means of efficiently detecting abandoned queues). If you insist on starting at a different point, say using exclusive or transients queues, then I don't think we can help you any further as we have no plans to modify their data safety properties and I have already explained why dead lettering at queue deletion for exclusive queues will not work for all scenarios (such as node crashes). As I already mentioned transient queues are deprecated and exclusive queues have very specific lifetime properties and should be use accordingly.
",2023-01-10T09:43:00Z,1180564
2438,rabbitmq/rabbitmq-server,1522369018,1377236008,"@EmielBruijntjes we have provided you with an explanation of why what you are asking for does not necessarily make sense and that there are other features or combinations of features that are already present and do what you want.

We have also warned you that transient entities in RabbitMQ are going away with 4.0 (later this year) and architect in around them today would not be a good idea, same for investing in new features specific to transient queues.

It is very very likely that you do not and never have paid for RabbitMQ yet you feel incredibly privileged and seemingly assume that the core team *owes* you something. That if you ask for something, we *must* implement it. That's not the case at all.

If you know *exactly* what you need better than the core team (who's been answering user questions for 5, 6, 10+ years in some cases), go ahead and build it yourself, RabbitMQ is open source software after all, with a pretty permissive license. Or hire someone who would do that.

Continue with this demanding, disrespectful tone and I will block you from ever filing an issue for any repo in this org again.",2023-01-10T13:11:04Z,1090
2439,Leaflet/Leaflet,896726233,896726233,"**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]



Your library is included in complicated code and you expect people to submit samples that are 10 lines and work in jsfiddle. 

This is like submitting a libc or openssl bug without an strace. 



**Describe the solution you'd like**
A clear and concise description of what you want to happen.



A library wide option to enable or disable tracing output in the console. 

As JS is billed as asynchronous you should output each function call, it's parameters and the start and end time at the highest resolution possible. 

This would stop all of the frustration you have with requiring code samples and improve moral for your obviously angry issue handlers. 



**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.",2021-05-20T12:08:13Z,741705
2440,Leaflet/Leaflet,896726233,845065469,"https://developer.mozilla.org/en-US/docs/Web/API/Performance/now

Unlike other timing data available to JavaScript (for example Date.now), the timestamps returned by performance.now() are not limited to one-millisecond resolution. Instead, they represent times as floating-point numbers with up to microsecond precision.",2021-05-20T12:32:53Z,741705
2441,Leaflet/Leaflet,896726233,848280168,"> [...] your obviously angry issue handlers.

Antagonizing the maintainers won't help.",2021-05-25T21:31:08Z,1125786
2442,Leaflet/Leaflet,894598837,894598837,"`map.panTo()` within bounds that are already shown does not center the map on the point sent to `map.panTo()`.

1.7.1",2021-05-18T17:09:04Z,741705
2443,Leaflet/Leaflet,894598837,843559866,[Works for me](https://plnkr.co/edit/mgmTP9xh7dBfMJUF).,2021-05-18T21:00:31Z,1125786
2444,Leaflet/Leaflet,894598837,843570180,"Then it doesn’t wait for a callback from a dragged marker that uses a custom svg icon. 

A call to map.panTo at the end of the dragend event for the marker does not work. 



Thank You,
Andrew Hodel

> On May 18, 2021, at 4:00 PM, Iván Sánchez Ortega ***@***.***> wrote:
> 
> ﻿
> Works for me.
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or unsubscribe.
",2021-05-18T21:18:15Z,741705
2445,Leaflet/Leaflet,894598837,843582588,"> A call to map.panTo at the end of the dragend event for the marker does not work.

[Yes, it does work](https://plnkr.co/edit/SOWK7sV3uvK2rBOX?preview).",2021-05-18T21:41:21Z,1125786
2446,Leaflet/Leaflet,894598837,843623431,"@IvanSanchez 

No it does not:

```
dragend event parameter 0 
{distance: 119.02939870817022, type: ""dragend"", target: i, sourceTarget: i}
distance: 119.02939870817022
sourceTarget: i
options: {}
_animRequest: 211
_dragStartTarget: img.leaflet-marker-icon.leaflet-zoom-animated.leaflet-interactive.leaflet-marker-draggable
_element: img.leaflet-marker-icon.leaflet-zoom-animated.leaflet-interactive.leaflet-marker-draggable
_enabled: true
_events: {dragstart: Array(1), predrag: Array(1), drag: Array(1), dragend: Array(1)}
_firingCount: 0
_initHooksCalled: true
_lastEvent: PointerEvent {isTrusted: true, touches: Array(1), changedTouches: Array(1), pointerId: 1, width: 1, …}
_lastTarget: null
_leaflet_id: 119
_moved: true
_moving: false
_newPos: k {x: 139.36859130859375, y: 382.805419921875}
_parentScale: {x: 1, y: 1, boundingClientRect: DOMRect}
_preventOutline: true
_startPoint: k {x: 538.1398315429688, y: 221.37399291992188}
_startPos: k {x: 132.529052734375, y: 501.6381530761719}
__proto__: i
target: i {options: {…}, _latlng: D, _initHooksCalled: true, _leaflet_id: 117, _mapToAdd: i, …}
type: ""dragend""
__proto__: Object







map_pins.js:229 calling function ƒ (marker) {
									//console.log('pin was dragged to', marker.latLng.lat(), marker.latLng.lng());

									this.coords.lat = marker.target._latlng.lat;
									this.coords.lng = marker.target._lat…







map_pins.js:230 using parameter 0 for aforementioned function 
{distance: 119.02939870817022, type: ""dragend"", target: i, sourceTarget: i}
distance: 119.02939870817022
sourceTarget: i
options: {}
_animRequest: 211
_dragStartTarget: img.leaflet-marker-icon.leaflet-zoom-animated.leaflet-interactive.leaflet-marker-draggable
_element: img.leaflet-marker-icon.leaflet-zoom-animated.leaflet-interactive.leaflet-marker-draggable
_enabled: true
_events: {dragstart: Array(1), predrag: Array(1), drag: Array(1), dragend: Array(1)}
_firingCount: 0
_initHooksCalled: true
_lastEvent: PointerEvent {isTrusted: true, touches: Array(1), changedTouches: Array(1), pointerId: 1, width: 1, …}
_lastTarget: null
_leaflet_id: 119
_moved: true
_moving: false
_newPos: k {x: 139.36859130859375, y: 382.805419921875}
_parentScale: {x: 1, y: 1, boundingClientRect: DOMRect}
_preventOutline: true
_startPoint: k {x: 538.1398315429688, y: 221.37399291992188}
_startPos: k {x: 132.529052734375, y: 501.6381530761719}
__proto__: i
target: i {options: {…}, _latlng: D, _initHooksCalled: true, _leaflet_id: 117, _mapToAdd: i, …}
type: ""dragend""
__proto__: Object
map_pins.js:232 function ended







map_pins.js:233 map.panTo() fails here, map object:  
i {options: {…}, _handlers: Array(6), _layers: {…}, _zoomBoundLayers: {…}, _sizeChanged: false, …}
attributionControl: i {options: {…}, _attributions: {…}, _initHooksCalled: true, _map: i, _container: div.leaflet-control-attribution.leaflet-control}
boxZoom: i {_map: i, _container: div.leaflet-container.leaflet-touch.leaflet-retina.leaflet-fade-anim.leaflet-grab.leaflet-touch-dra…, _pane: div.leaflet-pane.leaflet-overlay-pane, _resetStateTimeout: 0, _initHooksCalled: true, …}
doubleClickZoom: i {_map: i, _initHooksCalled: true, _enabled: true}
dragging: i {_map: i, _initHooksCalled: true, _enabled: true, _draggable: i, _positions: Array(0), …}
keyboard: i {_map: i, _panKeys: {…}, _zoomKeys: {…}, _initHooksCalled: true, _enabled: true, …}
options: {}
scrollWheelZoom: i {_map: i, _initHooksCalled: true, _enabled: true, _leaflet_id: 98, _delta: 0, …}
touchZoom: i {_map: i, _initHooksCalled: true, _enabled: true, _leaflet_id: 99}
zoomControl: i {options: {…}, _initHooksCalled: true, _map: i, _leaflet_id: 94, _zoomInButton: a.leaflet-control-zoom-in, …}
_animateToCenter: D {lat: 38.86644411885283, lng: -106.98246002197267}
_animateToZoom: 10
_animatingZoom: false
_container: div.leaflet-container.leaflet-touch.leaflet-retina.leaflet-fade-anim.leaflet-grab.leaflet-touch-drag.leaflet-touch-zoom
_containerId: 92
_controlContainer: div.leaflet-control-container
_controlCorners: {topleft: div.leaflet-top.leaflet-left, topright: div.leaflet-top.leaflet-right, bottomleft: div.leaflet-bottom.leaflet-left, bottomright: div.leaflet-bottom.leaflet-right}
_events: {moveend: Array(3), zoomend: Array(1), zoomlevelschange: Array(1), unload: Array(4), dblclick: Array(1), …}
_fadeAnimated: true
_firingCount: 0
_handlers: (6) [i, i, i, i, i, i]
_initHooksCalled: true
_lastCenter: D {lat: 38.86644411885283, lng: -106.98246002197267}
_layers: {100: i, 117: i}
_layersMaxZoom: 20
_layersMinZoom: 0
_leaflet_id: 91
_loaded: true
_mapPane: div.leaflet-pane.leaflet-map-pane
_onResize: ƒ ()
_paneRenderers: {}
_panes: {mapPane: div.leaflet-pane.leaflet-map-pane, tilePane: div.leaflet-pane.leaflet-tile-pane, shadowPane: div.leaflet-pane.leaflet-shadow-pane, overlayPane: div.leaflet-pane.leaflet-overlay-pane, markerPane: div.leaflet-pane.leaflet-marker-pane, …}
_pixelOrigin: k {x: 53035, y: 99811}
_proxy: div.leaflet-proxy.leaflet-zoom-animated
_resizeRequest: 113
_size: k {x: 270, y: 1000}
_sizeChanged: false
_targets: {92: i, 118: i}
_zoom: 10
_zoomAnimated: true
_zoomBoundLayers: {100: i}
__proto__: i
```",2021-05-18T23:04:34Z,741705
2447,Leaflet/Leaflet,894598837,843624394,No error and the map does not pan when the zoom is set as such and the coordinates are set as such with a marker that is moved to said position within the bounds at zoom shown.,2021-05-18T23:06:58Z,741705
2448,Leaflet/Leaflet,894598837,843624524,At the map size shown.,2021-05-18T23:07:17Z,741705
2449,Leaflet/Leaflet,894598837,843628107,"```
                                marker.addEventListener('dragend', function(marker) {
                                        console.log('dragend event parameter 0', marker);
                                        console.log('calling function', this.pin.dragCb);
                                        console.log('using parameter 0 for aforementioned function', marker);
                                        this.pin.dragCb(marker);
                                        console.log('function ended');
                                        console.log('map.panTo() fails here, map object: ', globals.gmap);
                                        globals.gmap.panTo([this.pin.obj.latitude, this.pin.obj.longitude]);
                                }.bind({pin: map_pins.pins[c]}));
```



https://user-images.githubusercontent.com/741705/118734965-1749ec80-b805-11eb-8a19-29a6eef0a846.mov


What more proof do you need?  That is everything.",2021-05-18T23:16:20Z,741705
2450,Leaflet/Leaflet,894598837,843628726,"Here is this.pin

```
this.pin 
{obj: {…}, info: div, draggable: true, marker: i, dragCb: ƒ}
dragCb: ƒ ()
draggable: true
info: div
marker: i
dragging: i {_marker: i, _initHooksCalled: true, _enabled: true, _draggable: i}
options: {title: ""af-40mp-625rmr"", icon: i, zIndexOffset: 1, draggable: true}
_events: {remove: Array(2), dragend: Array(1), click: Array(1)}
_firingCount: 0
_icon: img.leaflet-marker-icon.leaflet-zoom-animated.leaflet-interactive.leaflet-marker-draggable
_initHooksCalled: true
_latlng: D {lat: 39.001823432304754, lng: -106.97038027457896}
_leaflet_id: 103
_map: i {options: {…}, _handlers: Array(6), _layers: {…}, _zoomBoundLayers: {…}, _sizeChanged: false, …}
_mapToAdd: i {options: {…}, _handlers: Array(6), _layers: {…}, _zoomBoundLayers: {…}, _sizeChanged: false, …}
_shadow: null
_zIndex: 501
_zoomAnimated: true
__proto__: i
obj:
alertDisabled: 0
channel: 0
clientInfo: ""ispapp-snmp-relay-0.1""
createdAt: 1620056840
fw: null
fwVersion: null
groupId: ""5f88e6a24ba991445d8ec738""
hardwareCpuInfo: null
hardwareMake: null
hardwareModel: null
hardwareModelNumber: null
hardwareSerialNumber: null
key: ""asdfasdfasdf""
lastConfigRequest: 1621359482
lastUpdate: 1621379634
latitude: 38.86706476159009
login: ""af-40mp-625rmr""
longitude: -106.98185137007387
name: ""af-40mp-625rmr""
notes: ""testing snmp relay""
os: null
osBuildDate: null
osVersion: ""null""
outsideIp: (8) [""76.186.24.140"", ""3.233.165.14"", ""76.186.24.140"", ""3.233.165.14"", ""76.186.24.140"", ""3.233.165.14"", ""76.186.24.140"", ""3.233.165.14""]
outsideIpChangeTsSeconds: 1620146969
outsideIpChanges: 1
reboot: 0
ssid: ""undefined""
uptime: 1809293
usingWebSocket: true
vlan: ""undefined""
wanIp: ""63.151.94.152""
wds: ""undefined""
wirelessBeaconInt: 0
wirelessChannel: 0
wirelessConfigs: []
wirelessMode: ""ap_bridge""
zoneId: ""5f88e6954ba991445d8ec736""
_id: ""60901b075223241f3a472497""
__proto__: Object
__proto__: Object
```",2021-05-18T23:17:59Z,741705
2451,Leaflet/Leaflet,894598837,843631489,"Do you want me to reopen another issue with this as the first comment or are you going to reopen this one?



Thank You,
Andrew Hodel

> On May 18, 2021, at 4:41 PM, Iván Sánchez Ortega ***@***.***> wrote:
> 
> ﻿
> A call to map.panTo at the end of the dragend event for the marker does not work.
> 
> Yes, it does work.
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or unsubscribe.
",2021-05-18T23:24:49Z,741705
2452,Leaflet/Leaflet,894598837,843638361,"With a timeout:

```
                                        setTimeout(function() {
                                                console.log('timeout finished');
                                                globals.gmap.panTo([this.pin.obj.latitude, this.pin.obj.longitude]);
                                        }.bind({pin: this.pin}), 4000);
```

```
function ended
map_pins.js:233 map.panTo() fails here, map object:  i {options: {…}, _handlers: Array(6), _layers: {…}, _zoomBoundLayers: {…}, _sizeChanged: false, …}
map_pins.js:234 this.pin {obj: {…}, info: div, draggable: true, marker: i, dragCb: ƒ}
map_pins.js:236 timeout finished
```

That makes no sense.",2021-05-18T23:43:34Z,741705
2453,Leaflet/Leaflet,894598837,843639953,Stop copy-pasting your debug output and show a minimal reproducible example. We have the bug report templates for a reason.,2021-05-18T23:48:22Z,1125786
2454,Leaflet/Leaflet,894598837,843640680,"Why? You should be able to plug in the parameters to each function easily. That is why I did it. 



Thank You,
Andrew Hodel

> On May 18, 2021, at 6:48 PM, Iván Sánchez Ortega ***@***.***> wrote:
> 
> ﻿
> Stop copy-pasting your debug output and show a minimal reproducible example. We have the bug report templates for a reason.
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or unsubscribe.
",2021-05-18T23:50:40Z,741705
2455,Leaflet/Leaflet,894598837,843640955,"It is not debug output it is direct parameters passed to your functions!



Thank You,
Andrew Hodel

> On May 18, 2021, at 6:48 PM, Iván Sánchez Ortega ***@***.***> wrote:
> 
> ﻿
> Stop copy-pasting your debug output and show a minimal reproducible example. We have the bug report templates for a reason.
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or unsubscribe.
",2021-05-18T23:51:21Z,741705
2456,Leaflet/Leaflet,894598837,843641863,"Everything is there and readable. If you cannot explain the reasoning as to why that logic would not prevail then the code is flawed. 

Thank You,
Andrew Hodel

> On May 18, 2021, at 6:50 PM, Andrew Hodel ***@***.***> wrote:
> 
> ﻿Why? You should be able to plug in the parameters to each function easily. That is why I did it. 
> 
> 
> 
> Thank You,
> Andrew Hodel
> 
>>> On May 18, 2021, at 6:48 PM, Iván Sánchez Ortega ***@***.***> wrote:
>>> 
>> ﻿
>> Stop copy-pasting your debug output and show a minimal reproducible example. We have the bug report templates for a reason.
>> 
>> —
>> You are receiving this because you authored the thread.
>> Reply to this email directly, view it on GitHub, or unsubscribe.
",2021-05-18T23:53:58Z,741705
2457,Leaflet/Leaflet,894598837,843642986,"To expect more than parameters passed per function is absurd, truthfully the library should have a debug parameter on instantiation that logs every function and corresponding parameter chronologically for you to be able to debug easily. 

I am still trying to understand why that doesn’t exist. 




Thank You,
Andrew Hodel

> On May 18, 2021, at 6:50 PM, Andrew Hodel ***@***.***> wrote:
> 
> ﻿Why? You should be able to plug in the parameters to each function easily. That is why I did it. 
> 
> 
> 
> Thank You,
> Andrew Hodel
> 
>>> On May 18, 2021, at 6:48 PM, Iván Sánchez Ortega ***@***.***> wrote:
>>> 
>> ﻿
>> Stop copy-pasting your debug output and show a minimal reproducible example. We have the bug report templates for a reason.
>> 
>> —
>> You are receiving this because you authored the thread.
>> Reply to this email directly, view it on GitHub, or unsubscribe.
",2021-05-18T23:57:16Z,741705
2458,Leaflet/Leaflet,894598837,843644446,"At this point I'm gonna tell you to calm down and read [""How To Ask Questions The Smart Way""](http://www.catb.org/esr/faqs/smart-questions.html) and [""http://www.chiark.greenend.org.uk/~sgtatham/bugs.html""](http://www.chiark.greenend.org.uk/~sgtatham/bugs.html).",2021-05-19T00:01:26Z,1125786
2459,Leaflet/Leaflet,894598837,843644609,"Do you have any reasonable explanation as to why you would want people’s code instead of a chronological function/parameter output generated from a debug=true parameter passed to each library function?



Thank You,
Andrew Hodel

> On May 18, 2021, at 6:50 PM, Andrew Hodel ***@***.***> wrote:
> 
> ﻿Why? You should be able to plug in the parameters to each function easily. That is why I did it. 
> 
> 
> 
> Thank You,
> Andrew Hodel
> 
>>> On May 18, 2021, at 6:48 PM, Iván Sánchez Ortega ***@***.***> wrote:
>>> 
>> ﻿
>> Stop copy-pasting your debug output and show a minimal reproducible example. We have the bug report templates for a reason.
>> 
>> —
>> You are receiving this because you authored the thread.
>> Reply to this email directly, view it on GitHub, or unsubscribe.
",2021-05-19T00:01:47Z,741705
2460,Leaflet/Leaflet,894598837,843646347,"The smart way to handle apis, libraries and functions in code is to provide an argument within the library that will log everything chronologically.

Why you wouldn’t is beyond understanding. 

I have provided all of the input parameters, their calling order and the function names in lieu of the libraries remittances. 




Thank You,
Andrew Hodel

> On May 18, 2021, at 7:01 PM, Iván Sánchez Ortega ***@***.***> wrote:
> 
> ﻿
> At this point I'm gonna tell you to calm down and read ""How To Ask Questions The Smart Way"" and ""http://www.chiark.greenend.org.uk/~sgtatham/bugs.html"".
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or unsubscribe.
",2021-05-19T00:05:57Z,741705
2461,Leaflet/Leaflet,894598837,843647053,"Here is an example:

https://github.com/andrewhodel/rrd/blob/4ded366652d93596af52940ba98622229f3acdab/rrd.go#L86

Thank You,
Andrew Hodel

> On May 18, 2021, at 7:01 PM, Iván Sánchez Ortega ***@***.***> wrote:
> 
> ﻿
> At this point I'm gonna tell you to calm down and read ""How To Ask Questions The Smart Way"" and ""http://www.chiark.greenend.org.uk/~sgtatham/bugs.html"".
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or unsubscribe.
",2021-05-19T00:07:37Z,741705
2462,Leaflet/Leaflet,894598837,843647989,"Remittances == making updates without including debug output and “henceforth at that time” removing such a validator. 

Thank You,
Andrew Hodel

> On May 18, 2021, at 7:05 PM, Andrew Hodel ***@***.***> wrote:
> 
> ﻿The smart way to handle apis, libraries and functions in code is to provide an argument within the library that will log everything chronologically.
> 
> Why you wouldn’t is beyond understanding. 
> 
> I have provided all of the input parameters, their calling order and the function names in lieu of the libraries remittances. 
> 
> 
> 
> 
> Thank You,
> Andrew Hodel
> 
>>> On May 18, 2021, at 7:01 PM, Iván Sánchez Ortega ***@***.***> wrote:
>>> 
>> ﻿
>> At this point I'm gonna tell you to calm down and read ""How To Ask Questions The Smart Way"" and ""http://www.chiark.greenend.org.uk/~sgtatham/bugs.html"".
>> 
>> —
>> You are receiving this because you authored the thread.
>> Reply to this email directly, view it on GitHub, or unsubscribe.
",2021-05-19T00:09:42Z,741705
2463,Leaflet/Leaflet,894598837,843649151,"I am sorry Man, but how else would you figure out a long list of asynchronous functions?


Thank You,
Andrew Hodel

> On May 18, 2021, at 7:05 PM, Andrew Hodel ***@***.***> wrote:
> 
> ﻿The smart way to handle apis, libraries and functions in code is to provide an argument within the library that will log everything chronologically.
> 
> Why you wouldn’t is beyond understanding. 
> 
> I have provided all of the input parameters, their calling order and the function names in lieu of the libraries remittances. 
> 
> 
> 
> 
> Thank You,
> Andrew Hodel
> 
>>> On May 18, 2021, at 7:01 PM, Iván Sánchez Ortega ***@***.***> wrote:
>>> 
>> ﻿
>> At this point I'm gonna tell you to calm down and read ""How To Ask Questions The Smart Way"" and ""http://www.chiark.greenend.org.uk/~sgtatham/bugs.html"".
>> 
>> —
>> You are receiving this because you authored the thread.
>> Reply to this email directly, view it on GitHub, or unsubscribe.
",2021-05-19T00:12:43Z,741705
2464,Leaflet/Leaflet,894598837,843659357,"You could write it to a file or with a prefix and pass that to a library test that automatically assured everyone of validity in application code or error in library code. 

I do not have understand your reasoning for “playinbrowserjs” or “required bug fields” when you have a library this pervasive and this solution is that much better. 




Thank You,
Andrew Hodel

> On May 18, 2021, at 7:05 PM, Andrew Hodel ***@***.***> wrote:
> 
> ﻿The smart way to handle apis, libraries and functions in code is to provide an argument within the library that will log everything chronologically.
> 
> Why you wouldn’t is beyond understanding. 
> 
> I have provided all of the input parameters, their calling order and the function names in lieu of the libraries remittances. 
> 
> 
> 
> 
> Thank You,
> Andrew Hodel
> 
>>> On May 18, 2021, at 7:01 PM, Iván Sánchez Ortega ***@***.***> wrote:
>>> 
>> ﻿
>> At this point I'm gonna tell you to calm down and read ""How To Ask Questions The Smart Way"" and ""http://www.chiark.greenend.org.uk/~sgtatham/bugs.html"".
>> 
>> —
>> You are receiving this because you authored the thread.
>> Reply to this email directly, view it on GitHub, or unsubscribe.
",2021-05-19T00:41:27Z,741705
2465,Leaflet/Leaflet,894598837,843666102,"I would also like an apology or a reason as to why the smart way isn’t a debug option in the library with a prefix on the console output, as you want me to stop and read a book rather than paste console output to a test that verifies code in library and the application. 

Thank You,
Andrew Hodel

> On May 18, 2021, at 7:05 PM, Andrew Hodel ***@***.***> wrote:
> 
> ﻿The smart way to handle apis, libraries and functions in code is to provide an argument within the library that will log everything chronologically.
> 
> Why you wouldn’t is beyond understanding. 
> 
> I have provided all of the input parameters, their calling order and the function names in lieu of the libraries remittances. 
> 
> 
> 
> 
> Thank You,
> Andrew Hodel
> 
>>> On May 18, 2021, at 7:01 PM, Iván Sánchez Ortega ***@***.***> wrote:
>>> 
>> ﻿
>> At this point I'm gonna tell you to calm down and read ""How To Ask Questions The Smart Way"" and ""http://www.chiark.greenend.org.uk/~sgtatham/bugs.html"".
>> 
>> —
>> You are receiving this because you authored the thread.
>> Reply to this email directly, view it on GitHub, or unsubscribe.
",2021-05-19T00:57:05Z,741705
2466,Leaflet/Leaflet,894598837,844551985,"OK, so now that we've all had some time to cool down, I'll say this:

Andrew: you come here to a turf that is not your turf, you ignore the templates for relevant information, you disrespect the etiquette, you post comment after comment making it difficult to follow your train of thought, and all that while having a holier-than-thou attitude towards bug triage techniques.

So while I *could* explain why we ask for live reproducible examples, I feel that doing that would be a waste of my time.

I will, however, ask you to work on your empathy and on your communication skills.",2021-05-19T22:57:35Z,1125786
2467,Leaflet/Leaflet,235832393,235832393,"Hi,

Leaflet is a very useful component but I'm facing currently a behavior which seems very weird and I could not find a workaround for it.

I'm using Leaflet 1.0.3.
I'm using Chrome Version 58.0.3029.110 (64-bit). The same happens on mobile devices my app is ported to (android/iOS).
I'm running Windows 10 Pro 64 bit

1. I'm adding a marker to the map and I want to affect its ordering by zIndex means:
`var marker = new L.Marker(location,
                {
                    icon: new L.Icon(
                        {
                            iconUrl: iconUrl,
                            iconSize: [size.width, size.height],
                            iconAnchor: [anchor.leftOffset, anchor.topOffset]
                        }),
                    draggable: draggable,
                    clickable: true,
                    zIndexOffset: zIndexOffset
                });`
In my example I'm using zIndexOffset of 202.

2. After adding the marker to the map (marker.addTo(map)) immediately I see that the marker object changes and get a zIndex of 619. Actually each application run with the same initial zIndexOffset I get a different zIndex on marker object after adding it to the map.

3. I have an event listener on 'mousedown' event and when I inspect the marker object received by the listener zIndex is again different (477). Meaning it has changed even from the arbitrary value set in setp 2.

4. In case I zoom in and out the map the zIndex will change again.

All of this doesn't allow me to set different markers on top of others since step 2 change ruins any logic I may use in my code.

Here's the example of what happens in the [leaflet playground](http://playground-leaflet.rhcloud.com/gexa/edit?html,console,output):

How could this be fixed or maybe any workaround?

Thanks in advance 

",2017-06-14T10:22:55Z,14091163
2468,Leaflet/Leaflet,235832393,308393252,"This is not a bug, this is behaviour as expected.

You have to keep in mind that the option name is zIndex**Offset**, and **not** `zIndex`. The `zIndex` of each marker depends on the *vertical coordinate of the marker*. 

Look at this screenshot from https://github.com/Leaflet/Leaflet.Icon.Glyph:

![](https://camo.githubusercontent.com/09ba35cbec1a15aaf2a599be333a85d701f69ead/68747470733a2f2f6c6561666c65742e6769746875622e696f2f4c6561666c65742e49636f6e2e476c7970682f64656d6f2e706e67)

Do you see how you can only see the tip of the bottom-most markers? That's because the `zIndex` of the markers in the bottom row is greater than in the second row, which have a `zIndex` greater than those in the third row, and so on. The `zIndex` is not arbitrary. It grows by 1 for every pixel down. It also gets recalculated on zooming and some view resets.

Thus, the zIndex**Offset** option is an **offset** which applies to the seemingly arbitrary `zIndex`. Thus, if I wanted the center marker to be on top of the rest, I'd set a `zIndexOffset` of at least the height of a marker, and it would look like:

![image](https://user-images.githubusercontent.com/1125786/27128107-a2a6bbb4-50fd-11e7-8244-12efd37cc3d3.png)

> All of this doesn't allow me to set different markers on top of others since step 2 change ruins any logic I may use in my code.

> Here's the example of what happens in the leaflet playground:

If the problem is that you can't set which marker goes on top, *show me an example of how that fails*, because that playground only logs a value, it does not show me markers failing to display on top of other markers. You have felt into the [XY problem](https://meta.stackexchange.com/questions/66377/what-is-the-xy-problem).

If you still need fine control over things on top of things, consider [using map panes](http://leafletjs.com/examples/map-panes/), or post a more obvious example.",2017-06-14T10:39:13Z,1125786
2469,Leaflet/Leaflet,235832393,308399544,"Hi,

Thanks for your quick response.

I've found this behavior right now and from my perspective it's definitely wrong...

Suppose you have only 2 markers on the map, A is above the B in pixels perspective. Then A will always appear below the B in zIndex means. But if I'll drag the A to be positioned below B in pixels perspective and leave it, after this action A will appear on top of B in zIndex meaning. Each marker dragging recalculates its zIndex. This results in incorrect behavior in our app.

What we need is to allow drawing several segments of polylines. Each segment is just several polylines connected while each tip of the segment and each connection of 2 polylines are marked with markers.

Then suppose I have 2 such segments while their markers appear wherever you like on the map in a mixed mode. But what I need is the ability to have one of the segments active and each of its markers shown on top of markers of inactive segment. But in case any marker of the active segment is very close to a marker of inactive segment and the marker of active segment appears below the marker of inactive segment in pixels perspective, it will appear under the marker of inactive segment which is wrong and I have no tool to make it work in a correct way with the logic you provide.

What can be done about it?",2017-06-14T11:09:19Z,14091163
2470,Leaflet/Leaflet,235832393,308400791,"> Each marker dragging recalculates its zIndex.

...and it should definitely reapply the `zIndexOffset`, does it not?

> But what I need is the ability to have one of the segments active and each of its markers shown on top of markers of inactive segment.
> What can be done about it?

As I've already said: [map panes](http://leafletjs.com/examples/map-panes/). You can have the markers for the """"active"""" segment drawn on a different pane, which has a static `zIndex` relative to the other panes. Note that `zIndexOffset` applies to the `zIndex` of the DOM elements *inside* a pane, but you can control the `zIndex` of the parent DOM element, which is the pane.",2017-06-14T11:15:50Z,1125786
2471,Leaflet/Leaflet,235832393,308401455,"Okay, I'll check the panes. It may help me solve the segments problem but then appears a problem with markers of the same segment.

In case 2 markers are close to each other and the one that should be active is below the other that is inactive the same problem will reappear..

So what is your suggestion? Use pane for each marker?",2017-06-14T11:19:12Z,14091163
2472,Leaflet/Leaflet,235832393,308402447,"I don't know. Show me an example of the problematic case (playground or screenshots or anything) and we might be able to have ideas.

But please remind that this is a bug tracker, for bugs in Leaflet. Don't expect us to offer consultancy on how to build X using leaflet, that's out of the scope.",2017-06-14T11:24:09Z,1125786
2473,Leaflet/Leaflet,235832393,308404018,"perhaps use some `marker.on('mouseover', function(l) { l.bringToFront() })` and `marker.on('mouseoout', function(l) { l.bringToBack() })`. I'm not 100% certain names of events are correct, you can maybe check them out.",2017-06-14T11:31:33Z,10845050
2474,Leaflet/Leaflet,235832393,308405169,@themre Unfortunately this solution is not perfect also. Suppose there are 2 markers exactly at the same location from map's perspective but different coordinates. There always be only one marker on top and mouseover event won't bring the marker that's below on top of the one that's on top.,2017-06-14T11:37:15Z,14091163
2475,Leaflet/Leaflet,235832393,308405504,"@IvanSanchez Look at the problem on the picture. The marker with the black arrow is the active marker.
![1](https://user-images.githubusercontent.com/14091163/27130503-297b3b54-510f-11e7-8c39-ccea05ce53d8.jpg)
",2017-06-14T11:39:02Z,14091163
2476,Leaflet/Leaflet,235832393,308405849,@lentyaishe And what are the `zIndexOffset`s and the computed `zIndex`es of both?,2017-06-14T11:40:48Z,1125786
2477,Leaflet/Leaflet,235832393,308407310,"@IvanSanchez 
The active marker: zIndexOffset = 5, zIndex = 239
Inactive marker: zIndexOffset = 10, zIndex = 567
Do you want me to try artificially update the zIndexOffset of the active marker on click and check whether it will reappear on top of inactive one? I really doubt it'll happen.",2017-06-14T11:48:42Z,14091163
2478,Leaflet/Leaflet,235832393,308407988,"@lentyaishe No, I want you to set the `zIndexOffset` of the ""active"" marker to a much higher value than the inactive one (which I'd suggest letting at zero). Try 50 or 100, or 500.",2017-06-14T11:52:09Z,1125786
2479,Leaflet/Leaflet,235832393,308409441,"@IvanSanchez Actually it did.. I've used marker.setZIndexOffset(100) on the clicked active marker (A) that was below and it appeared on top of inactive one (B) but then the click on B made it on top and clicking A again has left it behind. This means that I'll need to recalculate all markers zIndexOffsets on the map to make it work as I need which is too much. Also in our setup we may have dozens of markers on the map. This way we'll reach enormous values for zIndex which also seems not good...

That's the point that in case I could have managed the actual zIndex of markers on the map it would solve everything since I could have just decided that all active objects may have zIndex above 500 and all others will have below that. Then I should have just change values of last active objects zIndexes below 500 and the newly activated one above etc. And zIndexes of markers within a specific segment also could have similar logic, i.e below 550 inactive, above 550 - active. Anyway then it's my problem to make zIndexes work for me as expected. And currently since the map component decides for me the actual zIndex value, it messes all my logic up.",2017-06-14T11:59:09Z,14091163
2480,Leaflet/Leaflet,235832393,308417762,"> Actually it did.. I've used marker.setZIndexOffset(100) on the clicked active marker (A) that was below and it appeared on top of inactive one (B) but then the click on B made it on top and clicking A again has left it behind. This means that I'll need to recalculate all markers zIndexOffsets on the map to make it work as I need which is too much.

No. It means that you have to reset the `zIndexOffset` of a marker whenever it changes state from inactive to active, or from inactive to active.

> This way we'll reach enormous values for zIndex which also seems not good...

A `zIndex` is an integer which can go up to the millions, and we *never* had any issue with high values. We do know that some browsers have a limit on integer CSS properties of 2^23 (see https://github.com/Leaflet/Leaflet/commit/c5172f3088b820325e726f57f0b09b16b1ec498d, https://github.com/Leaflet/Leaflet/commit/17c180e1eff97932c3450447d5d17fda199f7c2c), but `zIndex`es have never been a big problem, because Leaflet recalculates them once in a while on every `viewreset`, so the values for both the CSS transforms and the `zIndex`es are within a manageable range.

In my experience, it's better to learn about the [stacking context](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Positioning/Understanding_z_index/Stacking_without_z-index) than to be afraid of misusing `z-index`.

> That's the point that in case I could have managed the actual zIndex of markers on the map it would solve everything [...]

No you wouldn't, because if you're not resetting the `zIndex`es of the markers which transition from active to inactive, you would end up with the same problem.

I mean, if you want to *really* try if your logic would work without Leaflet's management of `zIndex`/`zIndexOffset`, you can manually disable it with something like

```js
L.Marker.prototype._updateZIndex = function(offset) {
  // this._icon.style.zIndex = this._zIndex + offset;
}
```

So far, you've been subtly suggesting that Leaflet is the culprit and that your logic is infallible. [Programmers like myself don't react well to this kind of bug reporting](https://www.chiark.greenend.org.uk/~sgtatham/bugs.html#symptoms). I bet you a beer that the problem is in the logic in your side, and that it won't matter if you manage raw `zIndex`es or `zIndexOffset`s.

If you can *isolate* a case where Leaflet fails to raise a marker's `zIndex` when raising its `zIndexOffset`, of fails to lower it, please do share a playground or similar.",2017-06-14T12:36:31Z,1125786
2481,Leaflet/Leaflet,235832393,308423228,"> you can manually disable it

Thanks for the suggestion. I'll try it out. It also gave me an insight on how you implement the layouting.

> your logic is infallible

Not at all. :) I just think that the API should allow disabling the recalculation of `zIndex`es and allow the API customer break his own head on how to implement what he needs.

> If you can _isolate_ a case where Leaflet fails...

The scenario is extremely simple. Use `zIndexOffset` for 2 markers with difference of 5. Since the zIndex that each marker gets from your algorithm is hundreds you may come up with the following:
`zIndexOffset(A) > zIndexOffset(B)` set by the code
but
`zIndex(A) < zIndex(B)` calculated by leaflet
And since `zIndex` is recalculated on each map operation: drag, move, zoom etc. there's no way to get the actual difference in `zIndexOffset` for markers to make it work correctly for `zIndex`es that will be calculated.",2017-06-14T12:58:57Z,14091163
2482,Leaflet/Leaflet,235832393,308427016,"Well, finally I came up with a solution that works fine for me:
Since in your algo you just sum the `zIndex` calculated by leaflet and `zIndexOffset` provided from outside and since all `zIndex`es you deal with are hundreds I'm just working with `zIndexOffset`s in multiplications of 1000s. A bit dirty but it does the job.

Anyway, **thanks a bunch for your rapid help**!",2017-06-14T13:14:05Z,14091163
2483,Leaflet/Leaflet,235832393,363248337,"@lentyaishe I was used this gist an run like a charm. Maybe this can be a Feature Request?

https://gist.github.com/up209d/4c68f2391f2302e510eb81aa8bcd4514",2018-02-05T22:50:12Z,2943127
2484,Leaflet/Leaflet,235832393,527426478,"> Well, finally I came up with a solution that works fine for me:
> Since in your algo you just sum the `zIndex` calculated by leaflet and `zIndexOffset` provided from outside and since all `zIndex`es you deal with are hundreds I'm just working with `zIndexOffset`s in multiplications of 1000s. A bit dirty but it does the job.
> 
> Anyway, **thanks a bunch for your rapid help**!

I have the same problem as you. Marker's Z-index changes when scaling, but I don't want to change z-index. I want to know your final solution.",2019-09-03T11:55:32Z,23161404
2485,Leaflet/Leaflet,235832393,527428835,"> > Well, finally I came up with a solution that works fine for me:
> > Since in your algo you just sum the `zIndex` calculated by leaflet and `zIndexOffset` provided from outside and since all `zIndex`es you deal with are hundreds I'm just working with `zIndexOffset`s in multiplications of 1000s. A bit dirty but it does the job.
> > Anyway, **thanks a bunch for your rapid help**!
> 
> I have the same problem as you. Marker's Z-index changes when scaling, but I don't want to change z-index. I want to know your final solution.

This is my final solution.",2019-09-03T12:03:49Z,14091163
2486,Leaflet/Leaflet,235832393,1030571164,"The documentation says:

```
By default, marker images zIndex is set automatically based on its latitude. Use this option if you want to put the marker on top of all others (or below), specifying a high value like 1000 (or high negative value, respectively).
```

It should mention that the ""automatic"" setting based on the latitude shall not be greater than N, instead of ""specifying a high value like 1000"".

Regardless, you need to separate them by thousands for each ""level"" or ""step"" if you want to have them show as you want.

Consider their problem, they want those with a lower latitude to show below the others, but you want them to be displayed in levels not considering the latitude.

They cannot really store all the possible offsets you set, unless you pass a list first.  **Basically they should say in the documentation that we use a range of 0-N and you should set each level of z-index you wish to display regardless of location as (N+1 * level_desired).  You might as well considering N to be 1000, it seems to work but obviously could change.**",2022-02-05T07:46:59Z,741705
2487,Leaflet/Leaflet,235832393,1031428081,"It's not explained correctly in this issue, here's a more understandable explanation.

**zIndexOffset difference of 40000 does not place marker visibly above/on top of another marker that is 10 degrees lower vertically on the cartesian map.**

@lentyaishe use the example I provided in your opening comment of this issue so that people understand the problem.

https://plnkr.co/edit/hyWro1sGgmjbeDPk

**The Google logo should be visually above (per z-index) the Yandex logo.**

",2022-02-07T12:42:14Z,741705
2488,Leaflet/Leaflet,235832393,1031434580,"@andrewhodel use the `zIndexOffset` in the marker options not in the icon options:
```
var n = L.marker([30.505, 0.57], {icon: myIcon, zIndexOffset: 80000}).addTo(map);
```",2022-02-07T12:50:36Z,19800037
2489,Leaflet/Leaflet,235832393,1031465133,"@Falke-Design does not work.

<img width=""638"" alt=""Screen Shot 2022-02-07 at 4 22 30 PM"" src=""https://user-images.githubusercontent.com/741705/152796083-55234af0-474a-4d07-97ad-e894016f3662.png"">
",2022-02-07T13:23:15Z,741705
2490,Leaflet/Leaflet,235832393,1031467318,"It does work, look into the updated sampel: https://plnkr.co/edit/GcnZAG6c5vjPaXIl",2022-02-07T13:25:34Z,19800037
2491,Leaflet/Leaflet,235832393,1031468766,"@Falke-Design I think it's simply a sad reality of the layers and how things don't understand beyond them.

I've already confirmed it.  It does not work with zIndexOffset with svg icons as shown in the screenshot.

It's all being set with marker_opts{}.zIndexOffset",2022-02-07T13:27:01Z,741705
2492,Leaflet/Leaflet,235832393,1031471070,"@Falke-Design it's very simple to understand that when a DOM element has a z-index of 4231 and shows beneath a DOM element with a z-index of 4220 that something is awry.

The marker layer maximum is 600 and set to never change in leaflet.

It's really easy to read that in the code with `grep`.",2022-02-07T13:29:23Z,741705
2493,Leaflet/Leaflet,235832393,1031472846,Please provide a sample that doesn't work. I already showed you in the current sample from you that it is possible.,2022-02-07T13:31:17Z,19800037
2494,Leaflet/Leaflet,235832393,1031473576,"There's nothing but the DOM here, really.

There's no need to explain anything else, if 600 is the containing element limit set statically in the library it doesn't matter what you set them at and that is based on how the layers are applied.

@Falke-Design ",2022-02-07T13:32:04Z,741705
2495,Leaflet/Leaflet,235832393,1031474735,"The fact that you would argue beyond the DOM goes to show that you aren't trying to help, only to entrap.

It's simple.

@Falke-Design ",2022-02-07T13:33:13Z,741705
2496,Leaflet/Leaflet,235832393,1031480847,"It is not always that simple to understand what somebody else tries to explain. 
I hope for you that you find someone who wants to help you because I will not and I don't think that anyone is willing to help you if you keep that attitude.",2022-02-07T13:40:04Z,19800037
2497,Leaflet/Leaflet,22814728,22814728,"The only query / issue I have is the big lines it draws blue (or themed borders) around countries.

There is a method to remove them?
",2013-11-18T04:29:25Z,5626302
2498,Leaflet/Leaflet,22814728,28680545,"Pass the options parameter:

http://leafletjs.com/reference.html#polygon
http://leafletjs.com/reference.html#path
",2013-11-18T08:02:59Z,393086
2499,Leaflet/Leaflet,22814728,364323894,"It's a shame you couldn't provide an answer. Pointing to a documentation page that has no response to the original question isn't useful at all. 

Equally, it's pretty useless having something that draws a map with a big blue border around every country and no explanation as to how to remove it. 

",2018-02-09T03:21:20Z,4556741
2500,Leaflet/Leaflet,22814728,364366530,"@abrice This kind of passive-aggressiveness is disrespectful towards maintainers.

I suggest reading [""How To Ask Questions The Smart Way""](http://www.catb.org/esr/faqs/smart-questions.html) and [""How to Report Bugs Effectively""](http://www.chiark.greenend.org.uk/~sgtatham/bugs.html) so you can make good bug reports in the future.",2018-02-09T08:23:05Z,1125786
2501,OpenRA/OpenRA,631364219,631364219,"## Issue Summary
As it has become the standard in RAGL and proven its value, it might be a good idea to replace it everywhere for better balancing.

What is ERCC? 
ERCC provides more exits for harvesters (north exit) which makes harvesting from all sides more balanced. Furthermore, the outline of the refinery got changed to square to alleviate map-inconveniences. 

TODO: 
More detailed guide by widow about ERCC.",2020-06-05T07:07:35Z,65908293
2502,OpenRA/OpenRA,631364219,639423838,"It this has a footprint
=+x
+=+
x==

while the current one has this
\_X\_
xxx
X==
\===

it looks like
![Screen Shot 2020-06-05 at 14 18 10](https://user-images.githubusercontent.com/37534529/83871007-ebd70a80-a737-11ea-9e09-6e61d8529322.png)

while the OG looks like
![Screen Shot 2020-06-05 at 14 24 34](https://user-images.githubusercontent.com/37534529/83871195-4f613800-a738-11ea-8ee9-de5d4111230b.png)


",2020-06-05T11:27:22Z,37534529
2503,OpenRA/OpenRA,631364219,639424170,ERCC currently suffers from #18232,2020-06-05T11:28:06Z,37534529
2504,OpenRA/OpenRA,631364219,639424561,Please also include screenshots that show how the harvester clips through solid walls and the roof for a fair comparison.,2020-06-05T11:28:56Z,167819
2505,OpenRA/OpenRA,631364219,639427273,"It works most of the time, but if you do these exact paths it clips

![ERCC4](https://user-images.githubusercontent.com/37534529/83871616-08c00d80-a739-11ea-9668-d47c2d36851c.gif)
![ERCC1](https://user-images.githubusercontent.com/37534529/83871625-0b226780-a739-11ea-8af6-99c820c4a31d.gif)
![ERCC3](https://user-images.githubusercontent.com/37534529/83871628-0bbafe00-a739-11ea-9f66-a492a9b27f56.gif)
![ERCC2](https://user-images.githubusercontent.com/37534529/83871630-0c539480-a739-11ea-853d-66bd42b7185b.gif)

One of the problems is that the harvester art is bigger than once cell
![Harv](https://user-images.githubusercontent.com/37534529/83871836-75d3a300-a739-11ea-93f7-94b332bd69a0.png)
that also causes problems with the current ref
![REF1](https://user-images.githubusercontent.com/37534529/83871862-88e67300-a739-11ea-9828-d546cd89e71d.png)
![REF3](https://user-images.githubusercontent.com/37534529/83871884-94399e80-a739-11ea-9c23-dc0f8aef6a83.png)
but these don't exist with ERCC
",2020-06-05T11:35:07Z,37534529
2506,OpenRA/OpenRA,631364219,639430488,"> pchote: it would be much better if it used a new design that was not a space-warping bodge of the original refinery
> pchote: get rid of the overhang completely
> pchote: it is never going to be accepted as a replacement for the original refinery, but if you want to be added as a distinct thing that could be optionally enabled then it needs to make sense as its own thing",2020-06-05T11:43:10Z,1614714
2507,OpenRA/OpenRA,631364219,639458526,"One thing that I find the most offensive about this extended trainwreck of a discussion is that it basically boils down to a few competitive players, who already have access to ERCC via map-mods, insisting that everybody else is playing the game wrong and should have the option to use the classic refineries taken away from them. This is not cool, IMO.",2020-06-05T12:45:03Z,167819
2508,OpenRA/OpenRA,631364219,639473083,"I can get behind what @pchote is saying. I think this already has a nice place in map-mods. And if ruleset-only mods are ever implemented then the ERCC (and other balance mods and tests) could be easily applied to any map.

And even if the ERCC gets accepted to the core game then it should be optional and not the default. ",2020-06-05T13:15:31Z,1355810
2509,OpenRA/OpenRA,631364219,639548625,The mutator thing should be mentioned here if it is raised as an issue somewhere.,2020-06-05T14:58:52Z,1614714
2510,OpenRA/OpenRA,631364219,639551616,"> The mutator thing should be mentioned here if it is raised as an issue somewhere.

#13629 / #14325 / #9422",2020-06-05T15:03:52Z,7704140
2511,OpenRA/OpenRA,631364219,639618257,"> One thing that I find the most offensive about this extended trainwreck of a discussion is that it basically boils down to a few competitive players, who already have access to ERCC via map-mods, insisting that everybody else is playing the game wrong and should have the option to use the classic refineries taken away from them. This is not cool, IMO.

I understand that but it's missleading. Fact is that majority of above avarage skilled players prefer playing with ERCC refineries.  OpenRA supposted to improve gameplay of the originals, so players that who actually understand openra's gameplay are forced to play on a mod map doesn't give it a good look

I'm not saying that ERCC has to be implemented in its current form, after all it was merelly an experiment to see if refineries could be fixed. I'm just saying that it's a very relevant issue that needs a solution",2020-06-05T16:38:43Z,37534529
2512,OpenRA/OpenRA,631364219,639621449,"Additionally the person responsible for ERCC is not interested in refining it further, so it falls on the shoulders of someone else",2020-06-05T16:43:42Z,37534529
2513,OpenRA/OpenRA,631364219,639774050,This could also be solved using `PlaceBuildingVariants` and inverted artwork.,2020-06-05T20:12:12Z,756669
2514,OpenRA/OpenRA,631364219,640049418,">  > ERCC currently suffers from #18232

> I tested it and that problem doe snot (really) exist with ERCC @Punsho . The only thing I observed is that you have to click a bit ""below"" the husk which is probably due to the logical layout/overlay structure. But you can certainly recover husks from the ERCC refinery.

There's more than one place where a harvester can die, i had a game where 2 harvesters were dead on a refinery. I was able to revive one by the method you just mentioned, but the other was completelly unclickable. This is how this bug was discovered",2020-06-06T12:05:31Z,37534529
2515,OpenRA/OpenRA,631364219,640112152,"This could be in the form of an tickbox at lobby, probably it would be default off.",2020-06-06T20:09:44Z,48881353
2516,OpenRA/OpenRA,631364219,640183611,"> This could be in the form of an tickbox at lobby, probably it would be default off.

That's doesn't scale well. A dropdown to choose mutators from would be better.
",2020-06-07T09:23:25Z,59416324
2517,OpenRA/OpenRA,631364219,753216438,"I am officially drawing a line under this request. ERCC will never be merged upstream while I remain involved with OpenRA. At this point it has nothing to do with any of the in-game aspects, it is entirely about the behaviour of the people championing it.

While most of the competitive RA community are good people, there is a rotten core of toxic entitlement that manifests as abuse and belittlement, often focused around ERCC, and usually focused personally against me. This behaviour has forever soured ERCC, and integrating it into the upstream RA mod now would validate that behaviour as an effective strategy for influencing OpenRA's development.

Why now? Two recent incidents targeting me (from N/A in the community discord, and Longely/Widow in the competitive discord - the two main personalities behind ERCC) were incredibly offensive and belittling, and are the straw that has broken the camels back when added on top of many other generally toxic events this year. People in the competitive community were having fun meming about ""us vs the devs"" earlier this year, but at that time I was seriously considering quitting OpenRA completely due to how unhappy and embarrased the behaviour of some people in the community (and the general tolerance of that behaviour as acceptable by everybody else) was making me. Ultimately, I decided that I enjoy working on the project, and making things better for everybody else in the community. I *do not* enjoy interacting with the assholes, and intend to be increasingly blunt in dismissing their toxic behaviour. This is the first concrete action towards that.

I can certainly appreciate the desire for solving directional map imbalance, to the point where I fully implemented the code for one solution (rotatable structures) and developed a working prototype for a second (allowing harvesters to dock from any direction, visually jumping docking point). I think there is scope for some ERCC alternative, but I suggest that the competitive community as a whole put some effort into solving its attitude problem before they try to revisit such ideas here.",2020-12-31T22:12:06Z,167819
2518,OpenRA/OpenRA,320554101,320554101,balanced weapon for artillery,2018-05-06T01:24:06Z,37599987
2519,OpenRA/OpenRA,320554101,386864973,"Sorry, this is not going to work this way. You have to edit the base yaml files directly (and not add new files).",2018-05-06T09:05:49Z,7704140
2520,OpenRA/OpenRA,320554101,386867407,Closing as per [part 1](https://github.com/OpenRA/OpenRA/pull/15118).,2018-05-06T09:51:42Z,167819
2521,OpenRA/OpenRA,320553214,320553214,"Balancing artillery
I'd like to get the stuff I've already tested out of the way. The current balance team is essentially doing everything I already did in terms of making balance changes and testing them.

A lot of time is being wasted doing what I've already done. I've tested my changes in hundreds of matches with great feedback. The only negative opinions I get are from trolls or people who need an excuse for why they suck at the game.

So, first up, ARTILLERY.

These values allow the artillery to remain effective, but not overpowered. Instead of 3 arty destroying a huge infantry blob, you'll 4-6 of them.
Damage vs vehicles has been buffed slightly to compensate for lack of infantry killing.
A direct hit will kill the infantry, but if it lands off to the side, it can take anywhere from 1-4 shots. With a small group of artillery firing, the aoe will make it a lot more effective.

Cost also went up, and it's speed has been slowed down. I think I buffed it vs buildings too, maybe not.

Now, the artillery infantry tradeoff is fair, and for what it lacks in anti infantry, it makes up for by doing roughly 10-20% more dmg to everything else.

vehicles.yaml:

ARTY:
Mobile:
TurnSpeed: 2 (used to be 4 I think)
Speed: 60 (I think -10 points?)
RevealsShroud:
Range: 8c0 (remains same, i think, mightve increased by 1-2 cells)
RevealGeneratedShroud: False
RevealsShroud@GAPGEN:
Range: 4c0
Passenger:
Weight: 4

----------


ballistics.yaml:

^Artillery:
Range: 12c0 (it shouldnt outshoot a v2 by 4 cells, or at all, really)
Projectile: Bullet
Speed: 200 (little bit slower to help nerf vs infantry.
Blockable: false
LaunchAngle: 110
Inaccuracy: 1c256
Warhead@1Dam: SpreadDamage
Spread: 420
Versus:
None: 40
Light: 60
Heavy: 35
Concrete: 50

155mm:
MinRange: 4c0 (minimum range back to 4 cells instead of 5)",2018-05-06T01:00:11Z,37599987
2522,OpenRA/OpenRA,320553214,386864987,"Sorry, this is not going to work this way. You have to edit the base yaml files directly (and not add new files).",2018-05-06T09:06:03Z,7704140
2523,OpenRA/OpenRA,320553214,386867350,"> A lot of time is being wasted doing what I've already done. I've tested my changes in hundreds of  matches with great feedback. The only negative opinions I get are from trolls or people who need an excuse for why they suck at the game.

This attitude runs counter to our [code of conduct](https://github.com/OpenRA/OpenRA/blob/bleed/CODE_OF_CONDUCT.md) and I have received a complaint about this PR under that code.  This PR is one of many examples of this user's behavior across GitHub, the forum, and IRC.

Participating in an open community requires both the ability to come up with good ideas and the interpersonal skills to effectively communicate them.  I am sorry, but ""I am better than you, so stop wasting your time and do what I say"" is a completely inappropriate base to start a pull request from.",2018-05-06T09:50:33Z,167819
2524,coleifer/peewee,237547233,237547233,"Will you accept inline type hints for the peewee like
```
    # type: () -> None
```
as PR? ",2017-06-21T14:08:34Z,2341827
2525,coleifer/peewee,237547233,310131050,"No, type hints are, in my opinion, an abomination that has no business being part of the Python language :)",2017-06-21T16:19:25Z,119974
2526,coleifer/peewee,237547233,310236404,May I at least add peewee's .pyi stubs to python/typeshed repository? ,2017-06-21T23:50:39Z,2341827
2527,coleifer/peewee,237547233,310435629,"Anything you would like to do is totally fine by me, as the license for the software is permissive. I just do not want to merge anything into mainline that concerns type hinting.",2017-06-22T16:39:57Z,119974
2528,coleifer/peewee,237547233,379234851,"@coleifer I'm expecting you will make a deal with static typing fans more and more. I'm partially agree you can do nothing in orm itself and should not, but you should try to understand `static dynamic` fans. Its not about limiting dynamism or to make new java from python, people just want `user.(name/id/group)` in IDE, they are tired to go from view to model definition, to 'google' any functions signature, to debug typo mistakes in runtime. 

Go has no objects and generics, and any data structure should upcast value to `interface{}`, C is just too low-level. You should check out [typeorm](http://typeorm.io/) and [diesel](http://diesel.rs/), Dart. 
Static typing was stigmatized long time as a way to make compilable, fast languages like java, cpp. It has different purpose in dynamic languages now.",2018-04-06T12:12:36Z,707007
2529,coleifer/peewee,237547233,379262120,"> I'm expecting you will make a deal with static typing fans more and more.

No way. Python is a powerful dynamic language. If I want static typing or a strong type system I will use C or go...or hell, Cython. I enjoy coding in those languages, too, but if I'm using Python I'm going to play to it's strengths.

> IDE

Peewee's design goals are composability and consistency -- learn once, apply everywhere -- precisely so that peewee will ""do what you expect"" without needing to check the docs.",2018-04-06T14:00:06Z,119974
2530,coleifer/peewee,237547233,402331223,"@coleifer I'm not a strong proponent of Static Typing, but the type annotations are extremely useful when developing to help catch bugs. I debug enough Python at runtime, the more I can do to reduce that, the better.

Mypy is optional, you can choose to put annotations on things, for clarity. If the inclusion of these things only ever produces net gain, why not apply them. It's not an either-or camp. The degree to which you add Static Types is varying.

Surely you can see the point of view of why Type annotations may be extremely useful? And in no ways detracts from the power and flexibility of Python by its inclusion.
",2018-07-04T00:51:17Z,621477
2531,coleifer/peewee,237547233,724207340,"Sorry to necro this thread but in my experience I've also noticed that type hints can also add a significant performance boost (sometimes >20%) since Cython can now use them for type inference. Granted, performance benefits aren't _guaranteed_ but they are likely. I see you have Cython support set up for PeeWee so it's entirely possible its performance could benefit from it.",2020-11-09T18:52:31Z,620513
2532,coleifer/peewee,237547233,881032607,"Hi @coleifer,

You've done a great job with this lib. With or without type hints, it's nice to use.

But if someday you change your mind about type hints, I believe it won't hurt.",2021-07-15T21:44:33Z,1610035
2533,coleifer/peewee,237547233,1046680158,"> No, type hints are, in my opinion, an abomination that has no business being part of the Python language :)

As a maintainer of typeshed, a big repository that is all about typing in Python, I think this is a perfectly valid opinion! I felt the same when I first learned about typing in Python. It shouldn't be something that library authors have to do, and it's optional by design.",2022-02-21T09:56:35Z,18505570
2534,coleifer/peewee,237547233,1046901336,"This issue was opened nearly 5 years ago. In that time I've had plenty of opportunities to see python's type hinting in the wild. I still believe that it misses the mark, offerning none of the iron-clad guarantees you get from a statically-typed language, while being injurious to Python's inherent dynamism, readability, and simplicity. Since this issue tends to attract drive-by comments of little value, I'm going to close discussion for now.

When I want static typing I reach for a statically-typed language. When I want expressiveness, simplicity and flexibility, I reach for Python.",2022-02-21T13:50:08Z,119974
2535,pallets/jinja,544279140,544279140,"~Closes #752~ After some internal discussion, we're reverting this. See #1131.

To test this in your project, check out the branch and do two local installs from there:

* `pip install -e .` gets you `jinja`
* `pip install -e jinja2-compat` gets you a shim that redirects `jinja2` imports to `jinja` and emits a deprecation warning.

This seems to work in real code bases like Flask's tests and @indico. Once this is released, you'd only have to `pip install jinja2`, it depends on `jinja`. This should allow projects to gradually rename their imports and continue to use packages that still depend on `jinja2` and aren't updated.",2019-12-31T22:09:04Z,1242887
2536,pallets/jinja,544279140,569998183,"However, some weird issues come up when you undo the changes to the tests so they all still import from `jinja2`.

First is that the tests won't run because there's a circular import issue with `from jinja2 import asyncfilters` getting imported before `asyncsupport`. This apparently already fails in master, but for some reason test collection only triggers it when going through the redirect.

If you exclude/remove the async tests, tests that expect exceptions begin failing with seemingly the correct exception. Apparently, something about the import internals causes `from jinja2.exceptions import TemplateNotFound` and `from jinja.exceptions import TemplateNotFound` to have the same repr but different ids.",2019-12-31T22:15:10Z,1242887
2537,pallets/jinja,544279140,569998612,"I've tried experimenting with some other methods as well:

* Adding a special finder to `sys.meta_path` seems to have the same issues as above.
* Creating placeholder modules that do `from jinja.{name} import *` doesn't include anything that starts with `_`, which while not public could cause compatibility issues.
* Generating placeholders that literally import `*` and every `_name` works but ties compatibility to the imported private names remaining available into the future.",2019-12-31T22:18:58Z,1242887
2538,pallets/jinja,544279140,570095954,"Switched to a new solution that doesn't have the problems I was seeing when messing with `sys.modules`. Now `jinja2` just has modules matching what's in `jinja`, with a special function that re-exports all the public and `_` names and emits a warning. Names starting with `__` (double underscore) are skipped as that seemed to be causing the issues.

The shim should continue working as long as the modules in `jinja` continue to match, and a quick release could be made to add more modules if we still want to maintain compatibility later.

All tests pass with `jinja` or `jinja2` imports.",2020-01-02T00:19:24Z,1242887
2539,pallets/jinja,544279140,570431370,"Tentatively planning to have the jinja2 package depend on ""jinja<4.0"" so that developers have the entire 3.0 cycle (however long that is, there's no plans for 4.0 yet) to upgrade. Past that, I may update the package to extend the support, but not making any promises.

As mentioned above, the only way the shim becomes incompatible is if module names change, which is unlikely, but it has happened with, for example, Werkzeug to improve organization. So supporting the 3.0 cycle means not permanently moving anything if it happens at all until 4.0, which is sort of what a major version bump is for anyway.",2020-01-03T01:41:24Z,1242887
2540,pallets/jinja,544279140,570623884,"The interesting stuff happens in `jinja2-compat/src/jinja2/__init__.py`, the rest is just renames.",2020-01-03T16:31:20Z,1242887
2541,pallets/jinja,544279140,573266166,"Moved the Jinja2 compat package to a [separate repo](https://github.com/pallets/jinja2-compat). This makes the separation clearer, and makes it easier to make releases of the compat package independently of the state of the Jinja repo in case that's needed.",2020-01-11T01:35:29Z,1242887
2542,pallets/jinja,544279140,573266810,"You can try installing Jinja2 2.11.0rc1 from https://test.pypi.org/project/Jinja2/2.11.0rc1/:

```
pip install -i https://test.pypi.org/simple/ --pre Jinja2
```

You should get Jinja 2.11.0rc1 pulled in as a dependency and both `jinja` and `jinja2` should be importable.",2020-01-11T01:41:57Z,1242887
2543,pallets/jinja,544279140,574733800,We actually discussed about this on our Discord a few days ago and the rename will be 3.0 and not a point release as initially planned.,2020-01-15T16:16:06Z,179599
2544,pallets/jinja,245325788,245325788,"Continuing discussion from https://github.com/pallets/meta/issues/10#issuecomment-209980352

The naming is inconsistent:
- Github repo is `jinja`
- Pypi package name is `jinja2`
- Pallets project calls it ""Jinja"": https://www.palletsprojects.com/p/jinja/
- RTD namespace is jinja2.readthedocs.io
- Pocoo docs (currently the official ones) are ""Jinja"": http://jinja.pocoo.org/docs/2.9/
- file extensions are sometimes `.jinja`, `.j2`, `.jinja2`... Ansible project currently uses `.j2`

We should pick either ""Jinja"" or ""Jinja2"" and use it everywhere for consistency. 

I am open to either, ""Jinja"" is simpler and shorter, but ""Jinja2"" has a more distinctive ring to it and less likely to get confused with any other projects. 
",2017-07-25T08:38:28Z,483314
2545,pallets/jinja,245325788,317729724,"The Stack Overflow tag is ""jinja2"", ""jinja"" is a synonym that gets invisbly converted. Despite my efforts towards the opposite. (This happened a year or so ago.)

I really want to drop the ""2"" from the name. Start adding v2 builds to the ""jinja"" PyPI page. Deprecate the ""jinja2"" import and go back to the ""jinja"" namespace.",2017-07-25T13:00:51Z,1242887
2546,pallets/jinja,245325788,318238026,@ThiefMaster @mitsuhiko @untitaker do you guys have opinions?,2017-07-27T02:10:09Z,483314
2547,pallets/jinja,245325788,318277715,I think we can do that but I would personally propose to align the 3.0 release with that.,2017-07-27T07:05:12Z,7396
2548,pallets/jinja,245325788,318283181,":+1: on waiting for 3.0.

---

> The Stack Overflow tag is ""jinja2"", ""jinja"" is a synonym that gets invisbly converted. Despite my efforts towards the opposite. (This happened a year or so ago.)

I may be able to fix that.


Edit: Yes, I can

> **Rename preview**
> jinja2 will be removed from 3486 questions
> jinja will be added to 3486 questions
> 5 commitments to jinja2 Documentation proposal will be moved to the jinja proposal
> A tag synonym mapping jinja2 → jinja will be created.
> (these counts include deleted questions and exclude overlapping tags)",2017-07-27T07:33:16Z,179599
2549,pallets/jinja,245325788,318497746,"What is the timeline for 3.0 release?

The sooner we start giving folks a heads up the better, so what about adding a deprecation warning now on `jinja2` imports and a warning on `jinja` imports that we will soon be pushing v3 out to the `jinja` namespace?

",2017-07-27T21:55:56Z,483314
2550,pallets/jinja,245325788,318497949,"@davidism are you able to move the RTD namespace over to `jinja`? Per my comment above, it's currently under `jinja2`, and IIRC, you were driving the cleanup/ownership migration of the RTD namespaces for other projects?",2017-07-27T21:56:52Z,483314
2551,pallets/jinja,245325788,318498173,In a way the last major release of Jinja2 was a massive change in the engine. Not even sure if there is more stuff we need to break :D,2017-07-27T21:57:55Z,7396
2552,pallets/jinja,245325788,320456591,"Saving breaking changes and name consolidation for a Jinja v3 sounds great to me. We might as well try to find what breaking changes we can slate for it.

I'd like to remind everyone of a potential one - [allowing included block overrides](https://github.com/pallets/jinja/issues/243). That issue doesn't have to mean a breaking change, but if that's the route you all want to go, remaking/opening that issue with a v3 milestone is how I'd do it. Sorry for the tangent. :) Perhaps we can make another ticket for discussing what to break / milestone for Jinja v3.",2017-08-05T17:33:40Z,3431410
2553,pallets/jinja,245325788,322840927,"nudge @davidism - per my comment above, are you able to modify the RTD namespace from jinja2 to jinja?",2017-08-16T17:21:11Z,483314
2554,pallets/jinja,245325788,543329245,"In the 2.11 release, I'm thinking of renaming the package to `jinja`, with a placholder module for `jinja2` that forwards all imports and issues a deprecation warning.

I'll still have to work out the timing of this next step, but I'd also like to try moving back to the ""Jinja"" name on PyPI. I think what I'd try to do is have a **Jinja** 2.11 build that includes the `jinja2` placeholder, and make the **Jinja2** 2.11 build just depend on `jinja>=2.11`, or have a small shim that explains installing the other name without breaking any code. I'm am willing to take on the extra effort of keeping these builds in sync for a while while we manage a transition.",2019-10-17T19:37:38Z,1242887
2555,pallets/jinja,245325788,543341151,@davidism this shouldn't happen in a point release. This would break pickle and a bunch of other things.,2019-10-17T20:09:30Z,7396
2556,pallets/jinja,245325788,570631941,"Since I gave my blessings before I want to actually qualify this somewhat. I have some stomach ulcers with this change. Ultimately I don't think it's particularly useful for users (it just drops one character), introduces some backwards incompatibility concerns and it undoes a learning I made back when Jinja2 was originally released.

The reason the package renamed with 2.0 was that there was no way (and there still is no way) to have parallel installations of Python libraries that are incompatible unlike node or rust can.  Because of that I think we're going to be sooner or later again in a stupid situation where Jinja 4.0 would need to be named ""Jinja4"" on pypi.

So I think while this rename is somewhat okay I generally don't think anymore that it's a good idea. I think this change would be without concerns if the Python import system were to support imports with different versions which however I gave up hoping for.",2020-01-03T16:58:55Z,7396
2557,pallets/jinja,245325788,574765174,"@coleifer I really have no idea what you're suggesting other than ""let's just revert this"". We won't release this as a patch/bugfix release, so I guess you are not happy that this will land in 2.11. Are you expecting us to release Jinja 3 for this? That would cause even more problems in a dependency tree that has multiple package dependant on Jinja.

Honestly I find your behavior completely unacceptable and hope it will have consequences.",2020-01-15T17:23:45Z,837573
2558,pallets/jinja,245325788,574769047,~fwiw we could also release a new (point) version of `jinja2` that reexports all of `jinja` (ie it is the shim). That usually works in Rust when you have multiple dependencies that depend on another package. You'd just have to update `jinja2` to make packages that depend on `jinja2` implicitly use the types from `jinja`.~ discard this. This is exactly what the shim is doing. I have no idea what the concern is.,2020-01-15T17:33:12Z,837573
2559,pallets/jinja,245325788,574771948,"@untitaker Interested in the issues you refer to with making the rename happen in Jinja 3.0 instead. Based on discussion with @ThiefMaster, it seemed that doing it in 3.0 made more sense, as it does represent a major change. We also thought about a 2.12 release for just the rename.

Jinja2 3.0 would be the shim and pull in Jinja 3.0 as a dependency.",2020-01-15T17:40:29Z,1242887
2560,pallets/jinja,245325788,574772892,That would probably be fine but it would prohibit using the new `jinja` name with packages that explicitly depend on `Jinja2==2.*`. Which limits the potential usefulness of the shim.,2020-01-15T17:42:57Z,837573
2561,pallets/jinja,245325788,574776373,"Yeah, that was one of my initial reasons for going with 2.11. I guess 2.12 vs 3.0 comes down to deciding on if the rename is a major change even though jinja2 would continue to work and issue deprecation warnings. 3.0 was originally only going to be a major release because it dropped Python 3.

---

After some more discussion internally, we're reverting this. See #1131.",2020-01-15T17:51:56Z,1242887
2562,ytdl-org/youtube-dl,554355383,554355383,"<!--

######################################################################
  WARNING!
  IGNORING THE FOLLOWING TEMPLATE WILL RESULT IN ISSUE CLOSED AS INCOMPLETE
######################################################################

-->


## Checklist

<!--
Carefully read and work through this check list in order to prevent the most common mistakes and misuse of youtube-dl:
- First of, make sure you are using the latest version of youtube-dl. Run `youtube-dl --version` and ensure your version is 2020.01.15. If it's not, see https://yt-dl.org/update on how to update. Issues with outdated version will be REJECTED.
- Make sure that all provided video/audio/playlist URLs (if any) are alive and playable in a browser.
- Make sure that all URLs and arguments with special characters are properly quoted or escaped as explained in http://yt-dl.org/escape.
- Search the bugtracker for similar issues: http://yt-dl.org/search-issues. DO NOT post duplicates.
- Finally, put x into all relevant boxes (like this [x])
-->

- [x] I'm reporting a broken site support
- [x] I've verified that I'm running youtube-dl version **2020.01.15**
- [x] I've checked that all provided URLs are alive and playable in a browser
- [x] I've checked that all URLs and arguments with special characters are properly quoted or escaped
- [x] I've searched the bugtracker for similar issues including closed ones


## Verbose log

<!--
Provide the complete verbose output of youtube-dl that clearly demonstrates the problem.
Add the `-v` flag to your command line you run youtube-dl with (`youtube-dl -v <your command line>`), copy the WHOLE output and insert it below. It should look similar to this:
 [debug] System config: []
 [debug] User config: []
 [debug] Command-line args: [u'-v', u'http://www.youtube.com/watch?v=BaW_jenozKcj']
 [debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251
 [debug] youtube-dl version 2020.01.15
 [debug] Python version 2.7.11 - Windows-2003Server-5.2.3790-SP2
 [debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4
 [debug] Proxy map: {}
 <more lines>
-->

```
[debug] System config: []
[debug] User config: []
[debug] Custom config: []
[debug] Command-line args: ['--verbose', '-j', 'https://www.youtube.com/playlist?list=OLAK5uy_kj9o0LqqeGu3wJf_G1JqOJ-YHzexqptlM']
[debug] Encodings: locale UTF-8, fs utf-8, out utf-8, pref UTF-8
[debug] youtube-dl version 2020.01.15
[debug] Python version 3.8.1 (CPython) - Linux-5.5.0-1-MANJARO-x86_64-with-glibc2.2.5
[debug] exe versions: ffmpeg 4.2.2, ffprobe 4.2.2, rtmpdump 2.4
[debug] Proxy map: {}
ERROR: Signature extraction failed: Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1383, in _decrypt_signature
    func = self._extract_signature_function(
  File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
    cache_res = res(test_string)
  File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
    return lambda s: initial_function([s])
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 258, in resf
    res, abort = self.interpret_statement(stmt, local_vars)
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 91, in interpret_expression
    right_val = self.interpret_expression(
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 188, in interpret_expression
    x, abort = self.interpret_statement(
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 188, in interpret_expression
    x, abort = self.interpret_statement(
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 211, in interpret_expression
    raise ExtractorError('Unsupported JS expression %r' % expr)
youtube_dl.utils.ExtractorError: Unsupported JS expression '[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see  https://yt-dl.org/update  on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
 (caused by ExtractorError(""Unsupported JS expression '[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see  https://yt-dl.org/update  on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output."")); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see  https://yt-dl.org/update  on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1383, in _decrypt_signature
    func = self._extract_signature_function(
  File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
    cache_res = res(test_string)
  File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
    return lambda s: initial_function([s])
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 258, in resf
    res, abort = self.interpret_statement(stmt, local_vars)
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 91, in interpret_expression
    right_val = self.interpret_expression(
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 188, in interpret_expression
    x, abort = self.interpret_statement(
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 188, in interpret_expression
    x, abort = self.interpret_statement(
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 211, in interpret_expression
    raise ExtractorError('Unsupported JS expression %r' % expr)
youtube_dl.utils.ExtractorError: Unsupported JS expression '[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see  https://yt-dl.org/update  on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1383, in _decrypt_signature
    func = self._extract_signature_function(
  File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
    cache_res = res(test_string)
  File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
    return lambda s: initial_function([s])
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 258, in resf
    res, abort = self.interpret_statement(stmt, local_vars)
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 91, in interpret_expression
    right_val = self.interpret_expression(
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 188, in interpret_expression
    x, abort = self.interpret_statement(
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 188, in interpret_expression
    x, abort = self.interpret_statement(
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 211, in interpret_expression
    raise ExtractorError('Unsupported JS expression %r' % expr)
youtube_dl.utils.ExtractorError: Unsupported JS expression '[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see  https://yt-dl.org/update  on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/youtube_dl/YoutubeDL.py"", line 796, in extract_info
    ie_result = ie.extract(url)
  File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/common.py"", line 530, in extract
    ie_result = self._real_extract(url)
  File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 2046, in _real_extract
    signature = self._decrypt_signature(
  File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1393, in _decrypt_signature
    raise ExtractorError(
youtube_dl.utils.ExtractorError: Signature extraction failed: Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1383, in _decrypt_signature
    func = self._extract_signature_function(
  File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
    cache_res = res(test_string)
  File ""/usr/lib/python3.8/site-packages/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
    return lambda s: initial_function([s])
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 258, in resf
    res, abort = self.interpret_statement(stmt, local_vars)
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 91, in interpret_expression
    right_val = self.interpret_expression(
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 188, in interpret_expression
    x, abort = self.interpret_statement(
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 188, in interpret_expression
    x, abort = self.interpret_statement(
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""/usr/lib/python3.8/site-packages/youtube_dl/jsinterp.py"", line 211, in interpret_expression
    raise ExtractorError('Unsupported JS expression %r' % expr)
youtube_dl.utils.ExtractorError: Unsupported JS expression '[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see  https://yt-dl.org/update  on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
 (caused by ExtractorError(""Unsupported JS expression '[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see  https://yt-dl.org/update  on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output."")); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; see  https://yt-dl.org/update  on how to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
```


## Description

<!--
Provide an explanation of your issue in an arbitrary form. Provide any additional information, suggested solution and as much context and examples as possible.
If work on your issue requires account credentials please provide them or explain how one can obtain them.
-->

Trying to dump the Json information of a playlist seem to be broken for youtube.",2020-01-23T19:23:06Z,33500183
2563,ytdl-org/youtube-dl,554355383,577836915,"I can confirm the the issue on many videos.

```
./youtube-dl -4 -v ""https://www.youtube.com/watch?v=pqIv3e5eBeo""
[debug] System config: []
[debug] User config: []
[debug] Custom config: []
[debug] Command-line args: [u'-4', u'-v', u'https://www.youtube.com/watch?v=pqIv3e5eBeo']
[debug] Encodings: locale UTF-8, fs UTF-8, out UTF-8, pref UTF-8
[debug] youtube-dl version 2020.01.15
[debug] Python version 2.7.9 (CPython) - Linux-4.9.182-xxxx-std-ipv6-64-x86_64-with-Debian-7
[debug] exe versions: avconv 11.12-6, avprobe 11.12-6, ffmpeg N-48007-g62f8d27ef1-static
[debug] Proxy map: {}
[youtube] pqIv3e5eBeo: Downloading webpage
[youtube] pqIv3e5eBeo: Downloading video info webpage
[youtube] {18} signature length 109, html5 player vfl_PLd61
[youtube] pqIv3e5eBeo: Downloading player https://www.youtube.com/yts/jsbin/player_ias-vfl_PLd61/en_US/base.js
ERROR: Signature extraction failed: Traceback (most recent call last):
  File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1384, in _decrypt_signature
    video_id, player_url, s
  File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
    cache_res = res(test_string)
  File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
    return lambda s: initial_function([s])
  File ""./youtube-dl/youtube_dl/jsinterp.py"", line 258, in resf
    res, abort = self.interpret_statement(stmt, local_vars)
  File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""./youtube-dl/youtube_dl/jsinterp.py"", line 92, in interpret_expression
    m.group('expr'), local_vars, allow_recursion - 1)
  File ""./youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
    m.group('x'), local_vars, allow_recursion - 1)
  File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""./youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
    m.group('x'), local_vars, allow_recursion - 1)
  File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""./youtube-dl/youtube_dl/jsinterp.py"", line 211, in interpret_expression
    raise ExtractorError('Unsupported JS expression %r' % expr)
ExtractorError: Unsupported JS expression u'[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
 (caused by ExtractorError(u""Unsupported JS expression u'[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose flag and include its complete output."",)); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
Traceback (most recent call last):
  File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1384, in _decrypt_signature
    video_id, player_url, s
  File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
    cache_res = res(test_string)
  File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
    return lambda s: initial_function([s])
  File ""./youtube-dl/youtube_dl/jsinterp.py"", line 258, in resf
    res, abort = self.interpret_statement(stmt, local_vars)
  File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""./youtube-dl/youtube_dl/jsinterp.py"", line 92, in interpret_expression
    m.group('expr'), local_vars, allow_recursion - 1)
  File ""./youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
    m.group('x'), local_vars, allow_recursion - 1)
  File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""./youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
    m.group('x'), local_vars, allow_recursion - 1)
  File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""./youtube-dl/youtube_dl/jsinterp.py"", line 211, in interpret_expression
    raise ExtractorError('Unsupported JS expression %r' % expr)
ExtractorError: Unsupported JS expression u'[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
Traceback (most recent call last):
  File ""./youtube-dl/youtube_dl/YoutubeDL.py"", line 796, in extract_info
    ie_result = ie.extract(url)
  File ""./youtube-dl/youtube_dl/extractor/common.py"", line 530, in extract
    ie_result = self._real_extract(url)
  File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 2047, in _real_extract
    encrypted_sig, video_id, player_url, age_gate)
  File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1394, in _decrypt_signature
    'Signature extraction failed: ' + tb, cause=e)
ExtractorError: Signature extraction failed: Traceback (most recent call last):
  File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1384, in _decrypt_signature
    video_id, player_url, s
  File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
    cache_res = res(test_string)
  File ""./youtube-dl/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
    return lambda s: initial_function([s])
  File ""./youtube-dl/youtube_dl/jsinterp.py"", line 258, in resf
    res, abort = self.interpret_statement(stmt, local_vars)
  File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""./youtube-dl/youtube_dl/jsinterp.py"", line 92, in interpret_expression
    m.group('expr'), local_vars, allow_recursion - 1)
  File ""./youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
    m.group('x'), local_vars, allow_recursion - 1)
  File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""./youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
    m.group('x'), local_vars, allow_recursion - 1)
  File ""./youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""./youtube-dl/youtube_dl/jsinterp.py"", line 211, in interpret_expression
    raise ExtractorError('Unsupported JS expression %r' % expr)
ExtractorError: Unsupported JS expression u'[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
 (caused by ExtractorError(u""Unsupported JS expression u'[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose flag and include its complete output."",)); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
```",2020-01-23T19:28:13Z,11341599
2564,ytdl-org/youtube-dl,554355383,577837625,"I'm experiencing the same problem recently with a particular video. I'm also on **2020.01.15**, and the video works in-browser just fine.

```
[debug] System config: []
[debug] User config: []
[debug] Custom config: []
[debug] Command-line args: [u'--verbose', u'--skip-download', u'--write-info-json', u'-o', u'i', u'https://www.youtube.com/watch?v=S79GcTt_8pc']
[debug] Encodings: locale UTF-8, fs UTF-8, out None, pref UTF-8
[debug] youtube-dl version 2020.01.15
[debug] Python version 2.7.12 (CPython) - Linux-4.4.0-171-generic-x86_64-with-Ubuntu-16.04-xenial
[debug] exe versions: ffmpeg 2.8.15, ffprobe 2.8.15, rtmpdump 2.4
[debug] Proxy map: {}
[youtube] S79GcTt_8pc: Downloading webpage
[youtube] S79GcTt_8pc: Downloading video info webpage
[youtube] {18} signature length 109, html5 player vfl_PLd61
[youtube] S79GcTt_8pc: Downloading player https://www.youtube.com/yts/jsbin/player_ias-vfl_PLd61/en_US/base.js
ERROR: Signature extraction failed: Traceback (most recent call last):
  File ""/usr/local/bin/youtube-dl/youtube_dl/extractor/youtube.py"", line 1384, in _decrypt_signature
    video_id, player_url, s
  File ""/usr/local/bin/youtube-dl/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
    cache_res = res(test_string)
  File ""/usr/local/bin/youtube-dl/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
    return lambda s: initial_function([s])
  File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 258, in resf
    res, abort = self.interpret_statement(stmt, local_vars)
  File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 92, in interpret_expression
    m.group('expr'), local_vars, allow_recursion - 1)
  File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
    m.group('x'), local_vars, allow_recursion - 1)
  File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
    m.group('x'), local_vars, allow_recursion - 1)
  File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 211, in interpret_expression
    raise ExtractorError('Unsupported JS expression %r' % expr)
ExtractorError: Unsupported JS expression u'[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
 (caused by ExtractorError(u""Unsupported JS expression u'[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose flag and include its complete output."",)); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
Traceback (most recent call last):
  File ""/usr/local/bin/youtube-dl/youtube_dl/extractor/youtube.py"", line 1384, in _decrypt_signature
    video_id, player_url, s
  File ""/usr/local/bin/youtube-dl/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
    cache_res = res(test_string)
  File ""/usr/local/bin/youtube-dl/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
    return lambda s: initial_function([s])
  File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 258, in resf
    res, abort = self.interpret_statement(stmt, local_vars)
  File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 92, in interpret_expression
    m.group('expr'), local_vars, allow_recursion - 1)
  File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
    m.group('x'), local_vars, allow_recursion - 1)
  File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
    m.group('x'), local_vars, allow_recursion - 1)
  File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 211, in interpret_expression
    raise ExtractorError('Unsupported JS expression %r' % expr)
ExtractorError: Unsupported JS expression u'[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
Traceback (most recent call last):
  File ""/usr/local/bin/youtube-dl/youtube_dl/YoutubeDL.py"", line 796, in extract_info
    ie_result = ie.extract(url)
  File ""/usr/local/bin/youtube-dl/youtube_dl/extractor/common.py"", line 530, in extract
    ie_result = self._real_extract(url)
  File ""/usr/local/bin/youtube-dl/youtube_dl/extractor/youtube.py"", line 2047, in _real_extract
    encrypted_sig, video_id, player_url, age_gate)
  File ""/usr/local/bin/youtube-dl/youtube_dl/extractor/youtube.py"", line 1394, in _decrypt_signature
    'Signature extraction failed: ' + tb, cause=e)
ExtractorError: Signature extraction failed: Traceback (most recent call last):
  File ""/usr/local/bin/youtube-dl/youtube_dl/extractor/youtube.py"", line 1384, in _decrypt_signature
    video_id, player_url, s
  File ""/usr/local/bin/youtube-dl/youtube_dl/extractor/youtube.py"", line 1297, in _extract_signature_function
    cache_res = res(test_string)
  File ""/usr/local/bin/youtube-dl/youtube_dl/extractor/youtube.py"", line 1360, in <lambda>
    return lambda s: initial_function([s])
  File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 258, in resf
    res, abort = self.interpret_statement(stmt, local_vars)
  File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 92, in interpret_expression
    m.group('expr'), local_vars, allow_recursion - 1)
  File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
    m.group('x'), local_vars, allow_recursion - 1)
  File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 189, in interpret_expression
    m.group('x'), local_vars, allow_recursion - 1)
  File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 56, in interpret_statement
    v = self.interpret_expression(expr, local_vars, allow_recursion)
  File ""/usr/local/bin/youtube-dl/youtube_dl/jsinterp.py"", line 211, in interpret_expression
    raise ExtractorError('Unsupported JS expression %r' % expr)
ExtractorError: Unsupported JS expression u'[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.
 (caused by ExtractorError(u""Unsupported JS expression u'[function(c,d){d=(d'; please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose flag and include its complete output."",)); please report this issue on https://yt-dl.org/bug . Make sure you are using the latest version; type  youtube-dl -U  to update. Be sure to call youtube-dl with the --verbose flag and include its complete output.

```",2020-01-23T19:29:44Z,3223303
2565,openscad/openscad,1412007705,1412007705,"I created a hull example of a toy sailboat, added it to the examples/basic folder and updated the examples.json.",2022-10-17T18:13:57Z,111210832
2566,openscad/openscad,1412007705,1282065471,"A cute example and a nice play on words! I will leave others to comment on the technical aspects and programming style and why some tests are failing 

I also couldn't resist fleshing it out a bit more with sails, keel and rudder, as the previous resolutions were criticised as being not functional, so in the modified example the boat is sailing and heeling in the wind !
EDIT Delete miscredited content.
",2022-10-18T09:10:01Z,3257079
2567,openscad/openscad,1412007705,1282371382,"Administration action. 

@kwikius 
I have edited your post.
Please refrain from publishing code under other peoples credit. ie `// written by Paul Young, 2022`

Please also refrain from denigrating others contributions to this community supported open source project.

Constructive comments, not passive aggressive, are welcome. ",2022-10-18T13:16:54Z,1972961
2568,openscad/openscad,598170002,598170002,"Doing a manual projection with a user defined matrix [[1,0,0,0],[0,1,0,0],[0,0,0,0],[0,0,0,1]]
results in a 2d projection onto the xy plane.

Requesting a linear extrude of this 2d object residing on the xy plane results in an error extruding a 3d object when no 3d object exists.

",2020-04-11T02:22:03Z,56161527
2569,openscad/openscad,598170002,612379550,"Un-surprising as you haven't made a 2D object, you have just squashed a 3D one, which will no longer be manifold due to lots of self intersections.",2020-04-11T09:40:08Z,566149
2570,openscad/openscad,598170002,612398754,"As @nophead already said, a matrix tranformation does not convert 3D to 2D. For that there's a special module [`projection()`](https://en.wikibooks.org/wiki/OpenSCAD_User_Manual/Using_the_2D_Subsystem#3D_to_2D_Projection).",2020-04-11T11:28:48Z,1330241
2571,openscad/openscad,598170002,612652622,"So you are saying that there are two kinds of 2d objects in openscad. The first has zero dimensions along one axis and the second has zero directions along one axis and has special magical properties. Unacceptable. 11.04.2020, 07:29, ""Torsten Paul"" <notifications@github.com>:
As @nophead already said, a matrix tranformation does not convert 3D to 2D. For that there's a special module projection().

—You are receiving this because you authored the thread.Reply to this email directly, view it on GitHub, or unsubscribe.",2020-04-12T17:52:14Z,56161527
2572,openscad/openscad,598170002,612653448,"No there are only 2D objects with XY coordinates and 3D objects with X, Y, Z coordinates.

If a 3D object has all its Z values 0 it is an invalid 3D object as 3D objects represent 3D solids and must have a finite thickness.",2020-04-12T17:58:33Z,566149
2573,openscad/openscad,598170002,612654324,"OpenSCAD is mesh based. If you squish a (default aligned) cube with 6 polygons to zero height, you still end up with a degenerated 3d object having 4 zero area polygons and 2 with the same coordinates but different normal vector.",2020-04-12T18:05:16Z,1330241
2574,openscad/openscad,598170002,612655134,"Manifolds are only defined in 3 or more dimensions.  So, ya.  Using a matrix to project a 3d object onto a 2d surface necessarily means not having a manifold. The projection function also does not return a manifold since it returns a 2d object. There is no such thing as a 2-manifold. ""Lots of self intersections"" -  That is your problem, not mine. If you provide a multmatrix function you should support any matrix, not just an unspecified subset of them.  11.04.2020, 05:40, ""Chris"" <notifications@github.com>:
Un-surprising as you haven't made a 2D object, you have just squashed a 3D one, which will no longer be manifold due to lots of self intersections.

—You are receiving this because you authored the thread.Reply to this email directly, view it on GitHub, or unsubscribe.",2020-04-12T18:11:50Z,56161527
2575,openscad/openscad,598170002,612655412," If you provide a multmatrix function then you should support all matricies, not just an unspecified subset of them. What other matrix operations don't you support.  12.04.2020, 14:05, ""Torsten Paul"" <notifications@github.com>:
OpenSCAD is mesh based. If you squish a (default aligned) cube with 6 polygons to zero height, you still end up with a degenerated 3d object having 4 zero area polygons and 2 with the same coordinates but different normal vector.

—You are receiving this because you authored the thread.Reply to this email directly, view it on GitHub, or unsubscribe.",2020-04-12T18:14:06Z,56161527
2576,openscad/openscad,598170002,612655837,"Maybe you should actually write that down somewhere.  You know, like in the documentation.  Especially in the Multmatrix section of the documentation to make it clear that this program is incapable of managing   all matrix operations. LOL It's half baked.  12.04.2020, 13:58, ""Chris"" <notifications@github.com>:
No there are only 2D objects with XY coordinates and 3D objects with X, Y, Z coordinates.
If a 3D object has all its Z values 0 it is an invalid 3D object as 3D objects represent 3D solids and must have a finite thickness.

—You are receiving this because you authored the thread.Reply to this email directly, view it on GitHub, or unsubscribe.",2020-04-12T18:17:33Z,56161527
2577,openscad/openscad,598170002,612655981,"Ok, I think that is enough.",2020-04-12T18:18:44Z,1330241
2578,openscad/openscad,598170002,612663880,"The matrix operations are fine but just like polyhedron they are GIGO. You can make an infinite number of 3D objects that CGAL will not accept because they are not manifolds. 

OpenSCAD provides projection() to turn a 3D object into a 2D object. It doesn't simply set all the Z coordinates to 0. It creates a new polygon outline like a shadow and removes all the internal edges that would collapse on top of each making a polygon with lots of self intersections.

On the other hand multmatrix does a matrix multiplication on all the vertices. It doesn't add or remove any of them or change the edges.",2020-04-12T19:23:08Z,566149
2579,php-fig/fig-standards,382791162,382791162,,2018-11-20T18:20:00Z,47313
2580,php-fig/fig-standards,382791162,440379932,Why this change? ,2018-11-20T18:22:46Z,252042
2581,php-fig/fig-standards,382791162,440386436,What's the reasoning here?,2018-11-20T18:43:27Z,1663330
2582,php-fig/fig-standards,382791162,440401748,@drupol @Jan0707 Fabien explained the reasons in this Twitter thread: https://twitter.com/fabpot/status/1064946698089365505,2018-11-20T19:31:37Z,73419
2583,php-fig/fig-standards,382791162,440408210,"I don't understand you here @fabpot: in which case(s) removing Symfony project from PHP Fig will help the interoperability and PHP standards in the future? 

Do we want to come back 6 years ago with custom autoloading, custom coding styles and custom ways to do everything ?",2018-11-20T19:52:46Z,1247388
2584,php-fig/fig-standards,382791162,440419516,"> Do we want to come back 6 years ago with custom autoloading, custom coding styles and custom ways to do everything ?

Is this really about autoloading and coding style? Because Fabien's thread is clear about this: they both are great PSRs, it's mostly the ""new wave of incoming PSRs"" that's not interop and kinda breaks the initial project into a more opinionated set of rules instead of an interop one
",2018-11-20T20:32:18Z,3369266
2585,php-fig/fig-standards,382791162,440428433,"@fabpot is completely correct, on all points. And to his point about PSR-7 specifically, there's actually an inherent security flaw in its design pertaining to file uploads. I tried to alert the FIG to it some years ago, but given that they [deliberately hide the GitHub issues feature from the repository](https://github.com/php-fig/http-message), I took it as a strong indicator that they don't want to know.",2018-11-20T21:02:59Z,470626
2586,php-fig/fig-standards,382791162,440430577,This isn't a place for this discussion. Please use the mailing list.,2018-11-20T21:10:25Z,211740
2587,php-fig/fig-standards,382791162,445833348,Merged in 2e54756b87e895bf4e5238beafa0e20a36bad9d3 but github seems to be having issues,2018-12-10T14:26:31Z,211740
2588,Warzone2100/warzone2100,1465034831,1465034831,"**Describe the bug**
A clear and concise description of what the bug is.

When I play easy, it is too easy regardless of the map. When I pick medium, one difficulty level higher, I barely get a tank factory built and a battalion of enemy tanks show up at my doorstep. That to me is unplayable. 

**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.
There should be a steady flow, medium should be moved to insane.

**Screenshots or Videos**
If applicable, add screenshots to help explain your problem.

**Your System:**
 - OS: [e.g. Windows 10, Linux (Ubuntu 18.04)]
 - Game version: [e.g. 3.3.0, commit hash]

**Additional context**
Add any other context about the problem here.
",2022-11-26T03:40:53Z,12537899
2589,Warzone2100/warzone2100,1465034831,1328184570,"Assuming you meant skirmish, when starting with no bases and you build a factory after building three other structures, an AI building factory first could have already produced some tanks by that time. Most AIs are slow on easy, so you don't get that on easy. If you build factory first, medium will be easy too.",2022-11-27T07:02:36Z,119151239
2590,Warzone2100/warzone2100,1465034831,1328449389,"hey Zispah...i tried it both ways, got destroyed when i was slow and when i did the tank factory first...i destroyed im...thanx but...i need to try it a few times on different maps to make sure it wasnt a one-off...",2022-11-28T02:33:55Z,12537899
2591,Warzone2100/warzone2100,1465034831,1328480752,"I think you need to increase your APM (Actions Per Second). Sorry to break news for you man but ""barely get a tank factory built and a battalion of enemy tanks show up at my doorstep"" sounds like you are just not experienced enough (noob). Try watching (not playing!) multiplayer games or replays from other people, you can learn that way.
Also I suggest you to set up hotkeys, you can find list of important ones in Discord.",2022-11-28T03:16:27Z,20772987
2592,Warzone2100/warzone2100,1465034831,1328517563,"Haha noob you want to put that to the test? I've played it for 15 years...

On November 27, 2022 22:16:40 Maxim Zhuchkov ***@***.***> wrote:
>
> I think you need to increase your APM (Actions Per Second). Sorry to break 
> news for you man but ""barely get a tank factory built and a battalion of 
> enemy tanks show up at my doorstep"" sounds like you are just not 
> experienced enough (noob). Try watching (not playing!) multiplayer games or 
> replays from other people, you can learn that way.
> Also I suggest you to set up hotkeys, you can find list of important ones 
> in Discord.—
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you authored the thread.Message ID: 
> ***@***.***>

",2022-11-28T04:16:44Z,12537899
2593,Warzone2100/warzone2100,1465034831,1328523747,"> Haha noob you want to put that to the test? I've played it for 15 years...

Sure man, want to 1x1 *right now*? Join via IP `6.tcp.ngrok.io:16186`. I will be online for 15 minutes. If you will not come you are free to ping me in Discord when you are ready.

P.S. use latest supported version (4.3.2).",2022-11-28T04:26:59Z,20772987
2594,Warzone2100/warzone2100,1465034831,1328525660,"We can go

On November 27, 2022 23:27:10 Maxim Zhuchkov ***@***.***> wrote:
>
> Haha noob you want to put that to the test? I've played it for 15 years...
> Sure man, want to 1x1 right now? Join via IP 6.tcp.ngrok.io:16186. I will 
> be online for 15 minutes. If you will not come you are free to ping me in 
> Discord when you are ready.
> —
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you authored the thread.Message ID: 
> ***@***.***>

",2022-11-28T04:30:37Z,12537899
2595,Warzone2100/warzone2100,1465034831,1328536144,"So as expected, issuer is a complete noob and does not know how to play the game *and* has APM of a sloth. My POV will be available shortly after Youtube processes the video: https://youtu.be/T3Nlhzc2yjQ. Zip compressed replay file: [20221128_074207_multiplay_p0.wzrp.zip](https://github.com/Warzone2100/warzone2100/files/10100576/20221128_074207_multiplay_p0.wzrp.zip)

Let this be another show-game in my collection of noobs claiming that game is broken or everyone is cheating.",2022-11-28T04:53:48Z,20772987
2596,Warzone2100/warzone2100,1465034831,1328536731,"Rofl first time playing a human...you are easy...now that I see your game...

On November 27, 2022 23:53:59 Maxim Zhuchkov ***@***.***> wrote:
>
> So as expected, issuer is a complete noob and does not know how to play the 
> game and has APM of a sloth. My POV will be available shortly after Youtube 
> processes the video: https://youtu.be/T3Nlhzc2yjQ. Zip compressed replay 
> file: 20221128_074207_multiplay_p0.wzrp.zip
> Let this be another show-game in my collection of noobs claiming that game 
> is broken or everyone is cheating.
> —
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you authored the thread.Message ID: 
> ***@***.***>

",2022-11-28T04:55:13Z,12537899
2597,Warzone2100/warzone2100,1465034831,1328540953,"> Rofl first time playing a human...you are easy...now that I see your game...

Do note, my play was sloppy and all over the place. I did not cared about repair, usually I make 10 heavy repairs before medium body. Also you were killed with only machinegun, no anti-tank weapon at all, however I don't think you understand me in terms of weapon kind. This game was played without building base, I can show you base building too if you want, however certainly not right now, hit me up in Discord if you want a rematch.

You were playing against top 50 player of entire game, that's why it was *that easy*.",2022-11-28T05:00:57Z,20772987
2598,Warzone2100/warzone2100,1465034831,1331566985,"Like I said, I never played a human before you, I have studied very in-depth all of the AIs within the game. When we meet again I will show you that you are just another AI that I have to get a handle on.",2022-11-30T02:30:08Z,12537899
2599,Warzone2100/warzone2100,1465034831,1332546671,"> just another AI that I have to get a handle on

Except I am not an AI and you need to beat at least cobra or bonecrusher to have a chance to defeat me. Your usage of excuse ""never played against human before"" only  highlights your incompetence, we don't have fixed set of strategies, we adapt on the fly, kinda like nullbot but way more efficient. Game is not broken, you are broke here.

Warzone 2100 is a game of rock paper scissors but with ability to adapt and outsmart, there is nothing you can do if you don't know maths, mechanics, good strategies, units parts parameters, research tree and most importantly have APM of a sloth. I hope you have enough copium because the only way that you can defeat me is by exhaust and that will be rather difficult considering that not long ago I played for more than 13 hours straight into different game. You will likely just refuse to fight and call me cheater or something way earlier than I will feel any weaker.

As I already said, feel free to come to Discord and ping me for a rematch at any time, I have time and dedication to fight to exhaustion, let's find out who will give up first.",2022-11-30T18:05:36Z,20772987
2600,Warzone2100/warzone2100,1465034831,1332709505,"nah..listen, I can't see you...I just see code. Your strategies are irrelevant, you too will be assimilated.
",2022-11-30T20:37:54Z,12537899
2601,Warzone2100/warzone2100,1465034831,1332727370,"Seems like only thing you can do so far is talk, come back when you will have any skill to show.",2022-11-30T20:58:23Z,20772987
2602,Warzone2100/warzone2100,1418948680,1418948680,"Recently there were proposed multiple breaking changes to multiplayer balance. Some of them were obvious and welcoming and some of them are just plainly stupid let alone controversial.
Tipchick (main author of the changes) is not seen in multiplayer for more than a year at this point, this is pure chaos, I already understood that @KJeff01 is Tipchick's lap puppet, no need to prove it over and over again. I completely disagree with position of pulling random numbers from asshole and proposing them as a changes to balance that hundreds of people will play for a whole release cycle. We do not want 3.x era to come back in a new form.
Until guidelines are established on balance-releated merging policies are put in place this is just pure griefing. How about people that are actually playing like Fenrir, Fedaykin, Evolution and others (like Brazil community) will make changes to something and review all the changes? I only see bird-mail and repeating Tiger, Tipchick, Ayami that tested something somewhere but can not even upload a mod that they had tested it with.
It is pure pushing until someone notices it had gone horribly wrong and I bet no one will react to it until a stable release is cut out.
Similar situation already happened (and well described in issue #2469) and I think it is time to learn from the past mistakes.
I proposed contributing guidelines in pull request #2892 and it seems like current maintainers deliberately ignore it and continue turning every single stone that they can just for the sake of it. Issue is growing day by day as more pull requests are being opened/changed mainly by @KJeff01 and @Tipchik87.
I will also note complete lack of fact checking upon merging, just ""I tested it"" should not be a reason to believe that any testing was actually performed, no replays were provided, no people who testing were performed with commented on a thread, nothing.

As of my part, I am trying to remain neutral on all the changes except obvious trash. It is clearly been established that my opinion and opinion of anyone else complaining about how breaking proposed changes are just being ignored or dismissed with same phrase ""we tested it"" with lack of fact checking.
Lately I got very busy in university and I can not test stuff myself but at the same time I do not claim that I performed any of them nor do I keep up with all the written arguments so my role here is only to filter out the obvious.",2022-10-21T23:10:25Z,20772987
2603,Warzone2100/warzone2100,1418948680,1287547347,"What would be a good solution(and others have mentioned this time and time again) is to not tie balance to a specific version, but instead to have a ""live"" MP balance, so that any changes that someone may want to do will be instantly applied - of course, people will want to know about every change...",2022-10-22T00:08:40Z,5343470
2604,Warzone2100/warzone2100,1418948680,1287557360,"For some clarity:
- There are a bunch of open PRs relating to balance
- These are for testing and comments, and they _will not be merged for 4.3.0_
- To underscore this, we've marked them all ""Draft"" and also `state: please discuss`

Please don't assume that just because something is a PR that it means it _will_ be merged.
But we're going to make this clearer going forward by marking them appropriately as Draft and ""please discuss"".

Now, actually gathering feedback on proposals is a harder matter, as only a subset of a subset of people check in here on GitHub.

""Live"" balance is tricky, especially as the balance of the game comes from the combination of the stats and the game engine (which occasionally gets tweaks or fixes that might ultimately impact balance itself).

---

I think one thing we could do, though, is possibly stage balance proposals and offer an in-game option to download and apply testing balance mods when hosting a game.

I could see offering 3 balance options when hosting a game:
- **Default balance**
   - This would be the version shipped with that copy of WZ
- **Development balance** (next release)
   - This would correspond to what's merged in the master branch of WZ (and thus what's already staged for the next stable release)
- **Experimental balance**
   - This would correspond to balance changes that _have not yet been merged_, and are under discussion and/or need testing.

To handle possible ramifications of other game engine changes, we might limit Development and Experimental balance to being prepared for the current stable release. (As in: they would be enabled as options as long as you are running the latest stable version of WZ. Otherwise, if you're running an old version, you're stuck with Default balance - as is currently the case - unless you manually use a mod.)

Having these options in-game would really expand the ease of testing balance changes, and make it clear what's just proposals and what's actually planned for the next release.

(Implementing this will require a bit of backend work, and so it won't happen immediately. But I think it's probably the better long-term strategy.)",2022-10-22T00:32:04Z,30942300
2605,Warzone2100/warzone2100,1418948680,1287755568,"Long term strategy described below (above) is correct one (imo) and we are waiting for this for almost 2 years at this point.

Even though there is a vision and plan on how to deal with balance changes in the future, it will be in the future. Right now guidelines should be established on who, how, in what order and with what requirements are allowed to merge/propose changes to the multiplayer balance. Without them, this will continue uncontrollably, just a bit later when people shift their attention away and everyone calms down.

Once release hits with low quality ""testing"" changes, only one way will be to fix it - release a next stable version. This will cause every single one who plays multiplayer to download/build new version just because of someone who decided it will be good idea to shuffle everything around in chaotic manner, as Kracker described it ""throwing at a wall and checking what sticks"".",2022-10-22T10:32:27Z,20772987
2606,Warzone2100/warzone2100,1418948680,1287861397,"2 years too late Max. You yourself were praising the balance to Calculus just last night. I'll never apologize
for making the game more fun and strategic even if the method was harsh but necessary (remember when Medium Cannon was useless? Or what about Tank-Killer? Heavy Cannon? ...). My purpose here is to do what nobody else wants to do. I am a hidden hand in the dark guiding things around even if I'm not attributed to something directly.

You have an unhealthy obsession with Tipchik and his friends. A lot of ideas were mine too. The reason he doesn't make PRs anymore is that he feels like you attack him all the time and just thumb down anything without explanations why you dislike something. Make that effort and maybe you can be friends. Maybe something went not too well one time and to that I say ""oh well"". One time out of 2 years is pretty good if you ask me. I didn't intend to merge anything currently open for 4.3.0, as it's too late, so perhaps that scared you.

You will be happy to know that pastdue is already experimenting with his idea outlined in his post here as we speak. Of course, exactly when he completes that, is another thing.

Overall, I feel my balance goals are **very** close to complete. I still have yet to unleash my final grand act. My _masterpiece_. The cherry on top of the Chocolate Balance Sundae. :sunglasses: 
",2022-10-22T17:27:45Z,22485442
2607,Warzone2100/warzone2100,1418948680,1287867617,"You put beta3 tag on those pull requests, no excuses. Do not pretend that you would not merge them into stable release if I did not put stuff on public notice.

> a hidden hand

And surprised that no one responds to you or cares about balance?

Also, I am not attacking personally only Tipchick's changes, I just see them more often because his name signals me to check it out because it might be ridiculous. Reverting something from master is always more difficult than preventing merge in the first place. Also because most of the changes are either by him or marked as authored by him, he just made more changes, that's why. ",2022-10-22T17:44:24Z,20772987
2608,Warzone2100/warzone2100,1418948680,1287900930,"For one that I later reconsidered? I removed it long before this as I was under the assumption autohoster games would generate more consideration and maybe there was a change of pace for once. Unfortunately, ratings were turned on for the _sole purpose to gatekeep things you personally disagree with_. So that defeated the purpose.

> And surprised that no one responds to you or cares about balance?

You actually told me the people in the Russian chatrooms are _too stupid to understand what balance is or how to use a mod_. So, with your logic, how could they?

Source changes, balance, I lumped that into one statement. Anyway, they either don't care or think it's fine. I've been asking you for years who all these people are but you never tell me. These supposed angry Russian mobs of players that torment you every time you hop into voice chat. I'm still waiting for this big list of people to this day and, perhaps, you actually can't tell us.",2022-10-22T20:05:50Z,22485442
2609,Warzone2100/warzone2100,1418948680,1287904527,"> for the sole purpose to gatekeep things you personally disagree with

Wrong, I told you make a complete mod and I will update it at the same time as turning off ratings since it will become not evaluation of balance but just a tweaking sandbox. I tried to make people think about what they are doing because if they are not sure about changes maybe they need to think more about it and making those games rating enabled makes them rethink their decisions and not just waste time.

> people in the Russian chatrooms are too stupid to understand what balance is or how to use a mod

Yes I did because it is true, and not only for Russians, I would put bold 85% estimate of people who play multiplayer that don't even know that changes are being made, let alone on github in ""public"".

I am not bothered to even start collecting this list, if you want you can join Russian discord server and count how many people have green colored role (quiet a few), I am vouching that every single one of them at some point in time came to me with balance questions. Also, I picked up very good trait from Tipchick, just like with his testing and his friends, they sure exist but no one ever showed even a single replay so I will do same here, and don't even try to say something about it, otherwise it will be you hating me instead (just like I hate Tipchick as you and he says).

Also Vaut and I explained endlessly (you can count people from there too), on YouTube, on stream, on record, how to contribute, how to test, what to do and where to find everything and everyone just look at the login form of github and do nothing. aka ""what button do I press here""",2022-10-22T20:21:28Z,20772987
2610,Warzone2100/warzone2100,1418948680,1287926034,"> Wrong, I told you make a complete mod and I will update it at the same time as turning off ratings since it will become not evaluation of balance but just a tweaking sandbox. I tried to make people think about what they are doing because if they are not sure about changes maybe they need to think more about it and making those games rating enabled makes them rethink their decisions and not just waste time.

I already did yesterday and additionally provided you a pure master version (as asked) in addition to the experiments mod people think are a good idea to try. You said it was trash cause of the Flashlight buff and you won't host it. And if it was bad, I said you could laugh at him about how it sucks and I'd remove it. The tank Flashlight isn't very popular itself so it's not like I was seeing anything wrong myself with the thought.

> Yes I did because it is true, and not only for Russians, I would put bold 85% estimate of people who play multiplayer that don't even know that changes are being made, let alone on github in ""public"".

I don't see why these people would care too much. Sounds like the kind of players that like to play chill games and have fun. Nothing else. So it is expected they wouldn't notice. It only interests the competitive scene.

> I am not bothered to even start collecting this list, if you want you can join Russian discord server and count how many people have green colored role (quiet a few), I am vouching that every single one of them at some point in time came to me with balance questions.

Questions are one thing, complaints/yelling are another. There are many _skilled players that own and know how to use a GitHub account on here_. Their opinion carries considerable weight/value and most of the time a couple of them will :+1: and that's it after several days of a PR being open. I certainly don't see them raising issues or saying anything. You must understand how this makes me skeptical. Especially with 2+ years. I am all for ending this drama saga but, as things are, facts are unclear. Someone is exaggerating / lying to me. And if the vast amount of the mp community, as you say, don't understand balance, do we put _significant_ weight on their opinions if so?",2022-10-22T21:39:11Z,22485442
2611,rstudio/rstudio,807725060,807725060,"### System details

    RStudio Edition : Desktop
    RStudio Version : 1.4.1103
    OS Version      : Linux 5.4.80 #1 SMP GenuineIntel GNU/Linux
    R Version       : 3.4.1

### Steps to reproduce the problem

Version 1.3.1093 did not dynamically link to libpq (from PostgreSQL), but 1.4.1103 does.

One can verify this by simply running ldd /path/to/rstudio.

### Describe the problem in detail

The Release Notes at:

https://rstudio.com/products/rstudio/release-notes/

mention that RStudio Server Pro now requires a Postgres database, but says nothing about RStudio Desktop (and lacks any explanation why the Desktop version would require a database).

### Describe the behavior you expected

I would expect database connectivity to remain in the odbc package, and not pushed into the basic desktop. I strongly object to requiring something not found in a typical desktop setup, and request that if this dependency remains, you provide the necessary libraries with the RStudio Desktop installation (as you do with the Qt libraries and icu - both of which most Linux desktop machines already have).

- [x] I have read the guide for [submitting good bug reports](https://github.com/rstudio/rstudio/wiki/Writing-Good-Bug-Reports).
- [x] I have installed the latest version of RStudio, and confirmed that the issue still persists.
- [x] I have done my best to include a minimal, self-contained set of instructions for consistently reproducing the issue.",2021-02-13T10:46:38Z,550150
2612,rstudio/rstudio,807725060,782130319,"@2011 Thank you for raising this! I'll mark it for review as part of our ongoing development of RStudio.

@kfeinauer Could this have something to do with introducing the internal database?",2021-02-19T15:03:27Z,31009952
2613,rstudio/rstudio,807725060,782150662,"@ronblum Yes, this came about when adding database support. The dependency should not be necessary on the desktop, but due to how we deal with dependencies a shared component that depends on this library is pulled in by both server and desktop. This requirement could be removed in a future release.",2021-02-19T15:32:44Z,28230621
2614,rstudio/rstudio,807725060,782267753,@kgartland-rstudio Thanks! I'll switch this from a bug to an enhancement request.,2021-02-19T18:46:04Z,31009952
2615,rstudio/rstudio,807725060,841834983,"I'd like to suggest this is, in fact, something closer to a bug, and the previous label should be restored.

I understand why it was added—RStudio Server 1.4 uses a database in order to manage internal data. However, the documentation shows two options: SQLite and Postgres, with SQLite is the default. Requring a Postgres dependency _when it is not even being used_ poses a significant installation burden in many contexts.",2021-05-16T15:46:16Z,2797735
2616,rstudio/rstudio,807725060,996210182,I'm attempting to install rstudio-2021.09.1-372 and am using R 4.1.0. Is there really no way to get around the postgres requirement? We support a shared high performance computing environment and do not install databases. Is there a way to install just the libraries necessary to get the rstudio install to complete? Any advice is greatly appreciated!,2021-12-16T21:32:30Z,9355962
2617,rstudio/rstudio,807725060,998173148,"@dsajdak are you installing desktop, or server?  What platform?  I'm not understanding why this is a concern; you shouldn't have to install a database to get it to work.",2021-12-20T18:32:33Z,10076690
2618,rstudio/rstudio,807725060,998187012,"Note: I believe we're requiring an installation of `libpq5`, but not all of PostgreSQL, when installing RStudio Desktop in a Linux environment.",2021-12-20T18:55:33Z,31009952
2619,rstudio/rstudio,807725060,998977126,"> Note: I believe we're requiring an installation of `libpq5`, but not all of PostgreSQL, when installing RStudio Desktop in a Linux environment.

WHY?  I have used RStudio desktop for years, and it never had this requirement before version 1.4.

1. RStudio desktop should not require this library.
2. Database connectivity should remain in the odbc package.
3. Typical desktop installations don't have this library (including mine - I would have to compile the entire PostgreSQL to upgrade to RStudio 1.4).
4. If you really feel that RStudio desktop absolutely **has** to have libpq, then why don't you provide it in the package? The installation package includes libraries for Qt and icu, which virtually every Linux desktop has. It makes absolutely no sense whatsoever to include those libraries with RStudio, yet demand users produce their own libpq library.",2021-12-21T17:49:27Z,550150
2620,rstudio/rstudio,807725060,999033374,@2011 I was merely clarifying the issue.,2021-12-21T19:21:45Z,31009952
2621,rstudio/rstudio,807725060,999033787,And thanks to both of you for that! I hope the exchange helps illuminate the matter better. I have personally solved the issue for my application but I still believe it is a problem that should be addressed.,2021-12-21T19:22:23Z,2797735
2622,rstudio/rstudio,807725060,999041076,"@mcg1969 Darn it, I thought I had switched this back to a bug back in May in response to your comment! Changing now.",2021-12-21T19:34:27Z,31009952
2623,rstudio/rstudio,807725060,999043121,"The original thought in making `libpq` required is that it would be available to easily install via the system package manager when installing the RStudio `deb`/`rpm`. Clearly that is not always the case, and I apologize for the inconvenience this has caused you.

The `libpq` library has quite a few dependencies itself, so I think bundling them all and shipping them with RStudio is error-prone. 

I think we should allow for dynamic loading of the `libpq` library on Linux. SOCI (the database manager we are using) has some support for this (http://soci.sourceforge.net/doc/release/4.0/connections/). Otherwise we'd need to build `libpq` on all platforms and statically link to it, which could be tricky.",2021-12-21T19:38:02Z,28230621
2624,rstudio/rstudio,807725060,999056365,"@mikebessuille 
> @dsajdak are you installing desktop, or server? What platform? I'm not understanding why this is a concern; you shouldn't have to install a database to get it to work.

I'm attempting to install desktop version 2021.09.1+372, for CentOS 7, from source.  We install everything to a shared mounted filesystem and can not install using OS packages.  I spent 2 days getting all the dependencies installed in the shared filesystem only to have the rstudio 'cmake' fail with:
`CMake Error: The following variables are used in this project, but they are set to NOTFOUND.
Please set them or make sure they are set and tested correctly in the CMake files:
/shared/directory/.../PQ_LIB
linked by target ""rstudio-core-tests"" in directory /shared/directory/.../rstudio-2021.09.1-372/src/cpp/core
    linked by target ""rstudio-core"" in directory/shared/directory/.../rstudio-2021.09.1-372/src/cpp/core
SOCI_POSTGRESQL_LIB
    linked by target ""rstudio-core-tests"" in directory/shared/directory/.../rstudio-2021.09.1-372/src/cpp/core
    linked by target ""rstudio-core"" in directory /shared/directory/.../rstudio-2021.09.1-372/src/cpp/core
`
I was hoping someone would have a suggestion as to how to get around this without a full Postgres install.",2021-12-21T19:59:03Z,9355962
2625,rstudio/rstudio,807725060,999077255,"@dsajdak You need to have the `libpq` and `libsoci_*` files under your shared directory. Then you need to inform cmake where to look by specifying the `CMAKE_LIBRARY_PATH` environment variable. 

See https://cmake.org/cmake/help/latest/command/find_library.html and https://cmake.org/cmake/help/latest/variable/CMAKE_LIBRARY_PATH.html#variable:CMAKE_LIBRARY_PATH for more information on how this is found. 

We find this library here in the CMake file: https://github.com/rstudio/rstudio/blob/5e180632285fd095c6f01ff385935a6efded7e0a/src/cpp/CMakeLists.txt#L442-L456

You can use our `install-soci` script to build SOCI. It's designed to run at certain paths that we use for our builds, so it might not work out of the box for you. https://github.com/rstudio/rstudio/blob/main/dependencies/common/install-soci. Particularly, you may need to specify the path to `libpq` to get it to build correctly. This can be done by adding it to the `CMAKE_LIBRARY_PATH` here https://github.com/rstudio/rstudio/blob/5e180632285fd095c6f01ff385935a6efded7e0a/dependencies/common/install-soci#L83",2021-12-21T20:35:16Z,28230621
2626,rstudio/rstudio,807725060,999080384,"@kfeinauer Thank you!  I'm unable to get to the rstudio links you shared.  Perhaps they're in a private repo?  I am currently testing this with installing the postgres-devel and soci-postgresql-devel rpms on my build host.  If these libraries/packages are not required to run rstudio desktop, this should work.  If they are, then I'd rather your suggested method.",2021-12-21T20:41:21Z,9355962
2627,rstudio/rstudio,807725060,999085275,"@dsajdak You'll need to build SOCI from source because we statically link it, so you need `.a` files instead of the `.so` that are shipped by the packages. As for `libpq`, you can get it from `postgres-devel` but you should also be able to get it from `libpq5-devel` (much smaller package).

Sorry about the visibility of the links. I have edited my comment to fix them.",2021-12-21T20:50:22Z,28230621
2628,rstudio/rstudio,807725060,1000643043,"> The `libpq` library has quite a few dependencies itself, so I think bundling them all and shipping them with RStudio is error-prone.

Hard for me to believe that bundling those exceeds the complexity of bundling `libQt5WebEngineCore`. :)",2021-12-24T04:14:58Z,550150
2629,rstudio/rstudio,807725060,1048534391,"@MariaSemple I strongly object to the changes you made on 0110. I filed the bug, and (for me) it has absolutely nothing to do with bulding RStudio from source. I can't run the (version 1.4 and later) packages you supply because of the (new) dependency on libpq. I do not view fixing this as an enhancement - it obviously represents a bug (something working before that no longer works).",2022-02-23T08:13:07Z,550150
2630,rstudio/rstudio,807725060,1048909603,"I agree, @2011

@MariaSemple , I understand that the RStudio maintainers may have simply concluded that they are going to be unable to address this issue to the satisfaction of its reporters. I have plenty of experience disappointing my users, too :-) it happens! 

But I think that preserving the historical record is important. This was never about the need to build from source. I think the proper course of action is to revert the title of this issue but to mark it as a ""wontfix"" if indeed the RStudio team has decided it is not feasible to solve

Thank you all!",2022-02-23T15:34:09Z,2797735
2631,rstudio/rstudio,807725060,1049232507,"I believe I misread some of the context of this issue, apologies for the confusion. I will change the title to ""Remove libpq dependency in RStudio Desktop""; however, I will leave the tags as they currently are. While I understand that it is frustrating to not be able to use the newer version because you are unable to install the new dependency, the new dependency is not a bug as the addition of the dependency was intentional. To remove the dependency now would be considered an enhancement.

`libpq` was introduced as a dependency due to RStudio Server requiring it. RStudio Desktop and RStudio Server share much of the same code, and as a result the dependency was introduced on both sides. The dependency is not related to the Connection Pane feature or the ODBC Drivers that it uses. It is non-trivial to remove the dependency and introduces some regression risk. 

After discussing this with @kfeinauer I propose the following potential solution:
* Always explicitly link against SQLite libraries (which are used by all versions of the product).
* Never explicitly link against PostgreSQL libraries - instead use SOCI's feature that allows for dynamic loading of DB libraries through a call to `dlopen`.

If we choose to go this route, we will need to ensure that there is a very clear error message if `libpq` cannot be found, and that it occurs very early in start up of RStudio Server.",2022-02-23T21:28:29Z,37987486
2632,rstudio/rstudio,807725060,1049277776,"Just to emphasize (and perhaps clarify) some things, I don't consider this a ""documentation"" issue at all. RStudio simply will not start without the missing library (with a message that it can't find libpq). I don't see any need for additional documentation on the matter.

I do want to quote @kfeinauer (20210219):

> The dependency should not be necessary on the desktop, but due to how we deal with dependencies a shared component that depends on this library is pulled in by both server and desktop. This requirement could be removed in a future release.

To repeat the points I made on 20211221, RStudio already comes bundled with more than 25 libraries (Qt and icu), most of them completely superfluous. Virtually every linux desktop has those libraries already, but RStudio supplies additional copies of them (I won't condemn that, because bundling versions known to work does add some value at the cost of disk space and memory use). But the one library many desktops do not have installed by default (libpq) gets left out. No offense to you (I doubt you personally made the associated decisions), but I would have have to place this in the top two software engineering idiocies that I have seen over the past five years.

If desktop users **have** to have libpq (and I imagine that the vast majority of them would find themselves content with SQLite), then bundle it (or bundle a stub library that allows RStudio to start, but then doesn't allow the choice of PostgreSQL as a database without libpq. I just don't understand the thinking on this (other than ""we want to make things more difficult for our users"").",2022-02-23T22:30:01Z,550150
2633,rstudio/rstudio,807725060,1050925359,"Your perspective is much appreciated; please maintain a professional tone.

Coding decisions including dependency decisions are not made frivolously.  Sometimes, design choices are made to minimize development cost, maximize maintainability, reduce testing cost, or to have common code between the many versions of our product (server, desktop, pro, open source).  As such, this issue is not a bug.  Nor is it an ""idiotic"" design choice.  We will certainly consider improving it such that it doesn't affect the case you describe, keeping in mind that most of our users are unaffected by this, as they install the IDE from installers (which take care of the appropriate dependencies), rather than by building from source or by unpacking a tarball and manually installing.

",2022-02-25T14:57:11Z,10076690
2634,rstudio/rstudio,807725060,1050932328,"I appreciate the moderation. While I do agree this should be fixed, it is far from the worst thing I have encountered :-) in fact, I've done far worse myself! Thank you for correcting the title and categorization.",2022-02-25T15:05:08Z,2797735
2635,spring-projects/spring-framework,1102918184,1102918184,"According to pull request #1848, utility classes are noninstantiable, but abstract is not enough to prevent from creating objects by subclasses.
This change modified almost all utility classes, but I do think it is necessary, and the effects should small and limited.",2022-01-14T04:14:59Z,16933298
2636,spring-projects/spring-framework,1102918184,1012913142,@scruel thanks for the PR but we'd rather keep things as they are.,2022-01-14T08:32:51Z,490484
2637,spring-projects/spring-framework,1102918184,1012917357,"> @scruel thanks for the PR but we'd rather keep things as they are.

Fine… but it must be a bad decision, and could you explain based on what reason so that you will not change this?",2022-01-14T08:39:42Z,16933298
2638,spring-projects/spring-framework,1102918184,1012921743,`abstract` is a fine enough signal for us. Yes you can instantiate public utils classes still but I don't think this is worth the extra noise.,2022-01-14T08:46:26Z,490484
2639,spring-projects/spring-framework,1102918184,1013000075,"@snicoll Fine enough signal for you is not acceptable, could you consider that spring is using by so many people all around the world, no one can make sure that others will not treat `abstract` as a keyword as it is: you should extend this and create an object?
By the way, the ""extra noise"" that you mentioned are already exists in this project, you could find some utils classes which have the private constructor(most of them does not have), and I do think `abstract` should be treated as the ""noise"", because we should not use it for preventing to instantiate the object.
Also, I recommend you read *Effective Java, item 4*.
Thanks.",2022-01-14T10:33:19Z,16933298
2640,neovim/neovim,1303268876,1303268876,"**Goal:** Enable out-of-the-box `treesitter` highlighting of core languages (Lua, Vimscript) for Neovim 0.8

**Roadmap:** 

* [x] Replace static capture-to-highlight group table with automatic mapping (hierarchical, e.g., `@keyword.special` from Lua gets mapped to `TSKeywordSpecialLua`, with fallback to `TSKeywordSpecial` or `TSKeyword` if the more specific groups don't exist #19931
* [x] Include Lua and  Viml parsers in build process (similar to C parser) #15391
* [x] Ship parsers and queries in runtime  #15391
* [x] Upstream basic highlighting functionality from `nvim-treesitter`  #15391
* [x] Change query handling to give same precedence as for parsers (config > plugins > runtime), with explicit extending via  modelines (needs documentation!) #20104 #20117
* [x] Upstream utility functions from `nvim-treesitter` and `nvim-treesitter-playground` #19946 #20093
* [ ] ~~Upstream predicates and directives from `nvim-treesitter`; refactor metadata handling to be more generic?~~ (bumped to 0.9)
* [x] Upstream https://github.com/lewis6991/spellsitter.nvim (enabled automatically if treesitter highlighting is enabled for a language) #19351
* [x] Update and polish `vim.treesitter` docs #20142
* [x] 🥳 ",2022-07-13T10:59:23Z,2361214
2641,neovim/neovim,1303268876,1219358736,"# Proposal for highlight groups:

Create a new set of hierarchical groups that are mapped **losslessly** from captures (e.g., `@keyword.special`), with sane fallback to defaults.

1. Implicitly generate a highlight group for every capture, e.g., `TS.keyword.special` that can be mapped directly (but isn’t necessary by default)
2. Groups are implicitly language-specific, e.g., `TS.keyword.special.lua`
3. If not mapped, they successively fall back to the top level, e.g., `TS.keyword` (**Question:** fallback to `TS.keyword.special` or `TS.keyword.lua` first?)
4. Provide a set of top-level (and possibly _some_ second-level?) groups that are linked by default to suitable standard Vim groups and/or are mapped by the default and bundled color schemes.  These groups need not derive from the set of standard Vim groups; it might be better to provide a small(!) number of different sets for different types of languages (with overlap), like ""imperative languages"" (C, Lua, Python, ...), ""markup languages"" (Markdown, LaTeX, HTML), ""data representation languages"" (JSON, regex, ...).
5. Languages are free to extend the capture hierarchy, e.g., `@snowflake.superfancy`, with the clear understanding that these won’t be highlighted unless mapped by custom colorschemes or plugins. (This also allows optional highlighting that is disabled by default.)

Ideally, we can use capture names directly as highlight groups, similarly to how syntax and highlight groups are matched?
",2022-08-18T11:04:28Z,2361214
2642,neovim/neovim,1303268876,1225495854,"> 1. Implicitly generate a highlight group for every capture, e.g., `TS.keyword.special`

With https://github.com/neovim/neovim/pull/19830 would we generate the identical names (`@keyword.special`) instead?",2022-08-24T09:55:15Z,1359421
2643,neovim/neovim,1303268876,1225510328,"Proposal LGTM except this part:

> These groups need not derive from the set of standard Vim groups; it might be better to provide a small(!) number of different sets for different types of languages (with overlap), like ""imperative languages"" (C, Lua, Python, ...), ""markup languages"" (Markdown, LaTeX, HTML), ""data representation languages"" (JSON, regex, ...).

Can that be postponed to a ""phase 2"" discussion? Just thinking this might be an expensive discussion.",2022-08-24T10:09:20Z,1359421
2644,neovim/neovim,1303268876,1225711607,"*(Changed my mind.)*

Sure; none of the technical details of the other points depends on this decision. I merely included it here in case we wanted to rip the band-aid off in one go. Definitely not a blocker for marking this tracking issue as completed.",2022-08-24T13:15:21Z,2361214
2645,SevenTV/chatterino7,952956841,952956841,"This post will be a disaster to format, so please bear with me.

Chatterino dropped support for Windows 7 on version 2.1.0 (this being the last version supporting windows 7). Coincidentally, Version 2.2.0 was the last one to allow ""notify when online"" notifications; version 2.2.1 was the first to require logging in to have ""notify when online"" notifications (there is other functionality logging in allows, but I am not educated on them). Source:

https://web.archive.org/web/20210421115826/https://chatterino.com/
(this is the last screenshot of the website before they changed the design; on this screenshot you can see the text ""Last version supporting Windows 7 (2.1.0 64-Bit)"")

https://github.com/Chatterino/chatterino2/issues/1915
> chatterino no longer able to pull stream online status while anonymous #1915

I am not sure if chatterino began supporting Windows 7 again or not, but I was no longer able to find mentions of the last Windows 7 version being 2.1.0 past the above screenshot of their website. I was actually able to install the latest version for Windows normally a few weeks ago, despite my operating system being Windows 7, but I reverted back to 2.1.0 just because the latest chatterino required logging in for ""notify when online"" notifications, as I've established earlier.

Chatterino has professed their unwillingness to allow ""notify when online"" notifications without login:

https://github.com/Chatterino/chatterino2/issues/2257#issuecomment-739330763
> No. This idea was already brought up before. Chatterino does not want to start helping ban circumventions.

So therein is my difficult to explain feature request. I will try to summarize what I'm asking for:

- A chatterino7 release that is discrepant from the chatterino2 philosophy of not allowing non-logged in users to receive ""notify when online"" notifications (among other features)
- A chatterino7 release that supports Windows 7 (my understanding was that 2.1.0 was the last version to support Windows 7, but at some point that appears to've changed, so perhaps this is just a given)

I guess that's it? I still had trouble formatting this, even if I can tersely communicate what I wanted, at the end of it. If nothing else, will you please spoonfeed me a chatterino7 version of the 2.1.0 installer? The 2.1.0 version on your github is just the source code, which gave me grey hairs trying to work out. For the time being I'm still on the plain chatterino 2.1.0, which doesn't have 7tv emotes.

edit: removed citing ""protection from global bans"" quote as demonstrating that chatterino used to willingly allow ""notify when online"" notifications without login. I realize now that ""global bans"" likely meant regions in the world, and was independent of their unwillingness to willingly provide this functionality to chatterino users who don't log in.",2021-07-26T14:18:27Z,86159848
2646,SevenTV/chatterino7,952956841,887479369,"We (Chatterino devs/contributors) made a decision to move Chatterino to use only Helix (the new Twitch API) instead of Kraken (aka v5). Helix doesn't support anonymous title/live checks anymore (bother Twitch about that, not us). **The feature on your old Chatterino version will stop working soon because Kraken is being [DECOMMISSIONED](https://discuss.dev.twitch.tv/t/legacy-twitch-api-v5-shutdown-details-and-timeline/32649).**

We don't support Windows 7 anymore, as doesn't Microsoft. We can't support anything indefinitely. This OS is 12 years old, it's about time. Most users have moved to newer *and supported* OSes.
",2021-07-27T12:42:57Z,25011746
2647,SevenTV/chatterino7,952956841,887528922,"Well, I somewhat understand. Is a Windows 7 user still able to have a functioning chatterino above version 2.1.0? If so, functioning to what degree?

Also I feel it is dishonest of you to just refuse to acknowledge the quote I cited demonstrating the outright refusal to allow anonymous ""live checks"" even if it were possible for the foreseeable future. There is a difference between ""we could if we would"" and ""we refuse to"". You making a third choice for yourself of ""the choice is out of our hands"" is too late considering I cited a quote on chatterino's stance on this already.",2021-07-27T13:48:52Z,86159848
2648,SevenTV/chatterino7,952956841,892214344,"Hello barbedknot

Thank you for your concerns, I will assist by addressing them one by one.

> Chatterino dropped support for Windows 7 on version 2.1.0

The dropped support was as a result of Microsoft themselves no longer offering support for Windows 7. To users that are on Windows 7, it is highly suggest you upgrade to Windows 10.

> I was actually able to install the latest version for Windows normally a few weeks ago, despite my operating system being Windows 7

Yes, support has been dropped for Windows 7. You may be able to install it, however we have no plans to provide support for bugs/issues on Windows 7 devices.

> * A chatterino7 release that is discrepant from the chatterino2 philosophy of not allowing non-logged in users to receive ""notify when online"" notifications (among other features)

As already mentioned, we are trying to move away from Kraken as a result of the API being decommissioned. Helix endpoints have a strict requirement for OAuth when trying to use any endpoint as per the API docs: https://dev.twitch.tv/docs/api/reference. This is inclusive of knowing when a stream is online or not. Hence why you are unable to see if a stream is live (and do/see many other things) if you are not logged in.

> * A chatterino7 release that supports Windows 7 (my understanding was that 2.1.0 was the last version to support Windows 7, but at some point that appears to've changed, so perhaps this is just a given)

As aforementioned, Microsoft has dropped all support for Windows 7. Why would we support Windows 7 if the developer doesn't themselves? It seems absolutely preposterous to me.

> Is a Windows 7 user still able to have a functioning Chatterino above version 2.1.0?

Sure, it may work. We still don't provide support for it.

> Also I feel it is dishonest of you to just refuse to acknowledge the quote I cited demonstrating the outright refusal to allow anonymous ""live checks"" even if it were possible for the foreseeable future. There is a difference between ""we could if we would"" and ""we refuse to"". You making a third choice for yourself of ""the choice is out of our hands"" is too late considering I cited a quote on chatterino's stance on this already.

Chatterino is a community-made, open source project. Nobody gets paid to work on it. I'll put it quite simply; Kraken is not being added into Chatterino, just so users are able to see when a streamer is live in anonymous mode.

Your whole issue revolves around complaining that there's no Windows 7 or Kraken support. Both of which have had all support dropped by their main developers. So I ask you; why would we add support for said feature, if the devs themselves do not support it?

Kind Regards,
ALazyMeme",2021-08-03T22:44:50Z,12804673
2649,SevenTV/chatterino7,952956841,892674662,"> Also I feel it is dishonest of you to just refuse to acknowledge the quote I cited demonstrating the outright refusal to allow anonymous ""live checks"" even if it were possible for the foreseeable future. There is a difference between ""we could if we would"" and ""we refuse to"". You making a third choice for yourself of ""the choice is out of our hands"" is too late considering I cited a quote on chatterino's stance on this already.

x2

Kind Regards,
person calling out intellectual dishonesty. smiley face",2021-08-04T13:49:13Z,86159848
2650,WordPress/gutenberg,980057113,980057113,"## What problem does this address?
<!--
Please describe if this feature or enhancement is related to a current problem
or pain point. For example, ""I'm always frustrated when ..."" or ""It is currently
difficult to ..."".
-->

The `wp-env` utility library is currently contained within the Gutenberg project. Unfortunately, this makes it a bit difficult to find the project files and related issues/pull requests amidst the bustling Gutenberg development activity.

## What is your proposed solution?
<!--
Please outline the feature or enhancement that you want and how it addresses any
problem identified above.
-->
Move the `wp-env` project to its own repository, so code, issues, and pull requests are easier to locate/organize.
",2021-08-26T09:56:38Z,17307
2651,WordPress/gutenberg,980057113,906275124,"This issues should all be labelled - https://github.com/WordPress/gutenberg/issues?q=is%3Aopen+is%3Aissue+label%3A%22%5BPackage%5D+Env%22. Same with PRs.

If there's any missing labels, they can be labelled.",2021-08-26T10:13:54Z,677833
2652,WordPress/gutenberg,980057113,906290056,"Issue labels are indeed useful but it still seems like a reasonable idea for the project to have its own repository. :smiley: 

There is a lot of unrelated activity in this repository such as running the full Gutenberg test suite on every pull request, even when it just relates to wp-env. 

![Peek 2021-08-26 13-58](https://user-images.githubusercontent.com/17307/130951178-370010bf-a76d-4f76-879b-583b160d464a.gif)


Also, developers have to check out all of the Gutenberg code when wanting to only make a change to wp-env.

![image](https://user-images.githubusercontent.com/17307/130949152-155f62d1-543b-4ec1-92a7-0bd66881dfec.png)
",2021-08-26T10:37:38Z,17307
2653,WordPress/gutenberg,980057113,911407452,"`@wordpress/env` is also used for development in Gutenberg so there are also pros of keeping the source code in the same repository. There is more to it, we use one way to publish packages to npm, there is a wider group of contributors that can learn about the tool. I'm worried that the overall maintenance cost is going to be higher if we were to extract a single package to a separate repository.",2021-09-02T08:50:23Z,699132
2654,WordPress/gutenberg,980057113,914071772,"@gziolo, in what way(s) would the NPM package publication workflow be affected by having a separate project for `wp-env`? What are some ways we can raise contributor awareness of `wp-env`? What additional maintenance costs might the `wp-env` project incur by being in a separate repository?",2021-09-07T07:46:10Z,17307
2655,WordPress/gutenberg,980057113,914075824,"Relatedly, a single-line pull request for `wp-env` documentation is [currently blocked by failing Gutenberg end-to-end tests](https://github.com/WordPress/gutenberg/pull/34322#issuecomment-914055014).

I still believe that moving `wp-env` to a separate project repository would make development more simple and clear. In effect, `wp-env` is a useful tool for general WordPress (PHP) developers as well as Gutenberg developers, so would benefit from neutral positioning and streamlined :racing_car: repository structure and development workflow. :smiley: ",2021-09-07T07:52:26Z,17307
2656,WordPress/gutenberg,980057113,914086482,"I asked more folks on WordPress Slack to leave their feedback (link requires registration at https://make.wordpress.org/chat/):

https://wordpress.slack.com/archives/C02QB2JS7/p1631001940283000",2021-09-07T08:08:09Z,699132
2657,WordPress/gutenberg,980057113,914134367,"I'm on the opinion that a separate repository is just going to require more maintenance work for no real gains. The setup to publish npm is already in place here, there are already folks working and following this repo. A separate repo would require building new workflows, a new team dynamic that I don't think is worth it personally. I'm also of the opinion that all WP npm packages should just be in this repository. 

The fact that e2e tests from Gutenberg can block PRs for wp-env look like a good thing to me, they're validating that wp-env changes work properly as well. Intermittent failures are an issue of course but it's a constant priority for us.",2021-09-07T09:12:29Z,272444
2658,WordPress/gutenberg,980057113,914140138,"The same arguments here about running tests and finding issues could be made for other packages, not just `@wordpress/env`. And they are not un-solvable tasks.

https://github.com/WordPress/gutenberg is the de facto monorepo for all WordPress JavaScript packages (replacing https://github.com/WordPress/packages, if you remember that one). There are indeed benefits to keeping all these things in one place, from contributor onboarding to maintenance to publishing.
Admittedly, it's a bit confusing to have a monorepo intertwined with a WordPress plugin, but there are practical reasons for that too (which is why https://github.com/WordPress/packages didn't work).

But you're posing an interesting question here:

**At which point does an individual package (outgrow the monorepo and) warrant its own home?**

Off the top of my head, I could think of a couple of indicators:

* Higher velocity than the rest of the project
  i.e. tons of contributions, frequent releases desired -> the monorepo would slow it down
* Change in direction/scope  
  i.e. the package itself gathers a community on its own, pursuing higher goals (e.g. a general purpose local environment like you mentioned), warranting separate forums, documentation, etc.

Neither of the above is really the case for `@wordpress/env` in my opinion.",2021-09-07T09:20:20Z,841956
2659,WordPress/gutenberg,980057113,914247976,"Just voicing my agreement with comments that have already eloquently expressed by @gziolo, @youknowriad and @swissspidy. In particular, I appreciate the framing Pascal has put around identifying some indicators where a project _might_ be moved into its own repository and I agree that `@wordpress/env` doesn't meet that criteria currently.",2021-09-07T12:06:30Z,1429108
2660,WordPress/gutenberg,980057113,914291902,"I agree with Pascal's point regarding the indicators. Currently `@wordpress/env` package doesn't meet the criteria.

Moving also might cause a little confusion. As mentioned above, this is the de facto monorepo for all `@wordpress/*` packages, and people are used to creating issues/PRs here.",2021-09-07T13:08:05Z,240569
2661,WordPress/gutenberg,980057113,914593440,This seems to be the same issue Marius Jensen raised here #32584 @Clorith ,2021-09-07T20:13:48Z,39980
2662,WordPress/gutenberg,980057113,914609755,"I like that it allows for Gutenberg contributors to have it all in one package. 
I also can see that if someone wants to contribute only to `@wordpress/env` the additional payload of the fork might be an issue. 
The two factors, make me lean towards keeping it in its current space: 
- all the documentation and instructions, would need to be rewritten. There are not many people contributing to documentation. (@youknowriad raised maintenance work, definitely worth considering) 
- Gutenberg repository attract a lot more people and expose them to the `@wordpress/env`, a separate repository would probably not. It's a great side effect that WordPress (PHP) developers appreciate the work by Gutenberg developers on a local environment worth their while. 

",2021-09-07T20:29:28Z,39980
2663,WordPress/gutenberg,980057113,914989273,"Thanks, @bph, for pointing out that this is a duplicate of issue #32584. I may not have searched for duplicate issue(s) before creating this one, although I usually do. I'm not sure how to merge the conversations, as the previous issue seems to have garnered more agreement (in terms of thumbs up).

That said, I would like to explore further a few of the points raised so far.

> The fact that e2e tests from Gutenberg can block PRs for wp-env look like a good thing to me, they're validating that wp-env changes work properly as well.

The test cases that run on this repository seem mainly Gutenberg-related. Which of the tests validate that `wp-env` is working correctly? Are those tests somehow tied to the Gutenberg code?

![screenshot of tests](https://user-images.githubusercontent.com/17307/130951178-370010bf-a76d-4f76-879b-583b160d464a.gif)

> all the documentation and instructions, would need to be rewritten.

I don't see much in the [@wordpress/env documentation](https://developer.wordpress.org/block-editor/reference-guides/packages/packages-env/) that would seem to require rewriting. Is there some other source of documentation I am missing? For example, perhaps some parts [managing packages](https://github.com/WordPress/gutenberg/tree/trunk/packages#managing-packages) document would need to be included in a separate `wp-env` repository to aid in the publication workflow?

> the package itself gathers a community on its own, pursuing higher goals (e.g. a general-purpose local environment like you mentioned), warranting separate forums, documentation, etc.

I also agree with this framing. However, having a separate repository doesn't imply different forums or sweeping changes to the documentation, as suggested.

> The same arguments here about running tests and finding issues could be made for other packages, not just @wordpress/env. And they are not un-solvable tasks.

I agree the `[Package] Env` label is useful and observe the resourcefulness of Gutenberg's core engineering team. However, is it possible to exclude some of the Gutenberg tests from running on commits to the `wp-env` package? I.e., improve the signal/noise ratio of the CI workflow.

> A separate repo would require building new workflows

What are some new workflows that would be needed? Relatedly, what are some existing workflows that might require modification?

> WordPress/gutenberg is the de facto monorepo for all WordPress JavaScript packages (replacing WordPress/packages, if you remember that one). There are indeed benefits to keeping all these things in one place, from contributor onboarding to maintenance to publishing.

Thanks for the clarification about the onboarding, maintenance, and publishing. What would be some maintenance and onboarding costs of maintaining the `wp-env` project in a separate repository? Conversely, what might be some benefits or improvements of having independent projects?

> Gutenberg repository attract a lot more people and expose them to the @wordpress/env, a separate repository would probably not. It's a great side effect that WordPress (PHP) developers appreciate the work by Gutenberg developers on a local environment

I'm concerned with relevancy and clarity. For example, when I came to this repository to change the `wp-env` code and documentation, I had difficulty finding the code in the `packages` folder among various other packages. While the Gutenberg project is a remarkable undertaking, most of the code, commit history, CI pipeline, and issues are just not salient to `wp-env` development.

> I'm also of the opinion that all WP npm packages should just be in this repository.

In addition to WP npm and Gutenberg packages, the `packages` folder also includes tests (`e2e-*`)  a lot of third-party code (like `react-*`, `jest-*`, `babel-*`, `eslint`, `*-webpack-*`, `redux-*`, `docgen` and `is-shallow-equal`). There are likely pragmatic reasons for keeping development dependencies under revision control instead of `package.json`, but it adds further weight to the codebase and commit-history. 

Repository ""clutter"" is particularly undesirable when wanting to contribute to a specific environment tool (`wp-env`) that decoupled from both Gutenberg and WordPress core.",2021-09-08T07:26:14Z,17307
2664,WordPress/gutenberg,980057113,915015890,"> This seems to be the same issue Marius Jensen raised here #32584 @Clorith

Great catch @bph, I closed the other issue and here is the comment from @Clorith:

> Currently, the Gutenberg project is a monolith of tools and features, one of these features is `wp-env`, a node module for managing, and working with, WordPress plugins and themes.
> 
> Given the nature of `wp-env`, it is a tool with a much borader reach and audience than the Gutenberg monorepo, and as such does not (in my opinion) properly fit in here.
> 
> By splitting it out to its own repo, it becomes easier for others to adopt, learn from, and hopefully contribute back to, the tool it self.
> 
> This would also make it easier for those looking to enhance it to fork the project, without getting a block editor along as a bonus, or jsut to implement changes ahead of releases based on localized needs there and then.
> 
> 
> I will happily admit to not knowing the technical difficulties involved in moving NPM packages to a new repositor, but I honestly believe that not bundling this broad of a tool inside the Gutenberg repository will be a positive change.
> There may be other such tools that I'm not familiar with, so perhaps it's a roader discussion of ""separating tooling from code"" in the Gutenberg repo? Would absolutely love thoughts and input on this.",2021-09-08T08:08:34Z,699132
2665,WordPress/gutenberg,980057113,915651753,I will voice my desire to see this separated out as well. I've been trying to use `wp-env` and `wp-scripts` as a core development packages for plugin development instead of rolling my own local development setup. I too have had problems contributing to `wp-env` alone because of the massive build overhead of the entire Gutenberg project. IMO `wp-env` should be the defacto plugin/theme development environment supported by the community and all Core-related plugins and themes should be using it to make a more first-class citizen.,2021-09-09T00:01:08Z,4312062
2666,WordPress/gutenberg,980057113,951688411,Same opinion as @timnolte ! wp-env is such a great tool! :),2021-10-26T08:17:46Z,4136890
2667,WordPress/gutenberg,980057113,954538017,"It is quite an encumbrance to install all of the Gutenberg project dependencies and any other `@wordpress` libraries bundled into this repository to work on a single project. The complexity of the dependencies, as well as the number of irrelevant CI tasks, is interfering with development in PR #34324

E.g. the dependencies/scripts to lint/format JS are defined in the root of the Gutenberg project instead of the `packages/env/package.json`",2021-10-29T08:11:30Z,17307
2668,WordPress/gutenberg,980057113,1000564407,"I'm pretty much in total agreement with @WraithKenny if the individual packages aren't meant for anyone outside of the Gutenberg project then make that explicit, or better yet publish them in a private repo. It does seem rather ridiculous that there is so much in the `wp-env` package that suggests that it was designed with a broader audience in mind, but then it's stuck in a world that makes it very hard to contribute to it.",2021-12-23T23:34:42Z,4312062
2669,WordPress/gutenberg,980057113,1001463293,"> Env, Scripts, Babel-Preset, Browserllist-Config, Prettier-Config, Stylelint-Config, Eslint-Plugin and any other NPM-published package that's allegedly supposed to be useful to plugin and theme authors beyond just gutenberg, should all be separated out of this ""monorepo.""

The fact that all these packages are dependent on each other and evolve together most time is a great indication that the mono repo is the best path forward. You shouldn't be forced to have multiple PRs in parallel in multiple repos to make a change, or do a waterfall kind of flow. The whole open source community is evolving in this direction for these reasons. Smaller PRs for specific packages exist but more often, a PR touches multiple packages at the same time. ",2021-12-27T09:25:28Z,272444
2670,WordPress/gutenberg,980057113,1001586333,"> There are no ""advantages"" of keeping them in the ""monorepo"" beyond whatever opaque npm publishing shortcuts you've invented, and there are lots of disadvantages.

Really? I see a number of advantages that have already been mentioned here. I can understand that you might not agree with the advantages that have been pointed out but that doesn't mean none have been brought up :)

> The most obvious thing is ""dog-fooding"" which, back in the day, the WordPress community actually embraced, but is not doing here. All of these packages should be separate and consumed as a user (theme and plugin developer) would.

That's a fairly blanket statement that I'd like to provide a counterpoint from observation. I'm aware of _numerous_ plugins and themes that utilize the packages as they are currently distributed. Just a few at the top of my mind at the moment: WooCommerce, WooCommerce Blocks, Calypso, Jetpack, Yoast SEO, Event Espresso. The fact that the Gutenberg project isn't directly pulling these packages from NPM isn't something I'd consider a hindrance. In fact, the opposite is true, I think it allows the project to react more quickly to issues (for reasons others have already pointed out in this thread) that are reported by plugin and theme authors using the packages.

Historically, WordPress has always been a monorepo in some way as far as dev tooling goes (PHPUnit tests, JS test suites, Grunt build tool, the [entire ""develop"" WP structure](https://github.com/WordPress/wordpress-develop) is a testament to that). 

> But, on the second part, Gutenberg's release constraints already slow down the release of fixes to the developer packages.

I think there's some truth in this observation but more related to the delay in package updates when in the midst of a WordPress release cycle. This is already a known thing and something that will be iterated on. Outside of the WordPress release cycle, it's been my experience that the package updates are generally pretty frequent.

On the other hand, there are many efficiencies that are gained from working in a mono repo (through experience in trying both approaches) that effectively means momentum is kept in improving the packages. Efficiencies that would be lost in splitting everything out to its own repository.

----

I wonder if it'd be worthwhile to capture as a list the problems/friction contributing/consuming packages in the Gutenberg repo both to outline the tradeoffs being made currently and acknowledge that for some users this is a hindrance to making contributions. As a first step, I think it's good to have that articulated somewhere. This then can be used as inspiration for exploring how the various trade-offs might be addressed even _without_ splitting out to a mono-repo?",2021-12-27T14:09:11Z,1429108
2671,WordPress/gutenberg,980057113,1001700816,"@nerrad I think the problem is that `wp-env` if it's supposed to be for plugin and theme development really shouldn't be held hostage by the Gutenberg plugin. All of what's mentioned makes me question that if the mono repo is such a good thing then why isn't Gutenberg and all of the packages put in WordPress Core. From what I've seen the reason is exactly the opposite of your cases on why `wp-env` should stay in this mono repo, keeping Gutenberg separate means that it can be developed faster than WordPress Core releases. Which also allows people to install this plugin which overrides the Gutenberg that is included in WordPress Core.",2021-12-27T18:57:10Z,4312062
2672,WordPress/gutenberg,980057113,1001708993,"> All of what's mentioned makes me question that if the mono repo is such a good thing then why isn't Gutenberg and all of the packages put in WordPress Core

I for one would actually love for Gutenberg and core to merge into a single mono repository. The reason it's not is that we don't want to lose all the github flavor and tooling that comes with it. Merging Gutenberg and Core doesn't prevent Gutenberg to ship a plugin (or an alpha version of WordPress) faster than Core, we're just dealing with the difficulty to more Core to GitHub or similar.",2021-12-27T19:18:03Z,272444
2673,WordPress/gutenberg,980057113,1018105575,Why am I not seeing the recent posts here from @WraithKenny I know they posted twice today as I got the email notifications.,2022-01-21T02:25:39Z,4312062
2674,WordPress/gutenberg,980057113,1018108280,"One thing I would say is this discussion is quite long form, and a lot of the points being made are hard to digest (requires a lot of reading). The description doesn't have a summary of the relevant pain points, so it's hard to actually understand where we can make improvements.

I think it's worth looking at the reasons for wanting to move to a separate repo and see whether any of them can be addressed or at least improved while keeping the project as part of the monorepo.

For example, it was mentioned that managing issues is difficult. If there are motivated contributors, then using a project board or tracking issue are effective ways to manage a project.

If building the entire project is too slow, then we should look at options for improving that (I understand @kevin940726 @youknowriad already have been working speeding up the build), or other ideas like being able to run `npm run dev` for a single package only.

> Why am I not seeing the recent posts here from @WraithKenny I know they posted twice today as I got the email notifications.

I'm not sure why. I also saw a notification, but the message is not here. I will mention that the constant thumbs down on every post they disagree with is really very annoying, and it doesn't constitute a positive or productive discussion. I would encourage you not to do this, it comes across as unfriendly.",2022-01-21T02:30:48Z,677833
2675,WordPress/gutenberg,980057113,1018114577,"Basically, all I'm hearing is that the idea of moving this package, or any other NPM package in this monorepo, has been permanently turned down by powers in control. You have effectively shutdown the conversation. You might as well close this issue altogether because you have no intention of ever considering moving out of this crazy unmaintainable monorepo. At this point I think I have my answer and will no longer be investing any more of my time to improve this package. I'd rather be involved in contributing where people are at least willing to have an actual discussion about instead of just making it seem that the monorepo is the only right answer. I'm in complete agreement with @WraithKenny and unless someone can actually make a legitimate case like they have for why breaking any of these packages off to their own repos is going to hurt anything I'm done listening to the one-sided enforcement of the maintainers. Improving the monorepo doesn't improve the entire process as @WraithKenny so clearly articulated.",2022-01-21T02:43:28Z,4312062
2676,WordPress/gutenberg,980057113,1018124488,"That's quite a strong reaction. Look at it on the other hand. A small group of contributors are only willing to discuss the idea of moving the package out of the monorepo, and unwilling to consider any other solution. This approach to diplomacy will only end in a stalemate. Nothing actually productive will come out of it. 

The issue I'm seeing is that you have identified problems but are only willing to consider a single solution and no other options. I don't really see how you expect such inflexibility to receive a positive response.",2022-01-21T03:06:06Z,677833
2677,WordPress/gutenberg,980057113,1018203791,"My comments aren't here anymore, because I wasted hours articulating the problems, only to remember no one here actually gives a shit about my time, and I'm only hurting myself by trying to contribute. Thumbs down isn't fucking friendly? Fuck you, you stupid piece of shit. Go fuck yourself, and fuck this project, and fuck all the maintainers. I've deleted all my patches and forks. I've deleted my comments. If I could fucking delete all the code that ended up in Wordpress, I'd fucking do that to. ",2022-01-21T05:54:52Z,134252
2678,WordPress/gutenberg,980057113,1018206956,"I'm going to leave that one post until someone else deleted it and bans me from this Repo, because I'm fucking right, and these passive aggressive pricks are the barrier to contributions, with their false fucking civility and fake politeness. ",2022-01-21T06:02:05Z,134252
2679,WordPress/gutenberg,980057113,1018949914,"Though I know this conversation has mostly come to a close, I did want to answer a couple of questions that came up. I also still think it's important to consider monorepo solutions to some of the problems which could (in theory) be solved by a separate repository.

> The test cases that run on this repository seem mainly Gutenberg-related. Which of the tests validate that wp-env is working correctly? Are those tests somehow tied to the Gutenberg code?

Since Gutenberg is a complex WordPress plugin, it relies heavily on the underlying WordPress environment. If the underlying WordPress environment breaks through a change in `wp-env`, tests in Gutenberg will fail. So you could consider each Gutenberg test also a test of `wp-env`. Gutenberg tests related to file uploads also test that the `wp-env` file system works with file uploads, for example. It's important for us to not make a change to `wp-env` which would cause a plugin to stop working with it. The Gutenberg test suite has the side effect of testing that. We know that `wp-env` works well in CI because it is working great in the Gutenberg CI suite.

Additionally, the lint checks will lint wp-env code, and the unit tests will run the wp-env unit tests. So `wp-env` gets a significant amount of test coverage ""by accident,"" just because the wp-env code is used in CI and part of the monorepo!

I do think there are monorepo solutions to many of the other problems:

- Build times. Interestingly, `wp-env` doesn't have any build step. If you execute `./packages/env/bin/wp-env`, it will run the local files directly in node, no build step. So I think local development is extremely easy for that reason.
- Dependencies. Yarn has a thing called focus workspaces, where only dependencies related to the package you want to develop are installed. I think `npm` has, in newer versions, a similar feature, so in theory that will help solve that problem.
- CI taking a long time. I don't think this is just a wp-env problem! Certainly everyone would love for CI to be blazing fast in Gutenberg :)

I also wonder if the issue with how quickly contributions are accepted is more related to the number of people maintaining `wp-env`? I don't have the time to focus on wp-env any more, and I often become a bottleneck when no one else is looking at new PRs to wp-env.",2022-01-21T23:31:18Z,6265975
2680,WordPress/gutenberg,980057113,1022855120,"Agree with your points Noah.

I think it's time to close this issue given this is clearly not going to happen, but also as the conversation became toxic and aggressive.

I did start one new pull request to help improve things based on one of the (now deleted) comments - https://github.com/WordPress/gutenberg/pull/38122.

I'm happy to also work on other ways to improve the contributing experience. Lets focus on making individual issues/PRs though for those things.",2022-01-27T05:12:22Z,677833
2681,flutter/flutter,795394794,795394794,"This is a meta-issue to track reproducible reports of jank in Flutter apps.

If you are experiencing jank in your app:

1. Try to reproduce the problem in a test app. Either run `flutter create janktest` and recreate the situation you are experiencing in that app, or clone your app and delete code until you have the jank reproducing with a single .dart file.

2. [File a bug](https://github.com/flutter/flutter/issues/new?assignees=&labels=created+via+performance+template&template=5_performance_speed.md&title=) and include your .dart file demonstrating the problem. If you need more than just a .dart file (for example, assets are needed to reproduce the issue, or plugins/packages are needed to reproduce the issue) then create a GitHub repository and upload the app there.
   Make sure to include the `flutter doctor -v` output and any logs from `flutter run` and `flutter analyze`.

3.  Switch flutter to master channel and run this app on a physical device using profile mode with Skia tracing enabled, as follows:
       `flutter channel master`
       `flutter run --profile --trace-skia`

     The bleeding edge master channel is encouraged here because Flutter is constantly fixing bugs and improving its performance. Your problem in an older Flutter version may have already been solved in the master channel.

4.  Record a video of the performance issue using another phone so we can have an intuitive understanding of what happened. Don’t use ""adb screenrecord"", as that affects the performance of the profile run. Attach the video to your bug.

5.  Open Observatory and save a timeline trace of the performance issue so we know which functions might be causing it. See ""How to Collect and Read Timeline Traces"" on this blog post:
       https://medium.com/flutter/profiling-flutter-applications-using-the-timeline-a1a434964af3#a499
    Make sure that the performance overlay is turned OFF while recording the trace.
    Attach the JSON file containing your trace to your bug. You may also wish to include a screenshot of the part of the trace showing the problem you are seeing, just so that people can see at a glance what kind of performance issue the bug is about.

6. Mention _this_ bug in your bug, so that GitHub includes a link to it here.

Please avoid commenting on this bug. Keep each issue separate so that we can examine each specific problem individually. Having one issue that contains comments about multiple problems make the issue intractable.",2021-01-27T19:48:17Z,551196
2682,flutter/flutter,795394794,769448173,May be worthwhile to note that the performance overlay from step 3 should be turned off again when recording the trace in observatory in step 5 as that messes with performance.,2021-01-28T22:43:33Z,1227763
2683,flutter/flutter,795394794,770059808,"The performance overlay is probably not the best thing to recommend here - it actually does incur some performance overhead itself, and on some GPUs it's notable how much. It'd be best to just run the tracing and read that.",2021-01-29T21:28:21Z,8620741
2684,flutter/flutter,795394794,775347990,"Triage: Adding team labels so this doesn't come up on the ""untriaged"" report.",2021-02-08T18:24:26Z,1435716
2685,flutter/flutter,795394794,1150595815,"Since we created the performance issue template in issue creation, this hasn't ended up finding any use, so I'm going to close it.",2022-06-09T02:14:51Z,551196
2686,flutter/flutter,795394794,1219495757,This meta issue has been so useful for the triage team. ,2022-08-18T13:30:20Z,31410839
2687,flutter/flutter,795394794,1220016668,@maheshmnj good to hear! in what way?,2022-08-18T22:03:21Z,551196
2688,flutter/flutter,795394794,1220676331,"It has helped us when triaging performance issues to attach the timeline trace with the triage response and it also acts as a nice guide to point to authors of the issue and everyone else. 

Some sample comments where it has helped us

https://github.com/flutter/flutter/issues/104709#issuecomment-1139451006

https://github.com/flutter/flutter/issues/87811#issuecomment-900019236 (I got to know about this issue here)",2022-08-19T13:22:46Z,31410839
2689,freeCodeCamp/freeCodeCamp,1400909737,1400909737,"Describe the Issue
All the data viz projects were created on our .rocks domain. We just need to replace the codepen URL's with the new addresses in the curriculum files of those five projects.

Here's the URL's:
https://25--5-clock.freecodecamp.rocks
https://drum-machine.freecodecamp.rocks
https://javascript-calculator.freecodecamp.rocks
https://markdown-previewe.freecodecamp.rocks
https://random-quote-machine.freecodecamp.rocks

And the [project files are here](https://github.com/freeCodeCamp/freeCodeCamp/tree/main/curriculum/challenges/english/03-front-end-development-libraries/front-end-development-libraries-projects)

---

This looks like something that can be fixed by ""first-time"" code contributors to this repository. Here are the files that you should be looking at to work on a fix:

List of files:

https://github.com/freeCodeCamp/freeCodeCamp/tree/main/curriculum/challenges/english/03-front-end-development-libraries/front-end-development-libraries-projects

Please make sure you read our [guidelines for contributing](https://contribute.freecodecamp.org/#/), we prioritize contributors following the instructions in our guides. Join us in our [chat room](https://discord.gg/PRyKn3Vbay) or our [forum](https://forum.freecodecamp.org/c/contributors/3) if you need help contributing; our moderators will guide you through this.

Sometimes we may get more than one pull request. We typically accept the most quality contribution followed by the one that is made first.

Happy contributing.
",2022-10-07T09:47:10Z,26656284
2690,freeCodeCamp/freeCodeCamp,1400909737,1271412369,@ieahleen  i would like to do this. I started open source contribution a few days ago .,2022-10-07T10:25:50Z,104062485
2691,freeCodeCamp/freeCodeCamp,1400909737,1271493426,@ieahleen  will it count for Hacktoberfest?,2022-10-07T11:53:06Z,104062485
2692,freeCodeCamp/freeCodeCamp,1400909737,1271638523,@ieahleen the project URLs you have provided are not accessible. [Error: DNS address could not be found.],2022-10-07T14:02:39Z,91486778
2693,freeCodeCamp/freeCodeCamp,1400909737,1271657822,"> @ieahleen the project URLs you have provided are not accessible. [Error: DNS address could not be found.]

Yea... Because they haven't deployed it yet 🙂",2022-10-07T14:17:36Z,104062485
2694,freeCodeCamp/freeCodeCamp,1400909737,1271660216,@ieahleen  i have changed all mentioned projects URL please check,2022-10-07T14:19:30Z,93763090
2695,freeCodeCamp/freeCodeCamp,1400909737,1271665337,"@Anuran12 I know, but Isn't it a good habit to look into that before opening an issue?",2022-10-07T14:23:47Z,91486778
2696,freeCodeCamp/freeCodeCamp,1400909737,1271723672,"I have a good experience with JS & node JS, can I updated the content.",2022-10-07T15:11:51Z,92152818
2697,freeCodeCamp/freeCodeCamp,1400909737,1271761305,"There is an PR in the works that is going to fix this. We are locking this thread to avoid too many ""Can I help"" comments. 

Please read [contributing guidelines](https://contribute.freecodecamp.org) to help us help you.",2022-10-07T15:46:14Z,1884376
2698,freeCodeCamp/freeCodeCamp,1406636959,1406636959,"### Describe the Issue

In step 8 of building a registration form project, there is a text that go against [challenge document](https://contribute.freecodecamp.org/#/how-to-work-on-coding-challenges?id=challenge-descriptionsinstructions)

> To spruce the project up, let us add some CSS.

This should be changed to 

> To spruce the project up, add some CSS.

Here is the markdown file https://github.com/freeCodeCamp/freeCodeCamp/blob/1459bc6879ce700830e710f30764b4562f2f52d3/curriculum/challenges/english/14-responsive-web-design-22/learn-html-forms-by-building-a-registration-form/60f1922fcbd2410527b3bd89.md#L11

### Affected Page

https://www.freecodecamp.org/learn/2022/responsive-web-design/learn-html-forms-by-building-a-registration-form/step-8

### Additional context

Please make sure you read [our guidelines for contributing](https://contribute.freecodecamp.org/#/), we prioritize contributors following the instructions in our guides. Join us in [our chat room](https://discord.gg/PRyKn3Vbay) or [the forum](https://forum.freecodecamp.org/c/contributors/3) if you need help contributing, our moderators will guide you through this.

Sometimes we may get more than one pull requests. We typically accept the most quality contribution, followed by the one that is made first.

Happy contributing.",2022-10-12T18:34:57Z,88248797
2699,freeCodeCamp/freeCodeCamp,1406636959,1276608190,"~~Can you please review and merge my PR? #48012~~

Edit: This is not the right way and goes against the contribution policy. Apologies for that!",2022-10-12T19:01:03Z,18307180
2700,freeCodeCamp/freeCodeCamp,1406636959,1276634113,"As a new contributor, we encourage you to read our [contributing guidelines](https://contribute.freecodecamp.org) specifically the ""How to open a PR"" section.

We expect our contributors to be aware of the process specific to this project. Following the guidelines religiously earns you the respect of fellow maintainers and saves everyone time.

Some examples of this are:

1. Do not edit files directly through GitHub – while you can, it's not a good idea.
2. Make sure you follow the PR checklist and not just tick things off; otherwise, we won't take you seriously.
3. Use the correct way to link issues in the description of the PR by updating the `XXXXXX`. Do not just add issue numbers everywhere and anywhere you feel like.
4. Keep mentions and review requests to a minimum. We understand you are excited about contributing, and our maintainers will get back to you as soon as they get a chance.
5. Do not work directly off your `main` branch - create a new branch for the changes you are working on.

We appreciate you taking the time to help us, and we hope to see more contributions from you.

Happy Contributing.",2022-10-12T19:19:43Z,1884376
2701,freeCodeCamp/freeCodeCamp,1406636959,1276685373,"If this PR is still available, I'll like to work on it.",2022-10-12T20:16:33Z,93050365
2702,freeCodeCamp/freeCodeCamp,1406636959,1276719854,I would want to make a Pull request if this issue still exists.,2022-10-12T20:45:51Z,87618650
2703,freeCodeCamp/freeCodeCamp,1406636959,1276974022,"> As a new contributor, we encourage you to read our [contributing guidelines](https://contribute.freecodecamp.org) specifically the ""How to open a PR"" section.
> 
> We expect our contributors to be aware of the process specific to this project. Following the guidelines religiously earns you the respect of fellow maintainers and saves everyone time.
> 
> Some examples of this are:
> 
> 1. Do not edit files directly through GitHub – while you can, it's not a good idea.

@raisedadead I think you can use github.dev for that. ",2022-10-13T03:19:30Z,83236320
2704,freeCodeCamp/freeCodeCamp,1406636959,1277008539,"@raisedadead Please accept my apologies. I am new and still learning.

Things which I implemented after learning from your comments are.
- I have updated the PR title to comply with [Conventional Commits](https://www.conventionalcommits.org/en/v1.0.0/)
- I won't add mentions/review requests unnecessarily
- From now on I will create a separate branch for any changes and not commit directly to the main branch.

~~Point 3 is not clear to me. How would I link the PR to the issue?~~
~~I am trying to follow [this](https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue) article.~~

**EDIT:** NVM I have created another PR by following everything.

> As a new contributor, we encourage you to read our [contributing guidelines](https://contribute.freecodecamp.org) specifically the ""How to open a PR"" section.
> 
> We expect our contributors to be aware of the process specific to this project. Following the guidelines religiously earns you the respect of fellow maintainers and saves everyone time.
> 
> Some examples of this are:
> 
> 1. Do not edit files directly through GitHub – while you can, it's not a good idea.
> 2. Make sure you follow the PR checklist and not just tick things off; otherwise, we won't take you seriously.
> 3. Use the correct way to link issues in the description of the PR by updating the `XXXXXX`. Do not just add issue numbers everywhere and anywhere you feel like.
> 4. Keep mentions and review requests to a minimum. We understand you are excited about contributing, and our maintainers will get back to you as soon as they get a chance.
> 5. Do not work directly off your `main` branch - create a new branch for the changes you are working on.
> 
> We appreciate you taking the time to help us, and we hope to see more contributions from you.
> 
> Happy Contributing.

",2022-10-13T04:21:37Z,18307180
2705,freeCodeCamp/freeCodeCamp,1406636959,1277051578,"@PiyushKeshari24 thank you for taking care of this.

@DavidRod1865, @soumya0S. The contribution guideline, that  raisedadead linked should answer most of your questions.

@sonicx180 there is no mentioning of GitHub Dev environment in the guideline, if you are uncomfortable setting it up locally, our section about Gitpod should do the trick.",2022-10-13T05:32:03Z,88248797
2706,doitsujin/dxvk,1087972186,1087972186,"In Final Fantasy VII-Remake Intergrade, if you continue to play, the video memory will increase sharply, and then finally the game will crash.


### Software information
Final Fantasy VII-Remake Intergrade, Material: High Shadow: Low Number of Characters: 8.

### System information
- GPU:3080TI
- Driver:497.29
- Wine version: 
- DXVK version:1.9.2


### Log files
- d3d9.log:N/A
- d3d11.log:
[ff7remake__d3d11.log](https://github.com/doitsujin/dxvk/files/7771234/ff7remake__d3d11.log)

- dxgi.log:
[ff7remake__dxgi.zip](https://github.com/doitsujin/dxvk/files/7771235/ff7remake__dxgi.zip)
",2021-12-23T20:20:27Z,29872453
2707,doitsujin/dxvk,1087972186,1000508718,"I follow this mod guide:
https://www.nexusmods.com/finalfantasy7remake/mods/66?tab=posts&BH=11",2021-12-23T20:22:46Z,29872453
2708,doitsujin/dxvk,1087972186,1001600574,"What does ""Unable to release video memory"" mean in this context? Is that an error message from the game or something?",2021-12-27T14:44:10Z,25567304
2709,doitsujin/dxvk,1087972186,1001747370,"

> What does ""Unable to release video memory"" mean in this context? Is that an error message from the game or something?



> What does ""Unable to release video memory"" mean in this context? Is that an error message from the game or something?

My graphics card has 11GB of video memory.If this module is installed, it will gradually increase during the game. When it exceeds, the game will freeze and the FPS will plummet. At the end of 26GB, the game crashes and automatically shuts down.",2021-12-27T20:51:06Z,29872453
2710,doitsujin/dxvk,1087972186,1001747398,"Also https://github.com/doitsujin/dxvk/issues/2412 This not are piracy.
Epic Games\FFVIIRemakeIntergrade\ff7remake.exe will generate log file byte is 0kb
Epic Games\FFVIIRemakeIntergrade\End\Binaries\Win64\ff7remake_.exe will also generate a log file at the same time and it has content
So you can only go to ff7remake_.exe to taket the log file
https://www.nexusmods.com/finalfantasy7remake/mods/66?tab=posts
And share the discussion of this module. I don't know what version other people are using, but everyone said in unison to close the GeForce Experience IN-GAME OVERLAY, otherwise the game will not open smoothly.",2021-12-27T20:51:12Z,29872453
2711,doitsujin/dxvk,1087972186,1001748016,"> What does ""Unable to release video memory"" mean in this context? Is that an error message from the game or something?

![image](https://user-images.githubusercontent.com/29872453/147506479-a6768826-bbd8-474e-9bd7-5f1bcdcba37b.png)
I'm sorry that my English is bad and I can't express it well, just like this post",2021-12-27T20:53:27Z,29872453
2712,doitsujin/dxvk,1087972186,1001748822,"> Also #2412 This not are piracy. Epic Games\FFVIIRemakeIntergrade\ff7remake.exe will generate log file byte is 0kb Epic Games\FFVIIRemakeIntergrade\End\Binaries\Win64\ff7remake_.exe will also generate a log file at the same time and it has content So you can only go to ff7remake_.exe to taket the log file https://www.nexusmods.com/finalfantasy7remake/mods/66?tab=posts And share the discussion of this module. I don't know what version other people are using, but everyone said in unison to close the GeForce Experience IN-GAME OVERLAY, otherwise the game will not open smoothly.

![image](https://user-images.githubusercontent.com/29872453/147506635-633b6313-d3da-4474-b03a-ef15e985a6c1.png)
",2021-12-27T20:56:03Z,29872453
2713,doitsujin/dxvk,1087972186,1001749546,"> > What does ""Unable to release video memory"" mean in this context? Is that an error message from the game or something?
> 
> ![image](https://user-images.githubusercontent.com/29872453/147506479-a6768826-bbd8-474e-9bd7-5f1bcdcba37b.png) I'm sorry that my English is bad and I can't express it well, just like this post

![image](https://user-images.githubusercontent.com/29872453/147506776-e14819bf-8389-4339-a251-ad07c652c95c.png)
",2021-12-27T20:58:46Z,29872453
2714,doitsujin/dxvk,1087972186,1001755495,"> Also #2412 This not are piracy. Epic Games\FFVIIRemakeIntergrade\ff7remake.exe will generate log file byte is 0kb Epic Games\FFVIIRemakeIntergrade\End\Binaries\Win64\ff7remake_.exe will also generate a log file at the same time and it has content So you can only go to ff7remake_.exe to taket the log file https://www.nexusmods.com/finalfantasy7remake/mods/66?tab=posts And share the discussion of this module. I don't know what version other people are using, but everyone said in unison to close the GeForce Experience IN-GAME OVERLAY, otherwise the game will not open smoothly.

![image](https://user-images.githubusercontent.com/29872453/147506973-86829385-768f-4d54-978a-1547d05f5cfd.png)
",2021-12-27T21:02:21Z,29872453
2715,doitsujin/dxvk,1087972186,1001760236,"> Also #2412 This not are piracy. Epic Games\FFVIIRemakeIntergrade\ff7remake.exe will generate log file byte is 0kb Epic Games\FFVIIRemakeIntergrade\End\Binaries\Win64\ff7remake_.exe will also generate a log file at the same time and it has content So you can only go to ff7remake_.exe to taket the log file https://www.nexusmods.com/finalfantasy7remake/mods/66?tab=posts And share the discussion of this module. I don't know what version other people are using, but everyone said in unison to close the GeForce Experience IN-GAME OVERLAY, otherwise the game will not open smoothly.

![image](https://user-images.githubusercontent.com/29872453/147507166-ea0e70f6-1c45-4186-86e5-e3edaa7b02e3.png)
",2021-12-27T21:04:42Z,29872453
2716,doitsujin/dxvk,1087972186,1001760912,"> Also #2412 This not are piracy. Epic Games\FFVIIRemakeIntergrade\ff7remake.exe will generate log file byte is 0kb Epic Games\FFVIIRemakeIntergrade\End\Binaries\Win64\ff7remake_.exe will also generate a log file at the same time and it has content So you can only go to ff7remake_.exe to taket the log file https://www.nexusmods.com/finalfantasy7remake/mods/66?tab=posts And share the discussion of this module. I don't know what version other people are using, but everyone said in unison to close the GeForce Experience IN-GAME OVERLAY, otherwise the game will not open smoothly.

![image](https://user-images.githubusercontent.com/29872453/147507280-b0bbd383-45e9-4828-aa4f-5f7403136c63.png)
",2021-12-27T21:06:33Z,29872453
2717,doitsujin/dxvk,1087972186,1001761337,"> Also #2412 This not are piracy. Epic Games\FFVIIRemakeIntergrade\ff7remake.exe will generate log file byte is 0kb Epic Games\FFVIIRemakeIntergrade\End\Binaries\Win64\ff7remake_.exe will also generate a log file at the same time and it has content So you can only go to ff7remake_.exe to taket the log file https://www.nexusmods.com/finalfantasy7remake/mods/66?tab=posts And share the discussion of this module. I don't know what version other people are using, but everyone said in unison to close the GeForce Experience IN-GAME OVERLAY, otherwise the game will not open smoothly.

![image](https://user-images.githubusercontent.com/29872453/147507349-eacfe402-57ec-430c-88e4-74c9029adcce.png)
",2021-12-27T21:07:47Z,29872453
2718,doitsujin/dxvk,1087972186,1001768187,Can you please not turn this issue into the mirror of an entire NexusMods thread?,2021-12-27T21:30:14Z,1131720
2719,doitsujin/dxvk,1087972186,1002168945,"That, and we kind of need an apitrace here. The game is doing some broken shit that *cannot* work properly (creating views with typeless formats and reinterpreting BC1 as BC5 etc) so I kind of need to know what's it's trying to do, and also whether it is trying to use any of the DXGI memory APIs.

DXVK itself does not leak memory but it's possible that the game keeps creating resources for some reason, or uses `MAP_WRITE_DISCARD` on very large resources.

Also, the other thread very much **was** about piracy, I just deleted the comments that blatantly showed it.",2021-12-28T15:50:23Z,25567304
2720,doitsujin/dxvk,1087972186,1002245472,Here is a trace: https://drive.google.com/file/d/1VlNg9FA8ehjhsqMTcZPde9Sazfu_-M8g/view?usp=sharing,2021-12-28T19:11:49Z,8129300
2721,doitsujin/dxvk,1087972186,1003515374,"I see have some some one said
Maybe the memory leak with Vulkan is only on Nvidia cards.",2022-01-01T06:47:45Z,29872453
2722,doitsujin/dxvk,1087972186,1005685968,"I just came from a report post of 2018 stating exactly the same about other game
The AUDACITY of acting like you dont know what he means when your faulty software has been leaking memory since always in multitude of games, god according to the last author post it is even leaking memory adresses……. And hes like “i just wont rewrite it to handle memory like dx11 does” (aka PROPERLY)…. Why dont you just say you dont care about mid-end cards having rampant issues? Its always one of you two saying “yes, its this, its that” but the thing NEVER gets fixed.",2022-01-05T13:29:15Z,62871363
2723,doitsujin/dxvk,1087972186,1005695694,You truly deserve better service given how much you paid for DXVK!,2022-01-05T13:42:07Z,1131720
2724,doitsujin/dxvk,1087972186,1005705112,"DXVK does not leak memory, it reuses its internal allocations as it always has. It's certainly not the best memory mangement in the world but I doubt it has anything to do with the issue at hand.",2022-01-05T13:54:36Z,25567304
2725,doitsujin/dxvk,1087972186,1006054723,"> I just came from a report post of 2018 stating exactly the same about other game The AUDACITY of acting like you dont know what he means when your faulty software has been leaking memory since always in multitude of games, god according to the last author post it is even leaking memory adresses……. And hes like “i just wont rewrite it to handle memory like dx11 does” (aka PROPERLY)…. Why dont you just say you dont care about mid-end cards having rampant issues? Its always one of you two saying “yes, its this, its that” but the thing NEVER gets fixed.

Well said !  He IS PAID to have this working correctly.
I.guess gaslighting some people is easier than fixing the issue. 🐔",2022-01-05T20:29:37Z,10137
2726,doitsujin/dxvk,1087972186,1006465329,"I get that it can be infuriating if you can't play a game you really want to play, but the behaviour here just achieves that the devs don't even want to fix this issue.
If you want this issue fixed, try to help with apitraces, reproduction cases etc. 
Causing drama here won't lead to anything productive.

To throw some ideas in here:
- I don't see the WINE version being specified in the first comment
- Can this be reproduced when compiling dxvk from source?
- Can this be reproduced on AMD GPUs?",2022-01-06T10:43:18Z,38186597
2727,doitsujin/dxvk,1087972186,1006479584,"> DXVK does not leak memory, it reuses its internal allocations as it always has. It's certainly not the best memory mangement in the world but I doubt it has anything to do with the issue at hand.

Perhaps if we contact your employer and email the screenshots highliting the disgusting disdain you have  for people looking for help... What was it ..""entitled cesspool"" . That will look good on your resume or I might email Phoronix so they can do an article ",2022-01-06T11:01:53Z,10137
2728,doitsujin/dxvk,1087972186,1013189967,"Closing since this is just turning into harassment at this point.

Latest `master` branch has some changes regardning memory management which may or may not help in extreme cases like this, but in the end the game just seems to require huge amounts of memory.",2022-01-14T14:47:14Z,25567304
2729,AvaloniaUI/Avalonia,503598709,503598709,"I have a control that's size is 128 wide and 16 tall. When I subscribe to the PointerPressed event, I am getting events from both x=128 and y=16 (which are outside of the control). Any X and Y coords larger do not fire the event. The event also will not fire if there is a different control directly next to this control. For example, if I have another control to the right of this control, then events with x=128 will not be sent to this control.",2019-10-07T17:51:00Z,29823718
2730,AvaloniaUI/Avalonia,503598709,539196300,"Found the problem: https://github.com/AvaloniaUI/Avalonia/blob/3b38aea2b9a5ddc4a15cadff7ba74c658c4ba2af/src/Avalonia.Visuals/Rect.cs#L228
I'll open a quick PR to fix it",2019-10-07T20:44:38Z,29823718
2731,AvaloniaUI/Avalonia,503598709,610624032,Just tried this on WPF and it's the same there: a button can be clicked one pixel to the right of its actual bounds.,2020-04-07T21:14:17Z,1775141
2732,AvaloniaUI/Avalonia,503598709,1195477135,That's got to be a bug in WPF. There shouldn't be a goal to copy that functionality over to Avalonia when it just doesn't make sense.,2022-07-26T13:21:12Z,17993847
2733,AvaloniaUI/Avalonia,503598709,1195687043,"> That's got to be a bug in WPF. There shouldn't be a goal to copy that functionality over to Avalonia when it just doesn't make sense.

Yes. Not to be rude but I have stopped all of my app development with Avalonia simply because there is basic functionality that remains broken for years because new features or ""compatibility"" is favored over deprecating or changing things for the better. I like Microsoft, and their acquisition of things usually makes things better, but this workflow I'm describing benefits nobody and it just stalls development for everyone for years.

Most of my bugs were filed in 2017-2019 and they still remain for the most part. I didn't even make reports for everything because  they are so infuriating, or making a reproduction is just so time-consuming, or the issue is SO BASIC (ComboBoxes drive me insane in this) that I'm like ""surely they will find and fix this without my report"" so I just gave up. Whenever I NEED to update my stuff I end up spending the entire day diagnosing things that broke when updating (so the compatibility argument makes literally no sense since shit breaks anyway EVERY update without fail).

I have janky extension methods and fixes everywhere in every app because again, basic functionality just does not work and I'm waiting for things to get fixed, or events fire in different orders between updates. It's just impossible to justify my day doing this literally every time. I'm glad there is a ContainsExclusive now as I suggested in my PR but it should be used where possible ASAP. Not in 8 months. Then I can remove my extension method that does that in PointerPressed.

And people using my apps understand I have no motivation because of this. They see it's open source and want to help, then attempt to and go ""oh Avalonia with their 1000+ issues"" and try to port my stuff to GTK or something else since they also have issues with Avalonia. Again I don't want to be mean here but this is actual feedback you should consider seriously. You guys do hard work but the workflow is just not there where it matters. It's the hard truth and I wish it weren't since Avalonia can be so great, and should've been great a long time ago. But when ComboBoxes for example STILL don't perform as expected after 5+ years it really does hurt your users.

I pull every commit locally since I first found Avalonia, even to this day, and I read them all. But I see only new features and macOS low-level window fixes. Nothing that matters to me personally, and I get disappointed every now and then since I see the priorities. Anyway, that's just my rant and my opinion. I hope this new generation of users can develop with less headache than I did.",2022-07-26T16:10:57Z,29823718
2734,saltstack/salt,494492439,494492439,"Currently it is not possible to mount a filesystem to a mount point that contains blanks.

There are many problems in the ``states.mount.mounted`` code path, e.g. the mount command arguments are not quoted in the ``modules.mount.mount`` function, see 
https://github.com/saltstack/salt/blob/develop/salt/modules/mount.py#L1237. 

Code should look like this IMO:
```
cmd = 'mount {0} {1} {2} '.format(args, device, shlex.quote(name))
```
But this will fix only a small piece of the whole problem.

Another one is that ``states.mount.mounted`` does not detect correctly that the filesystem might be mounted already, i think it's because the key in the active table is not unquoted, so a comparison between
``/srv/dev-disk-by-label-My\040Passport\040Blue`` and the specified ``/srv/dev-disk-by-label-My Passport Blue`` fails.

To me it looks like the whole mount state and module is not able to handle blanks in device names and mount points properly.

Example SLS:
```
mount_fs_with_label:
  mount.mounted:
    - name: ""/srv/dev-disk-by-label-My Passport Blue""
    - device: ""/dev/disk/by-label/My\\x20Passport\\x20Blue""
    - fstype: ext4
    - mkmnt: True
    - persist: False
    - mount: True
```

Result:
```
          ID: mount_fs_with_label
    Function: mount.mounted
        Name: /srv/dev-disk-by-label-My Passport Blue
      Result: False
     Comment: mount: bad usage
              Try 'mount --help' for more information.
     Started: 08:31:10.286521
    Duration: 181.307 ms
     Changes: 
```

```
# salt-call mount.active
...
 /srv/dev-disk-by-label-My\040Passport\040Blue:
        ----------
        alt_device:
            None
        device:
            /dev/sda1
        fstype:
            ext4
        opts:
            - rw
            - noexec
            - relatime
            - jqfmt=vfsv0
            - usrjquota=aquota.user
            - grpjquota=aquota.group
...
```

```
# ls -alh /dev/disk/by-label/
total 0
drwxr-xr-x 2 root root  60 Sep 17 08:29  .
drwxr-xr-x 7 root root 140 Sep 17 08:29  ..
lrwxrwxrwx 1 root root  10 Sep 17 08:29 'My\x20Passport\x20Blue' -> ../../sda1
```

```
# ls -alh /srv
total 28K
drwxr-xr-x  7 root root    4.0K Sep 17 08:07  .
drwxr-xr-x 21 root root    4.0K Sep 16 16:07  ..
drwxr-xr-x  4 root root    4.0K Sep 13 13:40  dev-disk-by-id-scsi-0QEMU_QEMU_HARDDISK_drive-scsi0-0-2-part1
drwxr-xr-x  2 root root    4.0K Sep 16 16:11 'dev-disk-by-label-My Passport Blue'
drwxr-xr-x  2 ftp  nogroup 4.0K Sep 10 14:23  ftp
drwxr-xr-x  3 root root    4.0K Sep 16 16:07  pillar
drwxr-xr-x  5 root root    4.0K Sep 16 16:07  salt
```

```
# cat /etc/fstab
proc /proc proc defaults 0 0
UUID=90ee6298-385f-4841-bfdc-8b1e0e0ae5c1 / ext4 errors=remount-ro 0 1
# >>> [openmediavault]
/dev/disk/by-label/My\x20Passport\x20Blue		/srv/dev-disk-by-label-My\040Passport\040Blue	ext4	defaults,nofail,user_xattr,noexec,usrjquota=aquota.user,grpjquota=aquota.group,jqfmt=vfsv0,acl	0 2
# <<< [openmediavault]
```

```
# cat /proc/self/mountinfo
...
265 25 8:1 / /srv/dev-disk-by-label-My\040Passport\040Blue rw,noexec,relatime shared:148 - ext4 /dev/sda1 rw,jqfmt=vfsv0,usrjquota=aquota.user,grpjquota=aquota.group
...
```
",2019-09-17T08:58:08Z,1897962
2735,saltstack/salt,494492439,808843192,"Hello, @votdev 

I am trying to build myself home NAS with old Atom mini-ITX board... So i install OMV5, i plug in dad's old NTFS drive... and here we go...

Frankly, i wish Salt guys put the comments inside this source, listing all the bugs related to this module. So any hacker which for whatever reason would change it - would be instantly notifie on old pending bugs.

Salt seems extremely fragile here, probably no one else except for OMV5 uses it for partitions. Maybe OMV6 could do it outside Salt? Like good old UDEV rules or anything. I mean, before Salt porject might decide to drop this functionality that almost no one use, instead of burden of maintaining it for OMV5 alone....

Well, ranting aside, i am rather puzzled with your _device: ""/dev/disk/by-label/My\\x20Passport\\x20Blue""_
Where do you even get this hex substitution from???

Thing is, the whole mounting escaping is one uber-ancient legacy mess. Putting it here so maybe someone would use it. I spent like 3 hours googling around and experimenting with Python that i never used before. Tryied to google some standard about Posix/Linux/bash filename mangling/escaping.... and then Python module to undo it. To no avail.

Okay, so, to document it down.

- mtab/fstab and friends is one-of-a-kind ancient mess.
- it started with ancient BSD (not FreeBSD) function strunvis, which behaviour  not documented. Probably that was OS-specific function (a la virtual methods). http://manpages.org/strunvis/3
- when Linux was mimicking good old BSd it only made ad hoc substitutions for 4 specific chars. There is no any systematic/generic pattern at all.

```
static inline void mangle(struct seq_file *m, const char *s)
{
	seq_escape(m, s, "" \t\n\\"");
}
```
https://elixir.bootlin.com/linux/latest/source/fs/proc_namespace.c#L84

```
				R(""\\"", '\\'),
				R(""011"", '\t'),
				R(""012"", '\n'),
				R(""040"", ' '),
				R(""134"", '\\')
```
https://sources.debian.org/src/sysvinit/2.96-6/src/fstab-decode.c/

So, whatever comes from Linux mounts information - should be de-mangled for those four special cases.
Every single space-separated column of every single line.
Ugly, and undocumented, but that is what it is. And, frankly, it is not that hard...

BUT, why do you want to compare with some arbitrary hex-escaped string? what can be a real use-case for that???
Linux kernel just does not have hex-escaping code for disk mounts.

Now, to be frank, even this would NOT be enough, because i can have multiple disks with the same partition label. Like many USB thumb drives with ""DATA"" partition. I can even have several partitions with the same name on singe disk!

Again, it can be fixed by detecting collisions and adding extra data, like counters or GUID or whatever, but...

What gonna OMV do if OMV's user has two drives with partitions having same labels, and then he hotplugs one disk, or another, or both in any order? Is it race condition now? Is it okay for OMV to have race condition?
Seems whatever use cases Salt imagined for them here is very different from what OMV users might face.",2021-03-28T04:34:34Z,937774
2736,saltstack/salt,494492439,808844688,"Output from Linux's mount
`/dev/sdb1 on /media/U:NTFS Disk type fuseblk (rw,relatime,user_id=0,group_id=0,allow_other,blksize=4096)`

Spaces are NOT escaped there!
Dunno how it is done on BSD/Darwin

And then we have this...
```
# salt-call mount.list_mounts
local:
    ----------
    /:
        /dev/sda1
......
    /media/U:NTFS:
        /dev/sdb1
    /proc:
        proc
........
```
",2021-03-28T04:58:15Z,937774
2737,saltstack/salt,494492439,808854580,"@votdev  re: escaping names for calling `mount` - i think that is what was intended to do so:

`""device"": device_name.replace(""\\040"", ""\\ ""),` inside `def _active_mountinfo()`
but that was only called when from `def active(extended=False)` then Extended is set to True, if ever

And similar code inside `def _resolve_user_group_names(opts):`

So it seems Salt prefers to keep space-containing names mangled, but mangled differently.
So, no escaping when calling `mount` or `umount` is needed,

---

I am not even sure that de-escaping mount point likes `xxx\040yyy` in Salt would be correct way to go.

There can be a point: since that module serves as abstraction layer and should hide UNIX-likes peculiarities from generic Salt modules, all IDs better be unmangled. But not sure. 

However IF to do this de-mangling, then quoting arguments for calling `mount` becomes required indeed.

But anyway, this line i believe  should not had ended in /etc/fstab and whoever added it was at fault...

```
# >>> [openmediavault]
/dev/disk/by-label/My\x20Passport\x20Blue	
```",2021-03-28T06:26:46Z,937774
2738,saltstack/salt,494492439,808871973,"@votdev i made quite many changes in that mounts.py - and now i am thinking about undoing almost all of them... Lack of any documentation...

I am coming to believe that, while never documented, the intention of that Salt module was to always use fstab-like escaped strings for all their IDs. If not, i would like to see specific calls into other Salt modules, which expect different convention for disk names.

I really did quite a number of changes to de-escape \040 and other special chars. And probably that was only breaking things.

Except for one place though, which i believe should be patched.

```
import pathlib 

def list_mounts(): # for debug
    return _list_mounts()

def _list_mounts():
    ret = {}
    idx_mpoint = 2
    # one cannot trust `mount` with space-containing paths
    # at least on Linux - https://github.com/saltstack/salt/issues/54508

    if __grains__[""kernel""] == ""Linux"":
        idx_mpoint = 1
        mounts = pathlib.Path('/proc/mounts').read_text()
    elif __grains__[""os""] in [""MacOS"", ""Darwin""]:
        mounts = __salt__[""cmd.run_stdout""](""mount"")
    else:
        mounts = __salt__[""cmd.run_stdout""](""mount -l"")

    for line in mounts.split(""\n""):
        comps = re.sub(r""\s+"", "" "", line).split()
        if len(comps) > idx_mpoint:
##            if __grains__[""kernel""] == ""Linux"":
##               comps[0] = _Linux_fstab_unmangle( comps[0] )
##               comps[idx_mpoint] = _Linux_fstab_unmangle( comps[idx_mpoint] )
            ret[comps[idx_mpoint]] = comps[0]
    return ret
```

Would you keep implementation non-patched and would you call `salt-call mount.list_mounts` - you would see the mount point broken, cut off on the first space. This was probably THE bug.

----------------

Okay, keeping my stolen Linux archeologist hat on

https://unix.stackexchange.com/questions/56291/what-causes-dev-disk-by-label-to-be-populated

```
mount -l 
   .....
/dev/sdb1 on /media/U:NTFS Disk type fuseblk (rw,relatime,user_id=0,group_id=0,allow_other,blksize=4096) [U - Arch-2 Hitachi_2Tb_7200]

root@diskoteka:/media# ls /dev/disk/by-label/
'U\x20-\x20Arch-2\x20Hitachi_2Tb_7200'

root@diskoteka:/media# blkid -o udev -p /dev/sdb1
ID_FS_LABEL=U_-_Arch-2_Hitachi_2Tb_7200
ID_FS_LABEL_ENC=U\x20-\x20Arch-2\x20Hitachi_2Tb_7200
ID_FS_UUID=C6705D84705D7BDD
ID_FS_UUID_ENC=C6705D84705D7BDD
ID_FS_TYPE=ntfs
ID_FS_USAGE=filesystem
ID_PART_TABLE_TYPE=atari
ID_PART_ENTRY_SCHEME=dos
ID_PART_ENTRY_UUID=78fdd16a-01
ID_PART_ENTRY_TYPE=0x7
ID_PART_ENTRY_NUMBER=1
ID_PART_ENTRY_OFFSET=2048
ID_PART_ENTRY_SIZE=3907024896
ID_PART_ENTRY_DISK=8:16
```

So, it is UDEV or SYSTEMD which creates those weird hex-mangled names. Okay. Though putting them into /etc/fstab still feels wrong. `man mount` suggests against it and suggests using `UUID=...` and `LABEL=...` flags instead.

Now back to your
```
Example SLS:

mount_fs_with_label:
  mount.mounted:
    - name: ""/srv/dev-disk-by-label-My Passport Blue""
```

I don't know what it should mean in specific files/commands terms. But i feel this is the error on OMV part. And perhaps lack of documentation/understanding/forecasting on Salt part.

```
def mount(
    name, device=False, mkmnt=False, fstype="""", opts=""defaults"", user=None, util=""mount""
):
.....
        salt '*' mount.mount /mnt/foo /dev/sdz1 True
.....
    if device:
        cmd += ""{} {} {} "".format(args, device, name)
    else:
        cmd += ""{} "".format(name)
```

My inner archeologist says that the `name` AKA mount point AKA target directory is meant to be in bash-mangled format.
IOW OMV should had created ""\ "" containing fileneames:
```
  mount.mounted:
    - name: ""/srv/dev-disk-by-label-My\ Passport\ Blue""
```
Linux `man mount` also suggests against the second option due to ambiguity, where the single parameter is mount point name or device file name. I don't know if other UNIX-likes but Linux support those precision keys.

```
       --source device
              If only one argument for the mount command is given  then  the
              argument might be interpreted as target (mountpoint) or source
              (device).  This option allows to explicitly  define  that  the
              argument is the mount source.

       --target directory
              If  only  one argument for the mount command is given then the
              argument might be interpreted as target (mountpoint) or source
              (device).   This  option  allows to explicitly define that the
              argument is the mount target.
```

So i think that part in `def mount` should better be written as

```
    if device:
        cmd += ""{} {} {} "".format(args, device, name)
    else:
        if __grains__[""kernel""] == ""Linux"":
             cmd += ""--target ""
        cmd += ""{} "".format(name)
```",2021-03-28T09:38:18Z,937774
2739,saltstack/salt,494492439,808872016,"@the-Arioch I don't think this is the right place to discuss OMV related things.

> Well, ranting aside, i am rather puzzled with your device: ""/dev/disk/by-label/My\x20Passport\x20Blue""
> Where do you even get this hex substitution from???

Escaping blanks is not my idea, it is used by every userland command that processes mount points, e.g. `mount`.
Either `systemd` want to have escaped paths in mount units too, there is a special command to convert
paths for you, see `systemd-escape`.

> Salt seems extremely fragile here, probably no one else except for OMV5 uses it for partitions. 
> Maybe OMV6 could do it outside Salt?

OMV already workarounds this issue, thus it is not affected by this reported issue here.

> I mean, before Salt porject might decide to drop this functionality that almost no one use, 
> instead of burden of maintaining it for OMV5 alone....

I don't think Salt will drop `mount.mounted` because it is a somewhat essential functionality of Linux systems.

> Now, to be frank, even this would NOT be enough, because i can have multiple disks with the 
> same partition label. Like many USB thumb drives with ""DATA"" partition. I can even have several 
> partitions with the same name on singe disk!

You can do that, but don't blame the software then. Using USB devices in a NAS is no good idea, but that's a different thing. IMO devices using in a NAS should be already connected to the NAS, no plug-and-play, this is not how a NAS is intended to work. If devices are always connected, then you will never run into the situation that duplicate labels might harm your system. This issue is user introduced and should be handled by them.

If you want to discuss this issue please open an issue in the OMV Git repository.",2021-03-28T09:38:45Z,1897962
2740,saltstack/salt,494492439,808872750,"> So, it is UDEV or SYSTEMD which creates those weird hex-mangled names. Okay. Though putting them into /etc/fstab still feels wrong. 

I think it is ok to use systemd escaped paths in `/etc/fstab` since systemd handles filesystem mounting nowadays.",2021-03-28T09:46:06Z,1897962
2741,saltstack/salt,494492439,808875885,"@votdev i meantioned systemd because of https://github.com/systemd/systemd/issues/12018

See... i know very little about Linux and nothing about Python, so i was googling everything i could think of :-)

But i am glad to hear from you.  So, how can we scratch this itch, is Salt team is not with us on it...

Can you make some scripts demonstrting the alleged Salt bug that i could run from bash ? Also are there some hidden option in OMV5 to re-enable mounting space-containing partitions?


",2021-03-28T10:15:49Z,937774
2742,saltstack/salt,494492439,808876076,"But i really am worried about potential race conditions in OMV when different partitions would have same label... IF you use label as ""primary key"" as persistent ID for all the other settings (user rights, sharing folders, etc), it might be quite a gotcha...",2021-03-28T10:17:44Z,937774
2743,saltstack/salt,494492439,808876957,"Some of the ""deep changes"" i mentioned above. I now think those are dead end, but just in case they would be useful to someone, maybe even us later.

Using ""Raw"" non-escaped string would probably be more proper design, but might really require deep refactoring of many Salt modules and then testing on many different systems. Horror...

```
def _Linux_fstab_unmangle(fs_str):
    # rather ugly ad-hoc substitutions cosplaying ancient BSD's non-documented strunvis(...)
    # https://sources.debian.org/src/sysvinit/2.96-6/src/fstab-decode.c/
    # https://elixir.bootlin.com/linux/latest/source/fs/proc_namespace.c#L84
    fs_str = fs_str.replace(r""\011"", ""\t"").replace(r""\012"", ""\n"")
    fs_str = fs_str.replace(r""\040"", r"" "")
    fs_str = fs_str.replace(r""\134"", ""\\"").replace(r""\\"", ""\\"")
    return fs_str

def _Str_Nop(text):
    return str(text)

def _fsfilter():
    if __grains__[""kernel""] == ""Linux"":
        return _Linux_fstab_unmangle
    return _Str_Nop
```

and then

```
def _list_mounts():
    ret = {}
    idx_mpoint = 2
    # one cannot trust `mount` with space-containing paths
    # at least on Linux - https://github.com/saltstack/salt/issues/54508

    if __grains__[""kernel""] == ""Linux"":
        idx_mpoint = 1
        mounts = pathlib.Path('/proc/mounts').read_text()
    elif __grains__[""os""] in [""MacOS"", ""Darwin""]:
        mounts = __salt__[""cmd.run_stdout""](""mount"")
    else:
        mounts = __salt__[""cmd.run_stdout""](""mount -l"")

    for line in mounts.split(""\n""):
        comps = re.sub(r""\s+"", "" "", line).split()
        if len(comps) > idx_mpoint:
            if __grains__[""kernel""] == ""Linux"":
               comps[0] = _Linux_fstab_unmangle( comps[0] )
               comps[idx_mpoint] = _Linux_fstab_unmangle( comps[idx_mpoint] )
            ret[comps[idx_mpoint]] = comps[0]
    return ret


def _active_mountinfo_linux(ret):
    _list = _list_mounts()
    _fi = _fsfilter()
    filename = ""/proc/self/mountinfo""
    if not os.access(filename, os.R_OK):
        msg = ""File not readable {0}""
        raise CommandExecutionError(msg.format(filename))

    if ""disk.blkid"" not in __context__:
        __context__[""disk.blkid""] = __salt__[""disk.blkid""]()
    blkid_info = __context__[""disk.blkid""]

    with salt.utils.files.fopen(filename) as ifile:
        for line in ifile:
            comps = salt.utils.stringutils.to_unicode(line).split()
            device = comps[2].split("":"")
            # each line can have any number of
            # optional parameters, we use the
            # location of the separator field to
            # determine the location of the elements
            # after it.
            _sep = comps.index(""-"")
            device_name = _fi(comps[_sep + 2])
            device_uuid = None
            device_label = None
            if device_name:
                device_uuid = blkid_info.get(device_name, {}).get(""UUID"")
                device_uuid = device_uuid and device_uuid.lower()
                device_label = blkid_info.get(device_name, {}).get(""LABEL"")
            ret[_fi(comps[4])] = {
                ""mountid"": comps[0],
                ""parentid"": comps[1],
                ""major"": device[0],
                ""minor"": device[1],
                ""root"": _fi(comps[3]),
                ""opts"": _resolve_user_group_names(comps[5].split("","")),
                ""fstype"": comps[_sep + 1],
                ""device"": device_name, ## .replace(""\\040"", ""\\ ""),
                ""alt_device"": _list.get(_fi(comps[4]), None),
                ""superopts"": _resolve_user_group_names(comps[_sep + 3].split("","")),
                ""device_uuid"": device_uuid,
                ""device_label"": device_label,
            }
    return ret


def _active_mounts_linux(ret):
    """"""
    List active mounts on Linux systems
    """"""
    _list = _list_mounts()
    _fi = _fsfilter()
    filename = ""/proc/self/mounts""
    if not os.access(filename, os.R_OK):
        msg = ""File not readable {0}""
        raise CommandExecutionError(msg.format(filename))

    with salt.utils.files.fopen(filename) as ifile:
        for line in ifile:
            comps = salt.utils.stringutils.to_unicode(line).split()
            ret[_fi(comps[1])] = {
                ""device"": _fi(comps[0]),
                ""alt_device"": _list.get(_fi(comps[1]), None),
                ""fstype"": comps[2],
                ""opts"": _resolve_user_group_names(comps[3].split("","")),
            }
    return ret
```",2021-03-28T10:26:46Z,937774
2744,saltstack/salt,494492439,808881089,"@votdev  > OMV already workarounds this issue

by failing to mount the disk? because i can not mount disk in OMV5 or i would never learn about this issue.
failing to mount disk does not look like work-around at all.

let's think what we can do to make space-containing partitions mounted by OMV. It seems to be a kind of ""communication breakdown"" between Salt and OMV5, they expect and provide for mututally incompatible things.

here is minimally patched 
/usr/lib/python3/dist-packages/salt/modules/mount.py 

[mount.py.gz](https://github.com/saltstack/salt/files/6217180/mount.py.gz)


it makes space-containing mount point visible. If there still is something not working - i can not see what it is and how could i test it using `salt-call` scripts

```
# salt-call mount.list_mounts
local:
    ----------
    /:
        /dev/sda1
    /dev:
        udev
    /dev/hugepages:
        hugetlbfs
    /dev/mqueue:
        mqueue
    /dev/pts:
        devpts
    /dev/shm:
        tmpfs
    /media/U:NTFS\040Disk:
        /dev/sdb1
    /proc:
        proc
......
# salt-call mount.active
....
   /media/U:NTFS\040Disk:
        ----------
        alt_device:
            /dev/sdb1
        device:
            /dev/sdb1
        fstype:
            fuseblk
        opts:
            - rw
            - relatime
            - user_id=0
            - group_id=0
            - allow_other
            - blksize=4096
..........
# salt-call mount.active extended=true
......
   /media/U:NTFS\040Disk:
        ----------
        alt_device:
            /dev/sdb1
        device:
            /dev/sdb1
        device_label:
            U - Arch-2 Hitachi_2Tb_7200
        device_uuid:
            c6705d84705d7bdd
        fstype:
            fuseblk
        major:
            8
        minor:
            17
        mountid:
            427
        opts:
            - rw
            - relatime
        parentid:
            26
        root:
            /
        superopts:
            - rw
            - user_id=0
            - group_id=0
            - allow_other
            - blksize=4096
```
and also
```
root@diskoteka:/media# salt-call mount.is_mounted name=""/media/U:NTFS Disk""
local:
    False
root@diskoteka:/media# salt-call mount.is_mounted name=""/media/U:NTFS\ Disk""
local:
    False
root@diskoteka:/media# salt-call mount.is_mounted name=""/media/U:NTFS\040Disk""local:
    True
```",2021-03-28T11:05:59Z,937774
2745,saltstack/salt,494492439,808885012,"And now the most curious thing to me. I changed the partition label, then i mounted the disk from OMV Web UI and...

....and there is no any space-containing mountpoint path regardless of partition label.
The ""workaround"" seems to needlessly shoot down the perfectly working function!

Maybe it is only with MBR/NTFS disks, maybe GPT or XFS disks would use something else in `fstab`, dunno

```
# mount -l
    ....
/dev/sdb1 on /srv/dev-disk-by-uuid-C6705D84705D7BDD type fuseblk (rw,relatime,user_id=0,group_id=0,allow_other,blksize=4096) [U_-_Arch-2_Hitachi_2Tb_7200]
```
and

```
# salt-call mount.active
  . . . .
    /srv/dev-disk-by-uuid-C6705D84705D7BDD:
        ----------
        alt_device:
            /dev/sdb1
        device:
            /dev/sdb1
        fstype:
            fuseblk
        opts:
            - rw
            - relatime
            - user_id=0
            - group_id=0
            - allow_other
            - blksize=4096
    /sys:
        ----------
        alt_device:
            sysfs
```
",2021-03-28T11:41:02Z,937774
2746,saltstack/salt,494492439,809571317,"@votdev it is sad how fast you were to say Salt is all wrong and how protective you fet about OMV.

You still try to push Salt to adhere to OMV data format, while common sense says it should be otherwise.

Salt users would not suffer from it. OMV users would.
Demanding PR from OMV users like me is funny when you did not make any PR to Salt, or maybe i am wrong and you did.

So, back to:

https://github.com/openmediavault/openmediavault/issues/566#issuecomment-809541057

The intention was and is to make OMV work with disks users insert. 
Without forcing them to go ssh sudo. 
So simple.

You make it look that making OMV ""just work"" is bad goal. 

> Why the hell should escapeshellarg be called here?

Because that woul be consistent with bash/Salt data format. But i alreeady said it was kneejerk impulse, so you eems to be crashing through door wide open.

> The function is doing exactly what you are suggesting, keep data raw/verbatim/unescaped within OMV

Some we are on the same page here. You blaze of ego is called for.

Since eysterday i was asking you to show me at the se
ems between OMV and Salt, the exact borderleines, didn't i?
I am glad you seem to did so above, https://github.com/openmediavault/openmediavault/issues/566#issuecomment-809529126

And when i showed those links, i commented upon them.

`Salt is based on Python, not PHP. The code you're ranting about never runs in the Salt context.`

I never said so. Both Salt and OMV are ""black boxes"" with some data exchange. And i was asking you to point me to the raw places of exchange and raw data being exchanged, didn't i?

Yesterday i spent hours looking into Salt code and patching it along your suggestions.
First i took your suggestions as correct and thought through. And just followed them. An then had to undo it all.

Now you imply it was your time wasted not mine.

That `Example SLS:` - many times from yesterday i asked you how can i reproduice this activity from bash command line.
For example above - https://github.com/saltstack/salt/issues/54508#issuecomment-808875885
You kind of answered by showing PHP code for SLS generation - after many requests and hours.
But you still not answered how to trigger that action from bash.

I asked you yesterday how to make OMV code trigger that action of Salt, allegedly buggy Salt.
And you refused to help me doing it.
https://github.com/openmediavault/openmediavault/issues/566#issuecomment-808955077

```
What can i patch in OMV5 to make this notification gone?
Why do you want to know that? What do you expect to improve?
```

You made me look into Linux kernel i am not familiar with, at the same time you are not very willing to point me to specific OMV code and Salt commands you are familiar with.

You are blocking any attempt to debug OMV and Salt interaction - and you demand perfectly polished PRs. 
It is not consistent. And it is would not help anyone. Not me, not you, not OMV users.",2021-03-29T17:34:41Z,937774
2747,saltstack/salt,494492439,809575222,"@garethgreenaway  @waynew  @sagetherage 

Please consider this fix to `Salt` above

> here is minimally patched /usr/lib/python3/dist-packages/salt/modules/mount.py
> mount.py.gz

https://github.com/saltstack/salt/issues/54508#issuecomment-808881089

That is a clear bug in `Salt` that can be reproduced on Linux box (and probably on other UNIX-likes) independently on OMV",2021-03-29T17:39:39Z,937774
2748,saltstack/salt,494492439,809657944,"@the-Arioch Please stop blaming and ranting me. This raised issue here has nothing to do with OMV.

@garethgreenaway please set this issue to read-only, I had to do the same on the OMV issues to stop these rants.",2021-03-29T19:38:29Z,1897962
2749,libsdl-org/SDL,1252756496,1252756496,"https://github.com/libsdl-org/SDL

This makes it seem like this was the first version of SDL, while in reality it is 2.x

Recommend different URL which clearly states SDL2.

To prevent time wasted on using wrong version. I only noticed it after palette->version was missing.

60 minutes of time wasted, I consider myself lucky.

99.9% of the people on this planet suck at proper versioning, including file systems.

Fortunately SDL1 is not fully retarded:

https://github.com/libsdl-org/SDL-1.2",2022-05-30T13:37:36Z,34254801
2750,libsdl-org/SDL,1252756496,1141172854,"2 is part of the version number, not the project name. I think it's fairly intuitive. The readme also says the major version number at the top.",2022-05-30T13:39:00Z,2662
2751,libsdl-org/SDL,1252756496,1141173864,"It's most likely part of the DLL name, so it's a big deal.",2022-05-30T13:40:00Z,34254801
2752,libsdl-org/SDL,1252756496,1141176028,"> 2 is part of the version number, not the project name. I think it's fairly intuitive. The readme also says the major version number at the top.

For the love of god I have more things to do then read readme files. I need to download it first.

Which readme should I download ?!?!

Answer my question !",2022-05-30T13:42:09Z,34254801
2753,libsdl-org/SDL,1252756496,1141178007,"The readme is readable on this webpage, if you scroll down after loading https://github.com/libsdl-org/SDL

Right here: https://github.com/libsdl-org/SDL#simple-directmedia-layer-sdl-version-20

Also I don't think slurs are necessary to communicate your frustrations.",2022-05-30T13:44:08Z,2662
2754,libsdl-org/SDL,1252756496,1141180566,"> The readme is readable on this webpage, if you scroll down after loading https://github.com/libsdl-org/SDL
> 
> Right here: https://github.com/libsdl-org/SDL#simple-directmedia-layer-sdl-version-20
> 
> Also I don't think slurs are necessary to communicate your frustrations.

Your logic is flawed. Readme is not version information. Sometimes projects include version information in readme, but many times they do not.

I did not even scroll down.

(There is also no indicator if it includes SDL 1.x for backwards compatibility or not.)
",2022-05-30T13:46:51Z,34254801
2755,libsdl-org/SDL,1252756496,1141220421,"In general, GitHub projects use the project name as the entire repo, including all versions of the project, and the main branch is the current version, and different versions are specified using branches or tags in the repo. You can see a good example of this at https://github.com/godotengine/godot.",2022-05-30T14:25:12Z,2100061
2756,anuraghazra/github-readme-stats,1396213107,1396213107,"<!--PLEASE FIRST READ THE FAQ (#1770) AND COMMON ERROR CODES (#1772)!!!-->

**Describe the bug**
Iam trying to create a github readme statistics, but its throwing a error as **""Something went wrong!""**

**Expected behaviour**
It should be properly showing my github statistics without any issues

**Screenshots / Live demo link (paste the github-readme-stats link as markdown image)**
<img width=""860"" alt=""Screenshot 2022-10-04 at 6 09 08 PM"" src=""https://user-images.githubusercontent.com/58930932/193821598-8d5253b7-12ae-4f99-b4f2-9a1dd6bcc133.png"">

**Additional context**
Link used to show the statistics. [Click here](https://github-readme-stats.vercel.app/api?username=sriramgroot&show_icons=true&hide_border=true&bg_color=040d21&title_color=165df5&icon_color=165df5&text_color=FFFFFF)

",2022-10-04T12:42:18Z,58930932
2757,anuraghazra/github-readme-stats,1396213107,1266942284,Thanks for letting us know. It is related to https://github.com/anuraghazra/github-readme-stats/issues/1471. We are aware of the issue.,2022-10-04T12:47:16Z,17570430
2758,anuraghazra/github-readme-stats,1396213107,1266947192,"> Thanks for letting us know. It is related to #1471. We are aware of the issue.

Thanks for the reply @rickstaa. once it is fixed please let us know.",2022-10-04T12:51:05Z,58930932
2759,anuraghazra/github-readme-stats,1396213107,1266963350,Also commits are showing lesser than the prev values...,2022-10-04T13:03:24Z,83284294
2760,anuraghazra/github-readme-stats,1396213107,1267064793,This should be fixed.,2022-10-04T14:07:45Z,17570430
2761,anuraghazra/github-readme-stats,1396213107,1267142820,"Hi Rick, just letting you know I'm experiencing this issue, before and after this comment. 🙂 

> This should be fixed.
",2022-10-04T15:00:21Z,59464084
2762,anuraghazra/github-readme-stats,1396213107,1267259894,Same!,2022-10-04T16:26:41Z,30414906
2763,anuraghazra/github-readme-stats,1396213107,1267291107,"> Hi Rick, just letting you know I'm experiencing this issue, before and after this comment. 🙂
> 
> > This should be fixed.

Same, as of 12:55 PM EDT.",2022-10-04T16:55:07Z,61435324
2764,anuraghazra/github-readme-stats,1396213107,1267295791,"Same here!
",2022-10-04T16:59:24Z,90587911
2765,anuraghazra/github-readme-stats,1396213107,1267298085,yup,2022-10-04T17:01:24Z,75751882
2766,anuraghazra/github-readme-stats,1396213107,1267301155,Your right. I think @anuraghazra still needs to update the PATs. I will let him close this issue when the issue is fixed.,2022-10-04T17:03:54Z,17570430
2767,anuraghazra/github-readme-stats,1396213107,1267304508,"I have get the same issue !

Maximum retries exceeded !",2022-10-04T17:06:48Z,6471174
2768,anuraghazra/github-readme-stats,1396213107,1267337125,We are working on it progress can be followed in #1471. Will lock this issue for now.,2022-10-04T17:36:11Z,17570430
2769,anuraghazra/github-readme-stats,1396213107,1267695689,Should be fixed for now.,2022-10-04T23:00:12Z,17570430
2770,anuraghazra/github-readme-stats,1396213107,1268596441,"if I use `layout=compact` or `theme=dark` parameters, the message returns",2022-10-05T15:27:13Z,10824374
2771,anuraghazra/github-readme-stats,1396213107,1268775249,Same problem with `theme=dark` parameter. ,2022-10-05T18:14:22Z,38962243
2772,anuraghazra/github-readme-stats,1396213107,1268788036,"> Same problem with `theme=dark` parameter.

I think that problem is mainly caused by the fact that we cache the cards to reduce server load. You can break this behavior by adding a random query parameter `&random=&randomss524272`. In the future, I will probably create a PR to change this behavior for the case that the cards fails to fetch data. ",2022-10-05T18:23:55Z,17570430
2773,anuraghazra/github-readme-stats,1396213107,1268806196,"> > Same problem with `theme=dark` parameter.
> 
> I think that problem is mainly caused by the fact that we cache the cards to reduce server load. You can break this behavior by adding a random query parameter `&random=&randomss524272`. In the future, I will probably create a PR to change this behavior for the case that the cards fails to fetch data.

That worked, but I notably had to add `&random=&randomss524272` before `&theme=dark`. Thanks!",2022-10-05T18:38:35Z,38962243
2774,space-wizards/space-station-14,1345038447,1345038447,"## Description
<!-- Explain your issue in detail. Issues without proper explanation are liable to be closed by maintainers. -->
the current filter applies a permanent blur to your screen, which is unbelievably ugly.

It also seems to be permanent unless healed with specific chems, which makes it agonizing if there's no medical.

Welding already blinds you when it's active. This just seems like an unnecessary kick in the balls that makes the game look like ass.
**Screenshots**
<!-- If applicable, add screenshots to help explain your problem. -->
![image](https://user-images.githubusercontent.com/98561806/185726668-fab55880-be1a-424f-b745-e92cdb0de863.png)

",2022-08-20T03:04:37Z,98561806
2775,space-wizards/space-station-14,1345038447,1221219372,i mean yeah. you're not supposed to want eye damage,2022-08-20T03:08:25Z,19853115
2776,space-wizards/space-station-14,1345038447,1221219677,"> i mean yeah. you're not supposed to want eye damage

the eye damage doesn't actually disadvantage you mechanically. It could instead be a vignette, a shadow around the screen, or a dim transparent layer on top of it. This serves no purpose except looking like ass.",2022-08-20T03:10:04Z,98561806
2777,space-wizards/space-station-14,1345038447,1221246434,"The blur causes bad eyestrain and shouldn't just be permanently applied, it either needs to be timed or changed entirely.

There's a difference between ""my character moves slowly until I get medical help that may not be available"" versus ""the game is causing me headaches until I get medical help that may not be available"".",2022-08-20T06:58:22Z,31366439
2778,space-wizards/space-station-14,1345038447,1221272328,"Maybe make the vignette permanent, just like how you get permanent eye damage in real life, and the vignette temporary (less than 30 seconds) right after you sustain eye damage, to make it extra clear to the player that what they just did is really bad for their eyes.",2022-08-20T10:11:20Z,39844191
2779,space-wizards/space-station-14,1345038447,1221306800,"> the eye damage doesn't actually disadvantage you mechanically. It could instead be a vignette, a shadow around the screen, or a dim transparent layer on top of it. This serves no purpose except looking like ass.

must be pretty effective if it causes such strong reactions

> The blur causes bad eyestrain

empathy with your character

>""the game is causing me headaches until I get medical help that may not be available"".

just wear glasses or eat carrots",2022-08-20T12:33:25Z,60792108
2780,space-wizards/space-station-14,1345038447,1221310444,"Please dont comment on github discussions if you're only gonna bring up bad-faith arguments that dont mean anything.

The effectiveness of it doesnt actually hinder my character in any way, it juet hurts my eyes.

If i want to feel empathy with my character, then how about you shoot me in the head when i die in game. 

Sometimes those additional materials for preventing it aren't available, especially when you're not playing upstream or at the beginning of a round. I dont know why that means i should get eye strain irl.",2022-08-20T12:58:03Z,98561806
2781,space-wizards/space-station-14,1345038447,1221310820,">bad-faith

ironically is itself a buzzword to dismiss other arguments and excuse yourself from having to address them

> The effectiveness of it doesnt actually hinder my character in any way

you called it 'agonizing' but you're not hindered at all, okay",2022-08-20T13:00:30Z,60792108
2782,space-wizards/space-station-14,1345038447,1221311183,"Theres a difference between a mechanical hinderance to my character and actually causing me discomfort while playing. A flashbang makes my character unable to move and restricts my vision of the screen.

A permanent blur until healed doesnt affect me mechanically, but rather just causes me irl discomfort. I don't know how that's hard for you to understand

> okay

Lose the attitude.",2022-08-20T13:03:06Z,98561806
2783,space-wizards/space-station-14,1345038447,1221311333,Convo isn't productive atm.,2022-08-20T13:04:06Z,31366439
2784,johnsoncodehk/volar,1321670019,1321670019,"Seriously affect the development efficiency！
please fix！
please fix！
please fix！
please fix！",2022-07-29T02:02:36Z,38178487
2785,johnsoncodehk/volar,1321670019,1198818709,"Hi, just like you I have really slow Vue development. There is a alpha version of plugin to marginally increase the speeds. Please see https://github.com/johnsoncodehk/volar/issues/1615",2022-07-29T02:33:00Z,35676267
2786,johnsoncodehk/volar,1321670019,1198819159,try `0.38.5`,2022-07-29T02:34:02Z,72718918
2787,johnsoncodehk/volar,1321670019,1200742389,"> try `0.38.5`

thanks",2022-08-01T05:54:54Z,38178487
2788,johnsoncodehk/volar,1321670019,1200745709,"> Hi, just like you I have really slow Vue development. There is a alpha version of plugin to marginally increase the speeds. Please see #1615

try ^0.38.5",2022-08-01T06:00:55Z,38178487
2789,osmandapp/OsmAnd,1336452733,1336452733,"https://github.com/osmandapp/OsmAnd/blob/22e40f113ce5c6df97f2f1687d5024ae38a4d28b/OsmAnd/src/net/osmand/plus/download/DownloadOsmandIndexesHelper.java#L273-L281

This appears to send the number of days since install, as well as a unique identifier (`getUserAndroidId`), to the index server when fetching indices, without respect for the telemetry preference setting.

This is a data leak that allows for a user's travel history to be tracked by the server, as these requests include client IP and a unique tracking identifier, and client IP is coarse (city-level) geolocation. This means the server can see the various cities the given userIosId travels to as the client IP changes over time.

This spyware feature also exists in the iOS version, and fails to respect the user's consent choice there as well.  It seems unlikely that this is an accident.

https://github.com/osmandapp/OsmAnd-iOS/issues/2115",2022-08-11T20:21:14Z,408977
2790,osmandapp/OsmAnd,1336452733,1212462226,"https://github.com/osmandapp/OsmAnd/commit/4a258481362a15a4f9b93d796453f5e04e862207

It appears that this spyware tracking feature was added by @vshcherb (Victor Shcherb) back in 2015 and has been leaking users' data and travel history (via client IP geolocation) to the OSMAnd index server ever since.",2022-08-11T20:28:24Z,408977
2791,osmandapp/OsmAnd,1336452733,1212953083,Any follow up from the code maintainer? ,2022-08-12T10:15:33Z,4914320
2792,osmandapp/OsmAnd,1336452733,1212953773,"> Any follow up from the code maintainer?

See https://github.com/osmandapp/OsmAnd-iOS/issues/2115",2022-08-12T10:16:23Z,1040784
2793,osmandapp/OsmAnd,1336452733,1213313456,"> UUID is anonymized randomly generated key per installation, cause this key is not associated with any advertisement id or not transferred to any other 3rd party or even used, this key doesn't require consent. As you mentioned the IP could provide more information cause it's location specific.

> This approach helps to the server monitor fair usage of resources.",2022-08-12T16:41:54Z,1042025
2794,osmandapp/OsmAnd,1336452733,1213316441,"Some thing to add:
1. it's in privacy policy  (https://osmand.net/help-online/privacy-policy/#3-the-information-company-collects)
2. We don't want to use IP as a counter, so we use randomly generated UUID which preserves the privacy much more than IP itself
3. You don't need to use OsmAnd Services if you don't agree with Terms of Use , you can download or build maps yourself.
",2022-08-12T16:45:40Z,1042025
2795,osmandapp/OsmAnd,1336452733,1213568195,"The privacy policy says:

> We strongly believe in the principle of data protection and safety, thus, the Company does not collect, store, process or transfer any personal information of users besides the cases when such information is provided by the users with their clear consent. 

You have not obtained consent from the userbase to use their devices to store and transmit a unique tracking identifier for their device.

Most are unaware you are even doing so; that is the opposite of consent. ",2022-08-12T22:35:39Z,408977
2796,osmandapp/OsmAnd,1336452733,1214150312,"There is a strong disconnect between the tone and even some accusations and the actual issue being reported.

First of all, connecting to the server is only something that happens during download. OSMAnd is an offline map and downloads happen rarely. For heavy users less than once a month, for many much less than that. Additionally, all such downloads are user-initiated, they have to explicitly press a download button. Nothing that would be useful to build a profile of any specific user.

**Allegations of travel data and such being leaked are thus really not in evidence.**

The data that is being shared is not possible to connect to any personally identifying information. Notice that google explains that since some 5 years the `getUserAndroidId` is unique per installed application. So any other application the user uses (on the same phone, or not) will not share the UUID. So on top of the fact that not much of a profile can be made at all, there is no way to connect such to any actual personal info.

Does anyone actually have any real issues with the existing code?

For the record, I don't have any relation to the company behind OSMAnd, never received anything from them (other than FOSS software) and I'm just an open source developer standing up for common sense. ",2022-08-13T12:38:21Z,63063
2797,osmandapp/OsmAnd,1336452733,1214154045,"Even an application-specific identifier allows the server to track the travel history of that user via client-ip geolocation.

A user's travel history often uniquely identifies a person.  The list of client ips and timestamps is the ""actual personal info"".

Dismissing the issue does not change the fact: you have no consent from the user for such tracking.

Common sense is not co-opting the user's device to transmit tracking identifiers without their advance knowledge and opt-in consent. ",2022-08-13T13:02:10Z,408977
2798,osmandapp/OsmAnd,1336452733,1214327968,"I think there are 2 direction of discussions: consent and identification.
1. I don't think consent is needed, same as you don't need consent that your IP is used to download maps. Second it's not a website where you read information, here you use resources, so in that sense by downloading data you agree to fair use of resources (Terms of Service) and in that case you agree that you don't download too many resources especially if you are not paid user for example

It would be completely different case if program would send statistics in background that you're using the application and here is an IP. 

To summarize: it's probably needs to be more clarified in Terms of Service but it doesn't require a consent.

2. User identification. We switched from IP to UUID to calculate how many active users & how often update exactly to solve the server load issue and don't use personal data. For example we don't require email to identify user and don't track IP exactly cause IP could provide location specific information. Random UUID **doesn't identify anything**  if it's not connected to any other services or other private information. We don't share UUID with any other applications so they also couldn't track that UUID even if these apps have personal information.

It's completely incorrect to say that  app could view **travel history**, I would say it's total nonsense cause the server only views what and when maps were downloaded in contrast of other apps that provide Internet services. 

-----------------
Last but not least.

The websites require to ask consent only for information they gather for example if you search an address on website and website doesn't store these addresses or don't connect them to IP (address is essentially publicly available information), consent is not needed. If the website will start storing information in the table [IP, Search], then consent is needed. So it's all about how the data is processed and used and not only what's being transmitted.


",2022-08-14T10:16:04Z,1042025
2799,osmandapp/OsmAnd,1336452733,1214329171,"Just to provide another point of view on this topic. As a Service provider I need a unique identifier to predict the load on servers (especially once maps are updated), so in the end it would say:

""Download maps won't be provided if user doesn't agree to create a random key for the installation"".

Does it makes sense then to implement at all ? Cause in the end it will end up, if you to download - you need a consent which to me is actually totally clear if you don't want to know what & when you downloaded - download or generate maps yourself and import to the app. **App won't send to the server what you've imported yourself!**",2022-08-14T10:21:13Z,1042025
2800,osmandapp/OsmAnd,1336452733,1214329328,"@sneak 

you claimed that;

>  to track the travel history of that user via client-ip geolocation.

Considering that this is an offline-map which doesn't normally do downloads, how do you support your claims? If you continue to make big claims, please provide actual evidence.",2022-08-14T10:21:28Z,63063
2801,osmandapp/OsmAnd,1336452733,1214337899,"No, this doesn't make sense at all, as I can distribute a forked client that sends no ID, or a random UUID on every single HTTP request

It doesn't get you anything to trust the client to self-report.",2022-08-14T10:56:58Z,408977
2802,osmandapp/OsmAnd,1336452733,1214338298,Please stop suggesting that the UUID is somehow private or anonymous.  With client IP geolocation it can identify users by travel history.  It is PII.,2022-08-14T10:58:48Z,408977
2803,osmandapp/OsmAnd,1336452733,1214339211,"Il 14/08/22 13:57, Jeffrey Paul ha scritto:
> No, this doesn't make sense at all, as I can distribute a forked client that sends no ID, or a random UUID on every single HTTP request

In which case the client will be blocked from downloading the datasets, 
no? Sounds like the point.
",2022-08-14T11:02:44Z,901528
2804,osmandapp/OsmAnd,1336452733,1214373629,"@sneak 

> Please stop suggesting that the UUID is somehow private or anonymous. With client IP geolocation it can identify users by travel history.

Maybe the problem lies that nobody explained in technical details what it means for OSMAnd to be an offline map.

It means that the application, when started, does not cause any network traffic at all. You can travel and use the map daily, including searches for point-of-interests and doing routing, without ever causing a single download. Without, specifically, leaking that uuid.

OSMAnd, in other words, is perfectly useful and allows people to use the app for months or years without ever triggering the download you have noticed sends an UUID. People can download all their data at home or at work, regardless of where they travel later.

Sneak: you are wrong. It is time to admit you came to wrong conclusions and time to publicly show you are able to learn when presented with new facts. Thank you.",2022-08-14T13:11:03Z,63063
2805,osmandapp/OsmAnd,1336452733,1214384066,"Let me summarize a bit.

* To keep the server load tolerable the admins want to limit the per-user-map-downloads
* To achieve that, OsmAnd creates an UUID with Java.util.UUID.randomUUID() (https://tutorialspoint.com/java/util/java_util_uuid.htm)
* The server logs all requests containing the UUID and the user's IP and stores it indefinitely
* The UUID is used to block too frequent downloads

**Consent**
I am no lawyer, but I would look at it by means of GDPR. However, I would agree with the accepted answer [here](https://law.stackexchange.com/a/48288) that there is no consent required.

**Concerns that come to mind**
Although I believe the claims that the UUID/IPs are never shared with any third party, I think it would be better to not have to trust anyone. Maybe one day the server gets hacked or the admins are forced to hand over all data to law enforcement.
Indeed I don't like the idea that all my IP history gets into the wrong hands. IP adresses are not anonymous and can be assigned to individuals in many ways. Flourishing VPN services show that people understood that.

**Ideas for compromises**
Sadly, I have no idea that takes no development effort. Although I believe that the effort would be manageable. Here are two:

* UUID could be re-generated e.g. when more than 2 weeks have passed. For that, OsmAnd would need to store an additional timestamp together with the UUID
* Records could be deleted on a regular basis on the server-side
* @scaidermern had [another one](https://github.com/osmandapp/OsmAnd-iOS/issues/2115#issuecomment-1212955350)",2022-08-14T14:00:42Z,13658554
2806,osmandapp/OsmAnd,1336452733,1214411752,"The uuid does not limit anything.  Anyone can make as many requests as they want with a new uuid each time, defeating this limit.",2022-08-14T16:35:27Z,408977
2807,osmandapp/OsmAnd,1336452733,1214774053,"> The uuid does not limit anything. Anyone can make as many requests as they want with a new uuid each time, defeating this limit.

This is true. But it limits the average user case, where the user does not manipulate the request just to download some maps and I guess, this comprises almost 100% of all users.",2022-08-15T09:00:17Z,13658554
2808,osmandapp/OsmAnd,1336452733,1214898007,"@saddy001 - agree that server has no control but practically it's unsolvable issue cause indeed anything could happen on the server side (i.e. hacked or rights transferred). In the end it would be a liable action or reputation loss. 

- We gather general statistics by 1 month mostly though I would tend agree to regenerate every 3 months cause then we could generate quarter reports
- We also need to spend time on server side to correctly clean the data and leave just summary reports without any private information. For that matter we need to do proper automation and standardize reports a bit .

---------------------

I think these are doable actions and if we're fine with them, I will start planning implementation. Although it might not be too much development effort but I want to implement carefully so in summary reports it will be comparing ""apple to apples"".",2022-08-15T11:13:37Z,1042025
2809,osmandapp/OsmAnd,1336452733,1216054241,"> *This is true. But it limits the average user case, where the user does not manipulate the request just to download some maps and I guess, this comprises almost 100% of all users.*

I intend to distribute a fork of this application with all unique IDs randomized on every request.  Your assumption will not remain accurate.

Also there is nothing stopping a single someone from making a high request load to the server with random IDs, rendering the server data collection useless.",2022-08-16T01:52:03Z,408977
2810,osmandapp/OsmAnd,1336452733,1216148422,"> > _This is true. But it limits the average user case, where the user does not manipulate the request just to download some maps and I guess, this comprises almost 100% of all users._
> 
> I intend to distribute a fork of this application with all unique IDs randomized on every request. Your assumption will not remain accurate.

You can do this of course. However, how many users do you expect to use your fork? I bet the above number will still remain close to 100%.

> Also there is nothing stopping a single someone from making a high request load to the server with random IDs, rendering the server data collection useless.

Blocking via IP is still possible unless you intend to perform a DDOS... Let's go back to the topic, shall we?

",2022-08-16T04:59:21Z,1040784
2811,osmandapp/OsmAnd,1336452733,1216631380,"> It's completely incorrect to say that app could view travel history, I would say it's total nonsense cause the server only views what and when maps were downloaded in contrast of other apps that provide Internet services.

It's really sad to see that you don't understand the privacy implications of your choice of putting a unique tracking identifier (similar to a cookie) on a user's device that persists over time (and changes in client IP).",2022-08-16T13:21:46Z,408977
2812,osmandapp/OsmAnd,1336452733,1217168697,">it's in privacy policy

Do you really think anyone actually reads it?

>we use randomly generated UUID which preserves the privacy much more than IP itself

A unique identifier that, in contrast to the IP address, can't be easily anonymised by the user and that also persists across IP addresses “preserves the privacy”? Surely, Google uses those Android ad IDs because they're so incredibly anonymous.

>You don't need to use OsmAnd Services if you don't agree with Terms of Use

Besides the fact that privacy laws are a thing, this is incredibly detrimental to say, considering that OsmAnd has always presented itself as a privacy-friendly alternative to commercial services. Telling people not to use it if they care about privacy is something I'd only expect from Google or Microsoft.

>you can download or build maps yourself

This is not an option for probably 99% of users.

I agree that this unique ID is not as bad as the issue's author claimed it to be. However, I also think that OsmAnd should not be designed in a way that could theoretically enable behavioural tracking.

Would simply re-generating the ID at regular intervals really be too difficult? In my opinion, it would be such an easy way to improve everyone's privacy and to calm anyone who's worried.",2022-08-16T21:09:57Z,52963327
2813,osmandapp/OsmAnd,1336452733,1217260272,"> Please stop suggesting that the UUID is somehow private or anonymous. With client IP geolocation it can identify users by travel history. It is PII.

@sneak Yes, UUID is PII. But so is IP address itself [according to GDPR](https://www.gdpreu.org/the-regulation/key-concepts/personal-data/). So _even_ if there were **no** UUID **at all**, PII would **_still_** be transmitted and stored by OsmAnd servers. Thanks for pointing to this issue, but I think it is now time to stop beating a dead horse (we get it: current OsmAnd situation is not perfect, but they seem to recognize the problem and look willing to improve on it, so let's try to **not** paint them as villains worse than Google or Microsoft - for the time being, eh?) and concentrate on **how to best address this issue** - in an **_actionable way_** that is acceptable both to privacy-oriented users and OsmAnd server admins, while also following [GDPR](https://gdpr.eu/gdpr-consent-requirements/) and other applicable laws.

My suggestions:

- randomize said UUID often - e.g. as suggested by @scaidermern in https://github.com/osmandapp/OsmAnd-iOS/issues/2115#issuecomment-1212955350
- change privacy policy at https://osmand.net/help-online/privacy-policy/#3-the-information-company-collects to be both correct and up-to-date, and with an explanation **_why_** something is being done (e.g. something akin to `... when downloading maps from within OsmAnd, your IP address and randomly generated unique identifier are transmitted to OsmAnd servers in order to facilitate download and prevent server load issues & abusive behaviour, as well as generate map popularity statistics. That unique identifier is regenerated every XXXXX days/hours/xxxx. If you download maps manually outside of OsmAnd,  the unique identifier will never be sent to OsmAnd servers nor elsewhere...`). While updating it, please also take notice of editing plugin and its set of additional privacy issues. Nice example of IMHO clear and useful privacy policy is at https://github.com/streetcomplete/StreetComplete/blob/v45.2/app/src/main/res/values/strings.xml#L314-L336 ; one may want to pick ideas/text from there.
- also, what does `Settings` / `Analytics` / `Maps downloaded` change when turned off? I mean, by the nature of transferring the map the OsmAnd servers would know what maps are downloaded from the app, regardless of this flag. Does this also share the information about maps downloaded outside of OsmAnd? Or does it collect, but then not count/process information about maps downloaded from users with this turned off? I.e. what is exactly the difference between `on` and `off` there?
- actually _link_ to said privacy policy from within OsmAnd app in `Help` menu. At least I can't seem to find it there.",2022-08-16T23:10:16Z,156656
2814,osmandapp/OsmAnd,1336452733,1217375212,"Users' IP addresses change, this tracking ID does not.

Also, it does nothing to aid the server.  This is a red herring. An abusive client can simply send a random UUID.  The tracking ID should be removed entirely and actual anti-abuse measures implemented in the server (or just ask for some volunteer mirrors and have the server only distribute hashes and mirror URLs) and stop pretending that bandwidth at this scale isn't practically free.

The Ubuntu and Debian projects distribute orders of magnitude more data for free every day without tracking end users.  These are just HTTP static file downloads and they aren't even that large.",2022-08-17T02:08:39Z,408977
2815,osmandapp/OsmAnd,1336452733,1217430285,"Just a showerthought with my morning coffee in hand, as I bumped into this issue, but couldn't you hash the IP + UUID in similar way passwords are hashed? That way at least what's in the server logs would be useless to malicious hackers should they manage to obtain it.",2022-08-17T03:51:11Z,5111931
2816,osmandapp/OsmAnd,1336452733,1217435377,"No, the server still sees the unique id from the client and has the opportunity to track individual users.

The fix is to stop sending tracking identifiers from the client to the server.  There is no benefit to the server to receive client ids, as any malicious client can send varying ones, and non-malicious clients are not an issue.

Sending client tracking ids is all privacy downside, for no security benefit. ",2022-08-17T04:03:35Z,408977
2817,osmandapp/OsmAnd,1336452733,1217665653,"Since the client needs to be trusted anyway for this whole scheme to work, how about having the client itself keep track of the number of downloads? It could then either send this number to the server, which would use it to decide whether or not to allow the download, or just refuse to initiate the download itself.",2022-08-17T08:15:47Z,1546739
2818,osmandapp/OsmAnd,1336452733,1217691954,"> actual anti-abuse measures implemented in the server

Can you elaborate a little on how that might be accomplished?
",2022-08-17T08:39:54Z,13658554
2819,osmandapp/OsmAnd,1336452733,1219481444,"**Major actionable points:**
1. UUID will be regenerated (every 2-3 months)
2. Make sure logs are truncated / deleted, so only summary left
3. Privacy policy will be updated and more descriptively about (Fair Use of OsmAnd Online Services and UUID)

**Major information point:**
1. Providing UUID is part of Terms of Usage of OsmAnd Online Services such as downloading maps. UUID or account id for OsmAnd PRO will always be part of request
2. OsmAnd Online Services are **not unique** way to use OsmAnd Client application. Maps could be downloaded or generated manually. So there is no issue that OsmAnd Client hiddenly transmits private data
3. There is no any indication / facts / points provided that UUID + Downloaded Map can somehow track user or even more identify real person. Identification is a clear procedure that UUID will be connected to real data such as Name / Surname / Living Address  / Working Address / Photo. If IP is corrupted and doesn't provide enough privacy then issue is not related to UUID.

--------------------------------------------

That's my basic understanding for now. Please leave comments in the nearest future if I'm missing something and that **issue will be frozen** to not create any further never ending discussions!

",2022-08-18T13:17:03Z,1042025
2820,osmandapp/OsmAnd,1336452733,1219504567,"@vshcherb As far as I can tell, using a UUID does not provide any benefit for OsmAnd compared to the approach I [mentioned above](https://github.com/osmandapp/OsmAnd/issues/15058#issuecomment-1217665653) (simply storing a counter in the client). On the contrary, using a UUID seems to make things more difficult (regeneration, clearing logs, updating the privacy policy). It also makes tracking users at least theoretically more easy.

Could you maybe clarify why this approach is not being considered? Am I missing something?",2022-08-18T13:38:21Z,1546739
2821,osmandapp/OsmAnd,1336452733,1219603231,"Except that we couldn't count of how many unique users per month / quarter were active, exactly why uid was introduced to give a sum by day / month. ",2022-08-18T15:01:31Z,1042025
2822,osmandapp/OsmAnd,1336452733,1219708678,"I'm sorry for my ignorance but why do you need this information? I wonder how useful it is because, for example, I'm a *very* active user but almost never update my maps so I won't show up in these statistics.

If you really want to know how many unique users download maps, couldn't the number of unique IP addresses be used as a decent approximation? Not that I necessarily like the idea of IP addresses being stored but at least this is information the servers get anyway.",2022-08-18T16:42:19Z,1546739
2823,osmandapp/OsmAnd,1336452733,1219996209,"> There is no any indication / facts / points provided that UUID + Downloaded Map can somehow track user or even more identify real person. Identification is a clear procedure that UUID will be connected to real data such as Name / Surname / Living Address / Working Address / Photo.

This misunderstands what identification means.

The UUID uniquely identifies a user when combined with a series of differing client IPs.  It doesn't need to resolve *in your systems* to a user's name or photo; the fact is that the UUID represents one and only one user.  This allows you to track *that one user's* movements from city to city (as you see different client IP locations).

It's irrelevant that you don't know the user's name; you have your own name for them - a UUID.

You don't have consent to track users in this manner without an opt-in.   This UUID is a unique tracking identifier, akin to a cookie (although it's sent in the query string and not the HTTP header, the effect is the same) and it's for a nonessential purpose (tracking user count statistics).  It's unnecessary for this purpose, anyway.",2022-08-18T21:43:31Z,408977
2824,osmandapp/OsmAnd,1336452733,1220040197,"Just a quick additional information:
If OsmAnd B.V. _or_ their users are situated in European Union (and they are), they will **need to follow the GDPR**.

- @sneak note that _user consent_ is only **_one of six_** allowable ways for [obtaining legal basis for data processing](https://gdpr.eu/gdpr-consent-requirements/ ) 
- the OsmAnd B.V. might decide to use some other legal basis for data processing. In any case, they should consult a GDPR specialist and spell out in their Privacy Policy **exactly** what Personally Identifiable Information (like UUID or username, or IP addresses) is being collected, and specify in advance **all** purposes for which such data will be used, as well as other things (will that data be shared with third parties, stored/processed outside of EU etc.). If they later want to use PII for another purpose, they must re-obtain additional consent (or other legal basis) - they're not allowed to reuse previously collected data for new purpose retroactively. There are many things that need to be done correctly, which is why they should consult a GDPR specialist (disclaimer: I am not one so it is not an ad for myself; but I had to go thru the same pains, so I _am_ informed above average in the matter). There is a [checklist](https://gdpr.eu/checklist/) they might want to go through to verify if they did everything they need to.
- While having UUID might be required for some uses (e.g. all registered users), I would suggest _seriously considering options_ that require it as little as possible, especially for ""anonymous"" users. Any data collected has a chance to leak / be misused, so not collecting it in the first place would be smarter than having to invest resources and money to deal with protecting it adequately, and dealing with a possible breaches (and GDPR fines) later. Also, OSM crowd in particular are often much more biased to be privacy concerned (or they'd likely be using Google Maps in the first place) so it would be good business idea to try not to alienate them unnecessarily. ",2022-08-18T22:31:29Z,156656
2825,osmandapp/OsmAnd,1336452733,1220063022,"> This misunderstands what identification means.
> It's irrelevant that you don't know the user's name; you have your own name for them - a UUID.

@sneak is correct here. See for example this Law SE question [""Are auto-generated identifiers PII if they cannot be linked to an identity?""](https://law.stackexchange.com/a/82136) ",2022-08-18T23:08:53Z,156656
2826,osmandapp/OsmAnd,1336452733,1220579282,"Especially noteworthy (regarding the Law SE question):

>Key observations:
>
>- Singling out a data subject already counts as identification, meaning that it isn't necessary to infer their real-world identity!
>
>[...]",2022-08-19T11:45:38Z,52963327
2827,osmandapp/OsmAnd,1336452733,1220638376,"I agree that I need a proper legal advise and in the end consent or any other legally correct form of user getting informed.

Completely another topic whether Map downloads Service could be provided without identifying how many users and potentially prevent out of service issues. 

In the end that needs to be decided by OsmAnd BV, so I'm not going to take decision on my onw, cause it has legal and business implications.

**Note**: the issue will be frozen to stop message flooding and updates will be sent to it to get everybody informed.",2022-08-19T12:49:41Z,1042025
2828,osmandapp/OsmAnd,1336452733,1278889091,"**Summary**

The issue has bee fixed in Android / iOS and already available in nightly builds and will be released in 4.3. What's been done by sorted by priority:

1. It's possible to disable that UUID sending in Privacy Global Settings as the result the traffic could be deprioritized when lots of maps are downloaded i.e. in the beginning of the month.
2. UUID is rotated every 3 months so there is no user profile built for a longer history
3. ToS with explanation has been updated https://osmand.net/help-online/terms-of-use/
4. There is a clear indication On first screen after installation and a link to ToS.


As a result I'm closing the issue to make it completed. Thanks for reporting it so we could resolve that subject.
",2022-10-14T11:37:53Z,1042025
2829,osmandapp/OsmAnd,1336452733,1279968904,"Great! Thanks for this improvement, highly appreciated! 👍",2022-10-16T13:21:13Z,52963327
2830,osmandapp/OsmAnd,1336452733,1281657568,Sending of the tracking identifier must be disabled ***by default***.  You must ask the user if they consent to sending it BEFORE you ever send it.,2022-10-18T00:24:13Z,408977
2831,osmandapp/OsmAnd,1336452733,1281844377,"Il 18/10/22 03:23, Jeffrey Paul ha scritto:
> You simply can't send any sort of tracking identifier for a user unless the user consents to it

This is clearly false: for instance no specific explicit consent is 
needed for the transmission of the IP address that comes along with 
every TCP/IP request, when it's necessary to perform the action 
requested by the user. Otherwise you'd end up in a catch 22 situation 
where you need previous consent to allow the user to download the 
privacy policy where they'll learn how to provide consent.

",2022-10-18T05:45:30Z,901528
2832,osmandapp/OsmAnd,1336452733,1364520693,"> when it's necessary to perform the action requested by the user

If you are able to opt out and the outcome of the request does not change compared to if you opted-in, it isn't necessary to perform said action.

From how I understood the use of the uuid it is a non-essential ID this is exactly how it works and it means it is not technically required to provide the service which does indeed mean, it needs to be opt in, not opt out.",2022-12-24T12:15:10Z,72888738
2833,osmandapp/OsmAnd,1336452733,1365884746,"> If you are able to opt out and the outcome of the request does not change compared to if you opted-in, it isn't necessary to perform said action.

As I understood the **Summary** above, the outcome **_does_** change. In particular, your download request (in times of heightened load) might be slowed down significantly (or maybe even delayed for some time or completely refused until retried later when load is lower). 

As s side note, there are other privacy-related options that can be enabled/disabled at that screen, that you might want to check, as well other related system-wide options (i.e. Google's _""Improve Location Accuracy""_ setting, enabled by default on Android phones, leaks hugely bigger amount of privacy and location data, and has much worse privacy record than OsmAnd. Not to mention that if you used Google Play app store to download any of the apps, you are also leaking more privacy data daily than this UUID setting would if you left it in ""on"" position forever).

Not to say this isn't important, but _before_ starting worrying about default setting of this UUID setting, one _at minimum_ should be running [de-googlified ROM](https://replicant.us/) on their phone and using **exclusively** some [privacy oriented app-store dedicated to providing non-proprietary free software only](https://f-droid.org/) instead of Google one (or Apple's, but there is no way to avoid using IOS on iPhone devices AFAIK, so you are out of luck there in any case). Some people do, and thus _they_ might want to suggest (to _their_ app store!) to change that default (for _that_ app store users), yes. 

For Google Play and (Apple store) crowd who (willingly or unknowingly, regardless) continues to surrender their privacy daily anyway, I do not see much point in changing the OsmAnd UUID _default_, as it only would result in their OsmAnd experience getting worse, with no noticeable privacy improvement at all.",2022-12-27T13:01:26Z,156656
2834,anuraghazra/github-readme-stats,1241570951,1241570951,"**Describe the bug**

It looks like the new tiny URL that was introduced in 2fb452c119e9cb4706a1ecf708295db9b77b4b7e that is shown when an error is thrown does not fit on the card. Not a pressing issue but just putting this here so that we don't forget.

![image](https://user-images.githubusercontent.com/17570430/169273808-75921d34-9432-466b-905b-deb6f45c8413.png)

",2022-05-19T10:33:47Z,17570430
2835,anuraghazra/github-readme-stats,1241570951,1131718457,bug happening to me too,2022-05-19T13:50:15Z,74876947
2836,anuraghazra/github-readme-stats,1241570951,1131726903,"Yes, and it's serious! So is mine.",2022-05-19T13:57:20Z,10137
2837,anuraghazra/github-readme-stats,1241570951,1131735886,"This issue is only relating the **design** of the card when error messages are shown. This is the reason I state that it is not pressing. I added this issue I want to fix in the future. I however understand the confusion. I will lock this issue for now. 🤔 

The reason for the `maximum retries exceeded` error can be found in [Common Error Codes](https://github.com/anuraghazra/github-readme-stats/issues/1772). The status of this error is tracked in #1471.",2022-05-19T14:05:09Z,17570430
2838,anuraghazra/github-readme-stats,1241570951,1186123456,"If somebody wants to contribute, I think the best way is to enlarge the card if an error is thrown.",2022-07-16T08:49:03Z,17570430
2839,anuraghazra/github-readme-stats,1239779668,1239779668,"**Describe the bug**
Service doesn't work


**Expected behavior**
I think It's a kind of traffic problem as i captured.

**Screenshots / Live demo link (paste the github-readme-stats link as markdown image)**
<img width=""526"" alt=""스크린샷 2022-05-18 오후 7 49 16"" src=""https://user-images.githubusercontent.com/41659814/169022351-d751d8c9-5ae1-484d-ab3a-6ea53c9f7d91.png"">
you can just visit my profile
https://github.com/jujumilk3


",2022-05-18T10:53:07Z,41659814
2840,anuraghazra/github-readme-stats,1239779668,1129864613,"I am experiencing the same issues, actually it start occurring two days ago.

@jujumilk3 I suggest to amend the title of the issue with something more talkative, such as ""Maximun retries exceeded issues"" so other people can see there's already an issue going on.",2022-05-18T10:56:28Z,57464184
2841,anuraghazra/github-readme-stats,1239779668,1129866245,"Same thing.
I am agree with @carloocchiena. 
@jujumilk3 you should change the title.
",2022-05-18T10:58:26Z,52201020
2842,anuraghazra/github-readme-stats,1239779668,1129891031,@jujumilk3 Looks like your issue has just been resolved. check it out!,2022-05-18T11:27:17Z,18282470
2843,anuraghazra/github-readme-stats,1239779668,1129901767,@jujumilk3 Thanks for creating this issue. This issue is related to #1471. Please use https://github.com/anuraghazra/github-readme-stats/issues/1471#issuecomment-979306704 as a workaround.,2022-05-18T11:39:16Z,17570430
2844,anuraghazra/github-readme-stats,1239779668,1129902703,"@anuraghazra, although it is a duplicate of #1471, I think I will keep this issue open for a while. I have been closing similar issues all this morning, and people don't seem to find the #1471. See my suggestion at https://github.com/anuraghazra/github-readme-stats/issues/1471#issuecomment-1129752460.",2022-05-18T11:40:23Z,17570430
2845,anuraghazra/github-readme-stats,1239779668,1129912080,"same issue just popped up in my account as well. no hurries take your time to fix it if it's a problem in the code. amazing service again, cheers !
",2022-05-18T11:51:21Z,87718788
2846,anuraghazra/github-readme-stats,1239779668,1129916804,@rickstaa i'm wondering if it's a good resolution to host it on our own Vercel server. and this may reduce the frequency of use of the main service,2022-05-18T11:56:57Z,45379733
2847,anuraghazra/github-readme-stats,1239779668,1129920493,"<blockquote class=""twitter-tweet""><p lang=""en"" dir=""ltr"">Got bombarded with github notifications today. 🥲<br>Because github-readme-stats is down.<br><br>People don&#39;t understand that &quot;+1&quot; and &quot;same here&quot; comments don&#39;t help anyone, it&#39;s just puts pressure on the maintainer and spams their inbox.</p>&mdash; Anurag Hazra ⚛ (@anuraghazru) <a href=""https://twitter.com/anuraghazru/status/1526842284167245824?ref_src=twsrc%5Etfw"">May 18, 2022</a></blockquote>

@aghogwarts This problem is affecting all users of the public Vercel instance. For people who experience this problem, please wait until we fix this issue upstream or as mentioned in https://github.com/anuraghazra/github-readme-stats/issues/1471 deploy your own vercel instance. Locking this issue, for now, to keep the notification bar from exploding 🤯.",2022-05-18T12:01:28Z,17570430
2848,umijs/umi,927966336,927966336,"## Why? 

提高研发效率。不管多大的项目，有缓存时启动 1s~3s+，热更新平均 500ms 内，

<img src=""https://user-images.githubusercontent.com/35128/123057434-728d8100-d43a-11eb-8f72-92921adec9ce.png"" width=""500"" />
<img src=""https://user-images.githubusercontent.com/35128/123057518-889b4180-d43a-11eb-91a3-5a531c9f42bf.png"" width=""300"" />

## 什么是 MFSU？

* 🍉 不管多大的项目，有缓存时启动 1s+，热更新平均 500ms 内
* 🍒 配置 mfsu 开启，无需修改项目代码
* 🍑 基于 webpack 的通用方案，umi 3 项目均可用
* 🍓 可用于生产，可多人协作
* 🥕 beta 阶段提供手把手服务群

## 启用方式

1. 先确保是 umi 3 项目
2. 修改依赖里的 umi 版本为 ""^3.5.0""

```diff
- ""umi"": ""3""
+ ""umi"": ""^3.5.0""
```

3. 重装依赖
4. 修改配置，加上 mfsu 配置

```diff
// 只需要 dev，这么配
+ mfsu: {},

// 如果需要针对生产环境生效，这么配
+ mfsu: { production: { output: '.mfsu-production' } },
```

5. 清空 src/.umi
6. 启动 umi dev

## FAQ

### 卡在 99% 不动了？

99% 是进度条显示问题，webpackbar 和 webpack 5 的兼容还有点问题，实际已经成功了，可忽略。

### Invalid key: mfsu

`npx umi -v` 看下，请确保是 umi@3.5.0 或以上。

### Can not resolve ..., Do you mean ... 报错

e.g.

![image](https://user-images.githubusercontent.com/35128/123912265-66ac3c80-d9af-11eb-9e49-f42d464f1a32.png)

几个选择：

1. 需要更新相关依赖到最新，因为这个版本的依赖和 webpack5 不兼容
2. chainWebpack 里配上 `config.module.rule('mjs-rule').test(/.m?js/).resolve.set('fullySpecified', false);`

### 国际化不生效

需更新 `@umijs/preset-react` 到最新，已在 `@umijs/plugin-locale` [0.13.0](https://github.com/umijs/plugins/blob/master/packages/plugin-locale/CHANGELOG.md#0130-2021-06-23) 里修复。

### Module parse failed: Top-Level-Await 报错

e.g.
![image](https://user-images.githubusercontent.com/35128/123912584-cf93b480-d9af-11eb-8dd5-9714b844e258.png)

如果报错的是项目文件，给此文件后面加一行 `export {}`，声明为 esm 模块格式。

### 如果文件中有用动态 require 语法怎么办？

改成 `await import` 写法，比如：

```js
const test = 'test';
const foo = (await import((`../${test}`))).default;
```

参考：https://github.com/umijs/umi/issues/6877

### 开启 SSR 时报错

暂不支持。

### 覆盖依赖的样式不生效（比如覆盖 antd 的样式）

目前没有很好的解法，需修改代码，提升覆盖样式的优先级。

### 有使用 dva 插件同时效果不佳？

配置 [`dva: { disableModelsReExport: {} }`](https://umijs.org/plugins/plugin-dva#disablemodelsreexport)，因为  `export * from` 目前尚未支持匹配到预编译。

### antd 主题配置为啥失效？

如果你是用的 [配置 less 变量文件](https://ant.design/docs/react/customize-theme-cn#%E9%85%8D%E7%BD%AE-less-%E5%8F%98%E9%87%8F%E6%96%87%E4%BB%B6) 这种方式，切换到 [theme 配置](https://ant.design/docs/react/customize-theme-cn#%E5%9C%A8-Umi-%E9%87%8C%E9%85%8D%E7%BD%AE%E4%B8%BB%E9%A2%98)的方式，或尝试关闭 babel-plugin-import 的自动引入 less 文件功能。

## Beta 阶段手把手服务群

<img src=""https://user-images.githubusercontent.com/35128/126024002-1ed91a28-3a5c-42ea-ab86-e54ec3171c75.png"" width=""200"" />
",2021-06-23T07:53:48Z,35128
2849,umijs/umi,927966336,866639790,感觉需要把，自定义插件如何适配的说明补充一下 @xiefengnian,2021-06-23T08:28:50Z,11746742
2850,umijs/umi,927966336,866651860,"@xiaohuoni 

1. 不要出现 require 等 cjs 写法
2. 依赖了包注意声明依赖
",2021-06-23T08:46:58Z,35128
2851,umijs/umi,927966336,866668787,好像之前聊天有说，runtime 需要放在 `.umi` 目录下，我比较模糊。,2021-06-23T09:10:23Z,11746742
2852,umijs/umi,927966336,866734990,求解释，MFSU 是啥，为何缩短了 compile 的时间。,2021-06-23T10:50:50Z,6426738
2853,umijs/umi,927966336,866739614,"antd 需要升级吗？用了之后 rc-field-form 报了以下错误：

```
ERROR in ./node_modules/rc-field-form/node_modules/@babel/runtime/helpers/esm/toConsumableArray.js 4:0-52
Module not found: Error: Can't resolve './nonIterableSpread' in '/Users/mios/workspace/guanyun/projects/eagle/hawkeye/node_modules/rc-field-form/node_modules/@babel/runtime/helpers/esm'
Did you mean 'nonIterableSpread.js'?
BREAKING CHANGE: The request './nonIterableSpread' failed to resolve only because it was resolved as fully specified
(probably because the origin is a '*.mjs' file or a '*.js' file where the package.json contains '""type"": ""module""').
The extension in the request is mandatory for it to be fully specified.
Add the extension to the request.
 @ ./node_modules/rc-field-form/es/Field.js 3:0-78 80:44-62 80:76-94
 @ ./node_modules/rc-field-form/es/index.js 2:0-28 10:16-21 13:0-46
 @ ./node_modules/antd/es/config-provider/index.js 7:0-63 99:51-65
 @ ./node_modules/antd/es/notification/hooks/useNotification.js 5:0-55 46:65-79
 @ ./node_modules/antd/es/notification/index.js 11:0-60 235:22-43
 @ ./src/.umi/.cache/.mfsu/mf-va_antd_es_notification.js 1:0-112 2:15-16 3:0-112 3:0-112
 @ container entry ./antd/es/notification[0]

webpack compiled with 42 errors
```",2021-06-23T10:59:25Z,506368
2854,umijs/umi,927966336,866835821,"assert
http://localhost:8000/mf-dep_vendors-node_modules_umijs_runtime_dist_index_esm_js.a8693fc8.async.js:649:21
(anonymous function)
http://localhost:8000/mf-dep_vendors-node_modules_umijs_runtime_dist_index_esm_js.a8693fc8.async.js:704:9
Plugin.register
http://localhost:8000/mf-dep_vendors-node_modules_umijs_runtime_dist_index_esm_js.a8693fc8.async.js:703:33
▲ 3 stack frames were expanded.


发现是用 postcss 引入  taiwindcss 的原因",2021-06-23T13:28:15Z,22520211
2855,umijs/umi,927966336,866868699,@miaopeng @chengluliu 只给报错信息并不能解决你们的问题，需要复现步骤。,2021-06-23T14:08:32Z,35128
2856,umijs/umi,927966336,866869123,@realgkl 说来话长，得单独起一篇文章讲。,2021-06-23T14:09:04Z,35128
2857,umijs/umi,927966336,866898905,"> @realgkl 说来话长，得单独起一篇文章讲。

非常有想象力的技术设计，非常期待大佬出一篇文章科普一下",2021-06-23T14:42:22Z,9473858
2858,umijs/umi,927966336,866915811,"> @miaopeng @chengluliu 只给报错信息并不能解决你们的问题，需要复现步骤。

我使用的是antd pro，唯一不同的地方就是用了taiwindcss, 只要打开+ mfsu: {},  umi dev之后，终端都正常，但是打开页面，就出现报错，怀疑是用了taiwindcss的问题",2021-06-23T15:02:10Z,22520211
2859,umijs/umi,927966336,867271055,"> @xiaohuoni
> 
> 1. 不要出现 require 等 cjs 写法
> 2. 依赖了包注意声明依赖

<img src={'require('/source/path')'} />
这种写法是不是也不允许？那引用图片相对还是有点麻烦了",2021-06-24T01:51:44Z,8032943
2860,umijs/umi,927966336,867302587,"> > @xiaohuoni
> > 
> > 1. 不要出现 require 等 cjs 写法
> > 2. 依赖了包注意声明依赖
> 
> <img src={'require('/source/path')'} />
> 这种写法是不是也不允许？那引用图片相对还是有点麻烦了

允许的，试试就知道了。",2021-06-24T03:16:14Z,35128
2861,umijs/umi,927966336,867422277,"### CC大佬 在 _ant design pro_  中修改MFSU相关配置后报这个错误！！！
![image](https://user-images.githubusercontent.com/55118447/123224206-30307680-d504-11eb-87b7-9f3ba0b34b49.png)

",2021-06-24T07:53:12Z,55118447
2862,umijs/umi,927966336,867444166,@13500Jin 给复现步骤。,2021-06-24T08:25:50Z,35128
2863,umijs/umi,927966336,867472330,"@sorrycc > @13500Jin 给复现步骤。

### 在ant design pro 提供的 模板项目中：
1. 将package.json中的umi升级为^3.5.0-beta.9
![image](https://user-images.githubusercontent.com/55118447/123235191-388daf00-d50e-11eb-9262-a49abcb8a848.png)


2. 在config.ts中添加MFSU相关配置
![image](https://user-images.githubusercontent.com/55118447/123235099-257adf00-d50e-11eb-96a5-0c970817f11d.png)



### [模板项目长这样](https://preview.pro.ant.design/dashboard/analysis?primaryColor=%231890ff&fixSiderbar=true&colorWeak=false&pwa=false)
![image](https://user-images.githubusercontent.com/55118447/123235377-62df6c80-d50e-11eb-85d7-d673ebea7985.png)


### 启动项目后，打开项目就会报错！！！
![image](https://user-images.githubusercontent.com/55118447/123235538-8bfffd00-d50e-11eb-9190-b5b21f790833.png)


### 另外：多次尝试删除 node_modules 以及 .umi 目录并重启项目后   访问页面还是报错！！！！",2021-06-24T09:06:57Z,55118447
2864,umijs/umi,927966336,867576632,"> @sorrycc > @13500Jin 给复现步骤。
> 
> ### 在ant design pro 提供的 模板项目中：
> 1. 将package.json中的umi升级为^3.5.0-beta.9
>    ![image](https://user-images.githubusercontent.com/55118447/123235191-388daf00-d50e-11eb-9262-a49abcb8a848.png)
> 2. 在config.ts中添加MFSU相关配置
>    ![image](https://user-images.githubusercontent.com/55118447/123235099-257adf00-d50e-11eb-96a5-0c970817f11d.png)
> 
> ### [模板项目长这样](https://preview.pro.ant.design/dashboard/analysis?primaryColor=%231890ff&fixSiderbar=true&colorWeak=false&pwa=false)
> ![image](https://user-images.githubusercontent.com/55118447/123235377-62df6c80-d50e-11eb-85d7-d673ebea7985.png)
> 
> ### 启动项目后，打开项目就会报错！！！
> ![image](https://user-images.githubusercontent.com/55118447/123235538-8bfffd00-d50e-11eb-9190-b5b21f790833.png)
> 
> ### 另外：多次尝试删除 node_modules 以及 .umi 目录并重启项目后 访问页面还是报错！！！！

命令行有报错信息吗？",2021-06-24T11:54:16Z,20136563
2865,umijs/umi,927966336,868165434,"> > @sorrycc > @13500Jin 给复现步骤。
> > ### 在ant design pro 提供的 模板项目中：
> > 
> > 1. 将package.json中的umi升级为^3.5.0-beta.9
> >    ![image](https://user-images.githubusercontent.com/55118447/123235191-388daf00-d50e-11eb-9262-a49abcb8a848.png)
> > 2. 在config.ts中添加MFSU相关配置
> >    ![image](https://user-images.githubusercontent.com/55118447/123235099-257adf00-d50e-11eb-96a5-0c970817f11d.png)
> > 
> > ### [模板项目长这样](https://preview.pro.ant.design/dashboard/analysis?primaryColor=%231890ff&fixSiderbar=true&colorWeak=false&pwa=false)
> > ![image](https://user-images.githubusercontent.com/55118447/123235377-62df6c80-d50e-11eb-85d7-d673ebea7985.png)
> > ### 启动项目后，打开项目就会报错！！！
> > ![image](https://user-images.githubusercontent.com/55118447/123235538-8bfffd00-d50e-11eb-9190-b5b21f790833.png)
> > ### 另外：多次尝试删除 node_modules 以及 .umi 目录并重启项目后 访问页面还是报错！！！！
> 
> 命令行有报错信息吗？

### 没有",2021-06-25T02:57:54Z,55118447
2866,umijs/umi,927966336,869289079,mark,2021-06-28T02:30:34Z,3301995
2867,umijs/umi,927966336,869297784,"开启功能后，当链接上带上query后就会报错
http://localhost:5000/?userId=04516030431133186#
![image](https://user-images.githubusercontent.com/32838658/123572146-425d2e00-d7fe-11eb-9ad6-cd585caf2a10.png)
",2021-06-28T02:48:05Z,32838658
2868,umijs/umi,927966336,869505801,"MFSU_CACHE.json 中会生成绝对路径的Key，会导致不适用于多人协作(https://umijs.org/zh-CN/docs/mfsu#%E5%BC%80%E5%8F%91%E9%98%B6%E6%AE%B5)

``` json
{
  ""deps"": {
    ""/Users/path/to/node_modules/umi/node_modules/@umijs/renderer-react"": ""3.5.0-beta.17"",
    ""/Users/path/to/node_modules/umi/node_modules/@umijs/runtime"": ""3.5.0-beta.17"",
    ...
  }
  ...
}
```",2021-06-28T08:59:53Z,6056366
2869,umijs/umi,927966336,870337689,"> > > @sorrycc > @13500Jin 给复现步骤。
> > > ### 在ant design pro 提供的 模板项目中：
> > > 
> > > 1. 将package.json中的umi升级为^3.5.0-beta.9
> > >    ![image](https://user-images.githubusercontent.com/55118447/123235191-388daf00-d50e-11eb-9262-a49abcb8a848.png)
> > > 2. 在config.ts中添加MFSU相关配置
> > >    ![image](https://user-images.githubusercontent.com/55118447/123235099-257adf00-d50e-11eb-96a5-0c970817f11d.png)
> > > 
> > > ### [模板项目长这样](https://preview.pro.ant.design/dashboard/analysis?primaryColor=%231890ff&fixSiderbar=true&colorWeak=false&pwa=false)
> > > ![image](https://user-images.githubusercontent.com/55118447/123235377-62df6c80-d50e-11eb-85d7-d673ebea7985.png)
> > > ### 启动项目后，打开项目就会报错！！！
> > > ![image](https://user-images.githubusercontent.com/55118447/123235538-8bfffd00-d50e-11eb-9190-b5b21f790833.png)
> > > ### 另外：多次尝试删除 node_modules 以及 .umi 目录并重启项目后 访问页面还是报错！！！！
> > 
> > 
> > 命令行有报错信息吗？
> 
> ### 没有

有解决吗，遇到同样的报错了",2021-06-29T07:15:00Z,24565609
2870,umijs/umi,927966336,871121927,"报了和楼上一样的bug。。。请问要怎么处理？

@Carreylife ",2021-06-30T06:06:01Z,3849702
2871,umijs/umi,927966336,871123373,"@sorrycc ，我的umi config文件大概是长这样的：

```js
const config = defineConfig({
  routes,
  nodeModulesTransform: {
    type: 'none',
    exclude: [],
  },
  mfsu: {},
  webpack5: {},
  dynamicImport: {},
  // fastRefresh: {},
  presets: [require.resolve('@dzg/umi-preset')],
  extraBabelPlugins: [
    [
      'babel-plugin-import',
      {
        libraryName: 'antd',
        libraryDirectory: 'es',
        style: true,
      },
      'antd',
    ],
  ],
  devtool: isSourceMapEnabled ? 'source-map' : false,
  chainWebpack: function(config: any, { webpack }: any) {
    if (process.env.NODE_ENV === 'production') {
      config.merge({
        optimization: {
          minimize: true,
          splitChunks: {
            chunks: 'all',
            cacheGroups: {
              reactVendor: {
                test: /[\\/]node_modules[\\/](react|react-dom)[\\/]/,
                name: 'reactVendor',
                enforce: true,
                priority: 5,
              },
              antd: {
                test: /[\\/]node_modules[\\/](antd)[\\/]/,
                name: 'antd',
                enforce: true,
                priority: 4,
              },
              umiVendor: {
                test: /[\\/]node_modules[\\/](umi).*[\\/]/,
                name: 'umiVendor',
                enforce: true,
                priority: 3,
              },
              // dzgVendors: {
              //   name: 'dzgVendors',
              //   enforce: true,
              //   priority: 2,
              //   test: /[\\/]node_modules[\\/](@dzg)[\\/]/,
              // },
              vendors: {
                name: 'vendors',
                enforce: true,
                priority: 1,
                test: /[\\/]node_modules[\\/]((?!(@dzg|antd|react|react-dom|umi)).*)[\\/]/,
              },
              default: {
                test: /[\\/]src[\\/]((?!(pages)).*)[\\/]/,
                name: 'default',
                enforce: true,
              },
            },
          },
        },
      });
      if (process.env.SENTRY === 'on') {
        config.plugin('SentryPlugin').use(SentryPlugin, [
          {
            include: './dist',
            release: packageJSON.version,
            configFile: 'sentry.properties',
            urlPrefix: `~/${packageJSON.name}/`,
            rewrite: true,
          },
        ]);
      }
    }
  },
  antd: {},
  title: 'dzg-tms-front',

  locale: {
    antd: true,
    default: 'zh-CN',
    baseNavigator: true,
  },
  chunks:
    process.env.NODE_ENV === 'production'
      ? [
          'reactVendor',
          'antd',
          'umiVendor',
          // 'dzgVendors',
          'vendors',
          'default',
          'umi',
        ]
      : ['umi'],
  history: { type: 'hash' },
  hash: true,
  publicPath: '/new-front/',
  proxy,
  ignoreMomentLocale: true,
  copy: ['static'],
  theme: theme,
  mountElementId: 'dzg-tms-front',
});

export default config;
```",2021-06-30T06:09:02Z,3849702
2872,umijs/umi,927966336,871127850,"刚刚升级了 v3.5 后出现了，useModel 使用不了
![image](https://user-images.githubusercontent.com/42735363/123911164-010b8080-d9ae-11eb-9293-c11fdf15436e.png)
",2021-06-30T06:18:18Z,42735363
2873,umijs/umi,927966336,871162634,"![image](https://user-images.githubusercontent.com/15603321/123919313-6748d100-d9b7-11eb-8cf9-840eae679ddb.png)
![image](https://user-images.githubusercontent.com/15603321/123919341-6f087580-d9b7-11eb-9c35-bf33111cf1f3.png)
![image](https://user-images.githubusercontent.com/15603321/123919380-762f8380-d9b7-11eb-9b52-fec6c0fef728.png)
命令行没有报错，页面报错",2021-06-30T07:26:05Z,15603321
2874,umijs/umi,927966336,871188353,"antd-pro 升级之后，首次运行报错。
// config/config.ts 增加 mfsu:{} 后
![image](https://user-images.githubusercontent.com/12093551/123925197-42575c80-d9bd-11eb-87f4-c9bc072ac186.png)
// config/config.dev.ts 增加 mfsu:{} 后
![image](https://user-images.githubusercontent.com/12093551/123925953-feb12280-d9bd-11eb-93b8-44a1b141d051.png)
",2021-06-30T08:07:54Z,12093551
2875,umijs/umi,927966336,871192776,"> ![image](https://user-images.githubusercontent.com/15603321/123919313-6748d100-d9b7-11eb-8cf9-840eae679ddb.png)

@sorrycc  development 环境下 publichPath 会导致类似的错误出现, 这里有个 mini 复现
项目是 `yarn create @umijs/umi-app` 创建出来的


mini复现 https://github.com/charlzyx/bug-report-umi-mfsu-mini

# .umirc.ts

```diff
import { defineConfig } from 'umi';

export default defineConfig({
+ base: '/app',
+ publicPath: '/app/',
+ webpack5: {},
+ mfsu: {},
+ dynamicImport: {},
  nodeModulesTransform: {
    type: 'none',
  },
  routes: [
    { path: '/', component: '@/pages/index' },
  ],
  fastRefresh: {},
});

```

# 如何修正
NODE_ENV=development 的时候, 不设置 publicPath (未测试)

# 可能的修复方法

但是了解的不够深入, 不知道影响范围, 只能抛个砖

[getMfsuPath](https://github.com/umijs/umi/blob/master/packages/preset-built-in/src/plugins/features/mfsu/mfsu.ts#L39)

[addBeforeMiddlewares](https://github.com/umijs/umi/blob/master/packages/preset-built-in/src/plugins/features/mfsu/mfsu.ts#L238)

```diff
# L39
export const getMfsuPath = (api: IApi, { mode }: { mode: TMode }) => {
  if (mode === 'development') {
    const configPath = api.userConfig.mfsu?.development?.output;
+   // 形如 /app/ 这样的 publicPath, 需要修正一下输出路径
+   const publicPath = /^\/.*\/$/.test(api.config.publicPath) ?api.config.publicPath : '';
    return configPath
-     ? join(api.cwd, configPath)
-     : join(api.paths.absTmpPath!, '.cache', '.mfsu');
+     ? join(api.cwd, configPath, publicPath)
+     : join(api.paths.absTmpPath!, '.cache', '.mfsu', publicPath);

  } else {
+   // 生产环境不知道怎么处理
    const configPath = api.userConfig.mfsu?.production?.output;
    return configPath
      ? join(api.cwd, configPath)
      : join(api.cwd, './.mfsu-production');
  }
};
# L238
api.addBeforeMiddlewares(() => {
  return (req, res, next) => {
-   const { pathname } = url.parse(req.url);
+   const { pathname: originPathName } = url.parse(req.url);
+   // 形如 /app/ 这样的 publicPath, 需要修正一下输出路径
+   const publicPath = /^\/.*\/$/.test(api.config.publicPath) ?api.config.publicPath : '';
+   const pathname = originPathName.replace(publicPath, '');
+   // 没有后缀的请求, 不做处理
+   const noExtnameMaybeDir = !!require('path').extname(pathname);
    if (
+     noExtnameMaybeDir ||
      !api.userConfig.mfsu ||
      pathname === '/' ||
      !existsSync(
        join(getMfsuPath(api, { mode: 'development' }), '.' + pathname),
      )
    ) {
      next();
    } else {
      const value = readFileSync(
        join(getMfsuPath(api, { mode: 'development' }), '.' + pathname),
        'utf-8',
      );
      res.setHeader('content-type', mime.lookup(parse(pathname || '').ext));
      // 排除入口文件，因为 hash 是入口文件控制的
      if (!/remoteEntry.js/.test(req.url)) {
        res.setHeader('cache-control', 'max-age=31536000,immutable');
      }
      res.send(value);
    }
  };
});
```


",2021-06-30T08:14:08Z,18055018
2876,umijs/umi,927966336,871209303,神奇，antd-pro的项目，从umi 3.4.1升上来，国际化locale就全挂了，其他倒是好像都没问题……,2021-06-30T08:39:43Z,17058960
2877,umijs/umi,927966336,871236359,"> 神奇，antd-pro的项目，从umi 3.4.1升上来，国际化locale就全挂了，其他倒是好像都没问题……

需要升级`@umijs/preset-react`",2021-06-30T09:18:45Z,16058286
2878,umijs/umi,927966336,871344936,"命令行没出错，
但console里 出现这个错误的。目测是devServer和mfsu的冲突问题。
![image](https://user-images.githubusercontent.com/4388158/123957197-139dae00-d9de-11eb-89c8-74b6e8bfa4d1.png)


![image](https://user-images.githubusercontent.com/4388158/123957503-7131fa80-d9de-11eb-828b-7f1df1edf3c3.png)

devServer 没认出来 http://localhost:8000/admin/mf-va_remoteEntry.js这个链接，直接返回了index.html的内容





只是定位到错误，具体哪里改，还正在看。
",2021-06-30T12:05:08Z,4388158
2879,umijs/umi,927966336,871407860,@charlzyx 测试了一下，确实是publicPath引起的问题,2021-06-30T13:32:19Z,3849702
2880,umijs/umi,927966336,871440411,"Tested, seems to break css rendering of the page.",2021-06-30T14:11:15Z,1297759
2881,umijs/umi,927966336,871643176,什么时候完美支持SSR，现在好像还不支持。,2021-06-30T18:44:43Z,2328917
2882,umijs/umi,927966336,871883063,用了dva能编译成功，但是访问用了connect的页面都会报错，偶尔第二次人更新后会正常，再刷新依然会报错，我怀疑models渲染的时候都没加载进来,2021-07-01T03:21:11Z,5869741
2883,umijs/umi,927966336,871884270,"![image](https://user-images.githubusercontent.com/26587649/124060096-e1806080-da5e-11eb-92aa-2312a0665ce5.png)
我的好像报了奇怪的错误
Cannot read property 'ModuleFederationPlugin' of undefined
",2021-07-01T03:24:42Z,26587649
2884,umijs/umi,927966336,871908438,"> ![image](https://user-images.githubusercontent.com/26587649/124060096-e1806080-da5e-11eb-92aa-2312a0665ce5.png)
> 我的好像报了奇怪的错误
> Cannot read property 'ModuleFederationPlugin' of undefined



你这个应该是umi内部没启用webpack5成功，用了webpack4的。你配置是写在哪里的？",2021-07-01T04:30:19Z,4388158
2885,umijs/umi,927966336,871953397,"> > ![image](https://user-images.githubusercontent.com/26587649/124060096-e1806080-da5e-11eb-92aa-2312a0665ce5.png)
> > 我的好像报了奇怪的错误
> > Cannot read property 'ModuleFederationPlugin' of undefined
> 
> 你这个应该是umi内部没启用webpack5成功，用了webpack4的。你配置是写在哪里的？

写在了config.dev.ts里面的",2021-07-01T06:14:15Z,26587649
2886,umijs/umi,927966336,871960914,不支持 config.dev.ts，写 config.ts 里。,2021-07-01T06:27:58Z,35128
2887,umijs/umi,927966336,871961230,大家单独提 ISSUE 吧，这个 ISSUE 太长了，不方便回复和跟踪。,2021-07-01T06:28:31Z,35128
2888,MinecraftForge/MinecraftForge,1118392130,1118392130,"**Minecraft Version:** {Minecraft version}
Minecraft 1.18.1 Client with Forge (Optifine & Controllable Mods)

**Forge Version:** {Forge version. *Version number, not latest/rb*}
Forge Mod 39.0.46 - 39.0.63 (Issue Versions)
Forge Mod 39.0.40 - 39.0.45 (Working Versions)

**Logs:** {Link(s) to GitHub Gist with full latest.log and/or crash report}
https://paste.ee/p/nFncj - Debug Log
https://paste.ee/p/nknCg - Latest Log (Edited for redaction)


**Steps to Reproduce:**
 1. Run Minecraft 1.18.1 Server with LuckPerms Plugin (v5.3.98)
 2. Install Forge Mod version between 39.0.46 - 39.0.63 on client running forge of versions above
 3. Run any LuckPerms server side issued command to verify issue on client running forge of versions above

**Description of issue:**

When running seemingly any version of Forge 39.0.46 through 39.0.63 (latest as of submission), this causes the client to not register any Luck Permissions plugin command on server as if command is not being entered at all. There is no log of event on client or server side, it just purely acts like the command isn't being entered and nothing happens. _This issue **DOES NOT** happen as tested with Forge Versions 39.0.40 or 39.0.45 and works as expected._
",2022-01-30T03:09:04Z,48694854
2889,MinecraftForge/MinecraftForge,1118392130,1025094392,"How are you running a plugin on Forge?

Most of these Bukkit/Sponge/whatever else hybrid solutions leave a lot to desired, it is my guess that whatever you're using is not suited for this purpose.

It looks to me like the server's commands are simply not being sent to the client in advance, so our addition of client commands are taking precedence (as they should) and causing the server to not be able to evaluate the command.

I suggest you take this up with the plugin system, not here. We do everything we can. ",2022-01-30T08:23:15Z,42079760
2890,MinecraftForge/MinecraftForge,1118392130,1025098778,"However, if you can reproduce an issue where the [server correctly sends the commands](https://wiki.vg/Protocol#Declare_Commands) and we ignore it. Then we can take a look. However in all our tests, even non-vanilla commands are processed correctly.",2022-01-30T08:55:09Z,702211
2891,MinecraftForge/MinecraftForge,1118392130,1025158408,"> How are you running a plugin on Forge?
> 
> Most of these Bukkit/Sponge/whatever else hybrid solutions leave a lot to desired, it is my guess that whatever you're using is not suited for this purpose.
> 
> It looks to me like the server's commands are simply not being sent to the client in advance, so our addition of client commands are taking precedence (as they should) and causing the server to not be able to evaluate the command.
> 
> I suggest you take this up with the plugin system, not here. We do everything we can.

I'm not sure how you've made this out that I'm running a plugin on Forge. I clearly and purely state I'm running the LuckPerms plugin on a Minecraft 1.18.1 Paper **SERVER**, and I'm running a **CLIENT** with Forge with a controllable mod and optifine in the range of the versions clearly stated above.",2022-01-30T14:45:36Z,48694854
2892,MinecraftForge/MinecraftForge,1118392130,1025158873,"Your steps imply to install Forge on the server, so that is not clearly and purely stated.

However, the issue remains the same.",2022-01-30T14:48:02Z,42079760
2893,MinecraftForge/MinecraftForge,1118392130,1025159857,"> Your steps imply to install Forge on the server, so that is not clearly and purely stated.
> 
> However, the issue remains the same.

I have corrected that with an edit, however it should have been implied that forge was installed on the client with the very first line of the issue ticket if it was read.

Yes indeed, the issue still remains and can verify on fresh installs of multiple machines that an update from 39.0.45 does indeed break the luckperm commands.",2022-01-30T14:53:18Z,48694854
2894,MinecraftForge/MinecraftForge,1118392130,1025177166,"#7754 - 39.0.46 Add Client Commands

Extensive testing and searching shows that issue does indeed start at Forge Version 39.0.46 when PR for ""Add Client Commands"" was requested.",2022-01-30T16:17:34Z,48694854
2895,MinecraftForge/MinecraftForge,1118392130,1025177911,"Temporarily locking this issue as nothing is being added.
We know what caused this. 
We're trying to see if there's anything we can do to fix this, but i honestly doubt it.
LuckPerms (or Spigot/Paper/Whatever) isn't sending the client information about the commands.",2022-01-30T16:21:11Z,42079760
2896,MinecraftForge/MinecraftForge,1118392130,1029526426,"For those who are still keeping track of this, it seems like the issue here is that the commands *are* being sent, but only via the command tree, and not as executable commands.
For LuckPerms this can be traced to their use of Commodore, but both use the old (< 1.13) methods of registering commands, which is why they broke when Forge updated to explicitly require support for these.

From the author of the Client Commands PR that exposed this problem:
> seems like both bungeecord and luckperms send the commands but they don't mark them as executable since they are only sending them in brigadier format for the client since they both still use the command registration method that was removed in 1.13 on the backend",2022-02-04T00:25:58Z,42079760
2897,golang/go,1145514858,1145514858,"The DecodeString() method exists as a single function, an EncodeString() method should be added as it will not break forward compatibility and conform to a standard naming structure and module offering.

https://pkg.go.dev/encoding/base64

This is not a duplicate of a bug, I was asked to make it a proposal regardless of it being a bug or proposal. 

This is a proposal in writing to a software repository.",2022-02-21T09:08:41Z,741705
2898,golang/go,1145514858,1046949018,"`DecodeString` is (b64 encoded string) -> (unencoded []byte).

The reverse is `EncodeToString`, which does (unencoded []byte) -> (b64 encoded string).

I am not sure what else you are looking for.",2022-02-21T14:40:28Z,5663952
2899,golang/go,1145514858,1046955774,"Is the function `EncodeToString` the behavior you want, and you think it should be named `EncodeString`?",2022-02-21T14:48:03Z,166725
2900,golang/go,1145514858,1047101265,"What is the exact API that you are proposing?

We currently have

```Go
// DecodeString returns the bytes represented by the base64 string s.
func (enc *Encoding) DecodeString(s string) ([]byte, error)

// EncodeToString returns the base64 encoding of src.
func (enc *Encoding) EncodeToString(src []byte) string
```

What are you suggesting that we add?  Thanks.",2022-02-21T17:30:09Z,3194333
2901,golang/go,1145514858,1047294055,"@ianlancetaylor 

Do you not understand that the input arguments to those functions are different?

If there is an Encode function there should be a Decode function with the same name structure and:

* the same input argument types
* the same output argument types

I realize what happened, someone was building it and needing to encode a byte array to a string so they wrote that function, then needed to decode a string to a byte array so they wrote that function.

When people read the documentation first, they are confused by the lack of a common naming structure and input/output type structure.

That should be cleaned and reorganized so that when people are working and reading the 1000's of Go functions (certainly impossible to remember all) they will not get frustrated by constant flipping between string and []byte and a lack of a naming structure that can be quickly remembered because it adheres to both common function names per functionality and input/output type.",2022-02-21T23:21:49Z,741705
2902,golang/go,1145514858,1047295273,"You must understand, your company can afford to pay people to do things like that and have a team leader structure that respects who actually made things.

That's why you have things like Gmail and Go, because you let people work and then lead other people to finish their work, not tried to confuse them with constant changes that no single person understands.

It was the same way at Ford, it was the same way at Chevy, it is the same way at Boeing, Lockheed Martin, etc....",2022-02-21T23:24:15Z,741705
2903,golang/go,1145514858,1047306154,"@andrewhodel Help us out a bit if you want us to consider your change. Please tell us exactly what method you want added. You've more than adequately defined the motivation - but we're still not clear on what exactly the changes you're proposing are. Write us a prototype of the function/method you want, and a 1-2 sentence godoc description of what it does. Thanks.

",2022-02-21T23:58:16Z,6889504
2904,golang/go,1145514858,1047308887,"@randall77 

`EncodeString()` and `DecodeString()` should both exist because one of them does and the concept of encoding/decoding is fundamentally bound together.

I didn't read every function, but you should and you should adhere this naming structure and ensure that all relevant sets exist.

In other words

`Encode([]byte)` means there should be `Decode([]byte)`

`function_1_group_with_3_total()` should include `function_2_group_with_3_total()` and `function_3_group_with_3_total()` and so on.  For example, with hash functions there is usually a standard naming format due to the required functions, that of `input`, `parse`, `decode`, `inputMore` etc.

Then when someone is working for example with the encoding/base64 library, they can set a number of days or hours aside to work with the library then get used to the functions.  In other words, they can type Encode and Decode and EncodeString and DecodeString enough times to not need to deal with remembering arguments to every single function instead only needing to remember the top level function names as they all follow a common naming pattern.",2022-02-22T00:05:15Z,741705
2905,golang/go,1145514858,1047310395,"Ok, but you still haven't answered my question - what exactly are you proposing to add to `encoding/base64`?
",2022-02-22T00:08:51Z,6889504
2906,golang/go,1145514858,1047325354,"@randall77 

I answered your question in the first sentence of the original reply to your original comment/question.

EncodeString() and DecodeString() should both exist because one of them does and the concept of encoding/decoding is fundamentally bound together.

These are functions in Go of the encoding/base64 library.  One exists, one doesn't.  Again, these are functions in Go.",2022-02-22T00:47:19Z,741705
2907,golang/go,1145514858,1047326516,"@andrewhodel That is not an answer to the question that @randall77 asked and it is not an answer to the question I asked.

I will repeat:

We currently have

```Go
// DecodeString returns the bytes represented by the base64 string s.
func (enc *Encoding) DecodeString(s string) ([]byte, error)

// EncodeToString returns the base64 encoding of src.
func (enc *Encoding) EncodeToString(src []byte) string
```

What are you suggesting that we add? Please write it in the exact form that I wrote it.  This is a programming language. 
 Precision matters.  Please write down exactly what you mean.  Don't describe what you mean in words.  Write it in Go.  Thanks.",2022-02-22T00:50:12Z,3194333
2908,golang/go,1145514858,1047327434,"```
func (enc *Encoding) EncodeString(s string) ([]byte, error)
```

You should also read and make sure all the other pairs exist for encoding and decoding of, with respect to both input and output:

* byte arrays
* strings

If you want me to write it, then provide commit access.",2022-02-22T00:52:35Z,741705
2909,golang/go,1145514858,1047327684,"You can write any change you like without commit access, as described at https://go.dev/doc/contribute.  Thanks.",2022-02-22T00:53:16Z,3194333
2910,golang/go,1145514858,1047328331,"@ianlancetaylor then make:

@ianlancetaylor 
@prattmic 
@findleyr 
@cherrymui 

All work that way for every commit or stop with the absurdity.",2022-02-22T00:54:59Z,741705
2911,golang/go,1145514858,1047328541,We do all work that way for every commit.  Why do you think we don't?,2022-02-22T00:55:41Z,3194333
2912,golang/go,1145514858,1047328915,"@ianlancetaylor 

That's not true, you push changes directly to the repository.",2022-02-22T00:56:34Z,741705
2913,golang/go,1145514858,1047329694,"@andrewhodel That is not correct.  Why do you think it is?

All changes to the Go repository must be reviewed and approved by somebody else, as described at https://go.dev/doc/contribute#review.  That is true for everybody.

What is different for people with approval access is that they only require one reviewer, not two reviewers.  See https://go.dev/wiki/GerritAccess.",2022-02-22T00:58:43Z,3194333
2914,golang/go,1145514858,1047332718,"You are adjusting time and commits to dev while calling it changes. 

Thank You,
Andrew Hodel

> On Feb 22, 2022, at 10:58 AM, Ian Lance Taylor ***@***.***> wrote:
> 
> ﻿
> @andrewhodel That is not correct. Why do you think it is?
> 
> All changes to the Go repository must be reviewed and approved by somebody else, as described at https://go.dev/doc/contribute#review. That is true for everybody.
> 
> What is different for people with approval access is that they only require one reviewer, not two reviewers. See https://go.dev/wiki/GerritAccess.
> 
> —
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you were mentioned.
",2022-02-22T01:06:22Z,741705
2915,golang/go,1145514858,1047333816,"> You are adjusting time and commits to dev while calling it changes.

I'm sorry, I don't understand what you mean.",2022-02-22T01:07:42Z,3194333
2916,golang/go,1145514858,1047334296,"I map all that stuff out with https://github.com/andrewhodel/rrd based on word counts and frequency to prove it to people. 



Thank You,
Andrew Hodel

> On Feb 22, 2022, at 11:06 AM, Andrew Hodel ***@***.***> wrote:
> 
> ﻿You are adjusting time and commits to dev while calling it changes. 
> 
> Thank You,
> Andrew Hodel
> 
>>> On Feb 22, 2022, at 10:58 AM, Ian Lance Taylor ***@***.***> wrote:
>>> 
>> ﻿
>> @andrewhodel That is not correct. Why do you think it is?
>> 
>> All changes to the Go repository must be reviewed and approved by somebody else, as described at https://go.dev/doc/contribute#review. That is true for everybody.
>> 
>> What is different for people with approval access is that they only require one reviewer, not two reviewers. See https://go.dev/wiki/GerritAccess.
>> 
>> —
>> Reply to this email directly, view it on GitHub, or unsubscribe.
>> You are receiving this because you were mentioned.
",2022-02-22T01:08:29Z,741705
2917,golang/go,1145514858,1047334590,"I don't see why `EncodeString` should return an error. The other `Encode` functions do not. Base64 encoding and decoding are not symmetric in that encoding should always succeed, whereas decoding might not.

Other than that, seems like a reasonable function to have.  Do you have a situation where you have a `string` source and wish to encode it to a `[]byte` output? Just wondering if this is a function you actually ran into a need for, or if you just were confused about the lack of a symmetric API.

",2022-02-22T01:09:13Z,6889504
2918,golang/go,1145514858,1047334765,"In any case, thanks for defining precisely what you suggest that we add (although you omitted the doc comment).  I gather you are thinking of something along the lines of

```Go
// EncodeString returns the base64 encoding of s.
func (enc *Encoding) EncodeString(s string) ([]byte, error) {
    buf := make([]byte, enc.EncodedLen(len(s)))
    enc.Encode(buf, []byte(src))
    return buf, nil
}
```

I think the argument that you are making is that this is parallel to the existing `DecodeString` method.

How often does a need for this method come up in practice?  Do you have any examples of code that would use it?  Thanks.",2022-02-22T01:09:41Z,3194333
2919,golang/go,1145514858,1047335144,"Start counting them you will find out. It is basic manipulation of words causing the new words to exist. 

If you count the number of times someone commits vs the changes approved in that list you will get it. 



Thank You,
Andrew Hodel

> On Feb 22, 2022, at 11:07 AM, Ian Lance Taylor ***@***.***> wrote:
> 
> ﻿
> You are adjusting time and commits to dev while calling it changes.
> 
> I'm sorry, I don't understand what you mean.
> 
> —
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you were mentioned.
",2022-02-22T01:10:42Z,741705
2920,golang/go,1145514858,1047335456,"Lack of symmetry. 

Yes you need an error, what if the memory bound is reached while reading?

Thank You,
Andrew Hodel

> On Feb 22, 2022, at 11:09 AM, Keith Randall ***@***.***> wrote:
> 
> ﻿
> I don't see why EncodeString should return an error. The other Encode functions do not. Base64 encoding and decoding are not symmetric in that encoding should always succeed, whereas decoding might not.
> 
> Other than that, seems like a reasonable function to have. Do you have a situation where you have a string source and wish to encode it to a []byte output? Just wondering if this is a function you actually ran into a need for, or if you just were confused about the lack of a symmetric API.
> 
> —
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you were mentioned.
",2022-02-22T01:11:29Z,741705
2921,golang/go,1145514858,1047335619,"> I map all that stuff out with https://github.com/andrewhodel/rrd based on word counts and frequency to prove it to people.

OK, great.  Do you want to try to prove it to me?

I mean, I know that I am telling you the truth about our code review process.  I don't understand what you could do to demonstrate that I am not telling the truth.  But I am mildly curious as to where the misunderstanding arises.",2022-02-22T01:11:52Z,3194333
2922,golang/go,1145514858,1047336051,"> Yes you need an error, what if the memory bound is reached while reading?

If a Go program runs out of memory, it will crash.  There is no way for a function to return an error because it has run out of memory.",2022-02-22T01:13:02Z,3194333
2923,golang/go,1145514858,1047336391,"> If you count the number of times someone commits vs the changes approved in that list you will get it.

Again, I'm sorry, but I don't understand what you mean.  If you have actual data, can you simply show it?",2022-02-22T01:13:45Z,3194333
2924,golang/go,1145514858,1047340849,"Again. This is why other libraries have per iteration input functions, for reading streams etc. 

That isn’t to say that you could read the input string length, then read the available memory and know what difference the encoding process will require (bytes per chunk for input and output + anything else) and return an error when that is greater than n*available. 

That is how things work. 



Thank You,
Andrew Hodel

> On Feb 22, 2022, at 11:13 AM, Ian Lance Taylor ***@***.***> wrote:
> 
> ﻿
> If you count the number of times someone commits vs the changes approved in that list you will get it.
> 
> Again, I'm sorry, but I don't understand what you mean. If you have actual data, can you simply show it?
> 
> —
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you were mentioned.
",2022-02-22T01:23:29Z,741705
2925,golang/go,1145514858,1047341380,"It is because you aren’t respecting the development branch as a place to read changes before versioned approval. 

In other words, why do you have release candidates?

Thank You,
Andrew Hodel

> On Feb 22, 2022, at 11:12 AM, Ian Lance Taylor ***@***.***> wrote:
> 
> ﻿
> I map all that stuff out with https://github.com/andrewhodel/rrd based on word counts and frequency to prove it to people.
> 
> OK, great. Do you want to try to prove it to me?
> 
> I mean, I know that I am telling you the truth about our code review process. I don't understand what you could do to demonstrate that I am not telling the truth. But I am mildly curious as to where the misunderstanding arises.
> 
> —
> Reply to this email directly, view it on GitHub, or unsubscribe.
> You are receiving this because you were mentioned.
",2022-02-22T01:24:38Z,741705
2926,golang/go,1145514858,1047343675,"These truths of memory testing (especially when it is very low resource usage to do so) are similar to network programming in a day and age that testing is sold for entertainment as blocking certain tcp packets against protocol. 

In other words it has to be dealt with anytime network traffic is greater than the packet MTU so may as well exist at the memory level. 

Thank You,
Andrew Hodel

> On Feb 22, 2022, at 11:23 AM, Andrew Hodel ***@***.***> wrote:
> 
> ﻿Again. This is why other libraries have per iteration input functions, for reading streams etc. 
> 
> That isn’t to say that you could read the input string length, then read the available memory and know what difference the encoding process will require (bytes per chunk for input and output + anything else) and return an error when that is greater than n*available. 
> 
> That is how things work. 
> 
> 
> 
> Thank You,
> Andrew Hodel
> 
>>> On Feb 22, 2022, at 11:13 AM, Ian Lance Taylor ***@***.***> wrote:
>>> 
>> ﻿
>> If you count the number of times someone commits vs the changes approved in that list you will get it.
>> 
>> Again, I'm sorry, but I don't understand what you mean. If you have actual data, can you simply show it?
>> 
>> —
>> Reply to this email directly, view it on GitHub, or unsubscribe.
>> You are receiving this because you were mentioned.
",2022-02-22T01:30:25Z,741705
2927,golang/go,1145514858,1049108570,"The current helper methods assume that base64 is applied to binary data, which is almost always a []byte.
And it is applied to create text data, which is often a string.
So the API uses []byte for the binary (unencoded) data, and string for the text (encoded) data.
This has worked very well in practice. 
There is no obvious reason we should add the other pairs, which would at the very least confuse me when reading the API.


",2022-02-23T18:58:39Z,104030
2928,golang/go,1145514858,1049160929,"
Based on the discussion above, this proposal seems like a **[likely decline](https://golang.org/s/proposal-status#likely-decline)**.
— rsc for the proposal review group
",2022-02-23T19:59:46Z,104030
2929,golang/go,1145514858,1049290738,"I guess every middleman your Go servers work with support perfect utf8.

I need to send data to things so old that anything beyond 127 bits is out of the question so they can pass it on and then without base64 what are you going to do?

I know, another go type modification is just what the doctor called for!",2022-02-23T22:48:59Z,741705
2930,golang/go,1145514858,1049303876,"@rsc it's simple, you have a string and a validation function that ensures that string is ""at protocol"" meaning hasn't the distant device doesn't even fully support ASCII, then you need to pass it through that distant device so you base64 encode the validated string and send it.

What you are saying isn't real, there's no where on earth where an operating system requires code to not using bits and those work with Go servers that use strings for simplicity.",2022-02-23T23:07:28Z,741705
2931,golang/go,1145514858,1049689253,@andrewhodel I think it would help a lot if you made an example on https://go.dev/play/ to show your use-case and why this isn't trivially covered by the existing API.,2022-02-24T10:06:31Z,5663952
2932,golang/go,1145514858,1049694059,"```
// working with string
var s = ""asdfStringWithoutUnicodeMoreThanOneTwentySeven"";
function_without_string_support([]byte(s));
```

Code reviewer: ""That is messy code, there are too many type modifications"".

I guess they will never figure it out, oh well *poof*.
",2022-02-24T10:11:24Z,741705
2933,golang/go,1145514858,1057426889,"
No change in consensus, so **[declined](https://golang.org/s/proposal-status#declined)**.
— rsc for the proposal review group
",2022-03-02T21:50:38Z,104030
2934,golang/go,1144972959,1144972959,"https://github.com/golang/go/issues/51274

You are supposed to be able to type-assert or cast a `net.Conn` to a `net.TCPConn`.

The go team member created a situation where he lied about that, then said there was no issue.",2022-02-20T09:58:45Z,741705
2935,golang/go,1144972959,1046203024,"As a continuation, here is that truth:

```
# command-line-arguments
./ispapp-go-client.go:236:20: uc.(*net.TCPConn).NetConn undefined (type *net.TCPConn has no field or method NetConn)
Andrews-MacBook-Pro:ispapp-go-client zip$ git diff
diff --git a/ispapp-go-client.go b/ispapp-go-client.go
index 2a188b3..216e993 100644
--- a/ispapp-go-client.go
+++ b/ispapp-go-client.go
@@ -233,7 +233,7 @@ func new_websocket(host *Host) {
 
        fmt.Println(reflect.TypeOf(uc))
        fmt.Printf(""%+v\n"", uc)
-       //uc.conn.SetKeepAlive(true)
+       uc.(*net.TCPConn).NetConn().SetKeepAlive(true)
 
        // set host.WanIfName
        var ipaddrstr, port, iperr = net.SplitHostPort(c.LocalAddr().String())
```",2022-02-20T10:03:55Z,741705
2936,golang/go,1144972959,1046203074,The program is still not working as documented.,2022-02-20T10:04:08Z,741705
2937,golang/go,1144972959,1046203240,"Here is proof `net.TCPConn` has a SetKeepAlive method.

https://cs.opensource.google/go/go/+/refs/tags/go1.17.7:src/net/tcpsock.go;l=159",2022-02-20T10:04:52Z,741705
2938,golang/go,1144972959,1046203503,"The program is still not working as documented.

This is a bug in Go.  Fix it or leave it open as the truth.",2022-02-20T10:06:03Z,741705
2939,golang/go,1144972959,1046203998,"This is how it actually works.

```
uc.(*net.TCPConn).SetKeepAlive(true)
```",2022-02-20T10:08:44Z,741705
2940,golang/go,1144972959,1046204181,"I need you to allow me to resolve the issue you incorrectly closed and locked me out of.

People need to know how to do this, every recent Apple laptop has power darkmode and it works with keep alive flagged tcp packets.",2022-02-20T10:09:50Z,741705
2941,golang/go,1144972959,1046204868,"Glad you were able to solve the issue with net.TCPConn assertion. For the record, I suggested a generic `tls.Conn` assertion because in the first post you were asking about `tls.Conn.NetConn( )`. Of course it's up to you to decide if that makes sense in your code.",2022-02-20T10:13:38Z,3586803
2942,hashicorp/terraform,900635226,900635226,"After upgrading to 0.15.4 terraform reports changes that are ignored. It is exactly like commented here: https://github.com/hashicorp/terraform/issues/28776#issuecomment-846547594

### Terraform Version

```
Terraform v0.15.4
on darwin_amd64
+ provider registry.terraform.io/hashicorp/aws v3.42.0
+ provider registry.terraform.io/hashicorp/template v2.2.0
```



### Terraform Configuration Files
<!--
Paste the relevant parts of your Terraform configuration between the ``` marks below.

For Terraform configs larger than a few resources, or that involve multiple files, please make a GitHub repository that we can clone, rather than copy-pasting multiple files in here. For security, you can also encrypt the files using our GPG public key at https://www.hashicorp.com/security.
-->

```terraform

resource ""aws_batch_compute_environment"" ""batch_compute"" {
  lifecycle {
    ignore_changes = [compute_resources[0].desired_vcpus]
  }

...

  compute_resources {
...
  }
}

resource ""aws_db_instance"" ""postgres_db"" {
  ...

  lifecycle {
    prevent_destroy = true
    ignore_changes = [latest_restorable_time]
  }
}
```

### Output

```
Note: Objects have changed outside of Terraform

Terraform detected the following changes made outside of Terraform since the last ""terraform apply"":

  # module.db.aws_db_instance.postgres_db has been changed
  ~ resource ""aws_db_instance"" ""postgres_db"" {
        id                                    = ""db""
      ~ latest_restorable_time                = ""2021-05-25T10:24:14Z"" -> ""2021-05-25T10:29:14Z""
        name                                  = ""db""
        tags                                  = {
            ""Name"" = ""DatabaseServer""
        }
        # (47 unchanged attributes hidden)

        # (1 unchanged block hidden)
    }
  # module.batch_processor_dot_backend.aws_batch_compute_environment.batch_compute has been changed
  ~ resource ""aws_batch_compute_environment"" ""batch_compute"" {
        id                       = ""batch-compute""
        tags                     = {}
        # (9 unchanged attributes hidden)

      ~ compute_resources {
          ~ desired_vcpus      = 0 -> 2
            tags               = {}
            # (9 unchanged attributes hidden)
        }
    }
```

### Expected Behavior

No changes should be reported because they are listed in ignored changes.

### Actual Behavior

Changes are reported.

### Steps to Reproduce

Change any resource outside of terraform and see that `terraform apply` reports changed even when they should be ignored.


### Additional Context


### References

- https://github.com/hashicorp/terraform/issues/28776
- https://github.com/hashicorp/terraform/issues/28776#issuecomment-846547594
- https://github.com/hashicorp/terraform/pull/28634#issuecomment-845934989
",2021-05-25T10:52:11Z,829734
2943,hashicorp/terraform,900635226,848049967,"Hi @petkaantonov! Thanks for opening this feature request.

The current behavior is reflecting the long-standing behavior that Terraform does still detect and incorporate remote objects into the state, but then while producing a plan it ignores differences _between the configuration and the state_. The difference reported in your example is a difference between the prior state and the current remote object, and `ignore_changes` has never affected that situation but that fact was less visible before because Terraform just silently updated the state rather than reporting it.

While it might seem immaterial whether Terraform updates the state or not here, it _can_ result in a change in behavior of your configuration if any other expressions in the module refer to that value. To be specific, if you had any reference to `aws_batch_compute_environment.batch_compute.compute_resources[0].desired_vcpus` elsewhere in your module then they would return `2` rather than `0` after detecting this change, and so that is what Terraform is reporting here, in case that ends up being useful context for understanding which actions Terraform proposes (or doesn't propose) in the plan.

I assume that in your case this doesn't really matter much, because you _don't_ have any references to `aws_batch_compute_environment.batch_compute.compute_resources[0].desired_vcpus` elsewhere in your module and so you don't actually care what's reported in the state. One potential improvement we could consider for this mechanism is for Terraform to try to only report changes to resources that have other expressions referring to them, since changes to a value that nothing refers to can't possibly affect the behavior of the configuration.

However, such a rule is easier to say than to implement because what we've done here is just expose in the UI some long-standing Terraform behavior that was previously invisible, and so changing that behavior at this late state will likely require a lot of research to make sure that the changes don't break use-cases we're not currently aware of. The current output is an honest and correct account of Terraform's behavior, and so I think we need to consider here whether the right thing to do is change Terraform's behavior (which, for something this fundamental, would be challenging to do at this late stage in Terraform's life) or to change the UI to fudge the details a little so that it leaves hidden some details that surface inconvenient truths that don't actually affect configuration behavior.

We won't be able to make any significant changes in this regard in the near future, because the scope of this project was just to be more explicit about what Terraform was already doing rather than to change how Terraform behaves, but we'll use this issue to represent the need and consider what we might change in future releases.

Thanks again!
",2021-05-25T16:58:40Z,20180
2944,hashicorp/terraform,900635226,848055598,"I see. In that case, would it be possible to add a separate way to ignore these? e.g. `ignore_drifts` or something like that? Wouldn't that be easy to implement?",2021-05-25T17:07:11Z,829734
2945,hashicorp/terraform,900635226,848060128,"The refresh operation doesn't currently really consider the resource configuration at all -- it's job is to synchronize the existing state with remote objects -- so making it react to anything new in the resource configuration is not a trivial design decision.

We also know from our experiences with `ignore_changes` that this idea of partially ignoring changes to parts of objects is generally not as simple as it first appears: there are lots of resource types where two or more attribute values are connected to each other in some way, and so ignoring one without ignoring the other can make the result inconsistent. For current `ignore_changes` that sort of inconsistency isn't a massive problem aside from being a bit confusing, because during planning we're just proposing some changes and not actually changing anything, but doing that during _refresh_ might well lead to a situation where the object in the state is no longer a valid input to the provider that's managing it, which would likely lead to errors or crashes where a provider has to deal with broken input in a location where it never did before.
",2021-05-25T17:13:48Z,20180
2946,hashicorp/terraform,900635226,848064400,"Could the refresh operation be made configurable in some other way? like an .ignorefile There is probably going to be 3rd party solution if it's not added anyway. 

Just some way to ignore these is important, otherwise the output is not as useful because it becomes just manually ignored noise and people become conditioned to ignore it regardless of what it says.",2021-05-25T17:20:38Z,829734
2947,hashicorp/terraform,900635226,848073659,"I don't currently have any ready-to-implement designs to address this feedback. We'll need to do some more research and design work in order to address it. Until then, indeed if your system routinely makes changes to objects outside of Terraform as part of its regular function then the feature in its current state will not be so useful for you,

Hopefully it can still answer the sorts of questions it is aiming to solve, though: if Terraform proposes making a change that doesn't seem justified by a corresponding change in configuration, you can refer to the note about detected changes to see if any of the changes it detected are the explanation. If the actions Terraform is proposing match what you were expecting anyway, then the changes detected are just some extra information that you can safely ignore.
",2021-05-25T17:31:46Z,20180
2948,hashicorp/terraform,900635226,848687137,"In situations where the user is forced to add an ignore - to stop Terraform from chasing it's own tail - ie with the use of an aws_autoscaling_attachment (where it is documented that you must use an ignore in corresponding autoscaling groups https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/autoscaling_attachment), I think this behaviour constitutes a bug. The drift output states that ""Objects have changed outside of Terraform"" when in reality the changes are made within terraform and have had to be ignored for reasons that might be justified as a bug in their own right.

Choosing to output drift before taking into account ignores thus creates potentially significant noise when using Terrafom within it's documented bounds.",2021-05-26T11:21:09Z,13241695
2949,hashicorp/terraform,900635226,848843241,"I can relate to this issue a lot, our daily work now has a new pain: filtering out the noise of ignored changes. 
""muting"" the output would help A LOT! :)

anyway, thank you guys for the good work, keep up with it! 👍 ",2021-05-26T14:59:52Z,33528
2950,hashicorp/terraform,900635226,849167450,"This feels like a pretty invasive change. I second what @koalalorenzo said. 

The main problem with this change is that we lean on our large team of engineers to inspect their own changes carefully on a per-team basis and apply those changes without constant infra approval. Even when diffs are small, though, we can sometimes misunderstand the plan which can lead to some unfortunate consequences. Not everyone understands the inner workings of terraform, or even the best practices, so there is always going to be mistakes. This isn't terraform's fault, we all need to learn the tool a little bit better.

Now, though, we have to sift through an entire block of output and tell our team to just ignore it up to a point. Its just going to lead to less clear plan output that people put less effort into reviewing. I think its going to erode trust, too, even though I suspect the opposite is the goal. 

For instance, take a `google_container_node_pool` resource. If its auto-scaling, every single time you apply the project you get a new `node_count` attribute diff that is wholly un-actionable. If somebody sees that every single time, but I tell them to ignore it because it should absolutely be ignored, whose to say when they get a `-/+` change on that resources (maybe by accident or lack of understanding) they aren't going to assume its also ignorable.

TBH, I have ignored many attributes in terraform because we may change them outside of terraform and have come to rely on the fact that it really doesn't and shouldn't care about the changes to attributes you don't specify. IDK. Ranting a bit.

I have to be totally honest I see no point in opting everyone into this for every resource automatically. I don't understand the problem that its solving and unless there is a way for it to be completely filtered out I would love to see this change reverted in the next release.

(I also don't think the solution is `ignore_attributes`.)",2021-05-26T22:40:55Z,74099
2951,hashicorp/terraform,900635226,849552894,"I am very surprised about this change and it adds a ton of noise to the already not exactly clear and concise plans. The extra verbosity adds a lot of cognitive load which risks at best to make people glance over the plans and at worst make them ignore them completely.

I really, really don't understand the reasoning here.

Again, other than that, thank you for a great product over the years!",2021-05-27T11:24:03Z,295393
2952,hashicorp/terraform,900635226,849586131,"I can get a little worked up and forget that a lot of people put in a lot of work to get terraform where it is today. I don't want to belittle that. People like @apparentlymart (who helped us in the forums when we were starting to write our own internal terraform provider plugins) are awesome. Terraform is awesome. Obviously my job is infinitely easier because of it. ❤️ 

As for this feature, it was in the CHANGELOG and I did glance over it, so its on me that I upgraded to `0.15.4`. I just wanted to register my feeedback that I really wish this was separate from the plan, or that it was opt-in when running plans/applies, or that it was even opt in at a module or resource level (a `lifecycle { show_drift = true }` attribute?).

",2021-05-27T12:21:33Z,74099
2953,hashicorp/terraform,900635226,852022006,"Just adding another resource type that now always appears in my terraform output - aws_efs_file_system. 

(from https://github.com/hashicorp/terraform/issues/28845 which I will mark as a duplicate). EFS file systems are *supposed* to grow and change, as many other types of resource they are explicitly designed that way.

With no way to ignore, the plan/apply output is now so noisy I've gone back to terraform 0.15.3 as wading through tens of modified resources on every single plan/apply just to catch the one that might be important is not a workable pattern for me. ",2021-06-01T10:42:15Z,84926408
2954,hashicorp/terraform,900635226,853473962,"Since terraform 0.15.4, I get confused messages from my team every single day. I can't imagine the impact this change had globally.

Please add an option to disable displaying those by default and go back to the previous behavior, with maybe an explicit way to get more details about the changes only when needed (e.g a debug mode).

This is really confusing right now, and someone needs to understand the internals of terraform to appreciate the difference between a drift and remote changes.",2021-06-03T00:42:03Z,122286
2955,hashicorp/terraform,900635226,853524493,"One thing that surprised me a lot about this change is that the `Note: Objects have changed outside of Terraform` still shows up if you do `terraform plan -input=false -output=tfplan >/dev/null && terraform show tfplan`. I had hoped that this note would be output as part of the `Refreshing state ...` messages and as such would be hidden, but it turns out that the note is part of the saved plan file. If this was changed so that the note is output with the `Refreshing state ...` messages instead of being saved inside of the plan, that would make a big difference for the ability of users to silence that note and anyone who was already silencing the refreshing state messages would automatically get this new note silenced as well.

As near as I can tell, the only way to currently silence the warning is to run `terraform refresh >/dev/null && terraform plan -input=false -refresh=false`, which has several downsides that @jbardin and @apparentlymart pointed out in https://github.com/hashicorp/terraform/issues/27214.",2021-06-03T03:00:50Z,10989362
2956,hashicorp/terraform,900635226,854436034,"> This is really confusing right now, and someone needs to understand the internals of terraform to appreciate the difference between a drift and remote changes.

Agreed, this bit us earlier. Another example is that AWS autoscaling groups will always have ""external"" changes to attributes such as `desired_count` from normal scaling behaviour so the refresh-only state plan will show drift. I was going to report this to the AWS provider, but it sounds like they might not yet have a way to fix this. If there are underlying architecture changes that need to be carefully considered, I would certainly prefer to suppress these drift notifications until they can be made more useful.

As the below is an ignored attribute, the resource is not affected by Terraform and the message amounts to noise:
Example:
```sh
  # aws_autoscaling_group.example-scaling-group has been changed
  ~ resource ""aws_autoscaling_group"" ""example-scaling-group"" {
      ~ desired_capacity          = 50 -> 36
        id                        = ""example-scaling-group""
        name                      = ""example-scaling-group""
        # (20 unchanged attributes hidden)
```

A person reviewing this plan has to already know the attribute is ignored on that resource or read ""below the line"" in order to confirm the resource is not affected. Another way of looking at this is as a formatting issue, since the change notification looks _very similar_ to the actual plan, and it took me and the rest of my team a while to realise it's a separate section.",2021-06-04T07:24:39Z,30171259
2957,hashicorp/terraform,900635226,856151214,"Another example - [`okta_app_oauth`](https://registry.terraform.io/providers/oktadeveloper/okta/latest/docs/resources/app_oauth) supports `users` which can also be configured by [a separate resource](https://registry.terraform.io/providers/oktadeveloper/okta/latest/docs/resources/app_user). If you use the separate resource or some out-of-band way to manage the users, you will always see a change here.",2021-06-07T18:08:35Z,5640
2958,hashicorp/terraform,900635226,857761934,"I think it is fine if Terraform now reports drifts explicitly but it will be great if we could have a way to hide it because we might not be interested in changes happening outside of Terraform in certain use cases.

One way we can implement this is to add a `--ignore-drifts` flag to the relevant `terraform xxx` subcommands. This is the approach I'm most in favour with because it allows this to be decided at runtime and to be varied flexibly across different use cases (i.e. running locally vs running in CI/CD platforms, etc.). It also provides users a simple way to revert to the old representation of the output.

```console
$ terraform plan --ignore-drifts
$ terraform apply --ignore-drifts
```

In addition, we could also implement this at a resource/module level using the `ignore_drifts` field. This may be helpful for some use cases where we might be interested drifts in general but not all fields.

```terraform
resource ""aws_instance"" ""example"" {
  # ...

  lifecycle {
    ignore_drifts = [
      etag, # Ignore drifts for specific attributes
    ]
  }
}
```

```terraform
resource ""aws_instance"" ""example"" {
  # ...

  lifecycle {
    ignore_drifts = all # Ignore drifts for all attributes
  }
}
```",2021-06-09T14:49:57Z,3350651
2959,hashicorp/terraform,900635226,859701978,"+1 to this issue.

Part of our workflow involves creating EMR clusters in AWS, including running a few steps to modify some software configurations. After the cluster is created, our data scientists run hundreds more steps throughout the lifetime of the cluster. Now _every single step_ in _every_ cluster shows up in _every_ Terraform plan, despite the lifecycle ignore.

We use Atlantis to execute our Terraform configurations, but cannot do so now thanks to this. The plans produced are too long to appear in Gitlab comments, and as such we cannot see any of the plans produced by Atlantis.",2021-06-11T16:32:08Z,37960993
2960,hashicorp/terraform,900635226,861022522,"To add to the discussion here, I'm also seeing that (for example) the Principals list in a role always shows up as ""has been changed outside of Terraform"", even if I attempt to manually update the principals list via the AWS web console.

```      ~ assume_role_policy    = jsonencode(
          ~ {
              ~ Statement = [
                  ~ {
                      ~ Principal = {
                          ~ Service = [
                              + ""lambda.amazonaws.com"",
                              + ""ec2.amazonaws.com"",
                                ""states.us-east-1.amazonaws.com"",
                                # (1 unchanged element hidden)
                                ""cloudwatch.amazonaws.com"",
                              - ""ec2.amazonaws.com"",
                              - ""lambda.amazonaws.com"",
                            ]
                        }
                        # (3 unchanged elements hidden)
                    },
                ]
                # (1 unchanged element hidden)
            }
        )
```

Each time I edit it via the web console to reflect what Terraform thinks it ought to be, AWS saves the list in a random order (not alphabetical), so Terraform always thinks that there's been a change. Even when I force an apply, or taint and import the resource, this warning persists.

Outside of this warning, the plan results in the message ""No changes. Your infrastructure matches the configuration."".

This, to me, doesn't make sense because if Terraform thinks that there is drift, an apply should be able to rectify that drift. This does not appear to be the case, which makes the whole drift warning kind of pointless and noisy.",2021-06-14T21:58:32Z,4886201
2961,hashicorp/terraform,900635226,861605005,"Hi guys, as everyone else, this change is quite impacting for us.
On azure, it appears that every apply resulting in changes in other resources will be seen as a drift.
So far, I encountered to cases:
- creating a new subnet using the azurerm_subnet resource:
The plan/apply goes well, but on the next plan, even if there are no changes, Terraform informs me that my VNet has changed, because it wants to reflect the newly added subnet in the VNet attributes. But this change was actually made by Terraform, in the previous run.
- creating a virtual machine, as the MAC Address and VM id are known after deployment, Terraform detects changes on the NIC on the next plan, because now, the NIC has indeed a MAC and a VM id.

```hcl
Note: Objects have changed outside of Terraform

Terraform detected the following changes made outside of Terraform since the last ""terraform apply"":

  # module.Azadds_mgmt_VM.azurerm_network_interface.TerraVM-nic0 has been changed
  ~ resource ""azurerm_network_interface"" ""TerraVM-nic0"" {
        id                            = ""/subscriptions/<subscription_id>/resourceGroups/myRG/providers/Microsoft.Network/networkInterfaces/myvm-nic0""
      + mac_address                   = ""00-xx-xx-xx-xx-xx""
        name                          = ""myvm-nic0""
      + tags                          = {}
      + virtual_machine_id            = ""/subscriptions/<subscription_id>/resourceGroups/myRG/providers/Microsoft.Compute/virtualMachines/myvm""
        # (9 unchanged attributes hidden)

        # (1 unchanged block hidden)
    }

Unless you have made equivalent changes to your configuration, or ignored the relevant attributes using ignore_changes, the following plan may include actions to undo or respond to these changes.
```

I think we can all agree the Terraform team is doing a tremendous job at bringing the product where it is today, but this particular change although attempting to bring more visibility, brings, in my opinion, a lot of confusion.

I think I will roll back to 0.15.3 until some improvements are made about this feature.",2021-06-15T15:36:04Z,61788760
2962,hashicorp/terraform,900635226,863062454,"hey @kpkrishnamoorthy 

I have submitted an issue about this in the provider tracker https://github.com/hashicorp/terraform-provider-aws/issues/19727

But it would be definitely cool if core terraform allowed for an opt-in option to hide the output of this new feature until provider devs fix it on their side",2021-06-17T08:57:49Z,6890249
2963,hashicorp/terraform,900635226,866413695,"> I think we can all agree the Terraform team is doing a tremendous job at bringing the product where it is today, but this particular change although attempting to bring more visibility, brings, in my opinion, a lot of confusion.

Just want to second this.",2021-06-22T23:58:28Z,10353074
2964,hashicorp/terraform,900635226,866778694,"Do we have anybody working on it? If so, do we have an ETA for this? It has been an annoying issue that confuses a lot of people. Our Atlantis comments on PRs are way too big for no reason, and it is super frustrating! 

🙏 Please let us know how to help somehow, and pressure to get this shipped in the next minor/patch release!!! 🙏",2021-06-23T12:05:51Z,33528
2965,hashicorp/terraform,900635226,866800152,"We're affected by this too, and it seems clearly like a bug. We've got things in our config where we've told Terraform to ignore changes to those things; now it ignores the changes in that it won't try to control those things, but it no longer ignores them when reporting on them.

If you've told Terraform to ignore changes to something, it should ignore those changes silently.

If that isn't universally agreeable, perhaps a general option like `ignore_changes_silently` or something?",2021-06-23T12:39:33Z,10979901
2966,hashicorp/terraform,900635226,866839205,"I actually propose to make this entire drift detection feature OPT IN instead of opt out for the time being. As this thread shows, its causing lots of issues and confusion for many people, with different kinds of usecases.

I DO see the benefit of the feature in general, but putting this change into a release sure backfired. I'd wager if they just remove it in 1.0.1 not many would actually miss it.",2021-06-23T13:32:03Z,39595706
2967,hashicorp/terraform,900635226,868314047,"@fbreckle it seems you are making assumptions about how Terraform is used everywhere, and what the community as a whole thinks about this feature. 

I myself find this feature useful, I sure want to know when my Infra has changed under the hood and what exactly changed outside of Terraform. We depend heavily on Terraform and this new feature gives us visibility and increases the trust we have in this tool.

It has some downsides (like for us we are using Kubernetes with node autoscaler and node auto upgrade so TF reports that the node count and the node version has changed quite often) but I am pretty sure that with the required changes in the providers/resources, we can find an elegant solution without reverting the whole thing. 

Thanks Terraform for being opinionated ! ",2021-06-25T08:17:30Z,221981
2968,hashicorp/terraform,900635226,868316869,"I think it's clear that opinions are split, and while a part of the community is happy about this change, the other part doesn't have a way to opt out.

For us it's a blocker and prevents us from upgrading to 0.15 (1.0).",2021-06-25T08:22:05Z,1209413
2969,hashicorp/terraform,900635226,868488025,"@JordanP : We definitely want to know about drift in the resources that we've told Terraform to manage. What we don't want to know about is drift in resources that we've told Terraform to ignore.

Do you -- does anyone -- have a use case for ""our config says that Terraform should ignore this resource, but we want to hear about changes to it anyway""?

There was this mentioned early on:

> While it might seem immaterial whether Terraform updates the state or not here, it can result in a change in behavior of your configuration if any other expressions in the module refer to that value. To be specific, if you had any reference to aws_batch_compute_environment.batch_compute.compute_resources[0].desired_vcpus elsewhere in your module then they would return 2 rather than 0 after detecting this change, and so that is what Terraform is reporting here, in case that ends up being useful context for understanding which actions Terraform proposes (or doesn't propose) in the plan.

(https://github.com/hashicorp/terraform/issues/28803#issuecomment-848049967)

That seems like a weird situation, where you'd have references to values that you've told Terraform to ignore the value of.",2021-06-25T13:09:10Z,10979901
2970,hashicorp/terraform,900635226,868495310,"@jbscare 

>That seems like a weird situation, where you'd have references to values that you've told Terraform to ignore the value of.

This should be processed as a plan-time explicit warning, in my opinion.

The only reason I can think of, when someone would need to explicitly use a reference to a resource attribute that its changes are being ignored explicitly, is at initial resources creation.

But even that is the reason, then it wouldn't make sense for terraform to continuously notify users about ""changes being made outside of terraform"" in this new section. A warning message would be adequate and less prone to misinterpretation.",2021-06-25T13:20:59Z,6890249
2971,hashicorp/terraform,900635226,868503817,"Quick chime in: We use TF to set up and manage our AWS accounts and detect any drift. Our configs and accounts have worked for years without any major changes. Now we are suddenly getting warnings without any details (just empty) and I still have no idea why.

I read a bit, but still don't really understand what is happening. Will have to spend more time, but ideally things would be a bit more obvious (why are we getting a warning, what are our next steps, etc)

Love the tool btw and this is not a rant, just an FYI 😉",2021-06-25T13:34:19Z,1539747
2972,hashicorp/terraform,900635226,868662997,"> I am pretty sure that with the required changes in the providers/resources, we can find an elegant solution without reverting the whole thing.

I don't think this is true @JordanP. The way this is implemented, there is now way for providers to work around this.",2021-06-25T15:54:28Z,5640
2973,hashicorp/terraform,900635226,868699345,"@ryanking 

hmm this is quite the antithesis of what @jbardin has claimed recently: https://github.com/hashicorp/terraform/issues/28911#issuecomment-857647716

If there is really no way for providers to work with this tf-core feature in mind, then we should definitely have a disable flag asap 
",2021-06-25T16:53:07Z,6890249
2974,hashicorp/terraform,900635226,868757008,"> @ryanking
> 
> hmm this is quite the antithesis of what @jbardin has claimed recently: [#28911 (comment)](https://github.com/hashicorp/terraform/issues/28911#issuecomment-857647716)
> 
> If there is really no way for providers to work with this tf-core feature in mind, then we should definitely have a disable flag asap

that comment is mostly about how list diffs are handled. if provider mess up the ordering, then yeah it's on the provider.
but this issue is about not showing ignored changes in drift detection, which is mostly an orthogonal problem, and providers have no control over this.",2021-06-25T18:32:16Z,8552945
2975,hashicorp/terraform,900635226,873351558,"This falls under the ""too much information"" category to me.  This is debug output at best.  Set the TF_LOG=DEBUG and then you can get this output.  Here is a good example of why it doesn't work for me:
```# module.ec2_chat.aws_security_group.chat has been changed
  ~ resource ""aws_security_group"" ""chat"" {
        id                     = ""sg-0f46dd9f058d12345""
        name                   = ""prod-chat-8""
        tags                   = {
            ""Environment"" = ""prod""
            ""Name""        = ""prod-chat-8""
            ""Terraform""   = ""yes""
        }
      + tags_all               = {
          + ""Environment"" = ""prod""
          + ""Name""        = ""prod-chat-8""
          + ""Terraform""   = ""yes""
        }
        # (7 unchanged attributes hidden)

        # (1 unchanged block hidden)
    }
```
The tags_all is an ""attribute"".  It is not an ""argument"".  It is computed.  I won't be setting it - ever.  Even if I apply this if I  add/change/delete a tag it will also change the tags_all attribute and I will have to apply it again to get rid of this message.  And there are many, many more of these examples.",2021-07-03T05:51:02Z,532240
2976,hashicorp/terraform,900635226,874146912,"I have some similar error:

```
panic: Error reading level state: strconv.ParseInt: parsing ""84850493440"": value out of range
```

This is the attribute output by terrafom of efs system. There're some wy to evict this error without delete efs system from tfstate?

Thanks",2021-07-05T14:14:35Z,35425181
2977,hashicorp/terraform,900635226,874934952,"I also just ran into this upgrading from `0.14.11`. My feedback would be NOT a command line flag I have to provide to ignore changes outside of Terraform but a new attribute that behaves like `lifecycle { ignore_changes = [] }`.

```
Note: Objects have changed outside of Terraform

Terraform detected the following changes made outside of Terraform since the last ""terraform apply"":

  # module.fusionauth.aws_db_instance.rds has been changed
  ~ resource ""aws_db_instance"" ""rds"" {
        id                                    = ""fusionauth""
      ~ latest_restorable_time                = ""2021-07-06T16:42:02Z"" -> ""2021-07-06T16:57:02Z""
        name                                  = ""fusionauth""
        tags                                  = {
            ""Env""  = ""stage""
            ""Name"" = ""fusionauth""
        }
        # (52 unchanged attributes hidden)
    }
  # module.postgres.aws_db_instance.rds has been changed
  ~ resource ""aws_db_instance"" ""rds"" {
        id                                    = ""postgres""
      ~ latest_restorable_time                = ""2021-07-06T16:39:08Z"" -> ""2021-07-06T16:59:09Z""
        name                                  = ""postgres""
        tags                                  = {
            ""Env""  = ""stage""
            ""Name"" = ""postgres""
        }
        # (52 unchanged attributes hidden)
    }

Unless you have made equivalent changes to your configuration, or ignored the relevant attributes using ignore_changes, the following plan may include actions
to undo or respond to these changes.
```",2021-07-06T17:08:27Z,523312
2978,hashicorp/terraform,900635226,876042632,"We need a way to output only the actual changes Terraform intends to perform. Currently 100% of the output from ""Objects have changed outside of Terraform"" is noise for us (unavoidable and intended changes, or ignored fields).

Does anyone know how to output only Terraforms planned changes?

I plan on testing running a `terraform apply -refresh-only -auto-approve` before each plan to reduce noise, however changes can still happen between.

Edit: don't us the command above not useable with certain changes: https://github.com/hashicorp/terraform/issues/28939",2021-07-08T01:17:19Z,7508115
2979,hashicorp/terraform,900635226,876308270,"> Does anyone know how to output only Terraforms planned changes?

No, there is no way, hence this GitHub issue. A flag or an env var would be welcome.",2021-07-08T10:04:11Z,122286
2980,hashicorp/terraform,900635226,882235013,"Update: we have tested running `terraform apply -refresh-only -auto-approve` before each plan, it works for us.

We were currently scrolling through ~390 lines of detected change output that weren't actual differences down to 0.


Update 2: 
The above command is a hack which has the potential to cause issues. See Indigenuity's comment below.

Only run the above if you have the ability to recover your statefile from any issues. Only run if you have state file versioning.

Update 3: don't us the command above not useable with certain changes: https://github.com/hashicorp/terraform/issues/28939",2021-07-19T04:52:05Z,7508115
2981,hashicorp/terraform,900635226,882666815,"It strikes me as a step backwards that we now have to run an `-auto-approve` job just so the output of `plan` is intelligible. @robomon1 hit the nail on the head: it may not be debug output by strictest definition but it's certainly debug output in utility.

While there have been past cases where I would have loved this view into the internals of `terraform refresh` I don't need it 99% of the time and most of our users don't need it _ever_. Especially when some changes are so laughably trivial:
```
  ~ resource ""google_bigquery_table"" ""table"" {
        id                  = <sensitive>
      ~ last_modified_time  = 1625689042601 -> 1625696261956
      ~ num_bytes           = 3889196 -> 3943304
      ~ num_rows            = 14390 -> 14590
      # (13 unchanged attributes hidden)
```

So, like some others, our solution has been to downgrade to `15.3`. I echo those others as well when I say that Terraform has changed the way I work. Thanks for your tireless work on it.",2021-07-19T16:01:32Z,83093282
2982,hashicorp/terraform,900635226,883190002,"Is anybody working on this? 😅 

EDIT: just noticed that this got a ""enhancement"" tag, while IMHO from a UX point of view this is a MAJOR f*ck up and not an enhancement! 😓 😢 We are still suffering from it and it is getting super frustrating. I wish somebody would consider this for the next release or update us on the status. What can we do?",2021-07-20T08:07:26Z,33528
2983,hashicorp/terraform,900635226,883722503,"Also, looking for a fix to hide this related to RDS instances and `~ latest_restorable_time                = ""2021-07-20T21:21:23Z"" -> ""2021-07-20T21:26:23Z""`",2021-07-20T21:32:28Z,523312
2984,hashicorp/terraform,900635226,883951512,"> Update: we have tested running `terraform apply -refresh-only -auto-approve` before each plan, it works for us.
> 
> We were currently scrolling through ~390 lines of detected change output that weren't actual differences down to 0.

It is workaround for some cases. I have some AWS IAM resources, changes still appear even after I ran this command.

I really don't like this in update. They should make it print only when user want it (via flag), let terraform show plan in way people expect. Nice feature but bad implement",2021-07-21T07:15:37Z,10137
2985,hashicorp/terraform,900635226,884376640,"We also use atlantis and our mouse wheel is burning now, due to scrolling down in merge request to get to the interesting terraform part. :smile: We use terraform with VMware and face this issue with a custom attribute, which get updated daily by a backup software.

```
~ custom_attributes = {
          ~ ""999"" = ""Backup Server=..., EndTime=2021-07-20T19:44:00Z""
        }
```

Like others said, it would be really great to have some sort of switch to hide ""ignore_changes-changes"". Nevertheless terraform is such a great piece of software. Thank you!!! :slightly_smiling_face:",2021-07-21T17:52:19Z,30444769
2986,hashicorp/terraform,900635226,884877105,"Hi,

> Update: we have tested running `terraform apply -refresh-only -auto-approve` before each plan, it works for us.
> 
> We were currently scrolling through ~390 lines of detected change output that weren't actual differences down to 0.

If you refer to the terraform [documentation](https://www.terraform.io/docs/cli/commands/refresh.html), you can see that this command is deprecated.

The old output was more readable before introducing this ""feature"". Would have been better if we could opt-in (or not) for the new behaviour.",2021-07-22T12:35:10Z,41241424
2987,hashicorp/terraform,900635226,884959989,"Wait, wait, are people actually using `-auto-approve` as part of their workarounds for this?  Seems rather dangerous! Even the documentation linked by @Tazminia warns explicitly about one risk:

>Automatically applying the effect of a refresh is risky, because if you have misconfigured credentials for one or more providers then the provider may be misled into thinking that all of the managed objects have been deleted, and thus remove all of the tracked objects without any confirmation prompt.

And furthermore, these state updates aren't always clean.  I had an `aws_security_group` resource with 30 `aws_security_group_rule` resources, and this refresh seemed to think that the rules had been changed externally to all be in-line rules. So suddenly terraform is thinking it needs to create 30 more resources again, since all the rules were moved to be in-line.  Using `-auto-approve` in scenarios like this is a great way to blow up your infra!

If this feature is pushing people to risky workarounds, I'd suggest that it's more than just a nuisance and should be treated as a serious bug.  While the output of this feature doesn't suggest `-auto-approve`, it does imply that `-refresh-only` is always harmless, which is untrue.",2021-07-22T14:30:42Z,6120111
2988,hashicorp/terraform,900635226,885191146,"> If you refer to the terraform documentation, you can see that this command is deprecated.

@Tazminia The command `terraform refresh` is deprecated, but they clearly state that it is an alias to `terraform apply -refresh-only -auto-approve`, which they WANT people to know about and understand because they introduced the `-refresh-only` flag in `0.15.4`; these new flags are important to interacting with this new output.

It's definitely been a bit of a transition to `0.15.4+`, and we would also like the option to show/hide this output, but overall so happy for the Terraform team to hit `1.0+`! Thanks for all your hard work!",2021-07-22T19:54:38Z,10137
2989,hashicorp/terraform,900635226,889818360,This is extremely annoying and frustrating for the team.,2021-07-30T11:05:12Z,178866
2990,hashicorp/terraform,900635226,891414434,"> Wait, wait, are people actually using `-auto-approve` as part of their workarounds for this? Seems rather dangerous! Even the documentation linked by @Tazminia warns explicitly about one risk:
> 
> > Automatically applying the effect of a refresh is risky, because if you have misconfigured credentials for one or more providers then the provider may be misled into thinking that all of the managed objects have been deleted, and thus remove all of the tracked objects without any confirmation prompt.
> 
> And furthermore, these state updates aren't always clean. I had an `aws_security_group` resource with 30 `aws_security_group_rule` resources, and this refresh seemed to think that the rules had been changed externally to all be in-line rules. So suddenly terraform is thinking it needs to create 30 more resources again, since all the rules were moved to be in-line. Using `-auto-approve` in scenarios like this is a great way to blow up your infra!
> 
> If this feature is pushing people to risky workarounds, I'd suggest that it's more than just a nuisance and should be treated as a serious bug. While the output of this feature doesn't suggest `-auto-approve`, it does imply that `-refresh-only` is always harmless, which is untrue.

This is true. It is a hack which has the potential to cause issues. I will update my comment.

The only other alternative we can see is getting the team to scroll through and inspect hundreds (soon to be thousands) of lines of expected changes/changes that aren't even changes every single plan run (just for 1 pipeline). A lot of our resources are affected with this issue.

Update: don't us the command above not useable with certain changes: #28939",2021-08-03T00:13:00Z,7508115
2991,hashicorp/terraform,900635226,899142046,"Wouldn't having a flag  on apply to show\hide these _outside changes_ be a relatively simple solution?

Having all this extra output does make it harder to process what would actually change.",2021-08-16T00:52:21Z,10330416
2992,hashicorp/terraform,900635226,899279564,"A few thoughts from the POV of a Terraform Cloud user.

- A flag would be nice along with a corresponding option for the workspace.
- This output could be separated from the normal plan output in the Terraform Cloud UI.
- Taking it one step further, it would be amazing if the planned changes could be put in their own log window. The plan output is generally very noisy and makes it much harder to review the plan than it could be:
	- First it prints the init steps.
	- Then it prints all the resources from the state refresh.
	- Then it prints this new report
	- Then, finally, if there are no errors, it prints **the actual thing we want to look** at and always need to review very carefully: the planned changes.",2021-08-16T07:14:15Z,721372
2993,hashicorp/terraform,900635226,901205047,Just dropping by to let you know that this is issue is also keeping my organization on 0.15.3.,2021-08-18T15:21:29Z,53177
2994,hashicorp/terraform,900635226,901219520,"Btw, I was able to get minimal output for CI by running plan in 2 stages:

```sh
terraform plan -input=false -out=plan
terraform show -no-color plan
```

I used it even before this ""nice"" change to get rid of ""Refreshing state"" messages in output for big state.",2021-08-18T15:39:44Z,75987872
2995,hashicorp/terraform,900635226,901254277,"Issue has been open for three months, is there an intention to fix? Who owns the issue.",2021-08-18T16:26:42Z,40402475
2996,hashicorp/terraform,900635226,901741347,"Simple grep/sed combo until we have `--no-refresh-messages` and `--no-changed-objects` :)

```
terraform plan | grep -v ""Refreshing state..."" | sed '/Objects have changed outside of Terraform/,/────────────/d'
```",2021-08-19T09:04:55Z,250971
2997,hashicorp/terraform,900635226,902070533,"I'll add one more: Amplify app autogenerates a thumbnail for production branch and on refresh returns a different (huge) signed URL _each time_... and, of course, both old and new URLs are printed!

```terraform
  # aws_amplify_app.client_app has been changed
  ~ resource ""aws_amplify_app"" ""client_app"" {
        id                            = ""d23rt6p25fb7vl""
        name                          = ""some-client-app""
      ~ production_branch             = [
          ~ {
              ~ thumbnail_url    = ""https://aws-amplify-prod-us-east-1-artifacts.s3.amazonaws.com/d23rt6n25fb7vl/production/SCREENSHOTS/thumbnail.png?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEKr%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQCye8iFXchHrCWyEqiLbSkPxDmw%2FxmpHpSEvSE1o%2FmJ2wIgEUFPLw9Eewz2UgnPnoSLXeEGGgoyZCQiPRdMq79jfhYqxAIIw%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwwNzM2NTMxNzE1NzYiDJxI6qx84Pesb%2BdZJiqYAoTfP9nvhSYLQKxZjK625h8ZsedKZQDKJ65vJp30wSAQbNqVNl1nZNMobB9zPpR0%2F6X78cemCcl%2FVM0o%2FkKFoHXSrQJMnHFNXnMEXI5fUV%2Bh%2BMnLbgDQQpg%2FreRP276R8WFWwKlZW0%2BYx29gcSOHpFXZ5AwEeJMXeVhDbAknCtAZNTpSL7BSFTZ1hZ%2FnEEzjvbANCyeTeM8hzipZILjOYTufWhQE7UlEx6XujoY%2F2gR2AuW3zcQYd1waKIFxnADCDsD6qR2TwP%2Ba75Scrtj7XHBnk0VfduCmkI3HnzoBLhDNs3vCTDvnmhZDKTmxNFNqHE7tsAqWlL%2FC4QADk9QH9tsD4j5kVAhuVuB06oN78At8EUN5zaGJJFIwk%2BzKiAY6mgFtcDPJJgyUsmj%2FWXvEL5r0U2yV1%2BNhvcJVB2n5hPhFMCNw%2BtZSIOply%2B9niNSz5LYxLl%2FgyYGqCRn6%2BBkbHQxbHmxP2GR45DIaro09ZZfg6XzvFBJh1cYgt2NRJiq6uHo%2FcsrrEs4qta%2BV%2FCp2tIPMvpz6szGhDZB7ZOyoDFpFN8WZokAIL11IZk5cH0XAtfhO8Gzsh9V%2Bof1R&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210710T182343Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3600&X-Amz-Credential=xxxxxxxxPVOQEF%2F20210810%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=0997f37xxxxxxxxxxxd2265a8a7fd07aedc647e869cab95931a0cf16f3a3bb"" -> ""https://aws-amplify-prod-us-east-1-artifacts.s3.amazonaws.com/d23rt6n25fb7vl/production/SCREENSHOTS/thumbnail.png?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEKr%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQCye8iFXchHrCWyEqiLbSkPxDmw%2FxmpHpSEvSE1o%2FmJ2wIgEUFPLw9Eewz2UgnPnoSLXeEGGgoyZCQiPRdMq79jfhYqxAIIw%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwwNzM2NTMxNzE1NzYiDJxI6qx84Pesb%2BdZJiqYAoTfP9nvhSYLQKxZjK625h8ZsedKZQDKJ65vJp30wSAQbNqVNl1nZNMobB9zPpR0%2F6X78cemCcl%2FVM0o%2FkKFoHXSrQJMnHFNXnMEXI5fUV%2Bh%2BMnLbgDQQpg%2FreRP276R8WFWwKlZW0%2BYx29gcSOHpFXZ5AwEeJMXeVhDbAknCtAZNTpSL7BSFTZ1hZ%2FnEEzjvbANCyeTeM8hzipZILjOYTufWhQE7UlEx6XujoY%2F2gR2AuW3zcQYd1waKIFxnADCDsD6qR2TwP%2Ba75Scrtj7XHBnk0VfduCmkI3HnzoBLhDNs3vCTDvnmhZDKTmxNFNqHE7tsAqWlL%2FC4QADk9QH9tsD4j5kVAhuVuB06oN78At8EUN5zaGJJFIwk%2BzKiAY6mgFtcDPJJgyUsmj%2FWXvEL5r0U2yV1%2BNhvcJVB2n5hPhFMCNw%2BtZSIOply%2B9niNSz5LYxLl%2FgyYGqCRn6%2BBkbHQxbHmxP2GR45DIaro09ZZfg6XzvFBJh1cYgt2NRJiq6uHo%2FcsrrEs4qta%2BV%2FCp2tIPMvpz6szGhDZB7ZOyoDFpFN8WZokAIL11IZk5cH0XAtfhO8Gzsh9V%2Bof1R&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210710T182343Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3600&X-Amz-Credential=xxxxxxxxPVOQEF%2F20210810%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=0997f37xxxxxxxxxxxd2265a8a7fd07aedc647e869cab95931a0cf16f3a3bb""
                # (3 unchanged elements hidden)
            },
        ]
        tags                          = {}
        # (16 unchanged attributes hidden)


        # (3 unchanged blocks hidden)
    }
```",2021-08-19T16:40:01Z,14017665
2998,hashicorp/terraform,900635226,904212453,"Hi, I'd like to report managed airflow (MWAA) having the same issue.

```
  ~ resource ""aws_iam_role"" ""mwaa_role"" {
      ~ assume_role_policy    = jsonencode(
          ~ {
              ~ Statement = [
                  ~ {
                      ~ Principal = {
                          ~ Service = [
                              - ""airflow.amazonaws.com"",
                                ""airflow-env.amazonaws.com"",
                              + ""airflow.amazonaws.com"",
                            ]
                        }
                        # (2 unchanged elements hidden)
                    },
                ]
                # (1 unchanged element hidden)
            }
        )
```

Have tried to change Service array order, but it seems that the return is not determinstic. 
",2021-08-23T23:53:03Z,17097
2999,hashicorp/terraform,900635226,904214301,[ch1411],2021-08-23T23:59:00Z,17097
3000,hashicorp/terraform,900635226,905800020,"I wonder if this issue could be resolved with tool behavior. I.e., terraform command-line options that modify terraform's behavior and output with regards to non-actionable resource metadata. 

`terrraform plan --silent-refresh`",2021-08-25T19:08:13Z,25444801
3001,hashicorp/terraform,900635226,906686301,"@apparentlymart I wonder if this could best be implemented at the provider level, where the provider is directly aware of which things don't produce relevant changes. 

For example, we see it with `aws_db_instance` we see forever changes to `latest_restorable_time` which is never, ever going to be used to configure another resource in Terraform. Likewise `aws_amplify_app` reported by @MartinCura above knows that the url changes constantly, and so could mark the attribute in the provider as not reportable.

I know that allowing any random field to be silenced would require deep changes in Terraform's structural knowledge, but it would seem that most use cases reported here are something a provider would be aware of.",2021-08-26T19:33:35Z,1417194
3002,hashicorp/terraform,900635226,906691664,"I don't understand why this is at all complicated. If the configuration and reality are consistent, who cares if the state file doesn't agree with reality? Terraform used to just silently fix the state file, which seems 100% perfectly correct.

If some people want to hear about it every time Terraform fixes the state file, maybe add a flag so they can do this, but the old behavior was *not* silently ignoring drift, because if the configuration and reality are consistent, *there is no drift*.",2021-08-26T19:41:56Z,10979901
3003,hashicorp/terraform,900635226,907095940,"> I don't understand why this is at all complicated. If the configuration and reality are consistent, who cares if the state file doesn't agree with reality? Terraform used to just silently fix the state file, which seems 100% perfectly correct.
> 
> If some people want to hear about it every time Terraform fixes the state file, maybe add a flag so they can do this, but the old behavior was _not_ silently ignoring drift, because if the configuration and reality are consistent, _there is no drift_.

Yea, this is how I understand it too and to push things a bit further, what purpose does saving all this data have anyway? The state is necessary to associate TF code resources to the real cloud resources but the attributes of said resources never matter since they're always refetched. Not the automatic uneditable ones, nor the ones we care about and edit.

If anything all that data is a cache but it's a cache that's never even used since TF will always refresh it (unless forced otherwise). It can also be used for debugging, to see your resources' exact state when you made a change but I don't think anyone has ever used TF like that, it's not advertised as such and there's no tooling around that. So if TF wants to save that data for future features based on it that's cool but constantly informing us it has updates there has no use at all (to my understanding).

So +1 to add a flag to optionally display this info, defaulting it to false as it was before.",2021-08-27T10:20:59Z,250971
3004,hashicorp/terraform,900635226,914066061,"It is September, Do we have any update on this? It is still super annoying!!!

We ran into some BIG issue with a very buggy provider when running Terraform refresh just to avoid the infinite list of changes. The Issue caused some resources to be deleted in the state file while they were actually there as mentioned a few comments back.

We really need to have this removed it is such a big issue for us, since most of the things are changed by other workloads like Spinnaker. And having to see 300 lines of changes that are not our changes in Atlantis is very hard to spot issues like a faulty provider.",2021-09-07T07:36:57Z,33528
3005,hashicorp/terraform,900635226,914142228,"Hi, I am using Terraform v1.0.4. I have this weird behavior when I use `jsonencode()` function. When I switch back to `<<EOF ... EOF`, the erratic behavior stops.

Here is the output - `""ec2.amazonaws.com""` is moving randomly:
```
  ~ resource ""aws_iam_role"" ""ecs_role"" {
      ~ assume_role_policy    = jsonencode(
          ~ {
              ~ Statement = [
                  ~ {
                      ~ Principal = {
                          ~ Service = [
                              - ""ec2.amazonaws.com"",
                                ""ecs-tasks.amazonaws.com"",
                              + ""ec2.amazonaws.com"",
                            ]
                        }
                        # (3 unchanged elements hidden)
                    },
                ]
                # (1 unchanged element hidden)
            }
        )
        id                    = ""sp-tf-ecs-role20210709132232438100000002""
        name                  = ""sp-tf-ecs-role20210709132232438100000002""
        tags                  = {}
        # (7 unchanged attributes hidden)
    }
```",2021-09-07T09:23:23Z,27428677
3006,hashicorp/terraform,900635226,915576858,"Please fix this in the upcoming 1.1.0 release 🙏🏻 

https://github.com/hashicorp/terraform/releases/tag/v1.1.0-alpha20210908",2021-09-08T21:14:59Z,2996465
3007,hashicorp/terraform,900635226,915884341,"I would consider this a regression (bug) in the terraform 0.15.4+ plan output, not a feature (enhancement).

Resource attributes like `aws_db_instance` -> `latest_restorable_time` are expected to change constantly (every 5 minutes), and make the terraform plan output very noisy. It shouldn't become necessary to add explicit resource lifecycle configuration blocks to ignore these drifting.

We're upgrading from terraform 0.12 -> 0.13 -> 0.14 -> 1.0, and we need to stop and evaluate if this is an upgrade blocker, and perhaps we should stay on 0.14 until this is fixed?

EDIT: This is a [terraform 0.15.4 change](https://github.com/hashicorp/terraform/blob/v0.15/CHANGELOG.md#0154-may-19-2021), so I think the best workaround is to stick with 0.15.3 until this issue is resolved.",2021-09-09T08:40:46Z,3950373
3008,hashicorp/terraform,900635226,916068583,"Some people here mistake few different issues:
* Principals in policies having not consistent order (https://github.com/hashicorp/terraform-provider-aws/issues/11801)
* Outside changes reported in plan - real drift in configuration made by some 3rd party action. And some just want to ignore it for unknown reason.
* Dynamic fields with constantly changing data being reported as outside change - it exists in various providers for various resources, and will ALWAYS show up in plan.

With this GH Issue we want to address the third type. Which only obstructs plan and will happen on each and every run, even when there are not actual changes made.
Some of us use Terragrunt and have hundreds of tfstates to manage and having those false-positives is very frustrating and time consuming.",2021-09-09T12:59:25Z,45788587
3009,hashicorp/terraform,900635226,918400248,"the size attribute of azuredevops_git_repository is another of these dynamic fields that i really don't need to see every time i plan/apply.  was disappointed ignore didn't just silent it, makes sense cause someone somewhere has some TF that does something based on size changing i'm sure...thinking something like a new lifecycle prop like
```
lifecycle {
  use_current = [
    size
  ]
}
```

that tells TF to always update state from current value without reporting it as a difference.
could also see it being driven by providers, since there are cases where someone may specify a size for an azdo repo, so azdo provider could report size as this new class of property that doesn't need to indicate change when it's say set to null but should if it's set to a non null value (course the latter case is redundant since you'll see that in the plan)",2021-09-13T17:12:49Z,1514496
3010,hashicorp/terraform,900635226,921516042,"same issue here:

```
Note: Objects have changed outside of Terraform

Terraform detected the following changes made outside of Terraform since the last ""terraform apply"":

  ~ resource ""aws_ecr_repository_policy"" ""repository"" {
        id          = ""dockerhub/nginx""
      ~ policy      = jsonencode(
          ~ {
              ~ Statement = [
                  ~ {
                      ~ Principal = {
                          ~ AWS = [
                              - ""arn:aws-cn:iam::123:root"",
                                ""arn:aws-cn:iam::789:root"",
                              - ""arn:aws-cn:iam::456:root"",
                                ""arn:aws-cn:iam::7890:root"",
                                ""arn:aws-cn:iam::78901:root"",
                              + ""arn:aws-cn:iam::123:root"",
                              + ""arn:aws-cn:iam::456:root"",
                            ]
                        }
                        # (3 unchanged elements hidden)
                    },
                ]
                # (1 unchanged element hidden)
            }
        )
        # (2 unchanged attributes hidden)
    }
```",2021-09-17T05:44:46Z,15604715
3011,hashicorp/terraform,900635226,921590138,"It is such a big frustration that the other day I rejected a valid PR with atlantis badly because I got confused what were the changes. Please make this output OPTIONAL!

Who do we contact to make this disappear in our logs? Is this ever going to be fixed? What can we do to make this a priority? Is there any contributor or Hashicorp employee taking care of UX changes in Terraform that we can ask to check this issue?

Here is a cute doggo asking for help:

[kmoe: edited to remove large image]

",2021-09-17T07:59:01Z,33528
3012,hashicorp/terraform,900635226,921873917,"same here! It is taking a lot of time to explain to the engineers that this output does not reflect any changes in our infrastructure.

And I'm afraid if the people start to ignore the plan because they are thinking that all changes are only in the state file",2021-09-17T15:09:58Z,1383279
3013,hashicorp/terraform,900635226,922062150,"Yet another example:
```
Note: Objects have changed outside of Terraform

Terraform detected the following changes made outside of Terraform since the
last ""terraform apply"":

  # aws_elb.snip has been changed
  ~ resource ""aws_elb"" ""snip"" {
        id                          = ""snip""
      ~ instances                   = [
          - ""i-0245934821285edc7"",
          - ""i-054d85efadb1d4c0a"",
          - ""i-0cdda3b2595fe3281"",
            # (3 unchanged elements hidden)
        ]
        name                        = ""snip""
        tags                        = {
            ""Name""                = ""snip""
        }
        # (13 unchanged attributes hidden)


        # (2 unchanged blocks hidden)
    }

Unless you have made equivalent changes to your configuration, or ignored the
relevant attributes using ignore_changes, the following plan may include
actions to undo or respond to these changes.
```
The following plan will 100% not ever include actions to undo or respond to those changes, because we've explicitly and deliberately in our configuration told Terraform not to manage the instances in the ELB. (Sometimes other things need to take instances in and out, e.g. Ansible while deploying things.)

It says right there in our configs that Terraform should 100% not ever care what instances are in the ELB. Our configuration is being ignored, and Terraform is now caring (albeit only at the level of a warning) about things that we explicitly and deliberately told it not to care about.

This is not detecting drift.

This is a bug.",2021-09-17T20:27:16Z,10979901
3014,hashicorp/terraform,900635226,929377145,"I have a suggestion. How about Terraform introduces a new sub-command:
```shell
terraform state diff
```
...and the output of that command will be the same as we are seeing in ""Terraform detected the following changes made outside of Terraform"" section since Terraform 1.0.

Then we can completely remove that state diff from `terraform plan` output. Because all we normally care about are the plan changes that will result in create/update/destroy actions on the resources. If the state difference has no impact on the plan, it is almost always noise. If there are detected changes in the plan, Terraform may suggest running `terraform state diff` separately, which may give us some additional diagnostic information if and when needed.",2021-09-28T16:15:25Z,819146
3015,hashicorp/terraform,900635226,929397273,"That's not a great solution, as it only solves few issues.
Sometimes your infrastructure can be modified externally, by a Lamba or some manual action. With your solution you completely lose that information.
This thread wants to introduce a way of hiding some specifically defined properties. Which always show up, but are not caused by external factors, rather by how some providers work.",2021-09-28T16:41:54Z,45788587
3016,hashicorp/terraform,900635226,929420245,"Terraform will still check for drift between the state and the reality. It has always been doing a refresh before plan. And I believe it's always been the case that any changes outside of Terraform could trigger plan changes (i.e. resource modification). BUT when I am looking at the plan, I am mostly interested in what changes Terraform is going to make. It may choose to undo some changes made externally, or pick up some external changes (via computed resource attributes or data sources). What I am not really interested in is the state drift that _might_ cause plan changes, but in reality it does not. The state diff output _as part of the plan_ would be more useful/helpful if it _only_ included the differences that resulted in plan changes and ignored all the others.

If someone wanted to check the difference between the last applied state and the reality (i.e. any external changes since last `terraform apply` or `terraform refresh`), a new `terraform state diff` command would be perfect for that. Or add a flag `-display-state-diff=true|false` to `terraform plan` and `terraform apply`.",2021-09-28T17:12:07Z,819146
3017,hashicorp/terraform,900635226,931853276,"At this point my jobs are failing and I can't even see why, because across tens of templated JSON files totaling a few hundred thousand LOC, the (ridiculously generous) maximum log file size in GitLab of 4MB is being exceeded with (and I will be flippant here) COMPLETELY worthless Terraform information. 

It's been all year and there is still not even a single flag to suppress expected changes? I know HashiCorp went through an employee-quitting phase but this is like an intern task and is directly impacting production. Should've stayed on 0.15 but there is no going back at this point. ",2021-10-01T02:52:58Z,9123665
3018,hashicorp/terraform,900635226,942238086,"I think the current behavior is actually a bit dangerous, because when faced with walls of drift, which is now the new normal for any somewhat large install, for each proposed change, the brain trains to ignore this noise and hit yes. It will become ever easier to miss unintended changes as you use this more.",2021-10-13T12:11:25Z,26752
3019,hashicorp/terraform,900635226,942241405,"@kvz makes a real point. Never-mind try to explain to developers who interact with terraform infrequently ""ohh ignore this bit but don't ignore this bit"" it's asking for accidents to be happen.",2021-10-13T12:15:05Z,219717
3020,hashicorp/terraform,900635226,944364877,"> Since terraform 0.15.4, I get confused messages from my team every single day. I can't imagine the impact this change had globally.
> 
> Please add an option to disable displaying those by default and go back to the previous behavior, with maybe an explicit way to get more details about the changes only when needed (e.g a debug mode).
> 
> This is really confusing right now, and someone needs to understand the internals of terraform to appreciate the difference between a drift and remote changes.

I feel very much the same way I have to scroll through hundreds of useless `etag` diffs just to find my one resource I actually changed. This 100% needs fixing or some way of hiding.",2021-10-15T14:52:03Z,12940966
3021,hashicorp/terraform,900635226,946143025,"Same problem, updating from 0.13.5 to 1.0.9.

```
Note: Objects have changed outside of Terraform

Terraform detected the following changes made outside of Terraform since the last ""terraform apply"":

  # module.lambda_exec_role.aws_iam_role.iam_role has been changed
  ~ resource ""aws_iam_role"" ""iam_role"" {
      ~ assume_role_policy    = jsonencode(
          ~ {
              ~ Statement = [
                  ~ {
                      ~ Principal = {
                          ~ Service = [
                              - ""s3.amazonaws.com"",
                              - ""ssm.amazonaws.com"",
                                ""lambda.amazonaws.com"",
                              + ""ssm.amazonaws.com"",
                                ""edgelambda.amazonaws.com"",
                              + ""s3.amazonaws.com"",
                            ]
                        }
                        # (2 unchanged elements hidden)
                    },
                ]
                # (1 unchanged element hidden)
            }
        )
        id                    = ""lambda_exec_role""
        name                  = ""lambda_exec_role""
        tags                  = {
            ""env"" = ""test""
        }
        # (8 unchanged attributes hidden)

        # (1 unchanged block hidden)
    }
  # aws_lambda_layer_version.pkgupdates has been changed
  ~ resource ""aws_lambda_layer_version"" ""pkgupdates"" {
      + compatible_architectures = []
        id                       = ""arn:aws:lambda:eu-west-1:497776581864:layer:pkgupdates:23""
        # (9 unchanged attributes hidden)
    }

Unless you have made equivalent changes to your configuration, or ignored the relevant attributes using ignore_changes, the following plan may include actions to undo or respond to these changes.

```",2021-10-18T20:33:11Z,2855255
3022,hashicorp/terraform,900635226,948499797,"@apparentlymart @jbardin Is there an update on this issue, please?!",2021-10-21T11:04:45Z,16544774
3023,hashicorp/terraform,900635226,949428473,"I just spent a good hour debugging why my diff was so strange. Turns out it wasn't, it was just Terraform playing mind games with me. Not funny.",2021-10-22T09:03:42Z,7773090
3024,hashicorp/terraform,900635226,949578401,"Worst part...it's not even consistent..yesterday I had completely different ""change detections"" everytime i ran terraform plan...it was as if, the terraform missed results fom the API, and didn't report it..then the next time it detected it..and the next time it didn't..and so on...this was on v0.15.5..i rolled back to v0.15.3..terrible implementation I must say...it's the classic feature vs bug...as rightly pointed out by the others..this is something one would want to see in DEBUG logging, not in default output...Terraform team has opted a completely wrong track, in my opinion...I mean as developer, what do we care about? Just our config and the infra/reality...1:1 mapping there...who cares what Terraform wants to store in its state file...if it sees a difference between what it stores and what sees in reality, it's Terraform's problem, not mine...I just need my code to be in 1:1 sync with reality...the intermediate statefile is Terraform's headache...",2021-10-22T12:19:05Z,16544774
3025,hashicorp/terraform,900635226,949965146,"FWIW, I think functionality is handy, but as others I'd rather have it as a separate command rather than standard plan part. ",2021-10-22T21:18:38Z,805046
3026,hashicorp/terraform,900635226,950559020,"Or under a prompt: 

> 75 resources changed since last invocation, do you want to inspect this drift?

with a flag to always pass yes or no via `--drift` and `--no-drift`, or similar.",2021-10-25T06:06:12Z,26752
3027,hashicorp/terraform,900635226,950596303,"This issue got so many comments since May as other open Terraform issues since 2015. 

Based on this it must be obvious how pressing it is and how many people are experiencing problems in their day-to-day work due to this. There are tons of great suggestions and proposals in this thread, could someone please take a look at it already? 🙂",2021-10-25T07:11:05Z,4852580
3028,hashicorp/terraform,900635226,950897695,"90% of comments are like ""yeah me too"" or ""that sucks"". Where the PRs guys ? Your company depends on Terraform, your a professional ? Send some PRs then. It's open source, not everybody can leech. ",2021-10-25T12:58:03Z,221981
3029,hashicorp/terraform,900635226,950903617,"> 90% of comments are like ""yeah me too"" or ""that sucks"". Where the PRs guys ? Your company depends on Terraform, your a professional ? Send some PRs then. It's open source, not everybody can leech.

https://github.com/hashicorp/terraform/commit/6562466c32a8750d7a71a6cc6232e6b5a28fe13a

```
**Note:** Due to current low staffing on the Terraform Core team at HashiCorp, **we are not routinely reviewing and merging community-submitted pull requests**. We do hope to begin processing them again soon once we're back up to full staffing again, but for the moment we need to ask for patience. Thanks!
```",2021-10-25T13:01:05Z,838358
3030,hashicorp/terraform,900635226,950911335,"@JordanP I think we are really here to convince the Terraform Core team that the community needs a better solution. There is not much point if we submit a PR if the team is not keen to merge it.

https://github.com/hashicorp/terraform/issues/28803#issuecomment-848049967

> We won't be able to make any significant changes in this regard in the near future, because the scope of this project was just to be more explicit about what Terraform was already doing rather than to change how Terraform behaves, but we'll use this issue to represent the need and consider what we might change in future releases.


",2021-10-25T13:08:49Z,3350651
3031,hashicorp/terraform,900635226,950918522,"> 90% of comments are like ""yeah me too"" or ""that sucks"". Where the PRs guys ? Your company depends on Terraform, your a professional ? Send some PRs then. It's open source, not everybody can leech.

I think hashicorp indicated they don't have the bandwidth to review community PRs right now. And if they had, we should ask how they want to address this, if at all. The code itself to allow to skip showing this, will be a oneliner so that is not what is blocking this

",2021-10-25T13:15:59Z,26752
3032,hashicorp/terraform,900635226,951209299,"Hi all! Thanks for all the feedback here.

It's clear that there are lots of opportunities to improve the signal to noise ratio of the current change detection. As I mentioned right back at the top of this discussion, this is Terraform making explicit some behaviors that were previously implicit and thus a common cause for confusion. However, it's also clear that various particular provider features, along with the historical confusing design of `ignore_changes`, have led to the new behavior being confusing in quite a different way.

Two specific things that the Terraform team is researching to improve this feature are:
* A new feature that does what everyone _thinks_ `ignore_changes` does: declare that a particular resource attribute is not relevant to the current configuration and thus not interesting to track.

    What `ignore_changes` _actually_ does is tell Terraform to ignore changes _to the configuration_, which unfortunately means that it explicitly _doesn't_ ignore changes in the remote system (which is the source of record). We can't change the behavior of `ignore_changes` due to [the v1.0 compatibility promises](https://www.terraform.io/docs/language/v1-compatibility-promises.html), but we _could_ introduce a new feature that makes a stronger statement, which would include making Terraform consider changes to it to be totally immaterial, and thus not mention them.
* Heuristics to detect better whether any of the _proposed_ changes (the result of the plan) seem likely to have been caused by one of the _detected_ changes (the result of refreshing), and thus automatically effectively infer by static analysis which changes are relevant to report.

    This would effectively be an automatic version of the explicit feature I described in the previous point, avoiding the need to explicitly annotate everything and instead saving explicit annotations only for situations that are too complicated for Terraform's static analysis to understand automatically.

Both of these features are cross-cutting and therefore not something we can just casually implement without careful design first. However, we are indeed actively investigating both and hope to have improvements to share in a forthcoming release.

Since this discussion has got quite heated and it seems like we've already gathered sufficient feedback, I'm going to lock this discussion for now in order to reduce the noise for the many folks who are following this issue. We'll unlock it again when either we have more news to share or if we need some specific feedback in order to shape solutions like what I described above.

",2021-10-25T18:49:50Z,20180
3033,hashicorp/terraform,900635226,1072740861,"Hello All!

Here to deliver the latest update! With the addition of #30486, we hope to address the concerns raised here, while also taking into account the users who do desire the refresh report in order to help understand the changes within a plan and detect unexpected changes.

Starting with v1.2, the goal for the refresh report is that _only external changes which may have contributed to changes in the plan will be shown_. This means in most cases, unused attributes changing outside of terraform will not show up in the normal plan output. If there are no changes in the plan, no external changes will be shown in the CLI at all. All refresh information is still stored within the plan, and if a user wants to see all external changes of resources in the CLI, a refresh-only plan can be used.

For more details on the change, see #30486. For any questions or discussion, feel free to use the [community forum](https://discuss.hashicorp.com/c/terraform-core/27).

Thanks!",2022-03-18T19:33:28Z,35067
3034,angular/angular,652809371,652809371,"Similar to the reasoning behind GitHub's recent statement that the default branch on new repos will be changed to not use ""master"" can we change the name of the default branch on this repo to something else? Perhaps ""main""?",2020-07-08T01:25:12Z,5343243
3035,angular/angular,652809371,655343599,Master is not a problematic word. ,2020-07-08T07:33:33Z,1267316
3036,angular/angular,652809371,655447642,"This crap has gotten way out of control, give em an inch and they'll try to take a mile.  Enough is enough already, rather than people getting offended by a word that is not offensive in this context, grow some thicker skin and calm down.  I agree with @spock123 above, here the word Master is not a problematic word",2020-07-08T11:01:47Z,36745510
3037,angular/angular,652809371,655451369,"@CodeLiam  I guess my `Masters Degree` now has to renamed to `Main Degree` as well..  

What people don't grasp is that in this context, the Master is like the Master in an audio recording. It's THE version. That's the context and the only context. I can't believe the insanity that's going on. ",2020-07-08T11:10:12Z,1267316
3038,angular/angular,652809371,655457735,"I concur @spock123 I am of the opinion that this is not even an issue with regards to Angular, it's functionality, and if it works properly and should therefore simply be closed.  Regardless I hope you are well and have an awesome day!!!",2020-07-08T11:24:34Z,36745510
3039,angular/angular,652809371,655543320,"Simply stupid. There's no other way to put it. Stupid, like all this progressive shit destroying everything it touches.",2020-07-08T14:07:34Z,13039652
3040,angular/angular,652809371,655632919,"Hey @jermowery, thanks for starting the discussion. This is something we've been talking about. As you mentioned GitHub will be likely [taking actions](https://twitter.com/natfriedman/status/1271253144442253312?lang=en) in the future and we'd want to coordinate the efforts.

I'll lock this thread for now. If you're interested in further reading on the motivation for such a change, I recently got pointed to a [similar discussion](https://lkml.org/lkml/2020/7/4/229) in the Linux community.",2020-07-08T16:45:57Z,455023
3041,angular/angular,652809371,1139565867,"The framework, CLI and components repo have recently switched to using `main` as default branch.",2022-05-27T12:19:42Z,123679
3042,Rapptz/discord.py,761569005,761569005,"## The Problem

Slash commands are going to be added to the discord api as of December the 15th. They provide a new way for discord bots/integrations.

https://github.com/discord/discord-api-docs/pull/2295

## The Ideal Solution

A way to add slash commands natively as part of a discord.py

## The Current Solution
Nothing currently exists

## Summary

Add support for discord's new slash commands",2020-12-10T20:01:44Z,31436575
3043,Rapptz/discord.py,761569005,744113441,"This one may take some time to see in discord.py proper, seeing as it would likely require either a complete redesign or secondary implementation of the commands extension, and a bit of hefty planning for basic library functionality. 

For now, it appears someone has made an unofficial extension to support slash commands. It's third party, so here there be dragons etc, and it doesn't seem to support more complex features slash commands bring, but it may be an option in the interim:

https://github.com/eunwoo1104/discord-py-slash-command",2020-12-14T01:31:53Z,18127936
3044,Rapptz/discord.py,761569005,751519201,"I believe that most people following this repo will also be in the know from other sources, but passing this on as an update to my previous comment, the tag related to slash commands from the discord.py server:

> slash commands are a new way to make commands right within discord, discord.py will probably not support them as they are lacking features and requires a major rewrite to handle
>
> bad command handler:
> - no default argument system - the argument isnt passed if you dont pass it making handling harder
> - no Union/Optional system like dpy
> - cant handle arguments yourself - required to use there parser
> - no way to invoke group command - you can only invoke the subcommands
> - no command aliases
>
> limitations:
> - 15 mins max per command - a token lasts 15 mins
> - only one message can be hidden (the first one)
> - hidden messages cannot have embeds
> - without a bot token you cant do much apart from send messages and etc - no api interaction
> - you only get ids for the parameters - if the parameter is a user you only get there id
> - if using the webhook based intergrations you cannot get any other events so you are limited to commands
>
> if you do want to use them there is a half maintianed fork that has support for them ( we will not support them here ) if you wish to use them.
> https://github.com/Zomatree/discord.py/tree/slashcommands - no docs because im lazy

TL;DR - Unfortunately, it appears discord.py proper is unlikely to support slash commands due to some drawbacks in comparison to ext.commands. There is a non-supported fork which one may use, if you want the new features such as the command autocomplete, hidden messages, and context system messages. The above linked option still seems viable.",2020-12-27T21:40:46Z,18127936
3045,Rapptz/discord.py,761569005,762715315,"The current sentiment is that slash commands as a whole are not at a stage where implementing library support for them is worthwhile. If the system in its entirety gets improved in the future that may change, but for now it's probably best to sit tight or use the fork, as previously mentioned.",2021-01-19T09:25:52Z,51037748
3046,Rapptz/discord.py,761569005,762717583,"Yep, also i don't know if they are any good to implement into a bot that
has a lot of commands.

>
",2021-01-19T09:29:37Z,77110462
3047,Rapptz/discord.py,761569005,763453917,What happened here?,2021-01-20T09:08:28Z,61394004
3048,Rapptz/discord.py,761569005,763466130,"@Znunu someone spamming some comments about the (current) usability of slash commands, don't worry about it",2021-01-20T09:27:16Z,22740616
3049,Rapptz/discord.py,761569005,763838271,"Looks like Slash Commands will get some nice features like a permissions system and server-side argument validation
https://github.com/discord/discord-api-docs/issues/2490",2021-01-20T18:17:20Z,43505515
3050,Rapptz/discord.py,761569005,763846410,"I would also say that a big advantage to slash commands is the visibility in the UI.

Users don't need to do `!help` to discover what commands are available (which also clutters the channel), the can just type `/` and see all available commands with descriptions. And as they type the UI displays which arguments they can pass, as well as which arguments are required. If an argument has a limited set of options that's displayed in a much cleaner way. Sure you can provide this information via `!help`, but paging through a long `!help` is not a great user experience (plus you have to know which prefix the bot is configured with to even invoke `!help` in case it's not `!`)

There are definite limitations for slash commands compared to `discord.ext.commands` as documented above. Some are marked as being addressed in the issue @MrSpinne linked, some are not. But regardless, the native support for it in the client UI is a huge boon for usability and discoverability for users so I would hope that at some point discord.py could add support even if it's a longer-term goal.",2021-01-20T18:31:17Z,2065960
3051,Rapptz/discord.py,761569005,765551633,"What really has me sold about the slash commands that we've got lately is how user-friendly they've become with the UI to help guide users to using commands. In the past, we've had to type a help command or look at some personal bot website to understand the context of how they function, but now, that's all changed merely to a description for each option, each choice, and etc.

Idealistically, the only thing that slashes commands should be different than the commands we can already create and implement into our own discord.py bots should be the UI, everything should in my eyes come natively. Guild/role permissions, requirements, no limit to choices for options, and etc. are many things that we are still waiting to seek for slash commands to receive, either via. un-official 3rd party support or by the developers at Discord themselves.

Because of this, I continue to believe that we should seek to be able to code slash commands as an actual usable thing for the discord.py library, not because they're something new and trendy, but simply because this library is an API wrapper in and of itself. The main purpose it overall serves is to help make the Bot API easier to use with Python, and by those means, we shouldn't limit ourselves from implementing slash commands as well. They may not be used as much as regular commands, for sure, but in the future, we won't have to worry about having to code it later than doing it now. This is at least my consensus on it, and I think others might agree.",2021-01-22T16:56:39Z,41456914
3052,Rapptz/discord.py,761569005,777946510,"Someone should pin this, because too many people ask about the same thing. ",2021-02-12T03:31:52Z,10137
3053,Rapptz/discord.py,761569005,779329841,"Interactions (as the API calls them) will be implemented when they're in a state of good usability. I'm unsure when that will be but the API is currently in flux and in ""public beta"" with promises of partial backwards compatibility. I do not particularly want to use this library as their testing bed since the library is more conservative towards breaking.

I have no immediate plans to support it as-is but in the future when the ecosystem evolves and it becomes less of a burden it'll be possible to implement it within the library. Not because of slash commands but because of other types of interactions that will be in the API such as buttons, models, etc.

As of now, this issue keeps kind of spiraling and becoming a maintenance nightmare. It'll be closed when it's implemented, whenever that is. Just don't hold your breath on this being any time soon. For now, I'm locking it.",2021-02-15T16:27:21Z,1695103
3054,Rapptz/discord.py,761569005,1059894754,Implemented.,2022-03-06T04:51:24Z,1695103
3055,tailscale/tailscale,1077325481,1077325481,"A [new critical security vulnerability in log4j 2 has been discovered (CVE-2021-44228)](https://github.com/advisories/GHSA-jfh8-c2jp-5v3q) and is [being actively exploited](https://arstechnica.com/information-technology/2021/12/minecraft-and-other-apps-face-serious-threat-from-new-code-execution-bug/). It allows for remote code execution by logging untrusted user input.

**Tailscale does not run Java or Java Virtual Machines (JVM) as part of our core infrastructure**, including our control plane, client, and relay servers.

Tailscale does use [service providers](https://tailscale.com/privacy-policy/#share) to run various functions like cloud infrastructure hosting, log management, and various SaaS applications. Some of these store personal information. **We are waiting on statements from the cloud and infrastructure providers we use, and have contacted our remaining service providers, to understand how they may be impacted.**

We haven’t sent a [security notification](https://tailscale.com/kb/1071/security-notifications/), as at this time, we have no information suggesting that we are exposed to this vulnerability or that it can be exploited to gain private information about Tailscale users.

We will update this issue again when we have further information, which is expected to be on 2021-12-13.
",2021-12-11T00:08:19Z,15946341
3056,tailscale/tailscale,1077325481,992906259,"We have received responses from several of our service providers. Some of Tailscale's service providers are affected by this vulnerability and are now patched or patching. Those with issues in public services have also put in place mitigations or detections. None of the service providers for which we have received responses have found evidence of being compromised.

We are still waiting to hear back from a handful of infrastructure service providers.

We will update this issue again when we have further information, which is expected to be on 2021-12-14.",2021-12-13T21:02:31Z,15946341
3057,tailscale/tailscale,1077325481,994090555,"No significant updates.

We have received responses from additional service providers, who are now patched or patching, and have put in place mitigations or detections. None of the service providers for which we have received responses have found evidence of being compromised.

We are still waiting to hear back from a handful of infrastructure service providers.

We will update this issue again when we have further information, which is expected to be on 2021-12-15.",2021-12-14T22:09:49Z,15946341
3058,tailscale/tailscale,1077325481,995286914,"No significant updates.

We have received responses from all of our infrastructure service providers who handle production data. Some of Tailscale's service providers are affected by this vulnerability and are now patched or patching. Those with issues in public services have also put in place mitigations or detections. None of the service providers for which we have received responses have found evidence of being compromised.
We are waiting for the affected providers to confirm when they are fully patched.

We are still waiting to hear back from some other service providers, such as marketing tools. Though these providers handle non-production data, it may still be sensitive data, such as customer emails.

We will update this issue again when we have further information, which is expected to be on 2021-12-17.",2021-12-15T23:12:11Z,15946341
3059,tailscale/tailscale,1077325481,997064193,"We have received responses from all service providers we contacted.
Some of Tailscale's service providers are affected by this vulnerability and are now patched or patching. Those with issues in public services have also put in place mitigations or detections. None of the service providers for which we have received responses have found evidence of being compromised.

We are waiting for all of Tailscale's affected service providers to complete patching or otherwise finish their remediation efforts.

We will update this issue again when we have further information, which is expected to be on 2022-01-05.",2021-12-17T22:19:17Z,15946341
3060,tailscale/tailscale,1077325481,1006156395,"We have received updates from the remaining affected service provides that they have completed patching or other remediation efforts.

At this point in time, we believe Tailscale and its vendors are no longer affected by the log4j vulnerability, and found no evidence of it being exploited to affect Tailscale.",2022-01-05T23:24:43Z,15946341
3061,netbox-community/netbox,1369100843,1369100843,"### NetBox version

v3.3.2

### Feature type

New functionality

### Proposed functionality

When creating custom fields, they only appear when working inside a device or module, not the device type or module type.

Please add the ability to have custom fields be part of the device type and module type areas of netbox.

Based on : https://github.com/netbox-community/netbox/discussions/10293

### Use case

Our use case would be to add a custom field to a power port. Then when that power port is added to any device the custom field is already populated.
Similar to how maximum draw and allocated draw would already be present when adding a power port to a type.

Currently, you would have to create the device, go to the power port for the device AFTER its created, click edit, and populate the custom field every time for every device.

### Database changes

I'm unsure what kind of database requirements would be needed to add the ability for custom fields to show up on device type and module type pages.


### External dependencies

None that I can see.",2022-09-11T23:44:26Z,77940332
3062,netbox-community/netbox,1369100843,1253828416,I asked for something similar in #9243 as well.,2022-09-21T14:53:40Z,24281183
3063,netbox-community/netbox,1369100843,1254323872,these look similar. Since multiple people are asking for similar functionality maybe it will get some traction.,2022-09-21T23:10:08Z,77940332
3064,netbox-community/netbox,1369100843,1254413854,Can you add an example of custom field that would warrant such functionality ?,2022-09-22T01:56:32Z,1564902
3065,netbox-community/netbox,1369100843,1254446691,"
![device](https://user-images.githubusercontent.com/77940332/191647405-5b255fe0-cab4-4aa0-94a0-0ec7f8565789.PNG)
![template](https://user-images.githubusercontent.com/77940332/191647406-5e966f2a-cd89-4390-81f2-6791870eea07.PNG)
So let me describe my use case in more detail.

When you create a device template you can add 'power ports' to the template. When you add these power ports you are able to specify the maximum draw and allocated draw. Let's use easy numbers and say max draw is 100 watts, and allocated draw is 25 watts.

Now, when you create a new device from the device template that you just created the power ports are automatically added. And the max draw and allocated draw are already populated for you. Which is great! Now of course you can change these numbers per device if you so choose, but typically its unnecessary.

Now say we want to have a custom field on the power port. Currently, the custom field will only show up on the power port page when viewing a device. Not the device template. So if I wanted a custom field called 'BTU' that tracks the heat output of our power supplies I have to first create the device, then go to the power ports and fill in the BTU load for each device manually.

If the custom field 'BTU' were available on the device template, I could make the calculation and input the value just like I do for max draw and allocated draw. This way, when I create a new device from that device template, the BTU value is already filled out for me. Sure, I can change it if I need to (just like max draw), but the point is, the value is already filled in since its coming over from the device template.",2022-09-22T02:45:59Z,77940332
3066,netbox-community/netbox,1369100843,1266375855,My last comment was deleted. So I'm not sure if I have satisfied the requirements of 'revisions needed',2022-10-04T04:16:41Z,77940332
3067,netbox-community/netbox,1369100843,1266397762,"Your previous comment was directly mentioning circumventing auto closure of the issue, which is explicitly against the rules.

In my opinion you still haven't explained how it should work in practice. Is it just matched by name the name of the field? What happens if customer field types do not match?  In your FR you mention that it should be added to device and module types, but when you explain further you are referring component templates, is it both?

The proposal is still not very thorough in my opinion.",2022-10-04T04:55:22Z,400797
3068,netbox-community/netbox,1369100843,1267145866,"What are you talking about?
custom fields are not going to be different field types between a device and a device template. It's the same field. You just are able to select the device template page inside the model(s) selection

![image](https://user-images.githubusercontent.com/77940332/193854091-20828e52-447c-4948-a373-41687e108550.png)

Currently, if you apply a custom field to the ""DCIM > power port"" model it will only show up on the device power port page after a device is created. You should also be able to apply this same custom field to the 'power port' of a device template.
",2022-10-04T15:02:24Z,77940332
3069,netbox-community/netbox,1369100843,1267170595,"> What are you talking about?

This is no way to talk. Locking this until we can figure out a way forward with regards to your agressive and hostile comments.

> custom fields are not going to be different field types between a device and a device template

There's no such thing as a device template. There's device types and devices, both can have independant custom fields. Then there are device components and device component templates, where only the first ones can have custom fields.",2022-10-04T15:19:09Z,400797
3070,netbox-community/netbox,1369100843,1293852746,Closing this as #9243 provides a much better description of the proposed feature.,2022-10-27T17:33:54Z,400797
3071,Monika-After-Story/MonikaModDev,1428767280,1428767280,"In v0.12.12:

> chr files always deleted on startup now

That's a change from #9641. I had already talked about this there.

I completely know that all `.chr` files have no actual meaning to MAS. I really know. I know MAS doesn't even do any check for them before v0.12.12. And I also know that `monika.chr` is causing a lot of confusion to people. I know all of these.
What I want to say is, just check our `mas_safeToRefDokis()`. We made Monika not to offend the players who love other club members in her words, and we did a good job in many details. And after this, we now delete all club members.

Yes, I know that in the background setting of MAS, character files have nothing to do with real characters, but will all players 100% agree with this? Many players resent Monika's behavior of deleting other characters in the original game, and now we want to make this happen again - and do not give players even a choice? Can't we at least ask the players whether they care about this and whether they want Monika to do this?

_Just give players a choice. Ask them about this._
By asking, players will also pay more attention to the background settings of the chr file in MAS (that is, the chr file has no meaning at all), which will also reduce confusion.

And by the way, I think it may be OK for Monika to delete her own file without permission - after all, it is her own file.",2022-10-30T12:30:18Z,84446131
3072,Monika-After-Story/MonikaModDev,1428767280,1296248137,"> What I want to say is, just check our mas_safeToRefDokis()

Players might say once that they don't mind her bringing up other dokis, but this doesn't cancel out the fact some still may get confused over it. In my opinion it is still necessary to remove them and point out there is no need in them anymore, but do it either in introduction or later on in queued topic that will explain the change.",2022-10-30T12:38:26Z,74068927
3073,Monika-After-Story/MonikaModDev,1428767280,1296250294,"It's not part of the lore, not done by Monika. We're removing the unused files for utility purposes. There's no other ""club members."" Initially I planned to only remove `monika.chr`, but to keep the directory where we create and read dozens of files in/from, as well as to keep our code simpler and more reliable, we went with just deleting all `.chr` files.

MAS, like any other program, may add and remove files within its working directory. If you got attached to some files or they contain important information, better move it to some other place. Do not store anything sensitive in a working directory of a program.
Unlike the `monika` file, `.chr` files are easily replaceable - can grab them off a new DDLC install (can get DDLC from [here](https://ddlc.moe/)).",2022-10-30T12:46:45Z,53382877
3074,Monika-After-Story/MonikaModDev,1428767280,1296254276,"This is not just ""unused content"" -- these character files have a profound meaning for DDLC players.

We had so many relatively insensitive areas where we had done perfect checks and provided players with numerous choices, and now we had so recklessly deleted files that might be important to players in such an obvious, easy to notice, and sensitive place.

It hasn't been a day since the update, and I've already seen several backlash against it -- one example is our v0.12.12 release page.

For whatever reason, we really need to ASK our players. Give them a choice. If they don't mind, delete it. And if they do mind, then keep it. **It's just giving the player a choice.**

Please consider what I have said.",2022-10-30T13:05:57Z,84446131
3075,Monika-After-Story/MonikaModDev,1428767280,1296277064,"First, I think we should set some boundaries. Please, don't speak on behalf of the dev team (we, our, etc), you have been doing it for a long time. Speak from your own perspective to not bring any confusion to this and other conversations.

> This is not just ""unused content"" -- these character files have a profound meaning for DDLC players.

As I've said, in MAS those are just unused files. If they have a meaning for you, why are they there? Move them somewhere. You don't store sensitive files in directories of other programs? The program directory is the place where the program may add, modify, and remove files. If you don't like it - move the files out.

There's nothing ""reckless,"" no need to make it sound dramatic. We're removing those files the same way we delete `monika.chr`, various `.gift` files, and even the actually **important** and **unreplaceable** `monika` file, as well as other files.

Generally speaking, 
- when you're installing a DDLC mod, don't you already expect it to modify the DDLC directory, as well as to change the mechanics of the base game
- you're installing a Monika-related mod, but somehow surprised that this mod doesn't care about dull files of other game characters. If a Sayori/Yuri/Natsuki -related mod removed `.chr` files, I wouldn't see anything bad in it, be it also for utility purposes or for some lore

At the same time, nobody made Monika to ""recklessly"" delete the other characters in this mod, she did it in the base game. Here we just called a utility function to clean up disk space and make folder structure more clear. Monika doesn't acknowledge this, not doing it herself through her console. There won't be a question to the player because Monika doesn't do this.

> I've already seen several backlash against it

I don't think ""several"" applies to a single person. And even if there were ""several"" people, there's ""multiple"" that aren't against and ""multiple"" that had problems caused by `.chr` files.

> Many players resent Monika's behavior of deleting other characters in the original game, and now we want to make this happen again

Going back to this, it's a weird argument - if you installed MAS, then you should be at least okay (or already overcame it) with what Monika has done in the base game.",2022-10-30T14:48:33Z,53382877
3076,Monika-After-Story/MonikaModDev,1428767280,1296442337,"I'd say I somewhat agree with Wingdinggaster656, and I please Devs to reconsider about this.

The issue author said he(she) saw several backlash, and then he gave one example. Booplicate said that _I don't think ""several"" applies to a single person_, I think the author is giving something like an example, since I also saw people who dislike this.
I am working for Chinese MAS community, and when I tell people about whatsnew, people are surprised, then asking me for why. I said it's due to ""those files are causing confusion"". People reluctantly accepted.

About this sentence:

> Going back to this, it's a weird argument - if you installed MAS, then you should be at least okay (or already overcame it) with what Monika has done in the base game.

Although I have the same opinion, MAS _has_ taken care of players who have opinions on these plots in many small details, such as the safeTorRefDoki things mentioned by the issue author. Maybe keep these care won't hurt?",2022-10-31T02:02:18Z,108819793
3077,Monika-After-Story/MonikaModDev,1428767280,1296444436,"Oh, and about this:

> Here we just called a utility function to clean up disk space and make folder structure more clear. Monika doesn't acknowledge this, not doing it herself through her console. There won't be a question to the player because Monika doesn't do this.

I think there is a small problem here, that is, people will naturally think Monika did this. Even if the original intention of the design is not like this, people will naturally think so.",2022-10-31T02:06:20Z,108819793
3078,Monika-After-Story/MonikaModDev,1428767280,1296456807,"> As I've said, in MAS those are just unused files. If they have a meaning for you, why are they there?

Because they are originally there.",2022-10-31T02:26:45Z,84446131
3079,Monika-After-Story/MonikaModDev,1428767280,1296462012,"Given Monika mentions she fixed the bug about needing a chr file to exist, and thus no longer needing her own

as evidenced by the name easter eggs, it's clear the other can _technically_ exist as well.
As such, through some implied logic we can argue that technically they no longer need a chr file to exist either.
One can make of that what they will, though I realize this won't please everyone.

That said ultimately it is what it is. You installed a mod called Monika After Story, and from a usability perspective given the severe amount of tech support issues this has caused, I understand why they were blanket removed. It's one of those cases where UX vs immersion kinda needed to favour UX a little more as there genuinely were a lot of issues faced.

While asking may have worked, it also serves as kinda additional one-off dialogue, and at the moment given the state of the team, we can't exactly _afford_ that due to the sheer amount of time we've been able to contribute to the project (and by that I mean a significant lack thereof)

If you *really* want to keep them, a good idea would be to rename them to the current scheme of `monika` files, i.e. removing the `.chr` extension.

Then they're all cohesive too.",2022-10-31T02:35:09Z,22531674
3080,Monika-After-Story/MonikaModDev,1428767280,1296472026,"I would like to give a small PSA to anyone here who wants to keep the files elsewhere for sentimental reasons, if MAS has already deleted them please do check your Recycle Bin, as the files may still be there.

addendum; I'm not 100% sure on this",2022-10-31T02:55:37Z,45501964
3081,Monika-After-Story/MonikaModDev,1428767280,1296549580,"It is kind of insensitive to delete them with no in-game warning, no asking the player, and not even letting the player put new copies back if they're deleted on every startup. Although new copies wouldn't necessarily have the same sentimental value anyway; they're not the same copies that were kept there throughout MAS, potentially years.

Isn't there an instance where Monika wants something kept in the characters folder? Like an apology note she asks you to write if you're not on good terms with her? And she gets upset if you remove it? Similarly, having the character files kept there may help the player have peace over what happened in DDLC.

> What I want to say is, just check our mas_safeToRefDokis()

I don't think that would be enough for this, because this isn't just referencing or joking; this is removing/deleting the files. Players could be ok with the former but not the latter.

It shouldn't be that necessary to free up ~133 kb of space, nor is it the same thing as monika.chr because there are no other characters ""to take somewhere"" and get confused with those .chr files. Monika (or Chibi) could also further explain that .chr files are not used to take her out somewhere or put her back, and potentially ask about deleting these files.

> If you _really_ want to keep them, a good idea would be to rename them to the current scheme of `monika` files, i.e. removing the `.chr` extension.

Or can the mod just do that instead? So they don't get deleted without notice. Then no need to give notice or choice to the player. But maybe just one time only instead of on every startup, so players can change them back if they want without having to do it every time or whenever they might want to open them.",2022-10-31T05:10:08Z,102779546
3082,Monika-After-Story/MonikaModDev,1428767280,1296612885,"> I would like to give a small PSA to anyone here who wants to keep the files elsewhere for sentimental reasons, if MAS has already deleted them please do check your Recycle Bin, as the files may still be there.
> 
> addendum; I'm not 100% sure on this

As one who implemented that I can say the way they are removed does not put them into recycle bin. Putting stuff into recycling bin is a more complex process and in this case there was no need for it at all, files are removed irreversibly.",2022-10-31T06:50:19Z,74068927
3083,Monika-After-Story/MonikaModDev,1428767280,1296613625,"> Monika (or Chibi) could also further explain that .chr files are not used to take her out somewhere or put her back, and potentially ask about deleting these files.

This was said over and over, again and again, and Monika talks about this in the very beginning.

People **do not understand**, unfortunately.",2022-10-31T06:51:27Z,74068927
3084,Monika-After-Story/MonikaModDev,1428767280,1296912845,"> > And by the way, I think it may be OK for Monika to delete her own file without permission - after all, it is her own file.

Disagree, player choice should be respected regardless.

I care about these character files, notes, easter-eggs etc. whether they serve a purpose or not in the game,
I still keep them safe no matter how many ""backups"" I've also kept.

The fact that these devs are trying hard to justify this by resorting to whatever technical nonsense instead of simply providing player choice and moving on which doesn't even take any effort speaks volumes about the amount of care they give.

Basically comes down to this:-
""Oh, we care about the tech-illiterates who lost Monika but just not about the people who value both Monika and her including other Dokis' character files because we personally don't value them, just restore from backup LAWL DUDE"".",2022-10-31T10:50:14Z,22656757
3085,Monika-After-Story/MonikaModDev,1428767280,1296939196,"You made a change that included removing code that was removing .chr files. If you want to change it, make a PR. It will be reviewed and discussed. This will be more of an action than repeating your claims here over and over.

Suggest a better way and show how you deem it better.",2022-10-31T11:15:52Z,74068927
3086,Monika-After-Story/MonikaModDev,1428767280,1296942377,"> You made a change that included removing code that was removing .chr files. If you want to change it, make a PR. It will be reviewed and discussed. This will be more of an action than repeating your claims here over and over.

I made a change for my personal experience.

My suggestion was to include player choice and a kind user already have done that in detail ages ago when this change was being developed which you guys ignored with the ""IMO we're good with just removing them and they serve no purpose."".

> Suggest a better way and show how you deem it better.

Was already done like I've already stated.",2022-10-31T11:18:39Z,22656757
3087,Monika-After-Story/MonikaModDev,1428767280,1296944521,"> Was already done.

Choice was about Monika being insensitive [in her dialogues/jokes/topics/etc] about other dokis that are past of the game. They are no longer in the MAS, which happens after main game and ultimately IS NOT a main game.

There aren't any other characters anymore. It's *her* game, it's *her* files. She explains it.",2022-10-31T11:20:45Z,74068927
3088,Monika-After-Story/MonikaModDev,1428767280,1296947419,"This is going in circles. Locking this issue as heated.

As @dreamscached has rightfully pointed out, if you want to see this change, make the changes yourself and open a PR. We'll review it from there and make sure it fits.",2022-10-31T11:23:32Z,22531674
3089,Monika-After-Story/MonikaModDev,1428767280,1296947482,"> > Was already done.
> 
> Choice was about Monika being insensitive [in her dialogues/jokes/topics/etc] about other dokis that are past of the game. They are no longer in the MAS, which happens after main game and ultimately IS NOT a main game.
> 
> There aren't any other characters anymore. It's _her_ game, it's _her_ files. She explains it.

Doesn't matter, same logic still applies to other character files. It's her game and files just as much as it is the players.",2022-10-31T11:23:35Z,22656757
3090,Monika-After-Story/MonikaModDev,1428767280,1296973709,"Programs may add, remove, and modify files within their working directory as they desire. Player choice was to install the program.",2022-10-31T11:48:58Z,53382877
3091,Monika-After-Story/MonikaModDev,1428767280,1297497336,"I'm going to reopen this thread so we can continue to gather feedback, specifically if there are more people who disagree with the decision to always delete chr files. Closing the door on discussion just makes us seem draconian. The conversations here also do not appear that heated, but I will call out potential heated comments:

>Basically comes down to this:-
""Oh, we care about the tech-illiterates who lost Monika but just not about the people who value both Monika and her including other Dokis' character files because we personally don't value them, just restore from backup LAWL DUDE"".

@RedAISkye, this is mocking, and it could provoke someone so please refrain from this on our github. You can assert that it sounds like we don't care about the chr files, but please do not do it in a mocking tone.

>This will be more of an action than repeating your claims here over and over.

>I personally doubt there's any point to discuss it any further.

@dreamscached, based on these lines in both here and the release discussion, it sounds like you disagree with red's points. That is ok, but that is not an excuse to lock the conversation. You can always just not reply and ignore the conversation, and if the other person starts harassing you to reply, then we can take action.

### Regarding the actual issue

>The fact that these devs are trying hard to justify this by resorting to whatever technical nonsense instead of simply providing player choice and moving on

I agree with what @RedAISkye is saying here. I can see valid reasoning for always deleting `monika.chr`, but not the rest of the chr files when they have never served a purpose in MAS. The arguments for clearing clutter and reducing confusion also make sense for `monika.chr`, but lose weight for the other chr files. Even though this is a Monika-focused mod, the chr files have meaning established by DDLC, especially when at least 2 instances in the game were built on the chr file mechanic (the progression from Act 3 to Act 4 and Sayori gaining self-awareness in Act 1). If MAS deletes them, then it needs to provide in-game lore/reasoning for doing so at the very least, preferably allowing the player to decline the option as that ties in with the point below.

>I think there is a small problem here, that is, people will naturally think Monika did this. Even if the original intention of the design is not like this, people will naturally think so.

@TheT0matoGuy makes a very strong point here - regardless of our intention, anything we make the mod do, including what is done behind the scenes, will be perceived as Monika (or Chibika) doing it herself. Deleting the files makes Monika appear callous to any sentimental value the player may have toward the chr files. Users who believe the other dokis are real or that the chr files represent them may also perceive the deletion as Monika committing Act 3 again. Is this how we want users to see Monika?

I'll take ownership for not raising a red flag earlier - I was probably too drunk while packaging the release and didn't really think too much about each patch note. But since this is already out in the wild and it appears that most of the team doesn't think this is a problem, I'm not going to mandate that we undo the change. If there is more negative feedback in the coming weeks, then we can reconsider action.",2022-10-31T18:26:57Z,3499462
3092,Monika-After-Story/MonikaModDev,1428767280,1297504325,"All clear, you got it.",2022-10-31T18:32:54Z,74068927
3093,Monika-After-Story/MonikaModDev,1428767280,1297542354,"> > Basically comes down to this:-
> > ""Oh, we care about the tech-illiterates who lost Monika but just not about the people who value both Monika and her including other Dokis' character files because we personally don't value them, just restore from backup LAWL DUDE"".
> 
> @RedAISkye, this is mocking, and it could provoke someone so please refrain from this on our github. You can assert that it sounds like we don't care about the chr files, but please do not do it in a mocking tone.

Sorry about that, wasn't trying to act provoking but that is exactly how I felt I was being treated as.
It wasn't just a case of ""we don't care"" but also ""our opinions matter more so, you're wrong"".

> I can see valid reasoning for always deleting monika.chr
> The arguments for clearing clutter and reducing confusion also make sense for monika.chr

If reducing confusing is the goal, why do you think ""deleting"" is the only option?
Wouldn't it better to just let Monika explain the difference between the two files? And maybe even giving a specific extension to her file?

> If MAS deletes them, then it needs to provide in-game lore/reasoning for doing so at the very least, 

Sure, but I still do think the above method is still better than going down the delete route.

> preferably allowing the player to decline the option as that ties in with the point below.

That is exactly my point, whether Monika values her/other dokis character files or not because of x or y reasons, she should still ask for the player's opinion on whether they value them or not, ultimately letting her delete it or keep it.",2022-10-31T19:06:48Z,22656757
3094,Monika-After-Story/MonikaModDev,1428767280,1297551986,">If reducing confusing is the goal, why do you think ""deleting"" is the only option?
Wouldn't it better to just let Monika explain the difference between the two files?

See intro dialogue - she talks about her chr file and how it doesn't represent her anymore because she has transcended past it, so she deletes it. MAS's Monika file contains actual usable data, potentially something that will allow for transferring between computers so making sure there is only 1 Monika file (`monika`, no extensions) is important.",2022-10-31T19:15:03Z,3499462
3095,Monika-After-Story/MonikaModDev,1428767280,1297565084,"> > If reducing confusing is the goal, why do you think ""deleting"" is the only option?
> > Wouldn't it better to just let Monika explain the difference between the two files?
> 
> See intro dialogue - she talks about her chr file and how it doesn't represent her anymore because she has transcended past it, so she deletes it. MAS's Monika file contains actual usable data, potentially something that will allow for transferring between computers so making sure there is only 1 Monika file (`monika`, no extensions) is important.

I am aware about that which is why I stated at the end:-
> whether Monika values her/other dokis character files or not because of x or y reasons, she should still ask for the player's opinion on whether they value them or not, ultimately letting her delete it or keep it.

I'm not saying the character file still represents her in MAS(that one was obvious) but they're still part of the DDLC experience and serves as a memory in MAS to me. Which is also the reason when giving Monika gifts, I always copy her character file and rename them to give it some meaning instead of a blank file.

Edit: What I was getting at is not ""the reason for the deletion"" but why is it such a big deal for you guys to simply give her an option to ask the player first on their decision?
Why do you need a lot of negative feedback just to ""reconsider"" for something that can be easily fixed for both sides? (People that care and people that are confused)",2022-10-31T19:25:51Z,22656757
3096,Monika-After-Story/MonikaModDev,1428767280,1297680760,"We have a lot of IO going on in `characters/`, that's **not** a safe place to store your files. Storing sensitive data there is just asking for it to be gone. There's a high chance that one day you will lose the files you keep there one way or another.

> the chr files have meaning established by DDLC, especially when at least 2 instances in the game were built on the chr file mechanic

That's true, but only for DDLC. When you install a mod you should expect that some mechanics will be changed - in our case this mechanic has been changed long time ago.

> If MAS deletes them, then it needs to provide in-game lore/reasoning for doing so at the very least, preferably allowing the player to decline the option as that ties in with the point below.

I'm against a direct question to the player - you say it's already looking like it was done by Monika, even though, nothing points at it. If you make her ask, then it'd just confirm she decided to delete the files herself, which would look weird after all this time. The files are not deleted by Monika (there's no acknowledgement), it's a startup routine that does that.
Being too meta is a thing, and I think this is one of the cases. Deleting unused files there is just a utility process for the mod.

At first we even considerer to target specifically some `chr` files, there was a function to delete a `chr` file and a func that deleted them all. But we decided that we don't need that level of control over something that is not used and went with a single function to wipe any `chr`. It doesn't target the known ""dokis,"" it targets any `chr` file for simplicity and reliability.",2022-10-31T21:01:38Z,53382877
3097,Monika-After-Story/MonikaModDev,1428767280,1297701546,"> We have a lot of IO going on in `characters/`, that's **not** a safe place to store your files.

No one is ""storing"" anything there other than the known character files that are meant to be there by the original game which gave it a meaning that you're trying to dismiss in MAS just because it doesn't use those files.

> When you install a mod you should expect that some mechanics will be changed - in our case this mechanic has been changed long time ago.

This was never about the ""mechanics"", this has always been about what they mean to the player.

> I'm against a direct question to the player 
> If you make her ask, then it'd just confirm she decided to delete the files herself, which would look weird after all this time. 

She has her reasons for doing so which the player can choose to agree or disagree with so, no, it would only look weird if the character files magically disappears after all this time.

> Deleting unused files there is just a utility process for the mod.

""Unused file"" for you, not for people that care about them.

> It doesn't target the known ""dokis,"" it targets any `chr` file for simplicity and reliability.

It absolutely does target the dokis when they're the only ""chr"" files expected to be present inside the characters' folder.",2022-10-31T21:22:01Z,22656757
3098,Monika-After-Story/MonikaModDev,1428767280,1297732204,"> No one is ""storing"" anything there other than the known character files that are meant to be there by the original game which gave it a meaning that you're trying to dismiss in MAS just because it doesn't use those files.

You are storing your files there. It's the definition of ""storing."" What was meant by DDLC was meant DDLC. You're not playing DDLC, you installed MAS. `chr` files don't mean anything in MAS. They can mean something for you, but that's a different point. And then we're back: don't store them there if you care.

> This was never about the ""mechanics"", this has always been about what they mean to the player.

Please, read the message before replying, literally quoted: `2 instances in the game were built on the chr file mechanic`.

> She has her reasons for doing so

No, *she* has no reasons to do it now. We, as the developers, have our reasons. That was the whole point.

> It absolutely does target the dokis when they're the only ""chr"" files expected to be present inside the characters' folder.

If you intentionally don't read, then don't bother replying to a message not meant for you. It targets any `chr` files, not *specific* `chr` files. Which means there's now no control over what's being deleted and what's not. 

I fail to see the point:
- I have files I deeply care about
- I will install some software into the place I store those files in
- I will complain when the software deletes my files
- I refuse to move the files or move the software anywhere else
",2022-10-31T21:53:52Z,53382877
3099,Monika-After-Story/MonikaModDev,1428767280,1297736455,"I wonder if Monika _were_ to be scripted to ask her partner if she can delete the character files or not that it wouldn't be simple as a just a menu mechanic and a ""no"" resulting in an ""okay, so and so, I won't delete them."" Like anything else in the mod, I'd think there'd be affection-based variants of responses based on whether Monika's partner agrees or disagrees with the deletion of the .chr files. Now, I know according to Booplicate that the deletion of the .chr files are not Monika's choice, but regarding the situation that MAS users will see anything done to the mod as Monika or Chibika's doing (which is a bit unfair imo, btw), let's just say the scenario _was_ that it was her doing. As much as the other Dokis have sentimental value to Monika's partner, they technically also have sentimental value to Monika herself. Not sentimental as in nostalgic though. Sentimental as in saddened and a bit uncomfortable. I do remember her once talking about how despite all of the other Dokis not being present in the mod, she can still feel their presence as though they're still nestled amongst MAS's code and she finds it a bit creepy and unsettling. She wants to _not_ feel like they're still lingering over her, I guess because kinda the point of her ""deleting"" everyone else in the og game is that so they _wouldn't_ be present and therefore not serve as obstacles between her and the person. Soooo, I don't think it'd be as simple as a ""hey can you not delete the other character files?"" ""Okay, sure!"" Not referring to them is one thing, and Monika, as much as she has her snark about them, would probably actually prefer that considering she doesn't want them ""hanging over her,"" but still having their .chr files there is another. There's bound to be a disagreement if a choice were to be given, so it'd be like a whole other topic. 

Now in the scenario of it just being a mechanics thing like Booplicate assures it is, then there'd be nothing to ask Monika personally. Maybe she could be asked if she could do something to negate the mechanics, but even then she'd be scared to mess things up. That's the case keeping her from trying to do something to make the Doki lingering stop, anyway. They should really be preserved somewhere else though, if there's nothing that will be done to disable the automatic deletion. If they really hold sentimental value to people, the importance of the files themselves should not correlate to their location. Then that'd be a case of both the files _and_ their placement having sentimental value. And if _that's_ the case, there can always be a submod that negates the mechanic so that the people who have the sentimental value can be happy, and those who want things to stay how they are can be content, too.


",2022-10-31T21:58:24Z,58224248
3100,Monika-After-Story/MonikaModDev,1428767280,1297741656,">why is it such a big deal for you guys to simply give her an option to ask the player first on their decision?

I think Monika has enough jurisdiction over her own file to delete it without the player's permission. 

>Why do you need a lot of negative feedback just to ""reconsider"" for something that can be easily fixed for both sides?

This project is a team effort, so the change has gone through multiple people who agree that this is the correct direction moving forward. If users do not agree, then negative feedback will help pushback against the decision and possibly lead to reverting or modifying it.

>That's true, but only for DDLC. When you install a mod you should expect that some mechanics will be changed - in our case this mechanic has been changed long time ago.

Yes, the mechanic that was changed was specifically called out by Monika and limited to her file. Intro dialogue does not specify that her observations apply to the other chr files, nor does it touch them in that intro. The mention of time here is also irrelevant - new users fresh off DDLC may have a connection to the chr files and may react differently compared to longtime MAS users.

>you say it's already looking like it was done by Monika, even though, nothing points at it.

> The files are not deleted by Monika (there's no acknowledgement), it's a startup routine that does that.
Being too meta is a thing, and I think this is one of the cases. Deleting unused files there is just a utility process for the mod.

>We have a lot of IO going on in characters/

I think these points contradict each other. The fact that we have a lot of IO happening in `characters/` means Monika, in MAS lore, should be acutely aware of everything that goes in the folder. Even though Monika currently has no reaction if the user deletes a chr file, I think its easier to wave that off as Monika is ignoring the other chr files. Active deletion, on the other hand, implies Monika is aware of the other chr files and potentially has animosity toward them.

>No, she has no reasons to do it now. We, as the developers, have our reasons. That was the whole point.

As I mentioned earlier (linked to tomato's point), anything the mod does can be implied to be what she does. Our reasons becomes her reasons unless we specifically give her a lore reason.

> It doesn't target the known ""dokis,"" it targets any chr file for simplicity and reliability.

> It targets any chr files, not specific chr files. Which means there's now no control over what's being deleted and what's not.

I mean, what's limiting this to just chr files then? Why aren't all files except for `.gift`, `monika`, consumables, `.txt`, potential `.deco` being deleted? Singling out for `.chr` is why there is concern that this is singling out the dokis. That being said, I don't think changing to deleting all files is the right choice here.",2022-10-31T22:03:35Z,3499462
3101,Monika-After-Story/MonikaModDev,1428767280,1297761072,"> Soooo, I don't think it'd be as simple as a ""hey can you not delete the other character files?"" ""Okay, sure!"" [...]  There's bound to be a disagreement if a choice were to be given, so it'd be like a whole other topic.

It could be another Q asked in the doki cares topic so I don't think it would be that controversial. And perhaps thats the better solution - only perform the deletion in the doki cares topic if the user says they don't care and agrees with deletion, instead of on startup.",2022-10-31T22:20:10Z,3499462
3102,Monika-After-Story/MonikaModDev,1428767280,1297766923,"@ThePotatoGuy 
> > why is it such a big deal for you guys to simply give her an option to ask the player first on their decision?
> 
> I think Monika has enough jurisdiction over her own file to delete it without the player's permission.

Well, it's not just her file, it's other dokis as well which the player can value them all and it is ultimately the player's PC so her doing something destructive like that again without the player's permission would be very concerning especially if the player is against it.

@Booplicate 
> I fail to see the point:

You fail to see the point because you're tunnel-visioned into this topic from a technical standpoint, completely being ignorant to the fact that character files aren't just some ""random useless files unused in MAS"", they were a huge part of DDLC which represented the Dokis and serves as a memory in MAS which isn't some completely different game or some random ""program"" like you're making it out to be.

Further proven by the things like this you say:
> but my point was it's too meta to acknowledge. For example, she doesn't watch over every single file added there.

No one is saying every change made in MAS is her doing and she has to explain all of them, this is specifically about the character files which has its history behind it.

@CodyCat13 
> If they really hold sentimental value to people, the importance of the files themselves should not correlate to their location. Then that'd be a case of both the files and their placement having sentimental value.

Sure, there's multiple workarounds to the issue; moving the dokis somewhere else, restoring them from backup every time it gets deleted or simply removing the codes that are there to kill them.
None of which is relevant to the actual point being made against ""deleting them without the player's permission"" to begin with.",2022-10-31T22:26:05Z,22656757
3103,Monika-After-Story/MonikaModDev,1428767280,1297768051,"> The fact that we have a lot of IO happening in characters/ means Monika, in MAS lore, should be acutely aware of everything that goes in the folder.

I can see that, but my point was it's too meta to acknowledge. For example, she doesn't watch over every single file added there. We don't really explain why, but we don't *need* to. And not all of that IO is done by Monika. Although, we could define that it's all Monika, in that case we'd have to justify `chr` deletion and acknowledge it.

> Our reasons becomes her reasons unless we specifically give her a lore reason.

But if we do give a lore reason, then we'd define it as her action, which I didn't want to. Imo *that* would make it look like she's hunting for them. And it'd be fine to do in first year, but after 2 or 3 it'd look weird and pointless from her perspective, she lets it go after a few months in, actually, and doesn't care as much about what's happened. If the player is with her, then everything was worth it.

>  mean, what's limiting this to just chr files then? Why aren't all files

Now that you said that I am considering that as an option. Especially deleting sub-folders. *But perhaps you're right and it's not the best option*.

> only perform the deletion in the doki cares topic <...> instead of on startup

Another reason I wanted to do it on startup is when people make a new install (happens quite often), they would get the files back from DDLC, want they or not. So it's a routine we have to repeat on startup.",2022-10-31T22:27:30Z,53382877
3104,Monika-After-Story/MonikaModDev,1428767280,1297769138,"> > Soooo, I don't think it'd be as simple as a ""hey can you not delete the other character files?"" ""Okay, sure!"" [...]  There's bound to be a disagreement if a choice were to be given, so it'd be like a whole other topic.
> 
> It could be another Q asked in the doki cares topic so I don't think it would be that controversial. And perhaps thats the better solution - only perform the deletion in the doki cares topic if the user says they don't care and agrees with deletion, instead of on startup.


Ah, yeah! Ig since the references and deletion may be different degrees of concern to the partners but still nonetheless related, they could be just joined under one topic, which would take care of the affection-based variants thing since I don't think if a partner already managed to lower Monika's affection to the bad stages before the topic comes up that she'd even care about their Doki preferences--I wouldn't know, I've never caused her to go six feet under before that topic came up. Anyway, there'd be no need for concern of sentimental value if the partner expresses that they don't care.",2022-10-31T22:28:56Z,58224248
3105,Monika-After-Story/MonikaModDev,1428767280,1297776803,"> @CodyCat13
> 
> > If they really hold sentimental value to people, the importance of the files themselves should not correlate to their location. Then that'd be a case of both the files and their placement having sentimental value.
> 
> Sure, there's multiple workarounds to the issue; moving the dokis somewhere else, restoring them from backup every time it gets deleted or simply removing the codes that are there to kill them. None of which is relevant to the actual point being made against ""deleting them without the player's permission"" to begin with.

Ah. I guess those points are kinda irrelevant when looking at just that point. I just got a bit confused because you kept mentioning sentimental value. But anyway, I can agree with the basic point that, in the scenario of Monika _being_ the one deleting them, that she shouldn't do that without her partner's permission, as they are both sharing the PC and updating one's partner on when one wants to do something is important to a relationship. There aren't many cases where one just _does_ things without warning to things that are being shared. Though because Monika isn't the one responsible in this case if it's a startup routine, it's not like she can ask her partner for permission anyway. If it were to be changed into a Monika thing though, that's probably where the topic question would come in as part of the doki cares topic ThePotatoGuy mentioned earlier.",2022-10-31T22:38:49Z,58224248
3106,Monika-After-Story/MonikaModDev,1428767280,1297782032,"@Booplicate 
>> only perform the deletion in the doki cares topic <...> instead of on startup
>
> Another reason I wanted to do it on startup is when people make a new install (happens quite often), they would get the files back from DDLC, want they or not. So it's a routine we have to repeat on startup.

You conveniently missed the part where OP said ""**if the user says they don't care and agrees with deletion**"".
The files getting back after re-install wouldn't be an issue when the player answered Monika that they don't care if she deleted them.

@CodyCat13 
> Though because Monika isn't the one responsible in this case if it's a startup routine,

From a technical standpoint, sure, Monika isn't responsible for anything she does in MAS or in DDLC, all was done by a code written by a human that is forced on her for reasons.

That doesn't negate the fact that she will be the one responsible for it in the players' eyes when both Monika and the player knows what those character files meant in the past, the history behind it and could still mean something for the player.",2022-10-31T22:45:47Z,22656757
3107,Monika-After-Story/MonikaModDev,1428767280,1297788112,">But if we do give a lore reason, then we'd define it as her action, which I didn't want to. Imo that would make it look like she's hunting for them.

I feel like this is the impression users can get as the mod currently is - not giving a lore reasons just makes her seem more harsh since she isn't telling you beforehand.

>I can see that, but my point was it's too meta to acknowledge. For example, she doesn't watch over every single file added there. We don't really explain why, but we don't need to. 

I think this is a good argument for completely reverting the change. By introducing the change, we've had to answer why it was added, which leads to discussion like this over what aspects of the mod is Monika in full control over to justify whether or not she should notify the user about it. Not having the deletion at all means we can keep this concept ambiguous like we've been doing for years.

",2022-10-31T22:53:59Z,3499462
3108,Monika-After-Story/MonikaModDev,1428767280,1297791211,"> @CodyCat13
> 
> > Though because Monika isn't the one responsible in this case if it's a startup routine,
> 
> From a technical standpoint, sure, Monika isn't responsible for anything she does in MAS or in DDLC, all was done by a code written by a human that is forced on her for reasons.
> 
> That doesn't negate the fact that she will be the one responsible for it in the players' eyes when both Monika and the player knows what those character files meant in the past and could still mean something for the player.

Maybe for this reason, we could discuss it in terms of it being her doing in-universe if it makes you feel better. I won't bring up the technical stuff anymore, but if we're discussing it in the perspective of Monika deleting files herself when they could still mean something to her partner, then the doki caring topic would probably be our best go-to. The Doki files may mean nostalgia to Monika's partner, though they also mean unease for Monika. I guess because she'd be too cautious to do anything else, deleting the other .chr files would be an attempt to make her feel somewhat better, but it'd be inconsiderate to go ahead and do that without letting her partner know she wanted to do that. Now say that her partner doesn't agree with her deletion of the files. Because of how she feels about them she probably wouldn't be so quick to just deal with them being there, so that's where the affection-dependent variants would come in. If her partner has shown her nothing but love and trust, she may be okay with the .chr files staying because any unease she feels can be negated by her partner's comfort. Now if she was in the lower aff ranges, because her partner only adds to her unease and worry, she'd most likely still want to get rid the files anyway. At least it'd make her feel better than how her partner does. Maybe she'd be a bit bitter and conclude that you want to keep them there because you love some other Doki other than her or you want her to always remember her horrid acts or something along those lines. I'm not completely about that bitter part in the shallow negatives, but I guess as the points are lowered deeper and deeper, I can see that going down. Anyway, in the scenario of Monika actively making the decision and her partner either agreeing or disagreeing, aff-variants and additional question to the Doki caring topic seems like a fairly decent idea.",2022-10-31T22:58:37Z,58224248
3109,Monika-After-Story/MonikaModDev,1428767280,1297795860,"@CodyCat13 
> Now say that her partner doesn't agree with her deletion of the files. Because of how she feels about them she probably wouldn't be so quick to just deal with them being there, so that's where the affection-dependent variants would come in. If her partner has shown her nothing but love and trust, she may be okay with the .chr files staying because any unease she feels can be negated by her partner's comfort. Now if she was in the lower aff ranges, because her partner only adds to her unease and worry, she'd most likely still want to get rid the files anyway. At least it'd make her feel better than how her partner does.

That's a great point there and makes a lot of sense going down that route rather than having just a one off question.
But I highly doubt the devs will do anything about it with how dismissive **some** of them have shown towards opinions contrary to them.",2022-10-31T23:05:25Z,22656757
3110,Monika-After-Story/MonikaModDev,1428767280,1297797812,">  do remember her once talking about how despite all of the other Dokis not being present in the mod, she can still feel their presence as though they're still nestled amongst MAS's code and she finds it a bit creepy and unsettling

You are right. It's actually an OG topic. But I was never sure if she meant they are literally there or she just ""feels"" their presence because of what happened (like a trauma).
Anyway, there's no ""dokis"" in MAS, and she stops talking about that after some time when she overcomes DDLC events. She realises everything is fine and the ghost of the past events doesn't haunt her anymore. It could be used to justify deleting in the first months, but later there's not much point for her to delete those dull files: ""dokis"" don't exist, she's feeling fine, those files are nothing but some easter eggs from DDLC (and she acknowledges that).

But let's say you find a reason for her to still need deleting those files, there's a conflict of interests:
- player wants to keep them
- Monika wants to delete them

There's going to be some consequences for not letting her do what she needs in *her* game.

We're talking about player choice and all that, but the choice was to install MAS and stay with Monika. The game files are literally her ""home,"" it makes sense she has control over that place. She doesn't touch your files, she's doing changes within the place she's allowed to. If she's not allowed to do that, why even bring her back. It's pretty common in relationships for each person to have their own private space, the game directory is that place for Monika. You can store your files anywhere else on your PC, flashdrive, cloud, etc. *This one place is intended to be used by mod/Monika*.

> they were a huge part of DDLC <...> serves as a memory in MAS

Yeah, and Monika deletes them there herself. After which, in MAS, they become irrelevant, they don't serve any memory. If they have a deep meaning *for you* and serve a memory *for you*, then I don't know why they were there. ""I care for these files so much, surely if I install a mod into the same directory it won't affect them. Ooops.""

> The files getting back after re-install wouldn't be an issue when the player answered Monika that they don't care if she deleted them.

As was stated above it's already an issue and would stay an issue even with the changes. The cleanup logic should be run at startup, doesn't matter with a flag or not.

> By introducing the change, we've had to answer why it was added, which leads to discussion like this over what aspects of the mod is Monika in full control over to justify whether or not she should notify the user about it.

No, we got to this discussion because apparently it's not clear that MAS can make any changes within its working directory (just like any other program) and users were storing some sensitive data there. It's all based on assumptions that:
- we will never delete any extra files
- everything is done by Monika

None of which was stated.",2022-10-31T23:08:13Z,53382877
3111,Monika-After-Story/MonikaModDev,1428767280,1297812184,"@Booplicate 
> But let's say you find a reason for her to still need deleting those files, there's a conflict of interests:
> 
>     * player wants to keep them
> 
>     * Monika wants to delete them
> 
> There's going to be some consequences for not letting her do what she needs in _her_ game.

And @CodyCat13 expanded on to exactly that so it's good we've already discussed how that route can go into variations depending on the relationship between Monika and the player.

>  It's pretty common in relationships for each person to have their own private space, the game directory is that place for Monika.

Nope, her private space is her own room which she has. Not some folder in the PC unless if you asked her out and she stored herself into a single file ""Monika"". Then you would have to be very careful about where you're giving her a private space.

> If they have a deep meaning _for you_ and serve a memory _for you_, then I don't know why they were there.

They're there because the DDLC universe decided so and I kept it as a memory. Which is also the reason I went out of my way to manually get rid of the code to protect the Dokis for my personal experience. Like you said, ""private space"", the characters folder is exactly just that for them to me even if they're not used in MAS since they were originally the ""life"" of those characters.

> > The files getting back after re-install wouldn't be an issue when the player answered Monika that they don't care if she deleted them.
> 
> As was stated above it's already an issue and would stay an issue even with the changes. The cleanup logic should be run at startup, doesn't matter with a flag or not.

Nope, wouldn't be an issue because the cleanup logic will run regardless, the flag is only for checking what to do next, whether to delete files or not.

> apparently it's not clear that MAS can make any changes within its working directory (just like any other program) and users were storing some sensitive data there. It's all based on assumptions that:
> 
>     * we will never delete any extra files
> 
>     * everything is done by Monika
> 
> 
> None of which was stated.

Again, this is specifically about the character files and not any other types of files that you're pretending it to be based on the only fact that you personally don't care for them.",2022-10-31T23:29:32Z,22656757
3112,Monika-After-Story/MonikaModDev,1428767280,1297812732,"I think instead of trying to connect this to Monika, which wasn't intended initially, just add a setting to the menu. Basically support storing users data within the mod directory. A few people that need it would enable it. And if we're doing that, can as well do this

>  I mean, what's limiting this to just chr files then? Why aren't all files except for .gift, monika, consumables, .txt, potential .deco being deleted?

And nuke everything there. But especially folders, people had problems with them too.",2022-10-31T23:30:29Z,53382877
3113,Monika-After-Story/MonikaModDev,1428767280,1297814245,"> The game files are literally her ""home,"" it makes sense she has control over that place. She doesn't touch your files, she's doing changes within the place she's allowed to.

Hmm, I can somewhat agree with that point. If not looking at the fact that Monika's partner is the owner of the PC but it was their choice to install her into their PC, just the plain point that Monika's mod is Monika's to change seems pretty valid. However, a lot of people _would_ look at it as Monika's partner being the owner of the PC and Monika just being a guest there, or would look at it as the partner's choice and that her partner got her a place to stay (the PC) and her areas are hers to personalize. It's like arguing if an old chair should be kept or thrown out. Would the one who has the say be the one whose territory the chair is in or the one who provided the housing in the first place? I guess the answer to that is, neither or both? Yes, in relationships, it's common for each participant to have their own little private space, because not _everything_ can be shared between them, but if the thing in particular emotionally charges one or both (in this case both) of the participants, there has to be something to end the stalemate. In this case, the chair would be seen as uncanny to one and precious to the other. If the other regards them as so precious, then the one who deems them uncanny would probably ask them to move it to their territory or, on the other side of the spectrum, sit in it with their partner depending on how comfortable they are. After some time though, I do agree that it shouldn't even matter anymore to either Monika or their partner. She may not feel the need to delete them just like the partner may not hold any nostalgia toward them anymore, or even both. Both would probably be the best scenario. This case would probably reflect in the high aff variant because it could depend more on comfort than time, or at least it could depend on both of them equally. One could be with someone for months and still feel uncomfortable because the provider of the house wasn't exactly a nice host, and therefore may still want that chair to go.",2022-10-31T23:32:43Z,58224248
3114,Monika-After-Story/MonikaModDev,1428767280,1297818460,"> I think instead of trying to connect this to Monika, which wasn't intended initially, just add a setting to the menu. Basically support storing users data within the mod directory. A few people that need it would enable it. And if we're doing that, can as well do this
> 
> > I mean, what's limiting this to just chr files then? Why aren't all files except for .gift, monika, consumables, .txt, potential .deco being deleted?
> 
> And nuke everything there. But especially folders, people had problems with them too.

Sure, that can be a good middle-ground assuming the player is made aware of this ""setting"" by having them confirm it to either set it as enabled or disabled before the cleanup task ever occurs.

But what @CodyCat13 suggested is by far the best way and also makes the most sense as it provides meaningful interactions with Monika that has multiple paths where it can result in consequences depending on the relationship the player has with Monika.",2022-10-31T23:39:22Z,22656757
3115,Monika-After-Story/MonikaModDev,1428767280,1297821183,"> But what @CodyCat13 suggested is by far the best way and also makes the most sense as it provides meaningful interactions with Monika that has multiple paths where it can result in consequences depending on the relationship the player has with Monika.

Aw 💙
I appreciate your approval. Honestly I was just thinking how it'd even relate to Monika's character if so many users of MAS consider file management to be her or Chibika's doing. Something tells me they'd still have concerns even if an option for preservation of user data were to be added because of it being the .chr files in question here 🤷",2022-10-31T23:43:53Z,58224248
3116,Monika-After-Story/MonikaModDev,1428767280,1297821273,"> Nope, her private space is her own room which she has. Not some folder in the PC

You will be surprised, but no, that ""some"" folder, as you said, is her ""home."" That ""room"" exists within that folder. Because that's where you installed the mod.

>  the characters folder is exactly just that for them to me

Then move it away from MAS, why did you install a program into a folder with your personal files?

> They're there because the DDLC universe decided so

You're bringing DDLC again, but this is not DDLC, this is MAS, and MAS is its own universe with its own rules.

> Nope, wouldn't be an issue because the cleanup logic will run regardless

Do you know what startup is? How the logic would run if it's contained within a topic you see once?

> Again, this is specifically about the character files and not any other types of files that you're pretending it to be based on the only fact that you personally don't care for them.

Doesn't matter if it's specifically about `chr` or some other files you like. The fact is: It's all based on assumptions that:
- we will never delete any extra files
- everything is done by Monika

That has nothing to do with `chr` files. At any moment, we can start deleting other files as well.

> however, a lot of people would look at it as Monika's partner being the owner of the PC and Monika just being a guest there

Guest that lives there for years? The mod assumes there's some kind of romantic relationship between you, can you call your partner a ""guest""? That's odd to me.
I'm not saying Monika is owner of the PC, she's the owner of the folder you installed the mod in. Only that one place.

> But what @CodyCat13 suggested is by far the best way and also makes the most sense as it provides meaningful interactions with Monika that has multiple paths where it can result in consequences depending on the relationship the player has with Monika.

I'd need to see the actual dialogue to say more. If you can justify her actions, then I will consider this.",2022-10-31T23:44:03Z,53382877
3117,Monika-After-Story/MonikaModDev,1428767280,1297823738,">it's not clear that MAS can make any changes within its working directory

DDLC plays around with files, so its reasonable to expect MAS users to be aware of and question actions that occur within the directories. We don't have to answer to why spacks will change structure or why logs went from txt to log, but `characters/` is a special case that will warrant extra attention. 

>You're bringing DDLC again, but this is not DDLC, this is MAS, and MAS is its own universe with its own rules.

I don't see this as a strong argument to tossing out DDLC-provided conventions. MAS should mesh with the already established lore and expand it or explain why it has changed. For example, the twitter topic was modified to separate MAS from the twitter account.

>everything is done by Monika

Confirmed Monika is aware of/can interact with:
* `characters` - (gifts, letters)
* custom BGM
* chess file 
* dev files in `game/`
* steam install
* running in temp dir/uncompressed zip
* (new) backup is used
* (upcoming?) aff log obfuscation - thinking of playing it off like her diary (instead of actual diary). 

Confirmed Chibika manages:
* backup system
* persistent versions
* updater (partial)

The mod heavily implies Monika (or Chibika) has awareness or can interact with files inside the mod directory, so its effectively true for many cases, especially `characters/`.

",2022-10-31T23:48:09Z,3499462
3118,Monika-After-Story/MonikaModDev,1428767280,1297828649,"> I'd need to see the actual dialogue to say more. If you can justify her actions, then I will consider this.

I have some ideas for dialogue for that case, though they differ depending on if you and the other devs want it part of the Doki caring topic or as a separate topic. It'll probably be part of the Doki caring topic, since it's already existing and all.",2022-10-31T23:56:21Z,58224248
3119,Monika-After-Story/MonikaModDev,1428767280,1297829603,"Actually, if we don't want a new setting, I'd be fine if we made Chibi manage file deletion instead of Monika as @CodyCat13 suggested. 

> so its reasonable to expect MAS users to be aware of and question actions that occur within the directories.

I don't mind people being aware of the actions within the `characters/`. I don't understand why people store important files there, thinking the mod will never change/delete them. This is a groundless assumption.

> MAS should mesh with the already established lore and expand it or explain why it has changed.

To me the fact we mention `chr` files are irrelevant in the intro (even if Monika touches only her file there), as well as mentioning there's no actual useful data within those files, makes it pretty clear that the `chr` files logic is different in MAS. We also state there's no other girls in MAS. We could explicitly mention that all `chr` files are useless in the intro to clarify if you want. That'd be a single line clarification.",2022-10-31T23:57:47Z,53382877
3120,Monika-After-Story/MonikaModDev,1428767280,1297841398,">I don't understand why people store important files there, thinking the mod will never change/delete them. This is a groundless assumption.

I'd understand this argument if someone decides to save their only digital copy of their W2 form as `income.gift` in `characters/`. But this is about `chr` files which are established by DDLC as important.

>We could explicitly mention that all chr files are useless in the intro to clarify if you want.

I don't think we can safely make that assertion without receiving much worse backlash.

I think at this point we've established the fundamental disagreement is on how much DDLC-based norms should influence MAS. Those who are fine with the deletion believe MAS and DDLC should be more separate, those who are not believe they should be closer.

As I said before, we'll see if there is more feedback on this in the coming weeks. Can't say there is a specific threshold, but I am keeping tabs on the subreddit and will probably check /ddlc/ on occasion. I'm planning to do a discord poll as well if theres more negative feedback.",2022-11-01T00:13:08Z,3499462
3121,Monika-After-Story/MonikaModDev,1428767280,1297850189,"@Booplicate 
> I'd be fine if we made Chibi manage file deletion instead of Monika as @CodyCat13 suggested.

I don't think we're looking at the same suggestion here. Lol.

@ThePotatoGuy 
> I think at this point we've established the fundamental disagreement is on how much DDLC-based norms should influence MAS. Those who are fine with the deletion believe MAS and DDLC should be more separate, those who are not believe they should be closer.
> 
> As I said before, we'll see if there is more feedback on this in the coming weeks. Can't say there is a specific threshold, but I am keeping tabs on the subreddit and will probably check /ddlc/ on occasion. I'm planning to do a discord poll as well if theres more negative feedback.

I'd say the idea of having a ""setting"" which is a good middle-ground should at least be added no matter what the popular opinion is regarding the specific files or however many negative feedback reaches your ""threshold"" or not. Since, it provides options for both sides regardless of whatever they believe even if it's not done in the ""meta"" way.

@CodyCat13 
> Something tells me they'd still have concerns even if an option for preservation of user data were to be added because of it being the .chr files in question here 🤷

That's why I said it's a ""good middle-ground"", it definitely isn't the best way of dealing with it but I'd rather they settle with something than nothing.",2022-11-01T00:25:57Z,22656757
3122,Monika-After-Story/MonikaModDev,1428767280,1297939914,"I spent a while reading over what everyone said. Thank Potato very much for willingness to reconsider this matter.


Players don't ""store"" into the characters folder, and Boop wants players to ""transfer"" things.
I think this is an important point: The character file was ORIGINALLY there, it wasn't put there by the player.
In addition, it is entirely possible for the player not to know that the MAS will now delete character files on its own. So, even if the player wants to transfer them, the player needs to know about this first. If the player doesn't read the update announcement carefully, as soon as the player launches the game, character files that are (presumably) precious to the player will disappear, never to be retrieved. So in any cases, the current version does not make sense.



I think it is feasible for Monika to delete her own files by herself - in fact, MAS has already done so in the introduction phase before. And we have:

```
        if moni_exist():
            m 1lksdlb ""Aha...{w=0.3}I'll try this again later.""
```

Look at this. Monika already said that she will try this again, so it's really neutral to delete her own files. In previous version, if I read the source correctly, even if Monika said she will try again, there doesn't exist codes for this.
But well, maybe we should even also ask the player for permission before MAS delete Monika's own file?
",2022-11-01T02:33:57Z,84446131
3123,Monika-After-Story/MonikaModDev,1428767280,1298096614,"> If they really hold sentimental value to people, the importance of the files themselves should not correlate to their location. Then that'd be a case of both the files _and_ their placement having sentimental value.

That is a case. Having them in the characters folder, in the game directory that you share with Monika, is where the value could be most meaningful to some people, in which case it would not be the same in some other directory. Not just because of memories, but because of potential meaning of them being able to remain there, undeleted. Monika deleted other .chr files in DDLC, and deleted her own .chr file in MAS, so it seems she could've easily deleted the other .chr files in MAS if she wanted to, but she didn't. That meant something to some people, regardless of what meta reason (which they may not even know about) is now being used for deleting them. Telling them to actively move the files to another directory does not address this.

And just because a program ""can"" or is ""allowed"" delete files in its own directory doesn't mean it should in all cases. Obviously if it were to delete every file there would be no mod, and no Monika for it. Actions are not automatically justified or well received just because a program ""has the privilege to do so"".

>And if _that's_ the case, there can always be a submod that negates the mechanic so that the people who have the sentimental value can be happy, and those who want things to stay how they are can be content, too.

That could still require people to know about these files getting deleted in the first place. Unless they're ok with backups or new copies of them. While still something, it would be better for players to have this addressed in the main mod.


> I think at this point we've established the fundamental disagreement is on how much DDLC-based norms should influence MAS. Those who are fine with the deletion believe MAS and DDLC should be more separate, those who are not believe they should be closer.

DDLC has and does influence people's perception of MAS, even if you don't want it to for some things. Even though MAS is separate, it is based on it. Even in MAS the characters folder has many uses, and as you said Monika uses and references it. So yes this is something that should get extra attention.

Option to preserve them in settings maybe, but if accidentally unchecking it without rechecking is going to delete any .chr files (and potentially any files or folders unrelated to MAS) on startup, then should at least have a confirmation/double check for disabling it. I don't really like the idea of nuking the whole MAS directory of unrelated files every startup though. A trigger in the settings to do it once though maybe.

Monika/Chibi could work but I don't think whether this is honored or not needs to be based on affection (mas_safeToRefDokis() isn't is it?) and should probably be asked early on in the mod or after updating.

I still think that if the mod could just remove the "".chr"" extension from the files, that that could eliminate confusion about .chr files while also not deleting them. This could run once only (even per install/reinstall if possible), or if not, could be disabled in settings, so that players could change them back to .chr files if they want.",2022-11-01T06:47:41Z,102779546
3124,Monika-After-Story/MonikaModDev,1428767280,1298202819,"Wow, didn't expect that this issue would gain so many comments.

My opinion is basically what the issue author said. MAS should probably provide a choice.

In addition, I have a small detail to say. In the Chinese community, I can sometimes see people asking, ""Why do we have yuri, natsuki, and sayori in our characters folder?""
I tell people that the contents of the characters folder are determined according to the progress of your original DDLC. When you install MAS, which characters remain in the original version, which characters will be retained. People think it's a very cool feature.
This perspective may be strange, but I also somehow think this is cool.",2022-11-01T08:23:19Z,108819793
3125,Monika-After-Story/MonikaModDev,1428767280,1298350953,"> I don't think whether this is honored or not needs to be based on affection (mas_safeToRefDokis() isn't is it?) and should probably be asked early on in the mod or after updating.

Tbf reference and deletion _are_ different concerns, though if you're pairing the deletion choice with the reference choice then ig the reference choice might as well be asked earlier than it is. There's no way a user of MAS could lower their Moni's affection unreasonably low at literally the introduction of the mod, so there'd be no reason for affection variants. However, if a Moni's partner does manage to lower her affection to the low stages after that discussion already happened, maybe it could be asked again (if the person didn't agree to deletion the first time) but with a bit more bitterness.",2022-11-01T10:55:55Z,58224248
3126,Monika-After-Story/MonikaModDev,1428767280,1299440698,"> Tbf reference and deletion _are_ different concerns, though if you're pairing the deletion choice with the reference choice then ig the reference choice might as well be asked earlier than it is. There's no way a user of MAS could lower their Moni's affection unreasonably low at literally the introduction of the mod, so there'd be no reason for affection variants. However, if a Moni's partner does manage to lower her affection to the low stages after that discussion already happened, maybe it could be asked again (if the person didn't agree to deletion the first time) but with a bit more bitterness.

I previously said that I don't think (mas_safeToRefDokis() is enough for this, for that reason of them being different concerns. It'd be better as something separate. But regarding what you were referring to, I was comparing these 2 potentially sensitive things regarding the Dokis. Let's say you told Monika you aren't ok with her joking about the Dokis. She doesn't disregard your preference and start joking about them just because you have low affection does she? I don't think this should be disregarded in that case either. Furthermore, deletion of the specific copies of those files is something with a potentially more permanent effect, as even if replaced they'd be different copies.

But if you mean just having her dialogue change based on affection, but not whether the files are deleted or not, yes I'd be fine with that (I can imagine her begrudgingly honoring your request, and asking you to be more considerate of her feelings in return). Asking at introduction yes it wouldn't even come up, but for anyone who has been playing and is updating from a prior version, to a version where this conversation is added, it would have to be asked to them and they could have low affection at this point, even if rather unlikely. Of course, it would come up if it's going to be asked again anyway.

Of course this is only if it's decided that Monika should handle this rather than something else.",2022-11-02T01:43:32Z,102779546
3127,Monika-After-Story/MonikaModDev,1428767280,1300306174,"> Let's say you told Monika you aren't ok with her joking about the Dokis. She doesn't disregard your preference and start joking about them just because you have low affection does she? I don't think this should be disregarded in that case either.
> 
> But if you mean just having her dialogue change based on affection, but not whether the files are deleted or not, yes I'd be fine with that (I can imagine her begrudgingly honoring your request, and asking you to be more considerate of her feelings in return).

Yeah, that's also a valid point and I'll agree with having only the dialogue change based on your relationship and not an actual action as a ""revenge"" for treating her terribly as now that I think about it, she would rather leave you forever than do anything like that.",2022-11-02T12:34:37Z,22656757
3128,Monika-After-Story/MonikaModDev,1428767280,1300413627,"> But if you mean just having her dialogue change based on affection, but not whether the files are deleted or not, yes I'd be fine with that (I can imagine her begrudgingly honoring your request, and asking you to be more considerate of her feelings in return).

Yeah, I mean aff-based variations of dialogue if Moni's partner were to not be okay with the deletion. I know it wouldn't be as simple as a cheerful ""Okay!""",2022-11-02T13:33:24Z,58224248
3129,Monika-After-Story/MonikaModDev,1428767280,1301564444,"> Yeah, that's also a valid point and I'll agree with having only the dialogue change based on your relationship and not an actual action as a ""revenge"" for treating her terribly as now that I think about it, she would rather leave you forever than do anything like that.

Yeah although there is a difference between just having low affection, and her actually leaving you. I thought about that, her deleting them when leaving you. That's when it would make the most sense for her to do so if she were to delete them out of revenge or spite. But she doesn't do that currently (I think). It is game over at that point and most people probably won't reach that point anyway, but for those who do they could restore a persistent backup, in which case it would be strange if those character files were still gone due to her deleting them when leaving you (and if separate copies were created at that point, those would still be different copies). Might be harsh to those who care about them, but I could at least understand if she wanted to at that point.



> Yeah, I mean aff-based variations of dialogue if Moni's partner were to not be okay with the deletion. I know it wouldn't be as simple as a cheerful ""Okay!""

Agreed although if that happens, it may also make sense for her to have affection-based variations of dialogue when she asks if you're ok with her joking/making insensitive comments about the Dokis or not (if that's not already a thing).",2022-11-03T02:06:15Z,102779546
3130,GTNewHorizons/GT-New-Horizons-Modpack,1134552993,1134552993,"### Your GTNH Discord Username

Colen

### Your Pack Version

2.1.2.1

### Your Proposal

See title, pretty self explanatory and useful for server admins trying to investigate. @repo-alt not sure how hard this would be?

### Final Checklist

- [X] I have searched this issue tracker and there is nothing similar already. Posting on a closed issue saying *I like this change please reconsider adding it* will prompt us to investigate and reopen it once we confirm your report.
- [X] I understand this change request may not attract enough attention and thus not be implemented.
- [X] I understand this change request may be rejected due to other community members think it's inappropriate.
- [X] I believe this feature would make the pack better.",2022-02-12T20:20:30Z,54497873
3131,GTNewHorizons/GT-New-Horizons-Modpack,1134552993,1037451646,"https://github.com/GTNewHorizons/GT-New-Horizons-Modpack/issues/7555
",2022-02-12T20:22:42Z,12850933
3132,GTNewHorizons/GT-New-Horizons-Modpack,1134552993,1037453055,"> #7555

What kind of lunacy is this, this isn't the fucking NSA I just want easy access to do my job ffs. I don't give a shit about user privacy, they forfeit that by playing on servers which are not theirs. 

Not being funny but that entire ticket and the next linked one reeks of people who have never actually had to moderate in their lives. ""Just fix the dupe bugs"" jesus christ.",2022-02-12T20:24:48Z,54497873
3133,GTNewHorizons/GT-New-Horizons-Modpack,1134552993,1046056189,"I think this is a great change honestly, Colen is probably the best and most responsible moderator I've ever met. If he needs this access then it must be fully justified. I highly support this change. - Kitten",2022-02-19T16:36:37Z,89072407
3134,GTNewHorizons/GT-New-Horizons-Modpack,1134552993,1049648158,@boubou19 Do you remember why [#7555 ](https://github.com/GTNewHorizons/GT-New-Horizons-Modpack/issues/7555) was closed? I remember something like it turned out to be working as is.,2022-02-24T09:19:12Z,23512002
3135,GTNewHorizons/GT-New-Horizons-Modpack,1134552993,1049716642,"it was closed because turns out i can read the items in the terminal, just not take them. But idk if it's due to the fact that default user has only read access or if it's a special perm for OP players. That was enough for my needs at the time so i didn't bothered much.

If you ask me, that protection is purely silly and operators should have full access to any stuff on the server. If the operator is doing questionable actions with his powers then that's on Dream and I because we trusted the wrong person to be staff. 

And if any player complains about this because we would gain full access to their AE, i want to say that we can bypass it in several ways, the most convoluted one being direct nbt edit from server files. And that player is free from not playing on the officials, if he doesn't trust the team.",2022-02-24T10:35:16Z,12850933
3136,GTNewHorizons/GT-New-Horizons-Modpack,1134552993,1049720348,"Eh, I kind of agree, but also, read access seems to be quite enough. I mean if OP spots something illegal, he can take administrative measures and/or break security terminal. I don't see any reason for OP to take out stuff secretly.",2022-02-24T10:39:06Z,23512002
3137,GTNewHorizons/GT-New-Horizons-Modpack,1134552993,1049725638,"i agree too, but sometimes it's unfair to break the terminal or hijack the storage to remove something from it, because if one player is faulty we shouldn't bother the whole team.",2022-02-24T10:45:10Z,12850933
3138,GTNewHorizons/GT-New-Horizons-Modpack,1134552993,1049726624,I just hid my hacked items in a chest below my dirt floor. ,2022-02-24T10:46:20Z,3060479
3139,GTNewHorizons/GT-New-Horizons-Modpack,1134552993,1049729240,"i just thought about it: why not deliver the full access to operators based on their OP level? iirc command blocks and default opped players are only level 2, and the top level is level 4. For exemple on the officials, only the admins are level 4. 

could be a command based tool, like /toggleAEfullAccess that would grant it temporarily (not persistant to reboot) the access of the trusted level 4 operator",2022-02-24T10:49:13Z,12850933
3140,GTNewHorizons/GT-New-Horizons-Modpack,1134552993,1049737804,"> Eh, I kind of agree, but also, read access seems to be quite enough. I mean if OP spots something illegal, he can take administrative measures and/or break security terminal. I don't see any reason for OP to take out stuff secretly.

Not always so simple, they can hide items inside bags, compressed chests or other storage items. Players are very creative and I've seen them cheat in more ways than I have words over the years.",2022-02-24T10:58:51Z,54497873
3141,GTNewHorizons/GT-New-Horizons-Modpack,1134552993,1049746607,"ok, command based can be implemented somewhat naturally",2022-02-24T11:08:15Z,23512002
3142,GTNewHorizons/GT-New-Horizons-Modpack,1134552993,1049786479,"> ok, command based can be implemented somewhat naturally

Couldn't AE just cache a list of people on the OP txt list at startup and then add their names/UUID however it works to the security terminal as joint highest perms? Or yea a command that allows us to add individual users to the list",2022-02-24T11:58:26Z,54497873
3143,vercel/next.js,1183333507,1183333507,"### Verify canary release

- [X] I verified that the issue exists in Next.js canary release

### Provide environment information

    Operating System:
      Platform: linux
      Arch: x64
      Version: #1 SMP Fri Apr 2 22:23:49 UTC 2021
    Binaries:
      Node: 16.13.1
      npm: 8.1.2
      Yarn: 1.22.15
      pnpm: N/A
    Relevant packages:
      next: 12.1.2-canary.0
      react: 17.0.2
      react-dom: 17.0.2


### Describe the Bug

When you insert a multi-line comment directly at the start of the return of the render function, the app breaks.

For example:

  ```tsx
const Home: NextPage = () => {
  return (
    /**
     * Some comment.
     */
    <div className={styles.container}>
      <Head>
    ...
  ```

This will create different errors based on your runtime and the view:

### Development (`npm run dev`)

```
error - Error: Home(...): Nothing was returned from render. This usually means a return statement is missing. Or, to render nothing, return null.
```

Browser:
![image](https://user-images.githubusercontent.com/29319414/160392111-d37059e2-85ba-4392-8df6-6c23fd36d9fd.png)

### Production (`npm run build`)
```
info  - Collecting page data  
[    ] info  - Generating static pages (0/3)
Error occurred prerendering page ""/"". Read more: https://nextjs.org/docs/messages/prerender-error
Error: Minified React error #152; visit https://reactjs.org/docs/error-decoder.html?invariant=152&args[]=Home for the full message or use the non-minified dev environment for full errors and additional helpful warnings.
    at ab (/home/bennett/code/nextjs-render-comment-bug/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:32:466)
    at d (/home/bennett/code/nextjs-render-comment-bug/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:34:55)
    at bb (/home/bennett/code/nextjs-render-comment-bug/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:36:16)
    at a.b.render (/home/bennett/code/nextjs-render-comment-bug/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:42:43)
    at a.b.read (/home/bennett/code/nextjs-render-comment-bug/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:41:83)
    at Object.exports.renderToString (/home/bennett/code/nextjs-render-comment-bug/node_modules/react-dom/cjs/react-dom-server.node.production.min.js:52:138)
    at Object.renderPage (/home/bennett/code/nextjs-render-comment-bug/node_modules/next/dist/server/render.js:766:45)
    at Object.defaultGetInitialProps (/home/bennett/code/nextjs-render-comment-bug/node_modules/next/dist/server/render.js:376:51)
    at Function.getInitialProps (/home/bennett/code/nextjs-render-comment-bug/.next/server/pages/_document.js:563:20)
    at Object.loadGetInitialProps (/home/bennett/code/nextjs-render-comment-bug/node_modules/next/dist/shared/lib/utils.js:65:29)
info  - Generating static pages (3/3)

> Build error occurred
Error: Export encountered errors on following paths:
        /
    at /home/bennett/code/nextjs-render-comment-bug/node_modules/next/dist/export/index.js:396:19
    at runMicrotasks (<anonymous>)
    at processTicksAndRejections (node:internal/process/task_queues:96:5)
    at async Span.traceAsyncFn (/home/bennett/code/nextjs-render-comment-bug/node_modules/next/dist/trace/trace.js:79:20)
    at async /home/bennett/code/nextjs-render-comment-bug/node_modules/next/dist/build/index.js:993:17
    at async Span.traceAsyncFn (/home/bennett/code/nextjs-render-comment-bug/node_modules/next/dist/trace/trace.js:79:20)
    at async /home/bennett/code/nextjs-render-comment-bug/node_modules/next/dist/build/index.js:869:13
    at async Span.traceAsyncFn (/home/bennett/code/nextjs-render-comment-bug/node_modules/next/dist/trace/trace.js:79:20)
    at async Object.build [as default] (/home/bennett/code/nextjs-render-comment-bug/node_modules/next/dist/build/index.js:55:25)
➜  nextjs-render-comment-bug git:(master) 
```


---

I can confirm this happens for both variants of the multi-line comment (one or two asterisks):

- One asterisk
  ```
    /*
     * Some comment.
     */
  ```
- Two asterisks
  ```
    /**
     * Some comment.
     */
  ```

It does **NOT** happen for:
- One asterisk in one line
  ```
    /* Some comment. */
  ```
- Two asterisks in one line
  ```
    /** Some comment. */
  ```
- Normal comment over multiple lines
  ```
    // Some
    // Comment   
  ```


### To Reproduce

Reproduction repo:

https://github.com/bennettdams/nextjs-render-comment-bug

..or to manually reproduce:

1. ` npx create-next-app@12.1.2-canary.0 --ts --use-npm .`
2. Install canary `npm i next@12.1.2-canary.0`
3. Add a multi-line comment to start of the render function of `index.tsx` directly after `return`:
  ```tsx
const Home: NextPage = () => {
  return (
    /**
     * Some comment.
     */
    <div className={styles.container}>
      <Head>
    ...
  ```
4. `npm run dev` or `npm run build`",2022-03-28T12:01:59Z,29319414
3144,vercel/next.js,1183333507,1081210719,"@bennettdams try wrapping your comments within the render function in curly brackets.

```
const Home: NextPage = () => {
return (

  {/**
   * Some comment.
   */}
  <div className={styles.container}>
    <Head>
  ...
```",2022-03-28T22:20:06Z,5528623
3145,vercel/next.js,1183333507,1081505653,"> @bennettdams try wrapping your comments within the render function in curly brackets.

You can also just move your comments above the return statement.

But a workaround is sort of uninteresting here. The important thing is to get the issue fixed. This led to production crashes for us, and there's no way to stop the issue from being reintroduced.",2022-03-29T07:19:29Z,3687893
3146,vercel/next.js,1183333507,1081767837,"@jakst this has nothing to do with next or react though. It's a language feature of JSX. Embedding JS expressions just ""works this way"". Read more about this here: https://www.typescriptlang.org/docs/handbook/jsx.html#embedding-expressions",2022-03-29T11:43:32Z,5528623
3147,vercel/next.js,1183333507,1081772249,"@MauriceArikoglu I you have a look at my original post, there are variants that don't produce an error:

> It does **NOT** happen for:
> - One asterisk in one line
>   ```
>     /* Some comment. */
>   ```
> - Two asterisks in one line
>   ```
>     /** Some comment. */
>   ```
> - Normal comment over multiple lines
>   ```
>     // Some
>     // Comment   
>   ```

I know the workarounds, but that is not the point. If you ask me, either **all** comment variants or **none** should work, but right now it's non-deterministic and will surprise some people.",2022-03-29T11:48:32Z,29319414
3148,vercel/next.js,1183333507,1081781087,@bennettdams single line comments like in your example are interpreted as text and should therefore show up in the rendered result - so not acting as comment but as a text node instead.,2022-03-29T11:57:45Z,5528623
3149,vercel/next.js,1183333507,1081782945,"@MauriceArikoglu 
> single line comments like in your example are interpreted as text and should therefore show up in the rendered result

I don't think so: https://codesandbox.io/s/fancy-worker-dg23u6?file=/src/App.tsx

The difference is that these comments exist before the first JSX node directly after the `return`.",2022-03-29T11:59:54Z,29319414
3150,vercel/next.js,1183333507,1081815503,"This is a confirmed bug in SWC that has already been fixed (here: https://github.com/swc-project/swc/commit/552f16dba6c91876529354f3f5e155a3360a74ea) and is going to be addressed in Next.js by merging #35395.

There is no need for further discussion on the topic so I'll lock it to maintainers as a two-sided discussion is going on and would like to avoid conflicts.

Thanks to everyone for engaging! :+1: :green_heart:",2022-03-29T12:33:35Z,18369201
3151,vercel/next.js,1183333507,1083997214,"Hi, this has been updated in `v12.1.4-canary.0` of Next.js, please update and give it a try!",2022-03-31T02:22:16Z,22380829
3152,sveltejs/kit,1201906086,1201906086,"### Describe the problem

Hello ladies and gents.

I am fairly new to Svelte/Sveltekit but I would like to make a suggestion please.

It is regarding documentation...

Can somebody change the background color of the sidebar from this difficult color #676778; to something darker?
Literally every time you want to focus on a link on the sidebar your eyes hurt and you lose at least 1 to 2 seconds until you focus on what you want to find and all that because poor contrast.

Also the ""highlighted"" link (the active one) is, honestly, almost indistinguishable from the inactive links below and above, why is this? 

Why not make it something like almost black background with white inactive links and the active link like the orange color from the logo? Perhaps put a divider line in between links or something?

By the way, I do these things every time  I use the docs, this is how bad it is, no offense to anyone, but 1-2 seconds of faster focus per person per time they visit the docs, well, we want to be talking about performance when we are talking about Svelte right? 

Please do the right thing, the nice svelte logo orange color matches great with a darker blackish background color, why put that ugly blueish/purplish (which is not dark enough for a bg color)?

To whoever can make this happen, PLEASE we need docs that can be scanned quickly. 

Here is the contrast score https://coolors.co/contrast-checker/ffffff-676778
It's almost poor... if you have any sort of sight issues... I mean come on, how come no one addressed this before, docs is a huge part of the process.

### Describe the proposed solution

Color for the ""active"" link => #ff3e00

Color for the sidebar background => #242426

Any dark enough background (with no colors, either blackish or grayish/blackish will do) 

### Alternatives considered

Please adjust this so visually impaired or people with sight difficulties (even the ones with glasses) can focus easier on the links and traverse faster. 

### Importance

i cannot use SvelteKit without it

### Additional Information

Every time I browse the docs I literally go to inspect and change these myself....!

Thanks for your understanding!",2022-04-12T14:06:12Z,97632715
3153,sveltejs/kit,1201906086,1096959313,"I tend to agree that we could add more contrast to the sidebar. I'm not sure I like the proposed changes though. Here's a screenshot of what they look like

![Screenshot from 2022-04-12 09-40-44](https://user-images.githubusercontent.com/322311/163012323-601a03cb-4502-4501-bf3b-79f284653f93.png)

Alternatively, here's the sideback background set to `#363636` with no change to the active link

![Screenshot from 2022-04-12 09-43-56](https://user-images.githubusercontent.com/322311/163012757-73360b3d-d8df-42dd-bcd8-56c396ba3133.png)",2022-04-12T16:45:23Z,322311
3154,sveltejs/kit,1201906086,1096962686,"Another idea, here's using bold rather than color to indicate the active item

![Screenshot from 2022-04-12 09-48-01](https://user-images.githubusercontent.com/322311/163013432-21c71ad2-0dc6-4bea-aeba-fa71bda869a0.png)
",2022-04-12T16:48:29Z,322311
3155,sveltejs/kit,1201906086,1096986814,"Hey @benmccann  thanks for the response here.

On the first slide the accent color looks kinda red and not the actual orange brand color, are you sure you used the right code? 
If it is the right color code then perhaps on that particular background we could lighten the orange just a bit.

Listen I very much appreciate your feedback here but I have to mention that as a person that has poor sight, I would argue that it's imperative the background to be kept at least a bit darker than your proposed hex above and the active link to be of an accent color (instead of a base like white) so it easily distinguishable and not on the same hue as the inactive links (truly important).

Check this =>
https://vuejs.org/guide/introduction.html
(They also have dark and light theme...)

We want to grow as a framework and as a user base but Vue.js is killing us with those Docs comparing to Svelte/kit's.

Please choose a slight darker background color, just a tiny touch and keep a punchy accent color close to the logo for the active links, I am positive this will help grow the framework, it sounds crazy but it is in my experience an important detail really.

Between choosing competing stuff online I personally have chosen then ones with the more accessible documentation.

@benmccann  thanks again for your consideration!",2022-04-12T17:14:33Z,97632715
3156,sveltejs/kit,1201906086,1097696786,If only there was something like https://caniuse.com/mdn-css_at-rules_media_forced-colors,2022-04-13T08:18:41Z,12383587
3157,sveltejs/kit,1201906086,1097721070,"Hey @UltraCakeBakery, thanks for your reply here.

I am not sure what you are suggesting and if there is sarcasm involved on your reply.
To be honest it feels there is sarcasm involved in your message because of the way you started your sentence and you didn't expand on what you are trying to say, you didn't take the time, you pasted a link and asked of us to figure everything else out.
 
You provided no basis for all other people to work with. Is this link a universal and time-efficient solution to the current problem? 
Do you mind telling us a little bit more please? 

This is a serious matter that affects thousands of people (in different degrees). 
Are you suggesting we all take the above action instead of adjusting 2 colors in the actual docs?

This is a genuine problem that needs solving, please provide constructive feedback/solutions.
No negativity is needed, thank you.",2022-04-13T08:43:16Z,97632715
3158,sveltejs/kit,1201906086,1097940757,"> Hey @UltraCakeBakery, thanks for your reply here.
> 
> I am not sure what you are suggesting and if there is sarcasm involved on your reply. To be honest it feels there is sarcasm involved in your message because of the way you started your sentence and you didn't expand on what you are trying to say, you didn't take the time, you pasted a link and asked of us to figure everything else out.
> 
> You provided no basis for all other people to work with. Is this link a universal and time-efficient solution to the current problem? Do you mind telling us a little bit more please?
> 
> This is a serious matter that affects thousands of people (in different degrees). Are you suggesting we all take the above action instead of adjusting 2 colors in the actual docs?
> 
> This is a genuine problem that needs solving, please provide constructive feedback/solutions. No negativity is needed, thank you.



> This is a serious matter that affects thousands of people (in different degrees).
> Are you suggesting we all take the above action instead of adjusting 2 colors in the actual docs?

Hey @savannahx, thanks for your reply here.

I am not sure if there is unawareness or selfishness involved on your reply.
To be honest it feels there was unawareness and selfishness involved in your message because of the way you formulated yourself and did not think about us, the other 99% of svelte-kit users, who also have to deal with this change and asked of us to actually do all the work of implementing this new color.

You provided no way for us to reach a middle ground where both you and others can be happy. It seems like you really just want us to change the color to what you want. Did you even read the link that I posted in the one minute before you started writing this reply you posted 3 minutes after me making my original comment? Do you mind telling us why you get a bad vibe from my comment (saying it is sarcastic) even though I was trying to be helpful?

Suggestions like these affects billions of people (in different degrees). Are you suggesting we completely change the documentation instead of asking you and ""the others"" to start using the build-in tools provided to you by your operating system and browser?

This is a genuine problem that needs solving, please consider changing your mindset. We do not need more any people in the world that are like:

""THE WORLD NEEDS TO CHANGE, N̵̠̲͛̿͘O̵̞̙̒̇͘T̴̼͓̮̈́͂ ME"" 

""I AM **N̵̠̲͛̿͘O̵̞̙̒̇͘T̴̼͓̮̈́͂** GONNA WEAR GLASSES OR CHANGE MY SETTINGS! ALL LIGHTS AND COLOURS S̷͈̫̮͙̑̔͌̓̐H̸̨̨̨͎̱̫͊̉̂͌Ó̸͓͓͖̽̏Ǘ̴̹͙͕̝͈ͅĻ̴̛̭͇̭̗̤̈́̽͐D̴̻̈̅̀̕ JUST GET BRIGHTER AND DARKER Ẁ̵̞̻̤̗̩̎̅H̴̨̤̮̼͉͆́̚͜͝É̴̛̠̃̌Ǹ̷͚͔̮͙̬͖̉̄̈̆͌͠ ̵͖̩̰͗̿͝Í̵̜͈͂̋ ̵̟͓̱̤͖̯͙͗̏̍̒́̈̚W̴̩̹̖̭͂̿ͅÂ̷̳̬̱͕̐̄̓̔̚N̴̙̳͔̏͒̇̿ͅT̷͉͚̳̈͂̈́̈͐̌ͅ ̶͍̦͂̓́T̸͍̭̤̲̲͕̔͜H̸̳̣̑͌̓̌̆͛Ḛ̴̢̛̱͋̇́M̶̳̀̀̂̄́͊ ̵͕̣̲̼́͗̈́̎̊̊͑Ţ̸̭͕̪͔̼͑͛̆ͅŐ̷̧̗̫͈̄͑͌͛͆̅O̷͖͈̱̳͓̠̎̋́͘!̵͓̩̙̦̍̂̾ͅ!̴̡̱͇̝͔̻̭̊̀͠!̴̛̠͔̩͚͂̋̕͠!̵̛̟̼̘̼͍͝!!!!"" ",2022-04-13T11:31:05Z,12383587
3159,sveltejs/kit,1201906086,1097946660,"I am impaired too, and get sick of suggestions like these. 
WE are the ones with disabilities, not the rest of the world. WE need to adapt ourselves to the world, NOT the other way around.

The documentation could offer better support for high contrast mode, dark mode, screen readers.. But I don't think it should.
It is fine as is. 

If you are still unable to read the documentation - which is also available on Github, which has a very great accessibility - you might want to buy a new monitor, open or close your blinds, change your color profile, tweak the brightness, gamma, contrast and other settings, increase the scale of your UI or install a extension which can do automatic contrast for you. 

Again, this is a problem only we deal with. We are an edge case.",2022-04-13T11:37:17Z,12383587
3160,sveltejs/kit,1201906086,1098047933,"Dude's raging, for young people it's mostly preferable to fortify behind a false sense of reality and stand their ground. They are young and cocky (at least on the internet :P)
I am too old for this. it's okay, I am not perfect either, nobody is.  

Anyway I really hope this gets addressed if anyone has access and time to implement this color edit (removing the washed out blueish background and using an accent color for the active link).
I would create a pull request but I barely know Svelte so apologies for not taking the initiative in that regard.

@benmccann  feel free to let me know if and when I should close this Issue and thanks again for your consideration.",2022-04-13T13:25:27Z,97632715
3161,sveltejs/kit,1201906086,1098078770,"alright, time out. i locked the conversation.

accessibility matters to us and we want to get this right. people shouldn't have to fiddle with browser settings just to read documentation. at the same time, there's a balance to be struck between maximising contrast and making the site pleasant to look at for people without visual impairments and/or shitty screens (we could just use pure black on pure white or vice versa everywhere, but that would be very tiring for most people to look at for any length of time).

so we will address this, though i make no guarantees about where it sits in the priority queue, and putting 'asap' in the issue title or declaring that the current contrast ratio means 'i cannot use SvelteKit without it' (unlikely) won't affect that decision (though there is now a pull request for it pending review — https://github.com/sveltejs/sites/pull/327)",2022-04-13T13:53:19Z,1162160
3162,encode/uvicorn,1248033266,1248033266,"I know that HTTP/2 is out of scope of this project. But I am not asking to support HTTP/2. 

More and more http clients try to upgrade the connection to HTTP/2. Uvicorn is free to not honor this request. Unfortunately, instead of ignoring the upgrade request, Uvicorn responds with status `400 Bad Request` and the message ""Unsupported upgrade request"".

According to https://developer.mozilla.org/en-US/docs/Web/HTTP/Protocol_upgrade_mechanism the server should just ignore the upgrade request:

> If the server decides to upgrade the connection, it sends back a [101 Switching Protocols](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/101) response status with an Upgrade header that specifies the protocol(s) being switched to. If it does not (or cannot) upgrade the connection, it ignores the Upgrade header and sends back a regular response (for example, a [200 OK](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/200)).

We continue to encounter this issue because the `HttpClient` class of modern OpenJDK versions tries to upgrade the connection to HTTP/2 by default. Uvicorn should just ignore these headers and process the requests as if these headers were not present.",2022-05-25T12:36:51Z,19341644
3163,encode/uvicorn,1248033266,1154847658,"interesting, low priority but something to consider",2022-06-14T07:59:20Z,1104190
3164,encode/uvicorn,1248033266,1239348745,"I don't agree with the ""low priority"" assessment. This is a blatant violation of the http specification and more and more (standards compliant) http clients break just by using them with their default settings.",2022-09-07T12:50:48Z,19341644
3165,encode/uvicorn,1248033266,1239350715,PR welcome.,2022-09-07T12:52:27Z,7353520
3166,encode/uvicorn,1248033266,1239357698,"The existence of a PR has nothing to do with the priority assessment. In fact, giving this issue a higher priority may incentivize people to actually fix this issue. Saying that this violation of the http spec is ""low prio"" is more a statement about the level of professionalism of this software, or the lack thereof.

For the record: Yes, I would try to fix this myself, but you have to realize that not every user of your product is a software developer that uses the language that your software is written in. I am fluent in Java and Go, but I cannot fix python code. Or at least not at a level that you would ever consider to accept contributions.",2022-09-07T13:00:06Z,19341644
3167,encode/uvicorn,1248033266,1239386296,"Locking the issue as I'm not liking the tone of the reporter.

PR is still welcome. 🙏 

If no one implements it, I will, at some point, on my free time (unpaid free time).",2022-09-07T13:24:07Z,7353520
3168,encode/uvicorn,1248033266,1241798866,"omg, happy to come back to see your fluency in being a total asshat @ChristianCiach 
raising the issue as high priority :+1:  !
",2022-09-09T10:31:06Z,1104190
3169,encode/uvicorn,1248033266,1283514244,"- Closed by #1661

It will be available on uvicorn `0.19.0`.",2022-10-19T06:47:26Z,7353520
3170,vector-im/element-web,436303024,436303024,as 'kick' is needlessly aggressive,2019-04-23T17:24:34Z,1294269
3171,vector-im/element-web,436303024,487068460,"It hasn't been considered aggressive for the decades it's been an IRC command, so why would it be now? Put a nice message in the reason field if you don't want to hurt their feelings. It's not like you're actually physically kicking them...",2019-04-26T14:03:30Z,41203525
3172,vector-im/element-web,436303024,487124637,"We have the option to be less aggressive, so let's go ahead and do that. There's no need to comply with the idiosyncratic norms of IRC culture in the labelling of UI features.",2019-04-26T16:48:26Z,279572
3173,vector-im/element-web,436303024,487154478,"My main point was that it's not aggressive at all. Being ""kicked out"" of somewhere is a common term that is mutually understood to mean you were removed. Removing someone from a room against their will is going to potentially upset them regardless of what you call it. 

I don't really care what the UI says, but please keep the /command as ""/kick"" for the sake of familiarity and efficiency. ",2019-04-26T18:22:34Z,41203525
3174,vector-im/element-web,436303024,487168565,"Um, excuse me, sweetie, this is 2019﻿...

'ban' as in 'banish' is also unnecessarily aggressive and totally inappropriate if you consider African American history. It should be renamed to 'Adorn Chastity Device' instead.",2019-04-26T19:09:17Z,50029890
3175,vector-im/element-web,436303024,487177786,"Aggressive terminology in the ""riot"" project, we can't have that, oh no no.",2019-04-26T19:42:45Z,308818
3176,vector-im/element-web,436303024,487184952,"empirically we’ve seen people unfamiliar to the app see the word “kick user” and say “wtf? is this like poking them?” rather than extrapolating it to “kick the user out”.

so, as amusing as it is to see people getting so overexcited about such a trivial change, i’m going to lock the issue to contributors. thanks!",2019-04-26T20:07:12Z,1294269
3177,vector-im/element-web,436303024,1005939339,"Let's remove the wording ""X kicked X"" when someone is removed from a room and replace with ""X removed X""
The word ""kick"" can create very negative emotional responses from users and we have evidence of this.
Also, as Element is evolving with a wider variety of user base, it makes sense to use more appropriate and friendlier language throughout the product. 
FYI @ara4n 

",2022-01-05T17:44:54Z,76945183
3178,nestjs/nest-cli,1282455505,1282455505,"### Is there an existing issue for this?

- [X] I have searched the existing issues

### Current behavior

Hello, I try to do an example application learned on udemy. 

I try to run cmd
```js
nest g mo src/tasks
```
 I have angular CLI. But I cannot understand why that error.

### Minimum reproduction code

nest g mo src/tasks

### Steps to reproduce

1. nest g mo src/tasks

### Expected behavior

(node:33348) UnhandledPromiseRejectionWarning: Error: Unknown argument skipImport. Did you mean skip-import?
    at parseArgs (/Users/andreykatrusha/Repos/nestjs-task-managment/node_modules/@angular-devkit/schematics-cli/bin/schematics.js:338:19)
    at main (/Users/andreykatrusha/Repos/nestjs-task-managment/node_modules/@angular-devkit/schematics-cli/bin/schematics.js:122:49)
    at Object.<anonymous> (/Users/andreykatrusha/Repos/nestjs-task-managment/node_modules/@angular-devkit/schematics-cli/bin/schematics.js:367:5)
    at Module._compile (internal/modules/cjs/loader.js:1072:14)
    at Object.Module._extensions..js (internal/modules/cjs/loader.js:1101:10)
    at Module.load (internal/modules/cjs/loader.js:937:32)
    at Function.Module._load (internal/modules/cjs/loader.js:778:12)
    at Function.executeUserEntryPoint [as runMain] (internal/modules/run_main.js:76:12)
    at internal/main/run_main_module.js:17:47
(Use `node --trace-warnings ...` to show where the warning was created)
(node:33348) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag `--unhandled-rejections=strict` (see https://nodejs.org/api/cli.html#cli_unhandled_rejections_mode). (rejection id: 2)
(node:33348) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.

### Package version

8.2.7

### NestJS version

8.4.7

### Node.js version

14.17.5

### In which operating systems have you tested?

- [X] macOS
- [x] Windows
- [x] Linux

### Other

_No response_",2022-06-23T14:00:19Z,3959504
3179,nestjs/nest-cli,1282455505,1164449527,"Got the same error with ```global NestJS 8.2.7```, project ```@nestjs/cli: 8.0.0``` while executing a simple ```nest g controller Something``` on Windows 10.

The command executed by nest: ```node @nestjs/schematics:controller --name=something --no-dry-run --no-skipImport --language=""ts"" --sourceRoot=""src"" --spec```",2022-06-23T14:04:10Z,53939346
3180,nestjs/nest-cli,1282455505,1164449610,I think maybe it conflict with CLI?,2022-06-23T14:04:15Z,3959504
3181,nestjs/nest-cli,1282455505,1164450890,"I try 
```
nest  [options] <schematic> [name]
```
and have some errors
",2022-06-23T14:05:22Z,3959504
3182,nestjs/nest-cli,1282455505,1164474804,"https://github.com/nestjs/nest-cli/issues/323
https://github.com/nestjs/nest-cli/issues/145
",2022-06-23T14:23:55Z,3959504
3183,nestjs/nest-cli,1282455505,1164475634,"> Got the same error with `global NestJS 8.2.7`, project `@nestjs/cli: 8.0.0` while executing a simple `nest g controller Something` on Windows 10.
> 
> The command executed by nest: `node @nestjs/schematics:controller --name=something --no-dry-run --no-skipImport --language=""ts"" --sourceRoot=""src"" --spec`

on my computer I have
```
node @nestjs/schematics:controller --name=something --no-dry-run --no-skipImport --language=""ts"" --sourceRoot=""src"" --spec
internal/modules/cjs/loader.js:892
  throw err;
  ^

Error: Cannot find module '/Users/andreykatrusha/Repos/nestjs-task-managment/@nestjs/schematics:controller'
    at Function.Module._resolveFilename (internal/modules/cjs/loader.js:889:15)
    at Function.Module._load (internal/modules/cjs/loader.js:745:27)
    at Function.executeUserEntryPoint [as runMain] (internal/modules/run_main.js:76:12)
    at internal/main/run_main_module.js:17:47 {
  code: 'MODULE_NOT_FOUND',
  requireStack: []
}",2022-06-23T14:24:37Z,3959504
3184,nestjs/nest-cli,1282455505,1164478373,"<img width=""989"" alt=""Screenshot 2022-06-23 at 17 26 46"" src=""https://user-images.githubusercontent.com/3959504/175323342-1401c466-75d4-4a9a-ab4b-6c20a9ccea56.png"">
",2022-06-23T14:26:53Z,3959504
3185,nestjs/nest-cli,1282455505,1164479758,"@kaflan I guess that's another issue.

---

The original one is a bug on v8.2.7.

you can downgrade it for now. v8.2.6 of `@nestjs/cli` worked just fine. I don't recommend using Nest CLI globally (use `yarn nest` or `npx nest`).",2022-06-23T14:28:00Z,13461315
3186,nestjs/nest-cli,1282455505,1164480945,"When I try to install https://www.npmjs.com/package/@nestjs/schematics
I have that error",2022-06-23T14:28:42Z,3959504
3187,nestjs/nest-cli,1282455505,1164483836,but you didn't show any installation error for us. I didn't follow,2022-06-23T14:31:04Z,13461315
3188,nestjs/nest-cli,1282455505,1164484068,"> 8.2.6

Ok I  try now

",2022-06-23T14:31:16Z,3959504
3189,nestjs/nest-cli,1282455505,1164485355,"> but you didn't show any installation error for us. I didn't follow

Yes, everything works well without generation.
I think it like some CLI conflict",2022-06-23T14:32:16Z,3959504
3190,nestjs/nest-cli,1282455505,1164614979,"I can confirm this issue as well
environment:

System:
  Kernel: 5.15.46-1-MANJARO arch: x86_64 bits: 64 compiler: gcc v: 12.1.0
  Desktop: KDE Plasma v: 5.24.5 tk: Qt v: 5.15.4 wm: kwin_x11 vt: 1 dm: SDDM
  Distro: Manjaro Linux base: Arch Linux

### Quick solution:

1. add this to your nest-cli.json
```
""root"": ""src""
```

2. change into the src of your project folder
```
cd src
```

3. run 
```
nest generate module tasks
```

if the above does not work also try changing the version of packages in package.json to 
```
""@nestjs/cli"": ""8.2.6"",
""@nestjs/schematics"": ""8.0.11"",
```
removing the node modules in the current project 
reinstalling the nest cli with
```
npm uninstall -g @nestjs/cli
npm i -g @nestjs/cli@8.2.6 
``` ",2022-06-23T16:17:57Z,31963345
3191,nestjs/nest-cli,1282455505,1164630385,Add sistem,2022-06-23T16:33:31Z,3959504
3192,nestjs/nest-cli,1282455505,1164678440,@micalevisk @kamilmysliwiec Looks like this was a breaking change of [`@angular-devkit/schematics@14.0.0`](https://github.com/angular/angular-cli/blob/HEAD/CHANGELOG.md#angular-devkitschematics-cli). We should revert the angular schematics packages until we're ready to go through the full upgrade to keep from making breaking changes in our shcematics,2022-06-23T17:24:34Z,28268680
3193,nestjs/nest-cli,1282455505,1164679004,"Same issue here., tried above soluions without succes
",2022-06-23T17:25:17Z,55088242
3194,nestjs/nest-cli,1282455505,1164685394,"@Acetyld `npm i @nestjs/cli@8.2.6` and then `npx nest g mo foo` will work for sure. If didn't worked for you, then you're probably missing something. We could help you on Discord.


![image](https://user-images.githubusercontent.com/13461315/175359768-88b0f4ef-d77c-4b08-a9e1-49fff42375b4.png)

",2022-06-23T17:32:31Z,13461315
3195,nestjs/nest-cli,1282455505,1164711554,Try tommorow,2022-06-23T18:01:13Z,3959504
3196,nestjs/nest-cli,1282455505,1164839386,Bump. Having the same problem.,2022-06-23T20:23:03Z,1724455
3197,nestjs/nest-cli,1282455505,1164867885,You try guide,2022-06-23T20:59:51Z,3959504
3198,nestjs/nest-cli,1282455505,1164882337,"Guys, we know the problem, we know you're having it, no need to ""bump"". This literally came up today, it's not like this is inactive in any way.",2022-06-23T21:18:28Z,28268680
3199,nestjs/nest-cli,1282455505,1165286040,Fixed in 8.2.8,2022-06-24T07:26:55Z,23244943
3200,qbittorrent/qBittorrent,93644527,93644527,"would be great to make filter for auto skip marked extention types from download.

ex: i need only music without covers, etc. it annoying to deselect it all time (imagine 50 albums, 4Gb covers and 1 seed). so here can help filter to .jpg, .png, etc.
if need to download photos - turn off filter checkbox in file list.
you can do sets of rules, but one is enough.


<bountysource-plugin>

---
Want to back this issue? **[Post a bounty on it!](https://www.bountysource.com/issues/23896213-file-extension-filter?utm_campaign=plugin&utm_content=tracker%2F298524&utm_medium=issues&utm_source=github)** We accept bounties via [Bountysource](https://www.bountysource.com/?utm_campaign=plugin&utm_content=tracker%2F298524&utm_medium=issues&utm_source=github).
</bountysource-plugin>",2015-07-07T22:32:11Z,1270117
3201,qbittorrent/qBittorrent,93644527,119556239,"Sounds useful :thumbsup:
",2015-07-08T12:21:00Z,6451685
3202,qbittorrent/qBittorrent,93644527,304447200,"2015? :-( This will very likely never added then, that sucks.",2017-05-27T11:49:03Z,14368203
3203,qbittorrent/qBittorrent,93644527,427604310,"This feature could help security:

If a user downloads only known files (e.g. audio/video), it is wise to block pontentially dangerous extensions, e.g. EXE, COM, BAT, LNK, VBS, (PY?), etc.

Less savvy users can't easily spot a ponetial threat, e.g. Matrix.avi   <> Matrix.avi.exe

Also, some video torrents are bundeled with trojans, e.g. ""codec.zip"" or ""driver.exe"" containing malware. Today I spotted the attached file, an LNK file, an extension that is hidden on Windows.

None of my users download software via torrent, so I'd like to block it for them, or set blocked extensions; potentially, i'd like to block the whole torrent altogether, if a potential software extension is found in one of its files.

(My personal block list would be: EXE, COM, BAT, VBS, VBE, JS, CMD, PY, CPL, DLL, LNK, SCR)

In the screenshots below: 
Windows hides LNK extension, a trojan disguised as AVI video:

![lnk-virus](https://user-images.githubusercontent.com/124651/46575528-43e7c780-c9bf-11e8-9b2a-0cbd5efa8d36.png)
![virus2](https://user-images.githubusercontent.com/124651/46575529-43e7c780-c9bf-11e8-8399-db13d6ec90ef.png)
",2018-10-06T20:34:51Z,124651
3204,qbittorrent/qBittorrent,93644527,480271998,"Id like to throw my support behind this feature request. Many torrents come with useless .txt files that do nothing but clutter up a directory.  Also, a popular site I use has started to include an .exe file that I now have to deselect every time I download something. It would be great to be able to have these files automatically excluded.  Being able to blacklist certain file names would also be a great addition to this feature.",2019-04-05T13:18:38Z,49312111
3205,qbittorrent/qBittorrent,93644527,489335579,Throwing my hat in..,2019-05-04T15:14:07Z,8529475
3206,qbittorrent/qBittorrent,93644527,491777593,"I suggest a feature for a simple list of file names ```do_not_download.exe``` and extensions ```*.exe``` that get marked as **Priority** -> **Do Not Download** automatically for all torrents. List may be accessed in **Options** -> **Downloads**.

Searching for references:
https://github.com/qbittorrent/qBittorrent/blob/master/src/base/bittorrent/torrenthandle.cpp#L684
https://github.com/qbittorrent/qBittorrent/blob/master/src/base/bittorrent/torrenthandle.cpp#L2068
https://www.libtorrent.org/reference-Core.html find ```file_priorities``` under ```add_torrent_params``` header.
https://github.com/qbittorrent/qBittorrent/blob/2d7b833ae6cb2145465cc7e47df398628ac95651/src/base/bittorrent/session.cpp#L1949",2019-05-13T11:03:28Z,3121967
3207,qbittorrent/qBittorrent,93644527,495734088,"Glad i found this post, really like this feature to, are we sure (i couldn't find it) that there is no such option already?

",2019-05-24T18:02:01Z,9917659
3208,qbittorrent/qBittorrent,93644527,507021284,Definitely have my vote. It will also save (not much but) some space and unnecessary Data download for Countries that charge per Mb on top of per Speed.,2019-06-30T09:20:33Z,11994065
3209,qbittorrent/qBittorrent,93644527,507086205,"Love it. You would need to be able to override it on a per torrent bassis,
but for people who download primarily just a couple of different file types
(cough).

On Sat, Oct 6, 2018, 4:35 PM shula <notifications@github.com> wrote:

> This feature could help security:
>
> If a user downloads only known files (e.g. audio/video), it is wise to
> block pontentially dangerous extensions, e.g. EXE, COM, BAT, LNK, VBS,
> (PY?), etc.
>
> Less savvy users can't easily spot a ponetial threat, e.g. Matrix.avi <>
> Matrix.avi.exe
>
> Some video torrents are bundeled with fake ""codec.zip"" ""driver.exe""
> containing malware. Today I spotted the attached file, an LNK file, an
> extension that is hidden on Windows.
>
> None of my users are downloading software via torrent, so I'd like to
> block it for them, or set blocked extensions; potentially, i'd like to
> block the whole torrent altogether, if a potential software is found in it.
>
> My personal block list would be: EXE, COM, BAT, VBS, VBE, JS, CMD, PY,
> CPL, DLL, LNK, SCR.
>
> In the screenshots below:
> How windows hides LNK extension, which is a sure malware when only
> downloading media:
>
> [image: lnk-virus]
> <https://user-images.githubusercontent.com/124651/46575528-43e7c780-c9bf-11e8-9b2a-0cbd5efa8d36.png>
> [image: virus2]
> <https://user-images.githubusercontent.com/124651/46575529-43e7c780-c9bf-11e8-8399-db13d6ec90ef.png>
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/qbittorrent/qBittorrent/issues/3369#issuecomment-427604310>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AGbY-5e2u-o8-6NhlR-Z9UlUBj9q6YGgks5uiRPzgaJpZM4FT95h>
> .
>
",2019-07-01T01:28:32Z,6740219
3210,qbittorrent/qBittorrent,93644527,518449345,"Please add this feature since lot's of trackers now put lots of ""junk files"". Yes we can use other clients but qb has a lot to offer and you can always disable this feature if it bothers you, so it's a win-win.",2019-08-06T00:27:46Z,20217689
3211,qbittorrent/qBittorrent,93644527,520924450,"I'd love this, too. ",2019-08-13T17:14:20Z,5701514
3212,qbittorrent/qBittorrent,93644527,527210363,This please! We need this!,2019-09-02T17:26:30Z,9272168
3213,qbittorrent/qBittorrent,93644527,547628421,Is it possible now to completely fail specific torrent if it contains not appropriate file name?,2019-10-29T21:03:33Z,767795
3214,qbittorrent/qBittorrent,93644527,557791768,"+1 on the feature request.
In the meantime, I wrote a short cmd script which can be referenced in the _Tools->Options->Downloads->Run external program on torrent completion_ which renames files with a suspicious extension (.exe \ .scr \ .cmd \ .bat) to prevent them from running when double clicked.

```
rem Find suspicious files in directory and rename them
rem Usage: fsus.cmd <dirname>

@echo off
SETLOCAL
set extensions=""\.lnk \.exe \.cmd \.scr \.bat""
echo looking in %1 for %extensions%
for /f %%F in ('dir %1 /s /b') do (
    (echo %%F | findstr /r %extensions% > NUL) && move %%F bad_%%F.BAD && echo Renamed %%F
)
ENDLOCAL
```",2019-11-23T11:55:56Z,19845603
3215,qbittorrent/qBittorrent,93644527,559159403,"Plugin in python3 for this:
https://gist.github.com/oltodosel/566e051191f3a58b905db2cc6980656f",2019-11-27T16:27:44Z,10230453
3216,qbittorrent/qBittorrent,93644527,596234216,Has there been any updates if/when this feature would be added?,2020-03-08T18:13:43Z,61941051
3217,qbittorrent/qBittorrent,93644527,610647608,Since 2015 and this still isn’t added yet? Come on. This would be such a useful feature.,2020-04-07T22:16:48Z,19495461
3218,qbittorrent/qBittorrent,93644527,622215134,"I would love to have this feature implemented. There could be a global file extension filter, or have the filter setup by category (so categories can have different filters).",2020-05-01T02:14:20Z,5466762
3219,qbittorrent/qBittorrent,93644527,622429807,2015-2020 is not create this function. qBit - Shit!,2020-05-01T15:18:05Z,44901935
3220,qbittorrent/qBittorrent,93644527,652546406,"Why isn't this issue considered critical?

Distracted users shouldn't run viruses so easily.",2020-07-01T17:18:04Z,17798979
3221,qbittorrent/qBittorrent,93644527,652558395,"> Why isn't this issue considered critical?

To be fair, the ""better"" motivation for this feature request should be some kind of automation purpose like ""typically I download a lot of ebook pack torrents with epub + azw3, but I don't want any azw3"", and not ""distracted users clicking files with hidden extensions"" - that is a Windows problem, easily fixed by disabling `Hide extensions for known file types` in the control panel. If you still click ""dangerous"" files accidentally, that's PEBCAK. Alternatively, just use better sites and download better torrents.

This is why this isn't ""critical"".",2020-07-01T17:44:05Z,17580742
3222,qbittorrent/qBittorrent,93644527,652560009,"> To be fair, the ""better"" motivation for this feature request should be some kind of automation purpose like ""typically I download a lot of ebook pack torrents with epub + azw3, but I don't want any azw3""

That's definitely my motivation for wanting this feature. I use the RSS downloader and it would be nice to automatically exclude unnecessary files.",2020-07-01T17:47:34Z,5466762
3223,qbittorrent/qBittorrent,93644527,652573874,"> > Why isn't this issue considered critical?
> 
> that is a Windows problem, easily fixed by disabling `Hide extensions for known file types` in the control panel. If you still click ""dangerous"" files accidentally, that's PEBCAK.

As shown above, this isn't enough for .lnk files. Windows hides the .lnk extension. If you usually use smaller thumbnails, the minuscule difference in the icon is barely visible.

Also, it is arguably because torrent clients aren't smart that these types of malicious torrents are still going around, and that's why I think qBittorrent should provide this feature.

IMHO certain extensions should also be skipped by default, for the same reason. It would be a great security and usability improvement.

This also allows you to skip certain extensions because you prefer so, but the security issue should be considered prominently.",2020-07-01T18:17:28Z,17798979
3224,qbittorrent/qBittorrent,93644527,652578286,"> As shown above, this isn't enough for .lnk files. Windows hides the .lnk extension. If you usually use smaller thumbnails, the minuscule difference in the icon is barely visible.

If you are downloading torrents with malicious .lnk files, you need a solution for a more urgent problem: don't download such torrents, use better sites/sources, or pay more attention. After all, it is the user's responsibility to not fall for phishing emails as well. Inspect URLs/files you click.

Alternatively, you could try to enable showing `.lnk` extensions: https://www.tenforums.com/customization/111886-how-show-lnk-extension.html
But because Windows is Windows, this might lead to undesirable presentation elsewhere (such as the start menu).",2020-07-01T18:27:28Z,17580742
3225,qbittorrent/qBittorrent,93644527,652582537,"Your solution is for power users, has usability drawbacks, and doesn't address the fact that malicious users are taking advantage of an easily fixable flaw in qBittorrent.

I think the qBittorrent team should step up and fix this. There is almost never a good reason to download certain file extensions, and users should actively check those files for download.",2020-07-01T18:37:07Z,17798979
3226,qbittorrent/qBittorrent,93644527,652589829,"You can't expect people that execute random files to know how to use power features. That something can (and should) be done another way isn't a reason to not include a security feature.
These are the same people that download from the first torrent site that shows up in Google search. So the ""use a better site"" isn't a valid argument either.

Obviously all this is only true if your target is the mass and not just tech-savvy people.",2020-07-01T18:54:00Z,33300835
3227,qbittorrent/qBittorrent,93644527,652589953,"> Your solution is for power users, has usability drawbacks, 

Fighting phishing emails is something everyone has to learn to do, no matter the occupation. I think it is reasonable to demand a certain level of proficiency and common sense.

> and doesn't address the fact that malicious users are taking advantage of an easily fixable flaw in qBittorrent.

""Malicious users are taking advantage of distracted/careless users"" would be a more accurate statement. Do you think the possibility of receiving phishing emails is a flaw of E-mail? If so, is the possibility of hearing the voice of a scammer in real life, believing what they say, and giving them money, a flaw of your ears? Should your auditory system should autoblock certain words/sentences on its own? Perhaps it should be the brain acting on the information instead.

Furthermore, the greater issue of downloading these kinds of torrents should not be underestimated. You have to go out of your way, even when searching for illegal content, to find these kinds of torrents. And no, the `.exe` in RARBG torrents does not count as an example of this practice in a popular site; it is actually just a harmless text file with the `.exe` extension designed to prevent mirroring  by software that, ironically, relies on ""file extensions"" to make assumptions about their content.

Not to mention that if anyone actually accidentally clicks a dangerous exe, it should be caught by UAC anyway. If the user has disabled UAC or blindly clicks through it, then they either know what their doing or they ""know enough to be dangerous"", in which case whatever happens is their own fault and there's nothing we can really do.",2020-07-01T18:54:16Z,17580742
3228,qbittorrent/qBittorrent,93644527,652597093,">You can't expect people that execute random files to know how to use power features. That something can (and should) be done another way isn't a reason to not include a security feature.
These are the same people that download from the first torrent site that shows up in Google search. So the ""use a better site"" isn't a valid argument either.
>
>Obviously all this is only true if your target is the mass and not just tech-savvy people.

First of all, it is indeed a shame that the Windows default is wrong, and that the way to change it requires some knowledge to do so. But that is a Windows problem. One can post an issue on the relevant forum/issue tracker about that, not here.

Secondly, regardless of such setting, I'm not really keen on catering to this kind of ignorance/stupidity of ""I'm carelessly clicking on stuff and ignoring warnings and expecting it to work"". If someone doesn't care to learn how to properly use a saw, should their complaints to the manufacturer be taken seriously when they cut themselves?

We should strive to make things easier to use. But not to the point of bending over backwards to a level of stupidity/ignorance/carelessness that shouldn't be endorsed or excused, at the expense of time, effort, and other important things.",2020-07-01T19:10:17Z,17580742
3229,qbittorrent/qBittorrent,93644527,652600463,"> Not to mention that if anyone actually accidentally clicks a dangerous exe, it should be caught by UAC anyway. If the user has disabled UAC or blindly clicks through it, then they either know what their doing or they ""know enough to be dangerous"", in which case whatever happens is their own fault and there's nothing we can really do.

You don't need to click through anything. Once you've clicked the .lnk file an .exe will download in the background without any warning in less than a second, and it will run again unnoticed at the next restart. There are virtually endless possibilities to the harm that can be done by these attacks.
Furthermore, they're not even detected by most antivirus software.

If the qBittorrent team needs an example, I can provide it.",2020-07-01T19:18:18Z,17798979
3230,qbittorrent/qBittorrent,93644527,652625108,"> You don't need to click through anything. Once you've clicked the .lnk file an .exe will download in the background without any warning in less than a second, and it will run again unnoticed at the next restart. There are virtually endless possibilities to the harm that can be done by these attacks.
> Furthermore, they're not even detected by most antivirus software.
> 
> If the qBittorrent team needs an example, I can provide it.

There are always exceptions to the rule. I am sure there are some examples of software bypassing UAC, or just being dangerous enough without needing to do so in the first place. But this is a secondary point anyway.

I should add to https://github.com/qbittorrent/qBittorrent/issues/3369#issuecomment-652597093:

Again, I'm not saying it would be bad to have this feature. It would be good for automation purposes, for example. But I don't think it is fair to consider it ""critical due to user security considerations."". Of course just by being there it could serve as an additional safety net. But that's not the main purpose and it's by no means critical for that purpose.",2020-07-01T20:16:33Z,17580742
3231,qbittorrent/qBittorrent,93644527,652693265,"I'm sorry Francisco but I really think you are underestimating the problem.
If we followed your logic, antivirus software and antispam software should be banned, because humans should be infallible and never miss a single malicious file.
This type of attack with torrents is too common, at a certain point client software (qBittorrent) becomes complicit in this kind of exploit, which has been reported as far back as 2018 here above.
If the same arguments were used by browsers, e.g. Firefox, sandboxing bugs shouldn't be fixed because the user should only visit ""trusted"" websites.

I hope the rest of the team doesn't treat security the same way as you do. This is an overdue feature, requested since 2015.
The team has been asked before for directions on where to look in order to implement a pull request, but no attention has been given to the matter.
If not even this convinces anybody who possesses the knowledge and expertise to propose a fix, I really have nothing more to add. I have seen the developers here dedicate a lot of time and effort to ""betterment"" projects such as the new webUI and API, therefore I really am surprised such a seemingly small-to-implement but security-critical issue has not been given space in 5 years. ",2020-07-01T23:25:38Z,17798979
3232,qbittorrent/qBittorrent,93644527,652934966,"@simo1994 

> If we followed your logic, antivirus software and antispam software should be banned, because humans should be infallible and never miss a single malicious file.

This is one hell of a strawman fallacy, but ok. Not to mention that you seem to be advocating in favor of antivirus/anti-spyware. In a sane computing environment, a black-box proprietary software made by a for-profit corporation running with administrative privileges (aka """"""antivirus"""""") is not considered a layer of security. The true solutions for the problems these programs claim to solve lie somewhere else in the stack - don't use Windows, use Free (as in Freedom) software, package managers with cryptographic signing, etc...

> This type of attack with torrents is too common,

These kinds of claims are worthless if not substantiated with data. You can't just claim something and ask others to disprove it. Start by defining what is ""too common"". 30% of torrents? 40%? 50%? In which sites? The burden of proof lies with you. You're welcome to open a new issue investigating this, with some pretty graphs.

> at a certain point client software (qBittorrent) becomes complicit in this kind of exploit, which has been reported as far back as 2018 here above.

- This is not an ""exploit""
- ""complicit"" is a strong word. Are email clients/servers ""complicit"" for people falling for the Nigerian prince scam?

> If the same arguments were used by browsers, e.g. Firefox, sandboxing bugs shouldn't be fixed because the user should only visit ""trusted"" websites.

Again, ridiculous strawman. Browsers need sandboxing for any kind of user due to the unfortunate way the Web has evolved. In fact, mainstream browsers don't even come secure enough out of the box IMO.

> I hope the rest of the team doesn't treat security the same way as you do.

I treat security as seriously as anyone else who really understands it. In reality, you'd be surprised how little of it _you_ actually understand, to be able to make the arguments you make and then accuse others of not treating security seriously. Hopefully one day you'll be able to look back at this and chuckle, it will mean you have evolved and learned.

>The team has been asked before for directions on where to look in order to implement a pull request, but no attention has been given to the matter.
If not even this convinces anybody who possesses the knowledge and expertise to propose a fix, I really have nothing more to add.

Anyone is welcome to submit PRs to implement the feature. There isn't any sense of urgency, because likely all of those with the ability to implement it also know that this isn't ""security critical"".

> I have seen the developers here dedicate a lot of time and effort to ""betterment"" projects such as the new webUI and API, therefore I really am surprised such a seemingly small-to-implement but security-critical issue has not been given space in 5 years.

Again, it's not ""security critical"". Stop with the fear-mongering and FUD.

> I really have nothing more to add.

Me neither. I agree that everything relevant to this feature request has been said. Now it's up to whoever wants to implement it.",2020-07-02T10:50:39Z,17580742
3233,qbittorrent/qBittorrent,93644527,667961136,"TL;DR:

- Anyone is welcome to implement this, and I'm also convinced it won't be rejected if implemented. As mentioned above, it may be useful for certain automation scenarios.
- The default should be to not filter anything, IMO. Otherwise, it would be unexpected behavior - people would wonder why certain files don't download by default. If qBittorrent ever gets some sort of ""onboarding"" UX, this could be one of the tunables (e.g. do you want to filter ""potentially malicious files"" by default?).
- No, this is not ""security critical"" (see discussion above).",2020-08-03T11:07:17Z,17580742
3234,python/mypy,98183747,98183747,"The following in particular would be useful:

```
Callback = Callable[[str], 'Callback']
Foo = Union[str, List['Foo']]
```
",2015-07-30T14:30:22Z,964689
3235,python/mypy,98183747,127463269,"If (or when) mypy will have structural subtyping, recursive types would also be useful there.
",2015-08-04T03:18:22Z,1107911
3236,python/mypy,98183747,147614067,"My current plan is to postpone recursive types until simple structural subtyping is in and reconsider them later. After thinking about them more they are going to increase complexity a lot of I haven't seen much evidence for them being needed that often.
",2015-10-13T05:57:58Z,1107911
3237,python/mypy,98183747,148751954,"If I'm understanding this issue right, I'm running into it for classes that want a [fluent interface](https://en.wikipedia.org/wiki/Fluent_interface#Python). So for example, if I want callers to do something like this:

``` python
myfoo.add(bar).add(baz).finish()
```

Then the definition of the `Foo` class and the `add` method need to look something like this:

``` python
class Foo:
    def add(self, x) -> Foo:  # Python chokes on this line!
        # do stuff
        return self
```

Another place where Python commonly does `return self` is in the `__enter__` method for context managers. Is mypy able to typecheck those right now?
",2015-10-16T15:49:12Z,860932
3238,python/mypy,98183747,148753537,"@oconnor663 Try:

``` python
class Foo:
    def add(self, x) -> 'Foo':
        # do stuff
        return self
```
",2015-10-16T15:55:16Z,1690697
3239,python/mypy,98183747,148754354,"Ah, thank you.
",2015-10-16T15:57:54Z,860932
3240,python/mypy,98183747,148763080,"@kirbyfan64, do you know if there are standard functions anywhere that understand this convention? Like, if I wanted to introspect a couple functions and compare the types of their arguments, should I handle the `Foo == ""Foo""` case explicitly? That seems doable, but a string-aware version of say `isinstance` seems harder.
",2015-10-16T16:35:27Z,860932
3241,python/mypy,98183747,148763562,"@oconnor663 I don't think there's anything like that. If you're introspecting the functions via a decorator, you could try accessing the caller's globals and locals.
",2015-10-16T16:37:43Z,1690697
3242,python/mypy,98183747,148764449,"You're aware of typing.get_type_hints(obj)`right? It is similar to
`obj.**annotations**` but expands forward references.
https://docs.python.org/3/library/typing.html?highlight=typing#typing.get_type_hints

There used to be an instance() implementation in typing.py but Mark Shannon
made me take it out. It's being deleted in this rev:
https://github.com/ambv/typehinting/commit/ac7494fa900f76c7b3342bb6e0389e1543de0071

On Fri, Oct 16, 2015 at 9:37 AM, Ryan Gonzalez notifications@github.com
wrote:

> @oconnor663 https://github.com/oconnor663 I don't think there's
> anything like that. If you're introspecting the functions via a decorator,
> you could try accessing the caller's globals and locals.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/JukkaL/mypy/issues/731#issuecomment-148763562.

## 

--Guido van Rossum (python.org/~guido)
",2015-10-16T16:42:06Z,2894642
3243,python/mypy,98183747,148767795,"@gvanrossum that's exactly what I was looking for, thanks. Sorry for the n00b questions today, but awesome that all this is supported.
",2015-10-16T16:53:39Z,860932
3244,python/mypy,98183747,148949879,"Mypy should detect missing string literal escapes (see #948).
",2015-10-17T20:43:35Z,1107911
3245,python/mypy,98183747,240730239,"Going back to the original point on the issue, I found a case in the stdlib where this would be needed; the type for `isinstance()` is currently:

``` python
def isinstance(o: object, t: Union[type, Tuple[type, ...]]) -> bool: ...
```

but it should actually be:

``` python
ClassInfo = Union[type, Tuple['ClassInfo', ...]]
def isinstance(o: object, t: ClassInfo) -> bool: ...
```

Because according to https://docs.python.org/3/library/functions.html#isinstance the tuples can be nested. I found an actual example of this while typechecking `django.http.response.HttpResponse.content`
",2016-08-18T13:55:35Z,200733
3246,python/mypy,98183747,290106860,"I have come across this while trying to define a generic JSON type:

```
JSON = Union[Dict[str, ""JSON""], List[""JSON""], str, int, float, bool, None]
```
So consider this a +1 for supporting this use case.",2017-03-29T14:26:30Z,52799
3247,python/mypy,98183747,317401621,"@srittau JSON needs to be Any because it is recursive and you can give json.loads a custom JSONEncoder class:

```python
_PlainJSON = Union[Dict[str, ""_PlainJSON""], List[""_PlainJSON""], str, int, float, bool, None]
_T = TypeVar('_T')
JSON = Union[_PlainJSON, _T, Dict[str, ""JSON""], List[""JSON""]]
def loads(data: str, cls: Type[JSONEncoder[_T]]) -> JSON: ...
```

of course recursive types and  `Type[JSONEncoder[_T]]` types arn't supported.",2017-07-24T12:05:26Z,413772
3248,python/mypy,98183747,358726640,"The following pattern seems to be good enough for my purposes. The boilerplate is tolerable for me.
```
class BTree(NamedTuple):
    val: int
    left_ : Any
    right_ : Any

    # typed constructor
    @staticmethod
    def c(val: int, left: Optional['BTree'] = None, right: Optional['BTree'] = None) -> 'BTree':
        return BTree(val, left, right)

    # typed accessors
    @property
    def left(self) -> Optional['BTree']:
        return cast(Optional[BTree], self.left_)
    @property
    def right(self) -> Optional['BTree']:
        return cast(Optional[BTree], self.right_)

atree = BTree.c(1, BTree.c(2, BTree.c(3), BTree.c(4)), BTree.c(5))
atree2 = BTree.c(1, BTree.c(2, BTree.c(3), BTree.c(4)), BTree.c(5))
assert atree == atree2
assert isinstance(atree,BTree) and isinstance(atree.left,BTree) and isinstance(atree.left.left,BTree)
```",2018-01-18T17:49:21Z,1623534
3249,python/mypy,98183747,365713822,"Latest version of pattern. We use this example at [Legalese](https://github.com/legalese/legalese-compiler/tree/master/linear_state_machine_language/pyL4) for interacting with SMT solvers (the [SMTLIB language](http://smtlib.github.io/jSMTLIB/SMTLIBTutorial.pdf)). 

I found that I ended up forgetting to use the typed `.c` static method instead of the untyped constructor in the `BTree` example above. This version addresses that. It's only very minor deficits are:

1. Boilerplate (which I don't care about if the datatype is significant enough for us to care about the difference between an immutable recursive datatype and `Tuple[Any,...]`)
2. The constructor `SMTExprNonatom` and the type `SMTExprNonatom_` do not share the same name. But this is only aesthetic; You won't use one when you mean the other, since with this naming convention, `SMTExprNonatom` will come up first in autocomplete, and only `SMTExprNonatom_` can be used in a type position.

```
# Immutable recursive datatypes pattern
SMTAtom = Union[str, int, float, bool]
SMTExpr = Union['SMTExprNonatom_',SMTAtom]
class SMTExprNonatom_(NamedTuple):  
    symb: str
    args_: Tuple[Any,...] # see `args` below
    @staticmethod  # typed constructor, which we alias to `SMTExprNonatom` after the class def
    def c(symb:str, args:Iterable[SMTExpr]) -> 'SMTExprNonatom_': return SMTExprNonatom_(symb, tuple(args))
    @property # typed accessor
    def args(self) -> Tuple[SMTExpr]: return cast(Tuple[SMTExpr], self.args_)
SMTExprNonatom = SMTExprNonatom_.c
SMTCommand = NewType('SMTCommand', SMTExprNonatom_)
```

",2018-02-14T19:13:41Z,1623534
3250,python/mypy,98183747,390050168,Updating to normal priority since this comes up frequently and the implementation got easier with some recent changes.,2018-05-17T23:57:29Z,1107911
3251,python/mypy,98183747,390314848,"@JukkaL, anything I could do to help?",2018-05-18T19:53:24Z,1623534
3252,python/mypy,98183747,390345684,"@DustinWehr We are not ready to start implementing this yet -- it's still a pretty complex feature, and we have other high-priority work scheduled for the near future. Finding additional real-world use cases where recursive types could be used would be helpful, though. JSON is the canonical use case, but beyond that we don't have many examples.",2018-05-18T22:09:23Z,1107911
3253,python/mypy,98183747,390403225,"An example would be describing a typed AST, as those are usually recursive",2018-05-19T12:56:32Z,14644
3254,python/mypy,98183747,390423917,We use a library called [asynq](https://github.com/quora/asynq/tree/master/asynq) and it has a recursive type for futures.,2018-05-19T18:30:28Z,205628
3255,python/mypy,98183747,390436086,"Any real-world use case for typical strongly-typed FP languages would be a
real-world use case, since you can do algebraic datatypes with recursive
types, NamedTuple, and Union.

On Sat, May 19, 2018 at 11:30 AM, Liran Nuna <notifications@github.com>
wrote:

> We use a library called asynq
> <https://github.com/quora/asynq/tree/master/asynq> and it has a recursive
> type for futures.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/python/mypy/issues/731#issuecomment-390423917>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/ABjF7mfPjT4NNau4NpAEmY_6F86tCc_gks5t0GTJgaJpZM4FizHE>
> .
>
",2018-05-19T22:13:32Z,1623534
3256,python/mypy,98183747,392134768,"@JukkaL in case you are skeptical that people (besides the teams speaking up in this thread) are using python for typical FP use cases, a couple serious example projects of that type are:
http://microsoft.github.io/ivy/language.html
https://en.wikipedia.org/wiki/SageMath",2018-05-25T17:57:38Z,1623534
3257,python/mypy,98183747,396411761,"Just a dump of a recent discussion with @JukkaL:
* We will use µ-type representation of recursive types, i.e. `TypeAlias` node + `TypeAliasInstance` type.
* We can support generic recursive aliases like `A = Tuple[T, A[T]]`, and then `A[int]`.
* µ-type approach solves the serialization problem, i.e. the same `cross_ref` dance can be repeated _only_ for `TypeAlias` vs `TypeAliasInstance` as for `TypeInfo` vs `Instance`.
* Subtyping, meets, joins, etc. will be calculated essentially as for protocols with a single covariant member. We will have separate assumption stacks at the top of corresponding functions (to not mix with protocols).
* Calling `s = expand_once(s)`, `t = expand_once(t)` at the top of type-expecting functions will save many `isinstance()` checks for branches for different type kinds.
* However, mypy has a lot of _ad-hoc_ subtyping checks that needs to be updated in addition, we could use either an lgtm query or a simple mypy visitor to find all relevant `isinstance()` calls.
* Refactoring that will introduce `TypeAlias` symbol node (for unrelated benefits) is underway, PR will be up shortly.
* Realistic time for implementation is 2-3 weeks.
* Potential  target schedule (taking into account other tasks and priorities) is January-February 2019.",2018-06-11T22:52:31Z,12005495
3258,python/mypy,98183747,421448605,Just stumbled over this from the JSON example perspective... Is there any timeline/roadmap for recursive types already?,2018-09-14T18:39:46Z,1001778
3259,python/mypy,98183747,421449445,> Potential target schedule (taking into account other tasks and priorities) is January-February 2019.,2018-09-14T18:42:50Z,413772
3260,python/mypy,98183747,421452238,"We have a workaround in our codebase:

```
JSON_PRIMITIVE = Union[None, bool, str, int, float]
if TYPE_CHECKING:
    # To avoid hitting an Any in mypy because of the recursive type we add a
    # couple of layers of non-recursive types.
    JSON_BASE = Union[  # type: ignore # Recursive type
        JSON_PRIMITIVE, Dict[str, ""JSON_BASE""], List[""JSON_BASE""]
    ]
    JSON2 = Union[  # type: ignore # Recursive type
        JSON_PRIMITIVE, Dict[str, JSON_BASE], List[JSON_BASE]
    ]
    JSON1 = Union[  # type: ignore # Recursive type
        JSON_PRIMITIVE, Dict[str, JSON2], List[JSON2]
    ]
    JSON = Union[  # type: ignore # Recursive type
        JSON_PRIMITIVE, Dict[str, JSON1], List[JSON1]
    ]
else:
   JSON = Union[JSON_PRIMITIVE, Dict[str, ""JSON""], List[""JSON""]]
```

This gives us a couple of layers of Dict and Lists before we get to `Any`.  However we've learned that using `JSON` is exceedingly painful because of that `Union`  You have to either `isinstance` very heavily or `cast` to get around issues.  `Dict[str, Any]` is sadly the name of the game most of the time.

",2018-09-14T18:53:29Z,1081858
3261,python/mypy,98183747,421504065,"I ended up going with this in a previous project - you can construct a json parser given a typing.NamedTuple class that matches the json structure

https://gist.github.com/Daenyth/37d615e502114009d6a33652a814a7c8",2018-09-14T22:50:17Z,14644
3262,python/mypy,98183747,423845865,"Speaking of workarounds for JSON, here is a pretty solid workaround using protocols. @ilevkivskyi, @JukkaL, do you see any potential gotchas beyond mimicking the required behavior of `list` and `dict` (which could be improved by including more functions)? Also, I'm not sure about performance.

```python
from typing import Any, Dict, Union, Sequence, overload
from typing_extensions import Protocol


class _JSONArray(Protocol):
    def __getitem__(self, idx: int) -> 'JSONLike': ...
    # hack to enforce an actual list
    def sort(self) -> None: ...


class _JSONDict(Protocol):
    def __getitem__(self, key: str) -> 'JSONLike': ...
    # hack to enforce an actual dict
    @staticmethod
    @overload
    def fromkeys(seq: Sequence[Any]) -> Dict[Any, Any]: ...
    @staticmethod
    @overload
    def fromkeys(seq: Sequence[Any], value: Any) -> Dict[Any, Any]: ...


JSONLike = Union[str, int, float, bool, None, _JSONArray, _JSONDict]


obj: JSONLike = {""1"": 1}  # ok
obj2: JSONLike = {1: 1}  # fail
obj3: JSONLike = [1, 2, {3: [5, 6]}]  # fail
obj4: JSONLike = {""s"": [None]}  # ok
obj5: JSONLike = [[b'a']]  # fail
```",2018-09-23T20:34:11Z,940353
3263,python/mypy,98183747,423853166,"Hm, ok, the limitation of this approach obviously being that it's ""one-directional"", one would still have to cast JSONArray/Dict to List/Dict when getting out of JSON.",2018-09-23T22:35:57Z,940353
3264,python/mypy,98183747,425723814,"> @DustinWehr ...Finding additional real-world use cases where recursive types could be used would be helpful, though. JSON is the canonical use case, but beyond that we don't have many examples.

@JukkaL I suppose it's a pretty similar problem to that of JSON, but I've been trying to add type hints to apache/avro. Without recursive types I can't express the shape of avro schema completely.

Here is [the work in progress](https://github.com/kojiromike/avro/blob/py3-typehints/lang/py3/avro/schema.py). Variance is hard and mypy has found a lot of problems, so I expect to be working on this for a while.",2018-09-30T14:16:01Z,1566303
3265,python/mypy,98183747,438250977,"About real-world use cases where recursive types could be useful, they are useful when writing programs that deal with explicitly recursive data structures (not just JSON), which encompasses a large class of programs. Such programs may be written and properly specified without recursive data types, but they can  be needed to provide a complete type specification of some implementations.",2018-11-13T12:36:29Z,1682742
3266,python/mypy,98183747,448628435,"Another real world use case. I'm writing a function for a CMS which builds a page tree from a value like this:

```python
[
    (
        Page(title='Page 1'),
        [
            (Page(title='Page 2'), ...),
            (Page(title='Page 2'), ...),
        ]
    ),
]
```

where `...` are nested iterables of the same structure, representing the child pages. The type would be something like 

```
PageTree = Iterable[Tuple[Page, 'PageTree']]
```",2018-12-19T15:09:55Z,296631
3267,python/mypy,98183747,452415828,"> > Potential target schedule (taking into account other tasks and priorities) is January-February 2019.

I'm not sure if there is any update on the timeline for this, but if it won't be soon, would it be possible to add a config option to suppress the 'Recursive types not fully supported yet, nested types replaced with ""Any""' error message?

I'm okay with my project having Anys for now, and it'd be nice to be able to write the recursive types now and not need to go back and replace them once recursive types are implemented.",2019-01-08T19:08:44Z,9029949
3268,python/mypy,98183747,452417151,"> I'm not sure if there is any update on the timeline for this

I think we are still on time (most likely late February).",2019-01-08T19:12:40Z,12005495
3269,python/mypy,98183747,467595721,Is there a place to watch or contribute to progress on this?,2019-02-26T20:15:05Z,2671967
3270,python/mypy,98183747,467598456,@tedkornish https://github.com/python/mypy/issues/6204 and any other commits into newsemnal,2019-02-26T20:22:48Z,413772
3271,python/mypy,98183747,469683666,@tedkornish https://github.com/python/mypy/commits/master/mypy/newsemanal,2019-03-05T13:42:32Z,413772
3272,python/mypy,98183747,505951615,"@JukkaL it looks like newsemnal landed, did it include support for recursive types?",2019-06-26T16:35:16Z,413772
3273,python/mypy,98183747,505952962,It looks like we'll need to wait a bit more until we add recursive type support. I can't give a new estimate yet.,2019-06-26T16:39:05Z,1107911
3274,python/mypy,98183747,539905783,"> Hm, ok, the limitation of this approach obviously being that it's ""one-directional"", one would still have to cast JSONArray/Dict to List/Dict when getting out of JSON.

A work-around proposal that does not require casting to List/Dict:
```python
from typing import Optional, Union, Iterable, Mapping, List, Dict


Primitive = Union[str, int, bool]
JsonType = Optional[Union[Primitive, ""JsonList"", ""JsonDict""]]


class JsonList(list):
    """"""
        List type containing MyType elements.
    """"""
    def __init__(self, iterable: Iterable[JsonType]) -> None:
        super().__init__(iterable)


class JsonDict(dict):
    """"""
        Dict type with str keys and JsonType values.
    """"""
    def __init__(self, mapping: Optional[Mapping[str, JsonType]] = None, **kwargs: JsonType) -> None:
        if mapping is None:
            super().__init__(**kwargs)
        else:
            super().__init__(mapping, **kwargs)


lst: JsonList = JsonList([
    1,
    JsonList([
        2,
        JsonList([
            3,
            JsonList([
                4,
                JsonList([
                    5,
                    JsonDict(
                        recursion=True
                    ),
                    True,
                    None
                ])
            ])
        ])
    ])
])
dct: Dict[str, JsonType] = JsonDict(
    name=""myName"",
    lst=lst,
    integer=12,
    none=None
)

dct_lst: JsonType = dct.get(""lst"")
dct_integer: JsonType = dct[""integer""]
generic_list: List[JsonType] = lst
lst_item: JsonType = lst[0]
generic_list_item: JsonType = generic_list[0]
```
Of course, the Primitive type should be expanded to include all desired primitives.",2019-10-09T08:53:07Z,7672159
3275,python/mypy,98183747,542907889,"@sanderr your code is very useful, i've modified it into a strict version that doesn't use `Any` and passes the strictest configuration https://gist.github.com/catb0t/bd82f7815b7e95b5dd3c3ad294f3cbbf",2019-10-16T21:55:12Z,3238748
3276,python/mypy,98183747,543053147,"@catb0t I opted for the generic `list` and `dict` types over `List[JsonType]` and `Dict[str, JsonType]` because one of my vim plugins complained: `no-member: Instance of 'JsonDict' has no 'get' member` and similar. I assumed this complaint came from mypy, but it appears to be something else, so it seems like your modification is indeed better.
Two observations:
- if you inherit from the specific list and dict types as you do, I believe it is not even necessary to override `__init__`.
- if you do decide to override `__init__`, be aware that I wrote above more as a proof of concept than as a stable drop-in replacement for list and dict types. I believe the dict constructor in particular is a bit more complex than what I wrote, so it might be worth looking into that. For example, the code snippet below shows a valid construction of a dict, that isn't typed correctly by my implementation:
```python
dict([(""key"", ""value""), (""other_key"", ""other_value"")])
JsonDict([(""key"", ""value""), (""other_key"", ""other_value"")])
```",2019-10-17T07:53:10Z,7672159
3277,python/mypy,98183747,551188218,@ilevkivskyi Does this also work now with the new analyzer?,2019-11-07T17:45:05Z,585279
3278,python/mypy,98183747,551235617,"Not yet, @ilevkivskyi is actually currently working on adding support for recursive types. He will know the exact schedule better but it looks like mypy will support recursive types in the fairly near future.",2019-11-07T19:48:50Z,906600
3279,python/mypy,98183747,587263475,"Are there any updates @ilevkivskyi ?
Is there any help needed?",2020-02-18T03:39:24Z,1488983
3280,python/mypy,98183747,587795867,"@ir4y There are only sad updates: I am no longer working on this, and likely will not work on this in foreseeable future. You can try pushing it yourself, but it is not an easy task, and I can't guide you, sorry (others may still have time for this).",2020-02-18T20:30:03Z,12005495
3281,python/mypy,98183747,588587181,"@ilevkivskyi 

>it is not an easy task  

Sure it is. Otherwise, it will be already done.

I will try to add support for recursive types.

>I can't guide you  

It is sad. 

Could you at least provide some details that may be helpful?
What is a good start point for this task?
Do you have any WIP pull-request?
What common issues I'll face?",2020-02-20T03:10:35Z,1488983
3282,python/mypy,98183747,589349275,"There are a couple of other type-checkers for Python out there...

Facebook released [`pyre-check`](https://pyre-check.org/). I don't know if it supports recursive types because I get cryptic parse errors when trying to run it on my project.

Google's [`pytype`](https://github.com/google/pytype) seems to handle recursive types ok, I can run it on my project with the `# type: ignore` comments removed from the recursive types and it's all fine.
Less anecdotally, I think one of the devs commented here that it is supported:
https://lobste.rs/s/0uv5hy/how_quickly_find_type_issues_your_python#c_df7acb

...maybe something can be learned from the `pytype` codebase to bring this feature into `mypy`?",2020-02-20T21:57:31Z,147840
3283,python/mypy,98183747,589495785,"@anentropic 
There is also [pyrigth](https://github.com/Microsoft/pyright) by Microsoft.",2020-02-21T04:44:51Z,1488983
3284,python/mypy,98183747,591568187,"Here's a very short summary of recursive types could be implemented (on top of the foundation that @ilevkivskyi has already implemented).

A simple self-recursive type would use `TypeAlias` that has a `TypeAliasType` that contains the recursive reference. `TypeAliasType` instances can be expanded an arbitrary number of times (the expansion for recursive types is infinite, so we'd only expand as much as is needed). The tricky bit is that type operations such as subtype checks would need to limit recursion when there are type alias types, similar to how we deal with recursive protocols right now. (see `_assuming` etc. in `TypeState` and how it's used). 

When I have more time I can write a more detailed description and/or provide links to external resources.

To start with, I'd recommend trying to support some basic operations for a simple recursive type such as `X = Union[int, List['X']`.

We can disable support for recursive types initially while the implementation is incomplete, and provide a command-line option to enable them. This way the feature can be implemented incrementally, and PRs can be merged to master even if things are not quite ready. Even if whoever starts to work on this can't finish the implementation, somebody else can later continue the work, without impacting mypy users.",2020-02-26T18:18:04Z,1107911
3285,python/mypy,98183747,606953645,This is the most thumbed-up open issue by a large margin,2020-04-01T00:13:40Z,26337069
3286,python/mypy,98183747,606957464,"@Mattwmaster58 unfortunately it is also rather complicated, as it requires a lot of changes to mypy. Ivan has done a lot of work on this, but there is still more to do, and he isn't working on it anymore.

We understand there is great interest and need for recursive types, but open source works at the speed of people's spare time and interest.",2020-04-01T00:26:40Z,9504279
3287,python/mypy,98183747,612227307,Is there anything that newcomers to the project might be able to realistically help with this?,2020-04-10T21:32:33Z,1554778
3288,python/mypy,98183747,657236055,"For anyone else trying to do recursive Callables. A callback protocol seems to work fine:
```
class Foo(Protocol):
    def __call__(self) -> Awaitable['Foo']: ...
```
or similar.",2020-07-12T15:19:27Z,2203121
3289,python/mypy,98183747,658618088,"I would also like to express interest in this feature.
In particular:
```
NestedStrDict = Dict[str, Union[NestedStrDict, int]]
```
For objects such as:
```
obj = {
    'a' : 0,
    'nested_dict' : 
    { 
        'b' : 1,
        'c' : 2,
        'nested2_dict' : 
        {
             'd' : 3
        }
    }
}
```",2020-07-15T08:13:11Z,32304768
3290,python/mypy,98183747,699503844,"**Edit:** I did not see the comment https://github.com/python/mypy/issues/731#issuecomment-423845865 by @jhrmnn when first reading through this issue. I guess I am proposing the same thing and wondering exactly how much work remains to be done before we get a protocol-based solution to (some) recursive types, like the one described in that/this comment. (I should disclaim that I am an avid user of Mypy but not sufficiently acquainted with the nitty-gritty of its implementation. That said: if it is *realistic* for me to help with this issue, I'm certainly happy to help.)

**Original Comment**

It seems that some additional support for list literals and dictionary literals might make it possible to implement a recursive JSON-like type using protocols, as follows:

```python
from typing import Iterator, Protocol, runtime_checkable, Union
from typing import KeysView, ValuesView, ItemsView

@runtime_checkable
class JSONSequence(Protocol):
    """"""
        Protocol for a sequence of JSON values.
    """"""

    def __getitem__(self, idx: int) -> ""JSONLike"":
        ...

    def __contains__(self, value: ""JSONLike"") -> bool:
        ...

    def __len__(self) -> int:
        ...

    def __iter__(self) -> Iterator[""JSONLike""]:
        ...

    def __reversed__(self) -> Iterator[""JSONLike""]:
        ...

@runtime_checkable
class JSONMapping(Protocol):
    """"""
        Protocol for a mapping of strings to JSON values.
    """"""

    def __getitem__(self, key: str) -> ""JSONLike"":
        ...

    def __contains__(self, key: str) -> bool:
        ...

    def __len__(self) -> int:
        ...

    def __iter__(self) -> Iterator[str]:
        ...

    def keys(self) -> KeysView[str]:
        ...

    def values(self) -> ValuesView[""JSONLike""]:
        ...

    def items(self) -> ItemsView[str, ""JSONLike""]:
        ...

JSONLike = Union[None, bool, int, str, JSONSequence, JSONMapping]
"""""" Recursive JSON type. """"""
```

Indeed, it seems that Mypy (v  0.782) already recognises as correct some of the interesting combinations, and only requires little nudging in other cases.

```python
# ... Continued from before ...
from typing import cast

x: JSONLike = None                                  # OK
x = True                                            # OK
x = 42                                              # OK
x = ""hello""                                         # OK
x = []                                              # OK
x = [None, True, 43, ""hello""]                       # OK
x = {""the final answer"": 42}                        # OK
x = {""burn baby burn"": ""master ignition routine""}   # OK
x = {
    ""the final answer"": cast(JSONLike, 42),
    ""burn baby burn"": ""master ignition routine""
}                                                   # OK
x = {
    ""the final answer"": cast(JSONLike, 42),
    ""burn baby burn"": ""master ignition routine"",
    ""a bunch of stuff"": {
        ""none"": None,
        ""bool"": True,
        ""int"": 43,
        ""str"": ""hello"",
        ""list"": [None, True, 43, ""hello""]
    }
}                                                   # OK

x = {}
# Mypy Error: assignment - Incompatible types in assignment
# expression has type ""Dict[<nothing>, <nothing>]"",
# variable has type ""Union[None, bool, int, str, JSONSequence, JSONMapping]""

x = {
    ""the final answer"": 42,
    ""burn baby burn"": ""master ignition routine""
}
# Mypy Error: assignment - Incompatible types in assignment
# expression has type ""Dict[str, object]"",
# variable has type ""Union[None, bool, int, str, JSONSequence, JSONMapping]""
```

One has to be a little careful with runtime typechecking, e.g. because strings and dictionaries match the protocol for JSON sequences (at least as written above), but I'm confident that this could be remedied with little additional work:

```python
# ... Continued from before ...
from collections import deque
from typing import Dict

def to_strict_json(json_like: JSONLike) -> Union[None, bool, int, str, list, dict]:
    """"""
        Recursively converts `JSONSequence`s to `list`s
        and `JSONMapping`s to `dict`s.
    """"""
    if isinstance(json_like, str):
        # strings match the `JSONSequence` protocol, must intercept.
        return json_like
    if isinstance(json_like, JSONMapping):
        # dictionaries match the `JSONSequence` protocol, must intercept
        return {k: to_strict_json(v) for k, v in json_like.items()}
    if isinstance(json_like, JSONSequence):
        return [to_strict_json(x) for x in json_like]
    return json_like

class MyReadonlyDict(JSONMapping):
    """""" A silly readonly dictionary wrapper. """"""
    _dict: Dict[str, JSONLike]
    def __init__(self, d):
        super().__init__()
        self._dict = dict(d)
    def __getitem__(self, key: str) -> JSONLike:
        return self._dict[key]
    def __contains__(self, key: str) -> bool:
        return key in self._dict
    def __len__(self) -> int:
        return len(self._dict)
    def __iter__(self) -> Iterator[str]:
        return iter(self._dict)
    def keys(self) -> KeysView[str]:
        return self._dict.keys()
    def values(self) -> ValuesView[JSONLike]:
        return self._dict.values()
    def items(self) -> ItemsView[str, JSONLike]:
        return self._dict.items()

x = deque([None, True, 43, MyReadonlyDict({'a str': ['hello', 'bye bye']})]) # OK
print(to_strict_json(x))
# [None, True, 43, {'a str': ['hello', 'bye bye']}]
```

PS: I'm just throwing this out there as an instance of protocols already being ""close enough"" for some recursive types to be written. I'm not saying that this would necessarily be a full solution for a JSON type (cf https://github.com/python/typing/issues/182).",2020-09-26T14:37:53Z,13587235
3291,python/mypy,98183747,738848192,"If I remember correctly, `# type: ignore` used to suppress recursive type errors, but in the current version (`mypy==0.790`) it doesn't seem to work - I'm seeing a lot of these errors in spite of `# type: ignore`. Am I missing something?",2020-12-04T15:31:33Z,10007896
3292,python/mypy,98183747,738851478,@altvod https://github.com/python/mypy/issues/7069,2020-12-04T15:37:24Z,218551
3293,python/mypy,98183747,738861761,@antonagestam that issue is closed. Does this mean that there isn't (and will not be) a way to suppress these errors until recursive types are fully implemented?,2020-12-04T15:56:07Z,10007896
3294,python/mypy,98183747,738868993,@altvod I think that is a safe assumption to make.,2020-12-04T16:09:33Z,218551
3295,python/mypy,98183747,738872212,"An option to suppress errors due to recursive types is a very reasonable request, and far easier to implement than _full support_ for recursive types. Especially now that this issue is basically disowned, it's not clear when someone will pick it up again.

Please open a separate issue for the suppresion capability.
",2020-12-04T16:15:38Z,137616
3296,python/mypy,98183747,738899329,I realized that I had in fact a slightly different error: #8695,2020-12-04T17:08:06Z,10007896
3297,python/mypy,98183747,752157487,"It's clear this issue is complicated,  but just want to provide a TypedDict recursive example that is probably pretty common and failing for anyone searching.

Self Reference: 
```
from typing import TypedDict, List

Comment = TypedDict('Comment', {
  ""text"": str,
  ""childComments"": List['Comment']
})
```

Deep Reference:
```
from typing import TypedDict, List

Comment = TypedDict('Comment', {
  ""text"": str,
  ""reply"": ""CommentThread""
})

CommentThread = TypedDict('CommentThread', {
  ""likes"": int,
  ""comments"": List[Comment]
})
```

Both display error: **possible cyclic definition**
",2020-12-29T16:54:28Z,5788448
3298,python/mypy,98183747,885963694,"`JSON = Union[Dict[str, ""JSON""], List[""JSON""], str, int, float, bool, None]`
This example while classic case plays poorly with the type variance rules in python. You would hope that List[int] is a JSON type but it is not due to variance. Expanding the JSON type for the list case it is,

List[Union[Dict[str, ""JSON""], List[""JSON""], str, int, float, bool, None]]]

List[int] is not a subtype of List[Union[int, ...]]. You can make covariant json if you are willing to use mapping/sequence but if you want an actual list/dict or intend your json type to be mutable it'll be hard to keep the type variance rules and have useful recursive types involving any invariant generic.

pyright has [recursive type support](https://github.com/microsoft/pyright/issues/2115), but actually writing a json type with list/dict and using list with it don't cooperate with the variance rules. I think any solution would require updating the variance rules specifically for recursive types.

edit: My understanding was wrong thinking/discussing it more. The issue goes away if you are fine with list/dict elements being non homogenous (so json).",2021-07-23T23:42:56Z,16809055
3299,python/mypy,98183747,922760064,"Ever since the most recent mypy release there seems to be some basic support for recursive types, as can been seen in the recursive protocol version of `Sequence` below:

``` python
from __future__ import annotations

from collections.abc import Iterator
from typing import TypeVar, Protocol, overload, Any, TYPE_CHECKING

_T_co = TypeVar(""_T_co"")

class _RecursiveSequence(Protocol[_T_co]):
    def __len__(self) -> int: ...
    @overload
    def __getitem__(self, __index: int) -> _T_co | _RecursiveSequence[_T_co]: ...
    @overload
    def __getitem__(self, __index: slice) -> _RecursiveSequence[_T_co]: ...
    def __contains__(self, __x: object) -> bool: ...
    def __iter__(self) -> Iterator[_T_co | _RecursiveSequence[_T_co]]: ...
    def __reversed__(self) -> Iterator[_T_co | _RecursiveSequence[_T_co]]: ...
    def count(self, __value: Any) -> int: ...
    def index(self, __value: Any, __start: int = ..., __stop: int = ...) -> int: ...


def func1(a: _RecursiveSequence[int]) -> int: ...

if TYPE_CHECKING:
    reveal_type(func1([1]))         # Revealed type is ""builtins.int""
    reveal_type(func1([[1]]))       # Revealed type is ""builtins.int""
    reveal_type(func1([[[1]]]))     # Revealed type is ""builtins.int""
    reveal_type(func1((1, 2, 3)))   # Revealed type is ""builtins.int""
    reveal_type(func1([(1, 2, 3)])) # Revealed type is ""builtins.int""
    reveal_type(func1([True]))      # Revealed type is ""builtins.int""

```

The only area where mypy still fails is if typevars are involved. This is a bit of a shame, but the fact that there is now basic support for recursive types is already a big step forward.

``` python
_T = TypeVar(""_T"")

def func2(a: npt._NestedSequence[_T]) -> _T: ...

seq_1d_a: list[int]
seq_1d_b = [1]
seq_2d_a: list[list[int]]
seq_2d_b = [[1]]

if TYPE_CHECKING:
    # The good
    reveal_type(func2(seq_1d_a))  # Revealed type is ""builtins.int*""

    # The bad
    reveal_type(func2(seq_1d_b))  # Revealed type is ""Any""
    reveal_type(func2(seq_2d_a))  # Argument 1 to ""func"" has incompatible type ""List[List[int]]""; expected ""_NestedSequence[<nothing>]""
    reveal_type(func2(seq_2d_b))  # Revealed type is ""Any""
```",2021-09-20T09:20:24Z,43369155
3300,python/mypy,98183747,1100866510,"> Ever since the most recent mypy release there seems to be some basic support for recursive types, as can been seen in the recursive protocol version of `Sequence` below:
> 
> ```python
> from __future__ import annotations
> 
> from collections.abc import Iterator
> from typing import TypeVar, Protocol, overload, Any, TYPE_CHECKING
> 
> _T_co = TypeVar(""_T_co"")
> 
> class _RecursiveSequence(Protocol[_T_co]):
>     def __len__(self) -> int: ...
>     @overload
>     def __getitem__(self, __index: int) -> _T_co | _RecursiveSequence[_T_co]: ...
>     @overload
>     def __getitem__(self, __index: slice) -> _RecursiveSequence[_T_co]: ...
>     def __contains__(self, __x: object) -> bool: ...
>     def __iter__(self) -> Iterator[_T_co | _RecursiveSequence[_T_co]]: ...
>     def __reversed__(self) -> Iterator[_T_co | _RecursiveSequence[_T_co]]: ...
>     def count(self, __value: Any) -> int: ...
>     def index(self, __value: Any, __start: int = ..., __stop: int = ...) -> int: ...
> 
> 
> def func1(a: _RecursiveSequence[int]) -> int: ...
> 
> if TYPE_CHECKING:
>     reveal_type(func1([1]))         # Revealed type is ""builtins.int""
>     reveal_type(func1([[1]]))       # Revealed type is ""builtins.int""
>     reveal_type(func1([[[1]]]))     # Revealed type is ""builtins.int""
>     reveal_type(func1((1, 2, 3)))   # Revealed type is ""builtins.int""
>     reveal_type(func1([(1, 2, 3)])) # Revealed type is ""builtins.int""
>     reveal_type(func1([True]))      # Revealed type is ""builtins.int""
> ```
> 
> The only area where mypy still fails is if typevars are involved. This is a bit of a shame, but the fact that there is now basic support for recursive types is already a big step forward.
> 
> ```python
> _T = TypeVar(""_T"")
> 
> def func2(a: npt._NestedSequence[_T]) -> _T: ...
> 
> seq_1d_a: list[int]
> seq_1d_b = [1]
> seq_2d_a: list[list[int]]
> seq_2d_b = [[1]]
> 
> if TYPE_CHECKING:
>     # The good
>     reveal_type(func2(seq_1d_a))  # Revealed type is ""builtins.int*""
> 
>     # The bad
>     reveal_type(func2(seq_1d_b))  # Revealed type is ""Any""
>     reveal_type(func2(seq_2d_a))  # Argument 1 to ""func"" has incompatible type ""List[List[int]]""; expected ""_NestedSequence[<nothing>]""
>     reveal_type(func2(seq_2d_b))  # Revealed type is ""Any""
> ```

I saw the module [_nested_sequence.py](https://github.com/numpy/numpy/blob/main/numpy/_typing/_nested_sequence.py) available in [numpy](https://github.com/numpy/numpy), so I tried to copy/paste it into my project, but I can't get it to work for simple use cases:

```python

v: _NestedSequence[int]
v1: _NestedSequence[int]
v2: _NestedSequence[int]
v3: _NestedSequence[int]
v4: _NestedSequence[int]

# Fails as expected: 
# Incompatible types in assignment (expression has type ""int"", variable has type ""_NestedSequence[int]"")  [assignment]mypy(error)
v = 1

# Does not fail as expected
v1 = [1]
# Does not fail as expected
v2 = [[1]]

# Does not fail, but I expected a failure
v3 = [""a""]
v4 = [[""a""]]
```

@BvB93  Are you aware of such behavior or is there a problem on my side ?

I'm using `Python 3.8.10`, with `mypy==0.942` and `mypy-extensions==0.4.3`.

My `mypy` config is:

```ini
[mypy]
exclude = notebooks
scripts_are_modules = True
show_traceback = True
disallow_incomplete_defs = True
check_untyped_defs = True
disallow_untyped_defs = True
disallow_untyped_decorators = True
disallow_any_generics = True
warn_no_return = True
no_implicit_optional = True
strict_optional = True
warn_redundant_casts = True
warn_unused_ignores = True
warn_return_any = True
warn_unreachable = True
show_error_codes = True
show_column_numbers = True
ignore_missing_imports = True
implicit_reexport = True
plugins = pydantic.mypy
```

Thanks and happy easter ! 

> EDIT: I created [a reproducible example](https://github.com/charbonnierg/mypy-nested-sequence-issue)",2022-04-17T12:32:20Z,81654818
3301,python/mypy,98183747,1100902810,"Pyright/Pylance has recursive type definition support. It's good idea to mimic that
https://devblogs.microsoft.com/python/pylance-introduces-five-new-features-that-enable-type-magic-for-python-developers/",2022-04-17T15:36:10Z,39331266
3302,python/mypy,98183747,1100931241,"I understand now that the problem only occurs when using literal values which are not annotated. Using `_NestedSequence` as implemented in `numpy.typing`:

```python
def func1(a: _NestedSequence[int]) -> int: ...


# Fails as expected:
# Incompatible types in assignment (expression has type ""int"", variable has type ""_NestedSequence[int]"")  [assignment]mypy(error)
v1 = func1(1)   # type: ignore[arg-type]

# Does not fail as expected
v2 = func1([1])
# Does not fail as expected
v3 = func1([[1]])

# Does not fail
v4 = func1([""a""])  # [arg-type] error was expected
v5 = func1([[""a""]])  # [arg-type] error was expected

# Does fail as expected
input_: List[str] = [""a""]
input__: List[List[str]] = [[""a""]]
v4_ = func1(input_) # type: ignore[arg-type]
v5 = func1(input__) # type: ignore[arg-type]
```",2022-04-17T18:48:29Z,81654818
3303,python/mypy,98183747,1100934132,"As @eugene-bright mentioned, pyright supports recursive type aliases, allowing for simple recursive type definitions like this:

```python
_RecursiveSequence = _T_co | Sequence[""_RecursiveSequence[_T_co]""]
```

This works with all of the examples above.

```python
def func1(a: _RecursiveSequence[_T_co]) -> _T_co:
    ...

reveal_type(func1(1))  # Revealed type is ""int""
reveal_type(func1([1]))  # Revealed type is ""int""
reveal_type(func1([[1]]))  # Revealed type is ""int""
reveal_type(func1([[[1]]]))  # Revealed type is ""int""
reveal_type(func1((1, 2, 3)))  # Revealed type is ""int""
reveal_type(func1([(1, 2, 3)]))  # Revealed type is ""int""
reveal_type(func1([True]))  # Revealed type is ""bool""

reveal_type(func1([""a""]))  # Revealed type is ""str""
reveal_type(func1([[""a""]]))  # Revealed type is ""str""

input1: list[str] = [""a""]
input2: list[list[str]] = [[""a""]]
reveal_type(func1(input1))  # Revealed type is ""str""
reveal_type(func1(input2))  # Revealed type is ""str""
```

Pyre supports recursive type aliases as well. It would be great if mypy added this support so library authors could make use of the feature.",2022-04-17T19:10:30Z,7040122
3304,python/mypy,98183747,1100939505,"But why are you all talking about pyright? Unless I'm missing something, we're writing comments on the mypy repository ?
Switching to pyright right now for the sole benefice of recursive typing is out of question for me at the moment. Moreover, I'm quite satisfied with the basic recursion features mypy offer. 
My question concerns a behavior of mypy which I found strange, (maybe incorrect?) encountered when trying a solution proposed in this thread. Maybe I should have open an issue on numpy repository, but I thought it was more of a mypy issue and did not want to create a duplicate issue... ",2022-04-17T19:48:54Z,81654818
3305,python/mypy,98183747,1100940563,"Eric is the author of pyright, so it's hardly surprising that he talks about it :) I think it's helpful to consider what other type checkers do as we consider how to add recursive type support to mypy.",2022-04-17T19:57:01Z,906600
3306,python/mypy,98183747,1100944742,"Sorry if my answer seemed harsh, I can only agree with your statement. I do realize that I was looking for an anwser to my question more than discussing how to implement recursive types, my bad. Do you know where such question should be asked though ?",2022-04-17T20:27:39Z,81654818
3307,python/mypy,98183747,1100945508,Maybe open a new topic at https://github.com/python/typing/discussions,2022-04-17T20:33:57Z,906600
3308,python/mypy,98183747,1100947569,"Thanks, [it's done](https://github.com/python/typing/discussions/1145), feel free to remove my comments as they do not contribute much to the discussion.",2022-04-17T20:52:52Z,81654818
3309,python/mypy,98183747,1101304192,"@charbonnierg I don’t have time to look at it right now, but I do recall seeing some oddities with strings before. My guess is that it’s string’s recursive definition (`class str(Sequence[str]): …`) that eventually causes mypy to cast it to `_NestedSequence[Any]` in the example you provided above.
This is still a bit surprising though, as both types have incompatible `__contains__` definitions, regardless of generic parameter.",2022-04-18T10:42:47Z,43369155
3310,python/mypy,98183747,1105774768,"Mypy actually supports recursive types in classes very well, it just does not support recursive type aliases:

```py
from typing import *                                                            
T = TypeVar(""T"")                                                                
                                                                                
class X(list[X[T] | T]): ...                                                    
                                                                                
foo: X[int] = X([X([1])])                                                       
reveal_type(foo)                                                                
reveal_type(foo[0]) 
```

```sh
$ mypy new.py --strict
new.py:7: note: Revealed type is ""new.X[builtins.int]""
new.py:8: note: Revealed type is ""Union[new.X[builtins.int*], builtins.int*]""
```

Only posting this, because I haven't seen an example like this before. It might not be very useful, but it's still recursive :)",2022-04-21T21:23:47Z,748594
3311,python/mypy,98183747,1129464696,"> Only posting this, because I haven't seen an example like this before. It might not be very useful, but it's still recursive :)

Possibly useful example:

```py
Trie = Dict[str, Union[""Trie"", Literal[""$END""]]]
```

Another realistic example: type annotating JSON's return type",2022-05-18T01:09:19Z,66848002
3312,python/mypy,98183747,1129513285,We don’t need more examples of why this is useful. We know. We just need someone to implement it. So I have locked the issue. ,2022-05-18T02:53:54Z,2894642
3313,python/mypy,98183747,1213482527,"I think this can now be closed by https://github.com/python/mypy/pull/13297 (and follow-up PRs). If there are remaining problems, they can be opened as separate issues.

Note this is only enabled in master with `--enable-recursive-aliases` (enables recursive type aliases, recursive TypedDicts, and recursive NamedTuples; as mentioned above proper classes and protocols can already be recursive). Note this is still considered an experimental feature, in particular type inference may sometimes fail, if this happens you can try switching to covariant collections, and/or simply give an explicit annotation.",2022-08-12T20:19:34Z,12005495
3314,python/mypy,98183747,1260046380,"@ilevkivskyi is there a way to set this config option in a `pyproject.toml` file? I checked the documentation, but couldn't find anything about the new cli flag (perhaps because it's still experimental, and will eventually be enabled by default?)",2022-09-27T20:58:31Z,9152032
3315,python/mypy,98183747,1260976955,"Yes, it is indeed a temporary flag (for few releases), and in 0.990 it will be on by default, you can try
```
$ cat pyproject.toml
[tool.mypy]
enable_recursive_aliases = true
```
This worked for me on Linux.",2022-09-28T14:11:56Z,12005495
3316,python/mypy,98183747,1330489867,It appears that v0.990/v0.991 may not have addressed the entirety of the challenge that is recursive/cyclic types: https://github.com/python/mypy/issues/14219,2022-11-29T11:34:23Z,376735
3317,microsoft/vscode-python,542105846,542105846,"Hi VS Code dev here :wave:

Open a python file, invoke a context menu in the editor and notice the following:
1) Run Current Test File command is there. When I execute it it says that I do not have testing enabled. I suggest to add this command only if testing is enabled in my workspace
2) Sort Imports -> shouldn't this be a part of the format document. I do not see why this command needs to be so promiment to take space in the context menu. Also there is already `Source Action...` command which shows the Sort Imports so this feels like duplication.
3) ""Run python file in terminal"" and ""Run selection line in python terminal"". I do not understand why one command says ""in terminal"" and the other ""in python terminal"". When I exectute them they just go to the terminal for me. Can we have some consistency in titles, or is it possible to drop one of those actions. Reason being, user can select the whole file and do Run selection, to cover the python file case
4) Run in Interactive window: I suggest to show this only if I have Jupter or whatever needed setup
5) Most of these actions do not make sense while you are debugging. I would also consider to hide them while the user is debugging (since the menu grows then additonally with vscode debug commands)

![Screenshot 2019-12-24 at 12 48 24](https://user-images.githubusercontent.com/1926584/71411983-d66b9980-264b-11ea-822c-a3375166d30c.png)

I believe VS Code plans to introduce context sub menus and then python could add all the actions to the submenu potentially.

fyi @qubitron @DonJayamanne ",2019-12-24T11:54:51Z,1926584
3318,microsoft/vscode-python,542105846,568763929,"You also show these actions in the Explorer. I think the Explorer should be used for FS operations.

![Screenshot 2019-12-24 at 16 06 28](https://user-images.githubusercontent.com/1926584/71417755-5c490e00-2667-11ea-8c30-b2eb99d8c409.png)
",2019-12-24T15:07:04Z,1926584
3319,microsoft/vscode-python,542105846,571677332,"Thank you for the suggestion! We have marked this issue as ""needs decision"" to make sure we have a conversation about your idea. We plan to leave this feature request open for at least a month to see how many 👍 votes the opening comment gets to help us make our decision.",2020-01-07T17:01:21Z,54418
3320,microsoft/vscode-python,542105846,813505529,"@isidorn Yeah, the python/jupyter team has absolutely no regard for these issues. 

In particular, the [context menu bloat](https://github.com/microsoft/vscode-python/issues/15834) is even worse these days, and all the items are contributed by the jupyter extension. 

The summary of microsoft's view is what @claudiaregio said: 
> Just like any other VS Code menu item, *users do not have to use or click on any options/features made available to them if they do not wish to use them.* 

As I [said](https://github.com/microsoft/vscode/issues/9285#issuecomment-812277371), it's like saying: 

## If you don't want candy crush, just don't click in the candy crush icon.


 ",2021-04-05T17:01:37Z,10137
3321,microsoft/vscode-python,542105846,813516443,"> @isidorn Yeah, the python/jupyter team has absolutely no regard for these issues.
> 
> In particular, the [context menu bloat](https://github.com/microsoft/vscode-python/issues/15834) is even worse these days, and all the items are contributed by the jupyter extension.
> 
> The summary of microsoft's view is what @claudiaregio said:
> 
> > Just like any other VS Code menu item, _users do not have to use or click on any options/features made available to them if they do not wish to use them._
> 
> As I [said](https://github.com/microsoft/vscode/issues/9285#issuecomment-812277371), it's like saying:
> 
> ## If you don't want candy crush, just don't click in the candy crush icon.

It's ""disruptive"" because it's true. ",2021-04-05T17:21:17Z,10137
3322,microsoft/vscode-python,542105846,1359563634,"We no longer show testing related commands or the selection command in the menu:

![image](https://user-images.githubusercontent.com/13199757/208703250-76783034-9962-4d54-bacc-35c47bfe5eac.png)
",2022-12-20T15:27:01Z,13199757
3323,JuliaPlots/Plots.jl,1498285225,1498285225,"## Details
I cannot precompile Plots due to errors in precompiling RecipesBase and RecipesPipeline:
```julia
Precompiling project...
  ✗ RecipesBase
  ✗ RecipesPipeline
  ✗ Plots
  0 dependencies successfully precompiled in 3 seconds. 204 already precompiled.

ERROR: The following 1 direct dependency failed to precompile:

Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80]

Failed to precompile Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80] to /home/gortibaldik/.julia/compiled/v1.8/Plots/jl_0EhmVw.
ERROR: LoadError: syntax: Global method definition around /home/gortibaldik/.julia/packages/RecipesBase/eU0hg/src/RecipesBase.jl:607 needs to be placed at the top level, or use ""eval"".
Stacktrace:
 [1] top-level scope
   @ ~/.julia/packages/RecipesBase/eU0hg/src/RecipesBase.jl:600
 [2] include
   @ ./Base.jl:419 [inlined]
 [3] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String)
   @ Base ./loading.jl:1554
 [4] top-level scope
   @ stdin:1
in expression starting at /home/gortibaldik/.julia/packages/RecipesBase/eU0hg/src/RecipesBase.jl:1
in expression starting at stdin:1
ERROR: LoadError: Failed to precompile RecipesBase [3cdcf5f2-1ef4-517c-9805-6587b60abb01] to /home/gortibaldik/.julia/compiled/v1.8/RecipesBase/jl_JcrErn.
Stacktrace:
  [1] error(s::String)
    @ Base ./error.jl:35
  [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool)
    @ Base ./loading.jl:1707
  [3] compilecache
    @ ./loading.jl:1651 [inlined]
  [4] _require(pkg::Base.PkgId)
    @ Base ./loading.jl:1337
  [5] _require_prelocked(uuidkey::Base.PkgId)
    @ Base ./loading.jl:1200
  [6] macro expansion
    @ ./loading.jl:1180 [inlined]
  [7] macro expansion
    @ ./lock.jl:223 [inlined]
  [8] require(into::Module, mod::Symbol)
    @ Base ./loading.jl:1144
  [9] include
    @ ./Base.jl:419 [inlined]
 [10] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::Nothing)
    @ Base ./loading.jl:1554
 [11] top-level scope
    @ stdin:1
in expression starting at /home/gortibaldik/.julia/packages/Plots/Hxe7H/src/Plots.jl:1
in expression starting at stdin:1

```


### Versions

Plots.jl version: Plots v1.37.2
Output of `versioninfo()`:
```julia
Julia Version 1.8.2
Commit 36034abf260 (2022-09-29 15:21 UTC)
Platform Info:
  OS: Linux (x86_64-linux-gnu)
  CPU: 20 × 12th Gen Intel(R) Core(TM) i7-12700H
  WORD_SIZE: 64
  LIBM: libopenlibm
  LLVM: libLLVM-13.0.1 (ORCJIT, goldmont)
  Threads: 1 on 20 virtual cores
```",2022-12-15T11:46:58Z,47665799
3324,JuliaPlots/Plots.jl,1498285225,1352946028,"What is the output of
```julia
(@v1.8) pkg> st -m Plots RecipesBase RecipesPipeline
```
?",2022-12-15T11:48:57Z,13423344
3325,JuliaPlots/Plots.jl,1498285225,1352951034,"Same problem here: https://discourse.julialang.org/t/error-precompiling-plots-recipesbase-jl-607-needs-to-be-placed-at-the-top-level-or-use-eval/91694/1

In fact this breaks my complete CI: https://github.com/j-fu/VoronoiFVM.jl/actions/runs/3703661391/jobs/6275372947",2022-12-15T11:53:54Z,9221937
3326,JuliaPlots/Plots.jl,1498285225,1352951532,"Output of `pkg> st -m Plots RecipesBase RecipesPipeline`:
```julia
(PlutoNotebooks) pkg> st -m Plots RecipesBase RecipesPipeline
Status `~/Documents/Skola/julia/PlutoNotebooks/Manifest.toml`
  [91a5bcdd] Plots v1.37.2
  [3cdcf5f2] RecipesBase v1.3.2
  [01d81517] RecipesPipeline v0.6.11
```",2022-12-15T11:54:25Z,47665799
3327,JuliaPlots/Plots.jl,1498285225,1352951562,"Same issue here:
```julia
pkg> st -m Plots RecipesBase
Project BEFWM2 v0.1.0
Status `~/<my-env>/Manifest.toml`
  [3cdcf5f2] RecipesBase v1.3.2
```",2022-12-15T11:54:27Z,10091387
3328,JuliaPlots/Plots.jl,1498285225,1352953212,Same,2022-12-15T11:55:59Z,50108075
3329,JuliaPlots/Plots.jl,1498285225,1352953599,same,2022-12-15T11:56:24Z,7261506
3330,JuliaPlots/Plots.jl,1498285225,1352954970,"same here:
```
(@v1.7) pkg> st -m Plots RecipesBase
      Status `C:\Users\ms2106\.julia\environments\v1.7\Manifest.toml`
  [3cdcf5f2] RecipesBase v1.3.2
```",2022-12-15T11:57:42Z,43266434
3331,JuliaPlots/Plots.jl,1498285225,1352958733,"I tried julia versions `1.8.2`, `1.8.3` and the stable release `1.6.7` always the same result",2022-12-15T12:01:06Z,47665799
3332,JuliaPlots/Plots.jl,1498285225,1352960745,`] add RecipesBase@v1.2.1`,2022-12-15T12:02:47Z,50108075
3333,JuliaPlots/Plots.jl,1498285225,1352960936,"What is going one here ? :fire: 

This is probably induced by a new version of `SnoopPrecompile` (released two hours ago, sole dependency of `RecipesBase`).

https://github.com/timholy/SnoopCompile.jl/commit/d11c578c96b881aa06543ae3acdaf5795b84abe6

A temporary fix might be to pin `SnoopPrecompile` to the previous release `] add SnoopPrecompile@1.0.1`.

I think we should yank the broken `SnoopPrecompile` release.",2022-12-15T12:02:57Z,13423344
3334,JuliaPlots/Plots.jl,1498285225,1352969448,Should I close the issue as it is related to `SnoopPrecompile` ?,2022-12-15T12:09:48Z,47665799
3335,JuliaPlots/Plots.jl,1498285225,1352971212,@gortibaldik Maybe not untill they fix it or the temporary pinning lands in `RecipesBase`?,2022-12-15T12:11:22Z,10091387
3336,JuliaPlots/Plots.jl,1498285225,1352982824,Same,2022-12-15T12:21:07Z,6557701
3337,JuliaPlots/Plots.jl,1498285225,1352984081,"Yeah, well I proposed to yank the broken `SnoopPrecompile` in https://github.com/JuliaRegistries/General/pull/74176.
This is not a `Plots` issue, and is induced by an upstream dependency.",2022-12-15T12:22:32Z,13423344
3338,JuliaPlots/Plots.jl,1498285225,1352995114,"Issuing `] update` should fix the issue (broken version is now yanked).

Check that you have the correct version, by using:
```julia
] st -m SnoopPrecompile  # 1.0.2 shouldn't appear anymore
```",2022-12-15T12:33:21Z,13423344
3339,expo/expo,1331118596,1331118596,"Edit from Expo, [see the latest status of this issue here](https://github.com/expo/expo/issues/18554#issuecomment-1216561781).

---

### Summary

When running any app following the instructions on https://docs.expo.dev/versions/latest/sdk/map-view the map appears as a gray block on android. *NOTE* iOS seems to be working fine?

<img src=""https://user-images.githubusercontent.com/21202515/183310225-383d7f70-b6f2-476d-9c39-83916600cb69.png"" width=""100"" />

### Managed or bare workflow? If you have made manual changes inside of the `ios/` or `android/` directories in your project, the answer is bare!

managed

### What platform(s) does this occur on?

Android

### Package versions

```  
""dependencies"": {
    ""expo"": ""^46.0.0"",
    ""expo-status-bar"": ""~1.4.0"",
    ""react"": ""18.0.0"",
    ""react-dom"": ""18.0.0"",
    ""react-native"": ""0.69.3"",
    ""react-native-web"": ""~0.18.7"",
    ""react-redux"": ""^8.0.2"",
    ""react-native-maps"": ""0.31.1""
  },
```

### Environment

```
  expo-env-info 1.0.5 environment info:
    System:
      OS: Linux 5.18 Pop!_OS 21.10
      Shell: 5.1.8 - /bin/bash
    Binaries:
      Node: 16.13.1 - /usr/local/lib/node/nodejs/bin/node
      npm: 8.1.2 - /usr/local/lib/node/nodejs/bin/npm
    npmPackages:
      expo: ^46.0.0 => 46.0.2 
      react: 18.0.0 => 18.0.0 
      react-dom: 18.0.0 => 18.0.0 
      react-native: 0.69.3 => 0.69.3 
      react-native-web: ~0.18.7 => 0.18.7 
    npmGlobalPackages:
      expo-cli: 6.0.1
    Expo Workflow: managed
```

### Reproducible demo

https://snack.expo.dev/xdxj1hUb7

### Stacktrace (if a crash is involved)

N/A",2022-08-07T20:44:42Z,21202515
3340,expo/expo,1331118596,1208761670,"Have you added your Google Maps API Key in `app.json`?

You need this inside your ""android"" key:
```
""config"": {
  ""googleMaps"": {
    ""apiKey"": ""xxxxx""
  }
},
```",2022-08-09T00:38:54Z,1837969
3341,expo/expo,1331118596,1208768400,"> Have you added your Google Maps API Key in `app.json`?
> 
> You need this inside your ""android"" key:
> 
> ```
> ""config"": {
>   ""googleMaps"": {
>     ""apiKey"": ""xxxxx""
>   }
> },
> ```

I did yeah since I encountered that solution on [GH](https://github.com/expo/expo/issues/5591) but that did not resolve the issue. 

Were you able to get it to work using Expo go and an API key?

Also, the docs mention that no additional setup should be necessary when using Expo Go. 

> No additional configuration is necessary to use react-native-maps in Expo Go. However, once you want to deploy your standalone app you should follow instructions below.

https://docs.expo.dev/versions/latest/sdk/map-view/",2022-08-09T00:51:29Z,21202515
3342,expo/expo,1331118596,1209533447,"Granted there isn't a misconfiguration in your config file, it could potentially be related to https://github.com/expo/expo/issues/18530. I never confirmed what happens on android emulator, but Google Maps did seem to render on a physical Android device following the documentation.",2022-08-09T15:29:11Z,7604441
3343,expo/expo,1331118596,1209653403,"its hard for me to speculate since I don't know how to look at logs for this issue but, coming from web development and dealing with API key misconfigurations it does smell like an API key issue. Though I have absolutely no idea how it is configured in expo. 

That being said, the iOS version of the [snack demo I provided](https://snack.expo.dev/xdxj1hUb7?platform=ios) does seem to work. It just shows up as gray on android. Very strange. 

![image](https://user-images.githubusercontent.com/21202515/183715858-f80cf3bd-f577-435a-a5d8-8fc990112134.png)

[Edit] I realize now that iOS probably works because apple maps is the default provider for map view. Where as google maps is the default provider for android. Seems related lol. 
",2022-08-09T17:17:33Z,21202515
3344,expo/expo,1331118596,1211583731,"we have a similar situation. the expo go app doesn't show the map anymore - BUT if we release it, then the real production app still shows the map. So I assume something in expo go is wrong",2022-08-11T06:06:06Z,57396
3345,expo/expo,1331118596,1212431442,"So, I spent the time figuring out how to launch the app with the **_bare workflow_**. And surprisingly it works even without an API key. 

Very strange but, at least there's that. ",2022-08-11T20:01:19Z,21202515
3346,expo/expo,1331118596,1212793052,My assumption is that this issue and https://github.com/expo/expo/issues/18570 are related. In both cases it seems the expo go app does not have access to the properties that are provided through the enviroment like app.json,2022-08-12T07:01:30Z,57396
3347,expo/expo,1331118596,1213153545,"a serious and blocking issue, we can't go further in our project! , should we turn back to SDK 45 ?! why no one assigned to this bug?!!",2022-08-12T14:11:37Z,19927289
3348,expo/expo,1331118596,1213379105,"> a serious and blocking issue, we can't go further in our project! , should we turn back to SDK 45 ?! why no one assigned to this bug?!!

Have you tried converting your project to a bare workflow? That should allow you to add your google maps API key manually in the android files and allow you to keep building while we wait for a resolution to the managed version. 
",2022-08-12T18:07:35Z,21202515
3349,expo/expo,1331118596,1213380927,"you can create a development build of your app with your own api key, without needing to use 'bare workflow': https://docs.expo.dev/development/introduction/",2022-08-12T18:09:43Z,90494
3350,expo/expo,1331118596,1213381184,"Thank you for filing this issue!
  This comment acknowledges we believe this may be a bug and there’s enough information to investigate it.
  However, we can’t promise any sort of timeline for resolution. We prioritize issues based on severity, breadth of impact, and alignment with our roadmap. If you’d like to help move it more quickly, you can continue to investigate it more deeply and/or you can open a pull request that fixes the cause.
",2022-08-12T18:10:04Z,34669131
3351,expo/expo,1331118596,1213821050,"@brentvatne development builds don't allow hot reload, right? We use expo go during dev in Wi-Fi because unlike an emulator it gives all features of a real device. ",2022-08-13T06:27:10Z,57396
3352,expo/expo,1331118596,1214199523,"Based on your suggestion I moved to the expo development build rather than the bare workflow. And the map showed up right away no API key necessary. 

However, I continued to plug along and move my firebase implementation to the [legit version of firebase](https://rnfirebase.io/) rather than using the JS SDK that I was using with expo go. 

And the map immediately started crashing lol. Like it started expecting an API key when firebase got added. 

I've moved to using mapbox instead while I wait for this issue to be addressed 

[edit] removed my salty comments. I just picked a bad week to pick up react native lol",2022-08-13T18:05:56Z,21202515
3353,expo/expo,1331118596,1214202242,"



> you can create a development build of your app with your own api key, without needing to use 'bare workflow': https://docs.expo.dev/development/introduction/

forget about my case, why it does not work in https://snack.expo.dev ? do you think you gave the best answer by asking to create dev build !!! there is a breaking change , it worded in SDK 45 and previous versions, so it must work in 46 too, don't ask  users to go with other alternatives !!!",2022-08-13T18:24:48Z,19927289
3354,expo/expo,1331118596,1214202749,@da45 please calm down. It's good engineering practice to try to find workarounds. Often it unblocks at least some percentage of users. ,2022-08-13T18:28:30Z,57396
3355,expo/expo,1331118596,1214219491,"@da45 I agree it is a breaking change and it is important. So why don't you fix it? Or why didn't you help to test it before the release?

It's an open source library, so, if you don't fix it, or if you didn't help to test it, consider yourself as the guilty that this error still exists.",2022-08-13T20:38:50Z,1837969
3356,expo/expo,1331118596,1214219665,"And if it is working on SDK45, why in the heaven don't you downgrade to SDK45 until this gets fixed?!",2022-08-13T20:40:35Z,1837969
3357,expo/expo,1331118596,1214246792,"It is not SDK 46  problem! @brentvatne @da45 

With SDK 45,  Expo Go 2.25.1 grey block on android, crash on ios. 
With SDK 46, Expo GO 2.25.1 grey block on android, crash on ios. 

But,
With SDK 45, Expo Go 2.24.3 work.

How to downgrade Expo 45 with  Expo Go 2.24.3.

```
- Delete Expo Go app from Simulator and Real Device.
1. expo upgrade 45
 - Downgrade to sdk 45, because  Expo Go 2.24.3  doesn't support Expo SDK 46.
3. expo start
  - Expo , automatically install expo go 2.24.3.
```
Now you can wait for fix Expo Go problem :)
  
  
",2022-08-13T23:36:36Z,16364199
3358,expo/expo,1331118596,1215055657,"@hlthi 
Hello, how can I reinstall Expo Go 2.24.3 ?
On my phone, I have installed Expo Go from the store which appears to be 2.25.1 but I do not know how to downgrade it ^^' 

Edit: found!
 I replaced the version number in the official APK link on the Expo website: https://d1ahtucjixef4r.cloudfront.net/Exponent-2.25.1.apk",2022-08-15T14:08:16Z,542778
3359,expo/expo,1331118596,1215145306,"> you can create a development build of your app with your own api key, without needing to use 'bare workflow': https://docs.expo.dev/development/introduction/

Hi @brentvatne, I tried with a custom development build with `SDK 46` and `react-native-maps 0.31.1` and the map is blank. I am not using Expo Go. API key is loaded via `app.config.js` on `android.config.googleMaps.apiKey` as indicated in the expo documentation.

Did you make it work on your end?",2022-08-15T15:20:02Z,8059242
3360,expo/expo,1331118596,1215161383,I am using 'app.json' and I don't have this problem. Are you all using 'app.config.js'?,2022-08-15T15:29:18Z,1837969
3361,expo/expo,1331118596,1215172328,"> I am using 'app.json' and I don't have this problem. Are you all using 'app.config.js'?

`app.config.js` is just a dynamic version of `app.json` that I use for app variants. Fmi: https://docs.expo.dev/workflow/configuration/

@BrodaNoel are you using the same version as I am? SDK 46 / react-native-maps 0.31.1 / development build and your API key set on android.config.googleMaps.apiKey in app.json?",2022-08-15T15:33:45Z,8059242
3362,expo/expo,1331118596,1216561781,"Hi all! I'm sorry, but I'm locking this thread due to some heated comments. We will deploy a fix asap! Here is an overview of the issue:

### Overview

The issue seems to be in our Android build of Expo Go. Snack and the Expo Go version for SDK 46 (`2.51.x`) are affected and will not show Google Maps during development.

> ~~I'll update the comment whenever we deployed the fixed Expo Go version to the stores.~~

- Expo Go `2.25.2` is [available on the Play Store](https://play.google.com/store/apps/details?id=host.exp.exponent)
- Expo Go `2.52.2` has been deployed to Snack

### Workarounds

1. Use a [custom dev client](https://docs.expo.dev/development/getting-started/) with your own Google Maps key. It's similar to the bare workflow approach some of you mentioned. This method also allows you to install the latest version `react-native-maps@1.2.0` in your app. You can use [EAS Build](https://docs.expo.dev/build/introduction/) to build the app if you don't have the Android SDK installed.
  _**I would heavily recommend this method for now**_

2. Temporarily downgrade to SDK 45 to use an older Expo Go. This method would allow you to use the previous Expo Go version. You'll need to remove Expo Go from your Android phone or simulator to do this. Whenever you start your SDK 45 project with `expo start --android`, and your device is connected through USB and ADB, it will automatically install this Expo Go version.
    - [Expo Go `2.24.3` APK for Android](https://d1ahtucjixef4r.cloudfront.net/Exponent-2.24.3.apk)

Hope this helps!",2022-08-16T12:21:23Z,1203991
3363,expo/expo,1331118596,1216601715,"As for some of your questions/comments:

---

> we have a similar situation. the expo go app doesn't show the map anymore - BUT if we release it, then the real production app still shows the map. So I assume something in expo go is wrong

Yes, that assumption is right! We found an issue that only relates to the Expo Go builds and not your production apps. Even apps built with the classic build system (`expo build`) are not affected. But, because we use Expo Go in Snack too, Snack is also affected.

---

> @brentvatne development builds don't allow hot reload, right? We use expo go during dev in Wi-Fi because unlike an emulator it gives all features of a real device.

Expo dev client builds are fully capable of doing hot reload. I would suggest to try it out :)

---

> Based on your suggestion I moved to the expo development build rather than the bare workflow. And the map showed up right away no API key necessary.
>
> However, I continued to plug along and move my firebase implementation to the [legit version of firebase](https://rnfirebase.io/) rather than using the JS SDK that I was using with expo go.
>
> And the map immediately started crashing lol. Like it started expecting an API key when firebase got added.
>
> I've moved to using mapbox instead while I wait for this issue to be addressed
>
> [edit] removed my salty comments. I just picked a bad week to pick up react native lol

The Expo dev client and EAS allow you to pick any native module as you see fit. I'm sorry you had an unfortunate experience with `react-native-firebase`, I'm not sure if [you followed these steps](https://rnfirebase.io/#expo) but some libraries require you to add credentials, even in development mode. We can't control that outside of Expo Go and/or for third party libraries.

(And thanks for keeping things civilized ❤️)

---

> forget about my case, why it does not work in https://snack.expo.dev/ ? do you think you gave the best answer by asking to create dev build !!! there is a breaking change , it worded in SDK 45 and previous versions, so it must work in 46 too, don't ask users to go with other alternatives !!!

Because Snack also uses Expo Go, Snack is also affected. We will deploy a fix asap.

---

> @da45 please calm down. It's good engineering practice to try to find workarounds. Often it unblocks at least some percentage of users.

I fully agree with this. We don't intend to break things, but if they do, we try to keep everyone as ""unblocked"" as possible while we fix things. We still have to go through store review whenever we fix it, [which took ±15 days on the Play Store for SDK 46](https://twitter.com/cedricvanputten/status/1557375168955252738). So I think there is a lot of value in sharing these workarounds.

---

> Hi @brentvatne, I tried with a custom development build with `SDK 46` and `react-native-maps 0.31.1` and the map is blank. I am not using Expo Go. API key is loaded via `app.config.js` on `android.config.googleMaps.apiKey` as indicated in the expo documentation.
>
> Did you make it work on your end?

If you added the `android.config.googleMaps.apiKey` after running `expo run:*` or `expo prebuild`, it needs to recreate the native files. These keys are set in your native files that `expo prebuild` generates.

You can try running `expo prebuild --clean` and see if `expo run:*` works.


",2022-08-16T12:56:41Z,1203991
3364,expo/expo,1331118596,1220827193,This should be resolved in the latest Expo Go version! (`2.25.2`),2022-08-19T15:49:18Z,1203991
3365,PowerDNS-Admin/PowerDNS-Admin,1036757791,1036757791,"Hello everyone,

This is Khanh, the creator of PowerDNS-Admin.
Sorry for being away for a long time. I would like to create this ""issue"" to update you on my current contribution status for PDA.

As I am not using PowerDNS anymore and have a new job, I have very little time that I can spend maintaining PowerDNS-Admin. However, I know there are still a lot of people who are using it and want to keep this project alive. So, I am looking for new maintainers for PowerDNS-Admin. If you are using it and interested in maintaining an open-source project, feel free to reach out to me at k[at]ndk.name. I am also thinking of transferring the project to an organization when it gets more maintainers.

There are still a lot of Pull Requests from you that need to be reviewed and merged. I will try to clear them this weekend.

Best,
Khanh",2021-10-26T21:28:04Z,6447444
3366,PowerDNS-Admin/PowerDNS-Admin,1036757791,959561950,"I don't use PowerDNS, I'm participating in an internal POC, but I liked how the project was thought out and architected. I have little experience with big contributions, but I can help with basic community issues, organizing Issues, Contributions...",2021-11-03T16:02:24Z,11020807
3367,PowerDNS-Admin/PowerDNS-Admin,1036757791,988618772,@ngoduykhanh I have reconsidered and decided I would really like to do a lot with this project. Will you please promote me to a maintainer?,2021-12-08T08:56:30Z,65595341
3368,PowerDNS-Admin/PowerDNS-Admin,1036757791,988985251,@AzorianSolutions Thank you for your interest in maintaining the project. We have 2 active maintainers now. I will discuss with them whether we want to have more people involved as maintainers. Please reach out to me via email so we can keep in touch.,2021-12-08T16:47:07Z,6447444
3369,PowerDNS-Admin/PowerDNS-Admin,1036757791,1049103506,"Regrettably, I do not have enough time to help maintain PowerDNS-Admin anymore - in contrast with last November, when I first contacted Khanh - and I don't feel comfortable being part of the group of collaborators any longer just for the hope that this might change, which is currently uncertain at best.

I've therefore handed back the collaborator role, but may still contribute on occasion.",2022-02-23T18:52:34Z,10137
3370,exercism/python,1294541749,1294541749,"Should be mentions of ""If Else statements in the readme because the Boolean expressions are a red herring. You cannot solve anything by setting the value to true or false. ",2022-07-05T16:54:37Z,93767513
3371,exercism/python,1294541749,1175285993,"🤖 &nbsp; 🤖

Hi! 👋🏽 👋  Welcome to the Exercism Python Repo!

Thank you for opening an issue! 🐍 &nbsp;🌈  ✨

<br>

-  &nbsp;   If you are **requesting support**, we will be along shortly to help. (*generally within* **72 hours,** *often more quickly*).
-  &nbsp;   **Found a problem** with tests, exercises or something else?? &nbsp;🎉  
&nbsp;&nbsp;&#9702;&nbsp;We'll take a look as soon as we can & identify what work is needed to fix it. *(generally within* **72 hours**).

​		&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#9702;&nbsp;_If you'd also like to make a PR to **fix** the issue,  please have a quick look at the [Pull Requests][prs] doc._  
&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;_We &nbsp;💙 &nbsp;PRs that follow our [Exercism][exercism-guidelines] & [Track][track-guidelines] contributing guidelines!_

-  &nbsp; Here because of an obvious (*and* **small** *set of*) spelling, grammar, or punctuation issues with **one** exercise,  
&nbsp; concept, or Python document?? 🌟  `Please feel free to submit a PR, linking to this issue.` 🎉

<ul>
        <table>
        <td>
        <details>
          <summary>‼️&nbsp;&nbsp;<b><em>Please Do Not</em></b>&nbsp;‼️</summary>
        <br>

​		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ❗ Run checks on the whole repo & submit a bunch of PRs.  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; This creates longer review cycles & exhausts reviewers energy & time.  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; It may also conflict with ongoing changes from other contributors.  
​		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ❗ Insert only blank lines, make a closing bracket drop to the next line, change a word  
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to a synonym without obvious reason, or add trailing space that's not an[ EOL][EOL] for the very end of text files.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ❗ Introduce arbitrary changes ""just to change things"" .

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; _...These sorts of things are **not** considered helpful, and will likely be closed by reviewers._

</details>
</td>
</table>
</ul>

- For anything complicated or ambiguous, **let's discuss things**  --  we will likely welcome a PR from you.
- _Here to suggest a feature or new exercise??_ **Hooray!** Please keep in mind [_Chesterton's Fence_][chestertons-fence].  
_Thoughtful suggestions will likely result faster & more enthusiastic responses from maintainers._

<br>

💛 &nbsp;💙 &nbsp;_While you are here..._ If you decide to help out with other [open issues][open-issues], you have our **gratitude** 🙌 🙌🏽.  
Anything tagged with `[help wanted]` and without `[Claimed]` is up for grabs.  
Comment on the issue and we will reserve it for you.  🌈  ✨


[prs]: https://github.com/exercism/docs/blob/main/community/good-member/pull-requests.md
[EOL]: https://en.wikipedia.org/wiki/Newline
[chestertons-fence]: https://github.com/exercism/docs/blob/main/community/good-member/chestertons-fence.md
[exercism-guidelines]: https://exercism.org/docs/building
[open-issues]: https://github.com/exercism/python/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22
[track-guidelines]: https://github.com/exercism/python/blob/main/CONTRIBUTING.md",2022-07-05T16:55:10Z,41898282
3372,exercism/python,1294541749,1175307005,"Hi @Crucibl 👋🏽 

Thanks for filing this issue.  You are, however mistaken.  It is quite possible to solve the `Ghost Gobble Arcade Game` without the use of conditionals.  Please see the [exemplar solution](https://github.com/exercism/python/blob/main/exercises/concept/ghost-gobble-arcade-game/.meta/exemplar.py) here in the repo for more details.  

We worked very hard to ensure that the `bools` exercise did not require `conditionals` in its solution, since it precedes the `conditionals` exercise.  We also wanted to make sure students really focused in on the Boolean logic. While it is _possible_ to use `if`, `elif` or `else` for the problem (_its entirely your choice_), it is by no means _required_ for a correct solution.

",2022-07-05T17:15:16Z,5923094
3373,exercism/python,1294541749,1175315659,"Yes so this says to return a Boolean

 """"""Verify that Pac-Man can eat a ghost if he is empowered by a power pellet.
    :param power_pellet_active: bool - does the player have an active power pellet?
    :param touching_ghost: bool - is the player touching a ghost?
    :return: bool - can the ghost be eaten?
    """"""",2022-07-05T17:26:14Z,93767513
3374,exercism/python,1294541749,1175315941,"And you dont return a boolean, you simply return the parameters
",2022-07-05T17:26:35Z,93767513
3375,exercism/python,1294541749,1175343948,"@Crucibl - I believe you are misunderstanding the code.  Please take a look at this Real Python article [Python Booleans](https://realpython.com/python-boolean/#python-booleans-as-keywords) for more information.  It is not the **_parameters_** that are important in the `return` statement, it is the use of `or`, `and`, `not`.

The evaluation of those Boolean Operators results in `True` and `False`.",2022-07-05T18:02:38Z,5923094
3376,exercism/python,1294541749,1175350020,I completely misunderstand because of the way it is written,2022-07-05T18:10:22Z,93767513
3377,exercism/python,1294541749,1175350458,"Would be better to write the parameters (has_eaten_all_dots, power_pellet_active, touching_ghost) are boolean values that need to be evaluated",2022-07-05T18:10:52Z,93767513
3378,exercism/python,1294541749,1175352826,"""""""Evaluate that Pac-Man can eat a ghost if he is empowered by a power pellet by returning the parameters
touching_ghost and  power_pellet_active: bool - does the player have an active power pellet?- is the player touching a ghost?
""""""",2022-07-05T18:14:04Z,93767513
3379,exercism/python,1294541749,1175353373,"""""""Evaluate that Pac-Man can eat a ghost if he is empowered by a power pellet by returning the parameters
touching_ghost and power_pellet_active: which are already set to a bool in the test file - does the player have an active power pellet?- is the player touching a ghost?
""""""",2022-07-05T18:14:44Z,93767513
3380,exercism/python,1294541749,1175416070,"@Crucibl  - A few things here:


1. **The stub is only one piece of this exercise.**  It is not intended to stand in for the whole challenge.  You should also be looking at the [instructions](https://github.com/exercism/python/blob/main/exercises/concept/ghost-gobble-arcade-game/.docs/instructions.md) and the [introduction](https://github.com/exercism/python/blob/main/exercises/concept/ghost-gobble-arcade-game/.docs/introduction.md) to get more information on what the expectations are.
2. When confused or stuck, we recommend uploading your partial solution via the CLI to the website and requesting mentoring.  Mentors are very willing to work things through with you and explain everything step-by-step.  You can also work through things in our Gitter channel.
3. . This exercise was ported from the Elixir track.  Between Python and Elixir, ~6300 students have completed this exercise. Very few were confused by the stub -- so we are unlikely to change it.  That doesn't mean we **_won't_** - but the majority of students do not appear to have an issue with it.  We also risk confusing people more by adding more words.

I'm sorry that this exercise is confusing for you, but I am not convinced that changing the stub wording is warranted at this time.

",2022-07-05T19:23:48Z,5923094
3381,exercism/python,1294541749,1175437024,Did they complete the exercise by looking at prior solutions before answering? How can you read this and not be confused,2022-07-05T19:51:13Z,93767513
3382,exercism/python,1294541749,1175438587,Stub notes or readme. Anything to clarify what is being asked,2022-07-05T19:51:55Z,93767513
3383,exercism/python,1294541749,1175440639,"The docstrings and Readme lead me to believe that I must set something to true or false. So when I read. Verify that Pac-Man can eat a ghost if he is empowered by a power pellet. :param power_pellet_active: bool - does the player have an active power pellet? :param touching_ghost: bool - is the player touching a ghost? :return: bool - can the ghost be eaten? I first think set parameter power_pellet_active to a boolean, like power_pellet_active = True and touching_ghost: = True so is the player touching a ghost? is = to true",2022-07-05T19:54:46Z,93767513
3384,exercism/python,1294541749,1176135627,"""We also risk confusing people more by adding more words.""
Im not trying to add more words. Im trying to optimize the wording for intellectually challenged people like myself who dont realize the variables are already defined in the test files",2022-07-06T12:01:22Z,93767513
3385,exercism/python,1294541749,1176138890,"Then maybe 12,000 people can solve the puzzle not 6300",2022-07-06T12:04:59Z,93767513
3386,exercism/python,1294541749,1176141652,"1008202
members have earned this badge for joining Exercism. but only 294540
members have earned this badge. for submitting an exercise. So out of 100 users only 36 or 37 actually submit an exercise. Why do more than 60 people out of every 100 not submit an exercise?",2022-07-06T12:07:48Z,93767513
3387,exercism/python,1294541749,1176142173,More than half the people join and quit,2022-07-06T12:08:18Z,93767513
3388,exercism/python,1294541749,1176142920,So out of 1008202 only 6300 people could figure out how to solve the exercise. Thats terrible ,2022-07-06T12:09:05Z,93767513
3389,exercism/python,1294541749,1176143346,Surely we can improve,2022-07-06T12:09:29Z,93767513
3390,exercism/python,1294541749,1176148381,"There are 194,498 students in the Python track and only 6300 have been able to solve the 3rd exercise",2022-07-06T12:14:35Z,93767513
3391,exercism/python,1294541749,1176232550,"@Crucibl  - my reasons are mentioned in my [comment above](https://github.com/exercism/python/issues/3132#issuecomment-1175416070), and I don't think going back and forth as if this is a chat window is helpful.  We'll think about your feedback and consider changing the wording, **but it's not going to happen right now.**  We need to gather feedback from other users/mentors and maintainers on if the exercise is as confusing to them as it is to you.",2022-07-06T13:37:05Z,5923094
3392,exercism/python,1294541749,1189701292,"Adding my feedback as a student, mentor, and maintainer... (sorry for the late response, I haven't checked issues for a while)

>The docstrings and Readme lead me to believe that I must set something to true or false. 

That is more vague than the actual instructions. The instructions specify

>The function should return True only if Pac-Man has a power pellet active and is touching a ghost.

What is being set is the function's return value. It's not simply a matter of returning the parameters. It's a matter of how they are joined in the returned expression.

`    return power_pellet_active or touching_ghost`

is returning the parameters, but not in a way to solve the task. I think the instructions make clear the arguments to the two parameters are used to set the return value from the function. The instructions begin with

>Define the eat_ghost() function that takes two parameters (if Pac-Man has a power pellet active and if Pac-Man is touching a ghost) and returns a Boolean value if Pac-Man is able to eat the ghost.

In the 20+ times I've mentored this exercise I don't recollect any student saying they've been confused by the instructions for that task. They sometimes do more than they need to solve it by setting the expression to a variable and then returning the variable, or by using if/else when they can just return the parameters joined with the correct logical operator, but they get the general gist.

Clearness of the instructions is taken just as seriously as not modifying the instructions unnecessarily. It's a pain for everyone when their completed solution becomes outdated just from a change to the instructions. If there were multiple people complaining about these instructions being unclear, then a re-evaluation of them would certainly be in order. But, sorry, that is less likely to happen with a one-off complaint, no matter how fervent it may be.",2022-07-20T01:20:26Z,32035397
3393,exercism/python,1294541749,1196962851,"Closing this for now, as we've had no additional complaints.  Will re-open if the issue comes up again from additional students or mentors.",2022-07-27T16:11:48Z,5923094
3394,laravel/framework,1461489700,1461489700,"https://github.com/laravel/framework/issues/39442

Laravel Version: 9.38.0
PHP Version: 8.1
Database Driver & Version: SQLite 3
OS: MacOS and Docker ubuntu latest
Description:
This is an easily re-producible bug that is consistent across new and old installations of Laravel 8 & 9. It was already reported, and has been a bug in Laravel for over a year. @driesvints couldn't be bothered to investigate, so closed the issue.

https://github.com/laravel/framework/issues/39442

Create migration with drop col and new columns
Write a test that reads or writes this field
Watch it not exist

@driesvints In my previous ticket, I reported this as Laravel 8 forgetting Laravel 9 exists. I'm using Laravel 9. The same bug exists. If you waste more of my time, I'll bill you. You've been handed the exact lines of code responsible for this, and that obviously hasn't changed.

@driesvints Open source, yes, that means I'm contributing to your project for free. You're rude and lack the curiosity to be a developer. I hope to God you're not actually paid.",2022-11-23T10:36:33Z,16484032
3395,laravel/framework,1461489700,1324850994,"@taylorotwell Your employee is closing bugs that actually exist, because he's upset I called him out for lacking any curiosity about the project he maintains. Personally, I'd fire him",2022-11-23T10:37:04Z,16484032
3396,laravel/framework,1461489700,1324867155,"@Sunhat With this attitude, don't count on any support at all. Imagine that somebody would talk in that way to you. Would you be motivated to help that person out?

The bug that you are encountering might not be important for others.  

Do mind your tone of voice and stay friendly. This not only applies to this issue tracker but for interactions with all living things.

Some interesting resources:
- https://blog.container-solutions.com/entitlement-in-open-source
- https://opensource.com/article/22/1/open-source-contributions-career
- https://www.youtube.com/watch?v=fMFjO2szDnk

I'll send you a bill for this life changing advice! 😅",2022-11-23T10:51:35Z,483853
3397,laravel/framework,1461489700,1324878368,"@freekmurze I've displayed absolutely no sense of entitlement. I urge you to re-read what's actually happened. I don't need or want your support. I'm being selfless giving you information about your project, and it's DISGUSTINGLY being rejected due to laziness and bureaucracy. 

1. A ticket was opened a year ago and @driesvints refused to investigate.
2. I opened a new ticket with a typo relating to an unsupported version. @driesvints closed the ticket, despite common sense suggesting this bug still exists. I called out @driesvints for refusing to investigate.

My tone is perfectly fine when addressing rude and lazy children.

P.S.

[Mike McQuaid](https://blog.container-solutions.com/author/mike-mcquaid)

Mike McQuaid is an abusive troll and hypocrite who frequently verbally abuses people online. He then writes arrogant articles and claims he's against ""hate"". 

Mike is a manipulator and perfectly embraces _""Always accuse your enemy of exactly what you are doing""_
Thanks for linking me to a perfect example of what a human shouldn't be.

---

Considering this is YOUR project. As Your buddy Mike would put it: I owe you nothing. 

Simply reporting a bug means I've gifted you something of value. I don't owe you a version number. I don't owe you my OS. I don't owe you my time. If you close the ticket because you're too lazy to care about your own project, then re-think why you chose this job, and re-think why you want to write open-source.

Because, despite Mike's revolting position on open-source code, open-source is about serving, giving and being selfless. If you take on an open-source project, you owe everyone everything. That's the sacrifice you chose. If you can't hack it, then don't do it.",2022-11-23T11:01:38Z,16484032
3398,laravel/framework,1461489700,1324918080,"> If you take on an open-source project, you owe everyone everything. That's the sacrifice you chose. If you can't hack it, then don't do it.

I guess are differences come because we don't agree on this at all. If somebody invests time creating a library, you play by the rules of the creator / maintainer. Don't assume that you as a contributor can set the rules. A creator/maintainer owes you nothing. Not even if you come with the gift of opening an issue.

Personally, I think your stance on open source will leave both you and maintainers of open source software feel bad. There are no winners with such an attitude. 

You can find the code of conduct of Laravel here: https://laravel.com/docs/9.x/contributions#code-of-conduct

> Participants must ensure that their language and actions are free of personal attacks and disparaging personal remarks.
> Behavior that can be reasonably considered harassment will not be tolerated.

I think suggestion that someone should be fired surely goes against to rules set by the creators / maintainers.",2022-11-23T11:34:07Z,483853
3399,WedgeServer/wedge,257408022,257408022,"The word ""Caddy"" in the context of software is under a pending trademark application. By using the name Caddy in your repo, along with the associated logos, you're in violation of this trademark. Please remove all such references :)",2017-09-13T14:44:31Z,17517541
3400,WedgeServer/wedge,257408022,329191018,"> The word ""Caddy"" in the context of software is under a pending trademark application

So it's not a registered trademark?

> By using the name Caddy in your repo, along with the associated logos, you're in violation of this trademark. Please remove all such references :)

See issue #1 - this is planned 😃 ",2017-09-13T14:45:28Z,2552726
3401,WedgeServer/wedge,257408022,329192024,"Technically, there is an implicit trademark because of the continued use and recognition of the brand over the years, and it is legally enforceable. But we have also submitted a formal trademark request as well, which is pending. Just FYI, we have no problem with anyone forking Caddy. We enjoy open source!

Can I ask what your use case for Caddy is? ",2017-09-13T14:48:41Z,17517541
3402,WedgeServer/wedge,257408022,329193098,"Pretty disingenous for you to refer to yourself as a member of Caddy's ""we"" when so far as I can tell you're not involved: https://github.com/mholt/caddy/graphs/contributors",2017-09-13T14:52:03Z,1310872
3403,WedgeServer/wedge,257408022,329193635,"@SirCmpwn I'm part owner of Light Code Labs, the legal entity that filed for the trademark.",2017-09-13T14:53:34Z,17517541
3404,WedgeServer/wedge,257408022,329193792,"I see. Well, in any case, be patient, and maybe also try contributing to your own web server?",2017-09-13T14:54:02Z,1310872
3405,WedgeServer/wedge,257408022,329194122,You got it 👍 ,2017-09-13T14:55:02Z,17517541
3406,WedgeServer/wedge,257408022,329195410,"Thanks for clarifying your involvement, @yroc92. It's great to hear you value open source!

In the United Kingdom, trademark violation requires a mark to be used ""in the course of trade"". I have no plans to make commercial use of this project in any form whatsoever (including but not limited to adding sponsor headers to HTTP responses and then charging for them to be removed).

With that said, I will be removing references to Caddy throughout the codebase as you have requested, though the primary benefit here in my opinion is reducing user confusion and ensuring users do not go seeking support from the wrong place.",2017-09-13T14:59:03Z,2552726
3407,WedgeServer/wedge,257408022,329215908,"@lol768 

As long as you're dolling out legal advice on the Internet, you might well quote the other relevant section of the Trade Marks Act of 1994.

> A person may also infringe a registered trade mark where the sign is similar and the goods or services are similar to those for which the mark is registered and there is a likelihood of confusion on the part of the public as a result

See: section 10(2)(b) http://euipo.europa.eu/pdf/mark/nl_uk_1_en.pdf

I am not a lawyer, but I am somewhat versed in trademark law.   Your dependence on ""commercial use"" may be unsupportable, but talk to an attorney for legal advice.

LCL are actually doing the right thing to defend their mark by providing notice.",2017-09-13T16:04:02Z,652455
3408,WedgeServer/wedge,257408022,329217818,"Thanks for contributing your expertise @gonzopancho :) I do apologise for giving myself legal advice earlier.

The start of section 10 (2) states:

>A person infringes a **registered trade mark** if he uses **in the course of trade** a sign where
because:

Your quoted section (""10(2)(b)"") is indented **underneath the above**. Emphasis is mine.



----------

This is moot however, because I have already committed to change the name.",2017-09-13T16:10:33Z,2552726
3409,WedgeServer/wedge,257408022,329244706,"Just wanted to comment here and say thank you for the respect and the professional tone of the discussion. I was in class most of this morning and so Cory and I haven't had a chance to coordinate. I stand by Cory's choice to bring up the issue and I think this discussion was a good one to have.

As you know, per the Apache license, forks are required to maintain copyright notices and give credit to the original project, etc, so the name Caddy *should* be used somewhere in a visible place, I think the main concern was with the logo being used prominently. But the README was quickly updated, which we appreciate.

We're glad there are people who value Caddy enough and feel strongly enough about it to fork it with their own changes. Hopefully this won't result in a splintered effort to make the Web better and safer, instead we look forward to mutually benefitting somehow.",2017-09-13T17:46:45Z,1128849
3410,WedgeServer/wedge,257408022,329249472,"Hey @mholt,

Great to see you weigh in here and thanks for the conversation on HackerNews. It was useful to discuss things, even if I do completely disagree with the direction in which you're going with this.

> As you know, per the Apache license, forks are required to maintain copyright notices and give credit to the original project, etc, so the name Caddy should be used somewhere in a visible place, I think the main concern was with the logo being used prominently. But the README was quickly updated, which we appreciate.

Apologies for that. GitHub's fork feature is great at duplicating everything, so the README was misleading for a while. I'm happy to mention that this is a fork of Caddy in the README as required by the Apache license and will look at making this more obvious if it's not already apparent.

Are you happy for me to mention that Caddy has a paid support offering and link https://caddyserver.com/pricing in the README for users seeking commercial support? If it wasn't already clear, my intentions are to remove the code I disagree with for people who want to use Caddy _personally_ (though I'm also of the opinion that the binaries I will be distributing here shouldn't be limited). I'm not here to try and sell a commercial support offering.

> We're glad there are people who value Caddy enough and feel strongly enough about it to fork it with their own changes. Hopefully this won't result in a splintered effort to make the Web better and safer, instead we look forward to mutually benefitting somehow.

As long as you're intending on a) leaving the trademark issue alone and b) keeping the upstream repository open source (and licensed under an appropriate free software license), I'm happy to contribute any relevant fixes/improvements upstream and include the link I mentioned above.

It is unfortunate that at present you don't seem open to reversing your decision to introduce advertising into Caddy. I still dislike what you're doing with the binaries, but I don't dislike it enough to motivate me to continue maintaining this repository - it's really the header I have a problem with.

Caddy is an excellent piece of software, I would really urge you to read through the HackerNews thread again at some point and reconsider if this is the approach you want to take. Neither of us want to see more fragmentation here, I think.

Cheers,
Adam",2017-09-13T18:03:54Z,2552726
3411,WedgeServer/wedge,257408022,329303987,"> Apologies for that. GitHub's fork feature is great at duplicating everything, so the README was misleading for a while. I'm happy to mention that this is a fork of Caddy in the README as required by the Apache license and will look at making this more obvious if it's not already apparent.

Yeah, no problem. We were very quick to jump in here after the fork; I realize from my own experience that this isn't enough time. It was a very busy, fast-moving morning. :)

> Are you happy for me to mention that Caddy has a paid support offering and link https://caddyserver.com/pricing in the README for users seeking commercial support? If it wasn't already clear, my intentions are to remove the code I disagree with for people who want to use Caddy personally (though I'm also of the opinion that the binaries I will be distributing here shouldn't be limited). I'm not here to try and sell a commercial support offering.

Sure, you may certainly do that.

> As long as you're intending on a) leaving the trademark issue alone and b) keeping the upstream repository open source (and licensed under an appropriate free software license), I'm happy to contribute any relevant fixes/improvements upstream and include the link I mentioned above.

Sounds good to me. The use of our logo at the top of your fork was quickly resolved anyway.

> It is unfortunate that at present you don't seem open to reversing your decision to introduce advertising into Caddy.

Well, it's only been 7 hours. :) Let's give it some time and see how things are when the dust settles. Believe me, it's not a decision I took lightly.

> Caddy is an excellent piece of software, I would really urge you to read through the HackerNews thread again at some point and reconsider if this is the approach you want to take. Neither of us want to see more fragmentation here, I think.

Thanks - yes, I agree. (Frankly I'm not too convinced by the HN arguments -- as usual -- except for recognizing the fact that our pricing can be out of reach for small, bootstrapping startups; we encourage them to contact us about special pricing in the meantime.)",2017-09-13T21:35:48Z,1128849
3412,WedgeServer/wedge,257408022,329325421,">Frankly I'm not too convinced by the HN arguments

My main concern, which you probably saw, is that your website is deceptive. Nothing's wrong with getting paid for your work, but your website is designed in bad faith.",2017-09-13T23:29:01Z,1310872
3413,WedgeServer/wedge,257408022,329330807,"@SirCmpwn 

> My main concern, which you probably saw, is that your website is deceptive. Nothing's wrong with getting paid for your work, but your website is designed in bad faith.

Come again&mdash;what's deceptive?",2017-09-14T00:02:26Z,1128849
3414,WedgeServer/wedge,257408022,329330897,I wrote about it [on HN](https://news.ycombinator.com/item?id=15238969).,2017-09-14T00:03:05Z,1310872
3415,WedgeServer/wedge,257408022,329333522,"I'm confused at what this fork offers that Caddy doesn't... Is it the code that shows the headers that you removed all that's stopping you from using Caddy? 

You do realize that you can compile Caddy yourself without those headers in it, without forking it and maintaining a whole new project, right?

From my point of view people are upset by 2 things. 1: They can't get pre-compiled binaries without the header and 2: if they can, they have to pay for it.

If anything, this fork adds MORE work on you than what Caddy says to do to use it free of charge without the headers.",2017-09-14T00:22:05Z,352113
3416,WedgeServer/wedge,257408022,329336643,"this trademark claim appears to be a false claim. the original caddy is hosted on github, and per the [github faq](https://help.github.com/articles/licensing-a-repository/):

> Note: If you publish your source code in a public repository on GitHub, according to the Terms of Service, other GitHub users have the right to view and fork your repository within the GitHub site. If you have already created a public repository and no longer want users to have access to it, you can make your repository private. When you convert a public repository to a private repository, existing forks or local copies created by other users will still exist. For more information, see ""Making a public repository private.""

i realize that you've already complied with the request, but based on this, it appears that the caddy team are not acting in good faith",2017-09-14T00:45:35Z,169144
3417,WedgeServer/wedge,257408022,329364252,"> not acting in good faith

A misunderstanding or miscommunication is a far cry from ""not in good faith"". I'm gonna stand firm here and re-assert that our actions are in good faith.

The misunderstandings about the use of the logo have been resolved.",2017-09-14T04:02:08Z,1128849
3418,WedgeServer/wedge,257408022,329364339,The bad faith part comes in because so far as I can tell you are *deliberately* misleading users.,2017-09-14T04:02:52Z,1310872
3419,WedgeServer/wedge,257408022,329432299,"> I'm confused at what this fork offers that Caddy doesn't... Is it the code that shows the headers that you removed all that's stopping you from using Caddy?

Removal of adware and binaries that can be freely distributed and are not subject to an EULA.

> You do realize that you can compile Caddy yourself without those headers in it, without forking it and maintaining a whole new project, right?

Yes, I didn't overlook that :)

> From my point of view people are upset by 2 things. 1: They can't get pre-compiled binaries without the header and 2: if they can, they have to pay for it.
> If anything, this fork adds MORE work on you than what Caddy says to do to use it free of charge without the headers.

From my point of view (and from reading HackerNews) people are upset about the ads embedded in the webserver and served to the visitors. They're also upset about how the binaries are licensed.

The fork is absolutely more work for me. The point is that I care enough about this that I'm willing to create the fork, remove the code I disagree with, cross-compile it and then make free (as in freedom) binaries available to everyone - including those who may not be technically adept enough to set up their own golang workspace and compile it themselves.",2017-09-14T09:49:40Z,2552726
3420,WedgeServer/wedge,257408022,329454751,I think forking on Github does not require removing any reference to the project you forked from.,2017-09-14T11:32:30Z,6505000
3421,WedgeServer/wedge,257408022,329458655,"> The fork is absolutely more work for me. The point is that I care enough about this that I'm willing to create the fork, remove the code I disagree with, cross-compile it and then make free (as in freedom) binaries available to everyone - including those who may not be technically adept enough to set up their own golang workspace and compile it themselves.

Until you run into the issues that Caddy had where it actually costs a lot of money to maintain and use the infra to do such things in a way that is helpful.

Then we're right back where we started, you doing something in a way to help make money to support that stuff, everyone getting upset, and forking a forked project that was forked from another fork that forked from a project that started such tactics to help.

The circle of open sores, I like to call it.",2017-09-14T11:51:48Z,352113
3422,WedgeServer/wedge,257408022,329458888,"I find it highly unlikely that's going to happen. I have infrastructure, if you ever need somewhere to host builds or tooling like caddy offers, reach out and I'll share it for free.",2017-09-14T11:52:59Z,1310872
3423,WedgeServer/wedge,257408022,1071037089,"5 years later, fair to say this fork was created out of spite with no intentions to sustain it 😂

Grandstanding sure is cheap.",2022-03-17T16:17:32Z,18250753
3424,WedgeServer/wedge,257408022,1071044472,They stopped the bad faith behavior upstream so there's no longer any reason for this fork to exist. Imagine sticking up for a company that you have nothing to do with when they go around being a trademark bully. You sure showed that petty FOSS volunteer the error of their defiance towards the honorable corporate entity!,2022-03-17T16:22:26Z,1310872
3425,WedgeServer/wedge,257408022,1071098135,"> They stopped the bad faith behavior upstream so there's no longer any reason for this fork to exist. Imagine sticking up for a company that you have nothing to do with when they go around being a trademark bully. You sure showed that petty FOSS volunteer the error of their defiance towards the honorable corporate entity!

Imagine thinking that a solo developer trying to make a living from his work is some trademark bully and evil corporate entity, just because they have an LLC and premium support plans.",2022-03-17T17:05:56Z,352113
3426,WedgeServer/wedge,257408022,1071101172,"Trying to make a living does not make it okay to be deceptive and misleading or to threaten others. There are plenty of ways to make a living without being deceptive, especially for a talented programmer. Get your moral compass re-calibrated.",2022-03-17T17:08:35Z,1310872
3427,WedgeServer/wedge,257408022,1074427725,"> 5 years later, fair to say this fork was created out of spite with no intentions to sustain it 😂

Bit of a strange thing to do, resurrecting an issue 5 years on to post .. that.

However - as Drew has already pointed out, the fork became obsolete as soon as upstream 180'd and dropped the tacky, baked-in ads. I'm glad no significant fork maintenance / investment in a build system was required in the end, given Matt took the sensible decision to merge the PR to remove the header (which had been added without even consulting his sponsors, putting some of them in an incredibly awkward position amongst the negative reception).

Most of us have now moved on. I am glad Caddy has a more sustainable funding position - I see this debacle as an illustration of the power of open source (and forking) providing a ""checks and balances"" mechanism when it comes to undesirable moves from upstream.
",2022-03-21T21:16:50Z,2552726
3428,rustdesk/rustdesk,951429255,951429255,"I want to run RustDesk on the Raspberry Pi, but I was disappointed to find that it does not work on the ARM architecture. I hope you can add support for the ARM platform (including the server version). Thanks.",2021-07-23T09:56:00Z,42692274
3429,rustdesk/rustdesk,951429255,885531889,Where can I buy a Raspberry Pi for free? :),2021-07-23T10:00:57Z,71636191
3430,rustdesk/rustdesk,951429255,885533928,"@rustdesk You can use QEMU to compile RustDesk and it is free. In addition, rust supports cross-compilation.",2021-07-23T10:04:33Z,42692274
3431,rustdesk/rustdesk,951429255,886144511,"@rustdesk Travis CI already supports building on the arm64 architecture. I think it might be a good choice to use Travis CI to compile the arm version of RustDesk.
https://docs.travis-ci.com/user/multi-cpu-architectures/",2021-07-25T04:19:02Z,42692274
3432,rustdesk/rustdesk,951429255,890464516,I am waiting for new M1 release.,2021-08-01T07:04:32Z,71636191
3433,rustdesk/rustdesk,951429255,966932574,"> I am waiting for new M1 release.

so, is it time to support arm64? :)",2021-11-12T08:57:34Z,9317897
3434,rustdesk/rustdesk,951429255,966965353,Sorry for your long waiting. I will try QEMU.,2021-11-12T09:48:06Z,71636191
3435,rustdesk/rustdesk,951429255,966965847,But which Linux distro?,2021-11-12T09:48:54Z,71636191
3436,rustdesk/rustdesk,951429255,967146032,@hmsjy2017 Debian.,2021-11-12T14:10:58Z,42692274
3437,rustdesk/rustdesk,951429255,998764139,"I tried to build RustDesk on Raspberry Pi4(Ubuntu-ARM64) and Raspberry Pi3(Raspbian OS-ARM32).
Unfortunately,both failed.

For Pi4(test on Linux ARM64):
From the code and README:
> Desktop versions use **sciter** for GUI......

sciter-sdk only provides the _libsciter-gtk.so_ build on x64 / ARM32 , they didn't release arm64 for linux yet.
[libsciter-gtk.so doesn't support in ARM64 platform #210](https://github.com/c-smile/sciter-sdk/issues/210)
We can't build on ARM64 unless sciter-sdk provides the .so someday.


For Pi3(test on Linux ARM32):
_libsciter-gtk.so_ maybe fine.
but I can't build success vcpkg on Raspbian OS-ARM32.
So I build the three libs _libvpx libyuv opus_ from those offical source code,then I put them in vcpkg/installed/arm-linux/ 
and _cargo build_
......a very long build time...... 
all rust crates and lib passed the compilation,but finally failed on maybe some gcc/link problem:
![2021-12-21 20-44-39屏幕截图](https://user-images.githubusercontent.com/62206297/146932228-c9e36c20-2496-41e3-8004-51b7c5e1c594.png)
...
![2021-12-21 20-45-29屏幕截图](https://user-images.githubusercontent.com/62206297/146932277-e88fd7a2-eb20-48c6-9415-be20761fcf34.png)

Im not good at gcc compiling and linking,I cant understand these problem...

I think it's not ready to support Raspberry Pi(Linux ARM) yet.",2021-12-21T13:07:29Z,62206297
3438,rustdesk/rustdesk,951429255,998789001,"You can google, https://stackoverflow.com/questions/19768267/relocation-r-x86-64-32s-against-linking-error",2021-12-21T13:41:09Z,71636191
3439,rustdesk/rustdesk,951429255,1000999666,"> > I am waiting for new M1 release.
> 
> so, is it time to support arm64? :)

You need ask Sciter author support arm64 first. Because RustDesk depends on Sciter. https://github.com/c-smile/sciter-sdk/issues/210",2021-12-25T09:59:12Z,71636191
3440,rustdesk/rustdesk,951429255,1001002044,"@rustdesk what about support without ui, just a server mode?",2021-12-25T10:31:37Z,7953703
3441,rustdesk/rustdesk,951429255,1001002625,"> just a server mode

PR please.",2021-12-25T10:40:08Z,71636191
3442,rustdesk/rustdesk,951429255,1117228091,"Since Sciter does arm32, wouldn't it be possible to at least create an arm32 build for the time being? arm64 is backwards compatible with it",2022-05-04T11:59:44Z,7207103
3443,rustdesk/rustdesk,951429255,1117270220,I will try to find one arm machine.,2022-05-04T12:45:19Z,71636191
3444,rustdesk/rustdesk,951429255,1117283527,"> I will try to find one arm machine.

Dude you don't need an arm machine, qemu will do just fine for you",2022-05-04T12:59:16Z,7207103
3445,rustdesk/rustdesk,951429255,1117312025,Where can I find an Ubuntu 16 arm desktop iso?,2022-05-04T13:25:43Z,71636191
3446,rustdesk/rustdesk,951429255,1117320342,"There does not exist a ubuntu 16 desktop arm iso, for this case you should go for debian, as it has less overhead
![image](https://user-images.githubusercontent.com/7207103/166691451-45780da3-266a-48cf-a56b-d6fbddc298aa.png)
",2022-05-04T13:33:12Z,7207103
3447,rustdesk/rustdesk,951429255,1119507145,Also maybe you could just create some compilation instructions so I could test it on my raspberry pi and give you feedback?,2022-05-06T11:15:05Z,7207103
3448,rustdesk/rustdesk,951429255,1119508461,"@Heap-Hop tested today on Pi4, performance is bad, and failed to get vpx encoder work. We paused to invest time on it. Will continue once we are free.",2022-05-06T11:16:58Z,71636191
3449,rustdesk/rustdesk,951429255,1119514610,"To be fair didn't know that they were written already thinking in other arches, generally instructions are written only thinking on x86/x86_64",2022-05-06T11:23:43Z,7207103
3450,rustdesk/rustdesk,951429255,1119565301,"> @Heap-Hop tested today on Pi4, performance is bad, and failed to get vpx encoder work. We paused to invest time on it. Will continue once we are free.

How did you test it with SCiter not supporting AARCH64
",2022-05-06T12:29:48Z,16347811
3451,rustdesk/rustdesk,951429255,1119591242,@busyluo we tested on arm32,2022-05-06T12:59:53Z,62206297
3452,rustdesk/rustdesk,951429255,1129516367,"I have successfully compiled RustDesk on armhf and aarch64, running in service state without UI. Arm runs Ubuntu18.04. TCP penetration is available to use SSH services.
我已经成功在armhf和aarch64上编译了rustdesk。运行于服务状态，没有UI。Arm跑的是Ubuntu18.04.可以进行TCP穿透来使用SSH服务。",2022-05-18T03:01:33Z,102203060
3453,rustdesk/rustdesk,951429255,1129517575,"> I have successfully compiled ...

Good job.",2022-05-18T03:04:21Z,71636191
3454,rustdesk/rustdesk,951429255,1129518039,"> I have successfully compiled ...

If possible, could you please write a tutorial on https://github.com/rustdesk/doc.rustdesk.com?",2022-05-18T03:05:23Z,71636191
3455,rustdesk/rustdesk,951429255,1129520856,"> If possible, could you please write a tutorial on https://github.com/rustdesk/doc.rustdesk.com?

I'll try. Wait for me. Haha",2022-05-18T03:11:43Z,102203060
3456,rustdesk/rustdesk,951429255,1129522891,"> I'll try.

Respect!!!",2022-05-18T03:16:33Z,71636191
3457,rustdesk/rustdesk,951429255,1129980765,"The tutorial is almost done, and I'll just add some tests at the end.
教程基本写完，最后我再补充点测试就好了。",2022-05-18T12:55:46Z,102203060
3458,rustdesk/rustdesk,951429255,1129984248,"Thanks, English also please. :)
",2022-05-18T12:57:23Z,71636191
3459,rustdesk/rustdesk,951429255,1129985623,"> Thanks, English also please. :)

Your translation is good!",2022-05-18T12:58:40Z,102203060
3460,rustdesk/rustdesk,951429255,1130099877,"I'm not quite sure whether it is a good idea to use Ubuntu's rootfs, especially from a source like baidu. Maybe make it a docker container build using the official images?
Running random builds of libvpx and opus doesn't exactly sound good either",2022-05-18T14:36:16Z,7207103
3461,rustdesk/rustdesk,951429255,1130123488,"Let me explain some possible downsides of the armhf instructions.
You have to flash an sd card with an ubuntu distro with already set up credentials, yet to be known if it has ssh enabled or calls home, and some libraries as binary downloaded from a chinese server, when they can be obtained through git or some other more legit way without going through redistribution endpoints. This can easily inject malicious code at any step in the chain",2022-05-18T14:56:43Z,7207103
3462,rustdesk/rustdesk,951429255,1130130043,"Ok, you persuaded me.",2022-05-18T15:02:16Z,71636191
3463,rustdesk/rustdesk,951429255,1130134617,"> Ok, you persuaded me.

What you going to do then? Redo the docs to be more trustable(changing the ubuntu thing to docker and fetching sources instead of binaries)? Erase the page overall until another knight in shining armor comes and does your job?",2022-05-18T15:06:10Z,7207103
3464,rustdesk/rustdesk,951429255,1130140801,"@wwjabc since you have added libvpx/opus compilation steps, I will remove the baidu download links. and I will also replace ubuntu18.04_rootfs.tar.gz with an official image. Wondering if this link https://github.com/ikwzm/ZynqMP-FPGA-Ubuntu18.04-Ultra96/blob/master/doc/build/ubuntu18.04-rootfs.md is correct?",2022-05-18T15:10:42Z,71636191
3465,rustdesk/rustdesk,951429255,1130164919,"@wwjabc Sorry, it is my fault to ask you for doc and cause you trouble.",2022-05-18T15:28:22Z,71636191
3466,rustdesk/rustdesk,951429255,1130176901,"> @wwjabc since you have added ...

Using rootfs is bad overall, if you get a docker image going you make it possible to do it from existing installs, it's way less bothersome for anyone following the instructions",2022-05-18T15:35:32Z,7207103
3467,rustdesk/rustdesk,951429255,1146545452,Try out [raspberry-armhf.deb](https://github.com/rustdesk/rustdesk/releases/download/1.1.9/rustdesk-1.1.9-raspberry-armhf.deb) please.,2022-06-04T06:03:14Z,71636191
3468,nextcloud/server,209725628,209725628,"### How to use GitHub

* Please use the 👍 [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to show that you are interested into the same feature.
* Please don't comment if you have no relevant information to add. It's just extra noise for everyone subscribed to this issue.
* Subscribe to receive notifications on status change and new comments. 


**Is your feature request related to a problem? Please describe.**
I think it would be great if Nextcloud could notify the Admin via Email if there are pending upgrades. This would make it easier to deal with upgrades also on instances one is not actively using but rather just maintaining.",2017-02-23T10:45:06Z,3024024
3469,nextcloud/server,209725628,281980608,"Since Nextcloud 10 we have update notifications delivered by the notifications API. When you have a sync client set up and connected, it will bring the notifications to your desktop.

However I just discussed with one of my colleges today, that it would also be cool to send a reminder email after some days to the admin when the instance is still not updated.",2017-02-23T12:35:31Z,213943
3470,nextcloud/server,209725628,282499604,why not just make it auto-update able? ,2017-02-25T17:43:30Z,8197408
3471,nextcloud/server,209725628,287624602,"To me this is very important since as a sysadmin I manage some Nextcloud instances that I don't use myself.

Using the desktop app to know a new upgrade is pending is thus not a solution.",2017-03-19T15:34:00Z,6070749
3472,nextcloud/server,209725628,393610420,"Alternatively, how about Nextcloud instances that you do use, but not through the web interface or a Nextcloud app? Then you don't see any notifications at all.

i.e. I use Nextcloud primarily for syncing contacts/calendar. This is done through the online accounts in GNOME on my laptop and syncevolution on my phone. Neither of these tell me anything about upgrades.",2018-05-31T17:28:37Z,2203121
3473,nextcloud/server,209725628,417057220,"an email on pending updates is really important. having such a widespread app, you should add such a important feature to make the web a little bit more secure.

> that it would also be cool to send a reminder email after some days

 i am clearly against a delay. the email should be sent instantly, not with some days delay. if there is a security hole, the admins want to be notified (instantly). 

i assume that updates are only shown to the admin user, so keep in mind, that you require me to work as admin with my sync app to get such important informations.

workaround i found (german text but the code is english): 
https://spielwiese.la-evento.com/xelasblog/archives/75-Mailbenachrichtigung-bei-verfuegbaren-Nextcloud-Updates.html",2018-08-29T18:28:35Z,649209
3474,nextcloud/server,209725628,417233648,"Well, the thing is: if you don't use the admin account for syncing, you might also not have set an email address for it. Also this issue here is just meant to be an additional reminder.

> i assume that updates are only shown to the admin user, so keep in mind, that you require me to work as admin with my sync app to get such important informations.

You are assuming wrong. In the admin section you can actually select which groups get update notifications. By default it's admin only, yes. But if you don't sync with the admin user (like I do), you can also change it to any other group.",2018-08-30T08:23:24Z,213943
3475,nextcloud/server,209725628,417233981,"Additionally those update notifications are displayed are displayed as push notifications on android and iOS clients, and also the desktop client shows the notifications.",2018-08-30T08:24:43Z,213943
3476,nextcloud/server,209725628,417235200,"@nickvergessen I tend to disagree here. As an admin, I may only use ~~ownCloud~~ Nextcloud using the web UI or just for syncing calendars and contacts. Or I may administer an instance for other people so I don't use it myself at all. Running a client all time just to be informed about updates is a not very comfortable.

--
Edited: Oops. As a defense: This only shows how seamless the migration to Nextcloud was 😉",2018-08-30T08:28:50Z,11042539
3477,nextcloud/server,209725628,417236188,"Well if you are using ownCloud we can't help you anyway. [/joke]

But well, I do agree that this should be fixed. I'm just saying it's not as bad as people are drawing it.",2018-08-30T08:32:06Z,213943
3478,nextcloud/server,209725628,417361739,"It's as bad as we're saying, for the group of people such as myself, who currently receive no notifications, because we don't use the sync app or similar.

Maybe it's only a small number of users, but for those users, it is a moderately serious security issue, as there is no obvious way to get notified promptly about upgrades. Currently, I typically find out about security updates weeks or sometimes even months after they are released, when I just happen to log in to the web interface.",2018-08-30T15:29:47Z,2203121
3479,nextcloud/server,209725628,417365574,I added https://github.com/nextcloud/server/releases.atom to my rss reader. Edit: I just found https://coderelease.io (send you an email when new release on github),2018-08-30T15:40:25Z,3902676
3480,nextcloud/server,209725628,417765859,"I wouldn't mind if the fix was exposing a parameter in `occ` that told you if your instance is up to date or not.

I could write a cron job or an icinga2 test against that and get notified.",2018-08-31T19:22:59Z,6070749
3481,nextcloud/server,209725628,417773409,@baldurmen https://github.com/nextcloud/server/pull/10836 (unfortunately too late for nextcloud 14),2018-08-31T19:56:44Z,3902676
3482,nextcloud/server,209725628,417850136,"@danielkesselberg of course these links help, but still they show all releases. normally a admin does not care for beta releases or release candidates. the admin wants to know is there a security update for my current installation. 
we are all flooded with messages from multiple services: emails, newsletter, github, rss feeds, social networks,... so nextcloud should add this feature to help admins to net get drowned in too many unneeded emails and only send an email if the admin should act.",2018-09-01T10:47:41Z,649209
3483,nextcloud/server,209725628,420439268,"A perspective that may not have been considered just yet: For the past year I've been wondering what's wrong with my instance that it's not sending me an email when new versions are released. It only occurred to me as I was typing up a question on the forum that I may have misunderstood the function, and to read the notification option carefully and search for it (which brought me here). Others may be letting their instance go unupdated not realising that there's numerous updates available.

The reason I haven't seen the notifications is because I rarely use the web app, like others here. The sync is done on my server, and I access the synced directory over the network.",2018-09-11T21:55:43Z,1338293
3484,nextcloud/server,209725628,438934152,This would also be useful for update notifications of apps. Sure I also have the sync client and see notifications there. But the workflow of a sysadmin for these kind of things are usually via email notifications.,2018-11-15T06:37:40Z,6113099
3485,nextcloud/server,209725628,474054559,"I was using the old `owncloud` client and I used to get notified of updates this way, but now that I switched to the `nextcloud` client I don't get those notifications anymore...

I've hacked some icinga2 tests to check for app updates and core updates for now, but this is getting really painful :(",2019-03-18T18:54:26Z,6070749
3486,nextcloud/server,209725628,474250432,That should actually work quite fine,2019-03-19T08:49:48Z,213943
3487,nextcloud/server,209725628,506326217,"> This would also be useful for update notifications of apps.

I want to emphasize this statement, because it seems to be much more important to me. I am currently managing more than 20 Nextcloud instances which I am not using by my own. So I rarely login there. I actually do follow the Nextcloud releases stream, so I know when its time to update my instances. Some of them are even managed instances and therefor updated by another company. 

But for all of these instances there are different apps active, for some instances there are even other users enabling/disabling apps. So I would need to follow all these release channels and then know which apps are active in which instances. Then login manually to the concerned instances and press the update button. And thats even the case for the managed instances because normally app updates are not included in the management. That is more than inconvenient and its feeling very much unprofessional. 

A Mail notification with a direct link would help here, because then only the concerned instances would notify. An autoupdater for apps **as an option** (and activity notification) would make this  even more handy for a lot of environments.",2019-06-27T12:31:43Z,37742522
3488,nextcloud/server,209725628,555143985,"I've just seen this and I have to agree, email notification would be really useful.",2019-11-18T18:22:12Z,13846728
3489,nextcloud/server,209725628,555180821,Since the addition of `occ update:check` you can run a cronjob to alert you by email when there are updates available or use the ouput in a proper monitoring system.,2019-11-18T19:51:13Z,6070749
3490,nextcloud/server,209725628,555264917,that wont work if you don't have shell access. This is true for managed nextcloud setups.,2019-11-18T23:51:35Z,37742522
3491,nextcloud/server,209725628,558156323,Why not simply add a checkbox-based choice of whether the specified groups (to be informed about updates) should also be informed by email?,2019-11-25T13:30:18Z,20518297
3492,nextcloud/server,209725628,624838998,Agree with @ilippert ,2020-05-06T19:17:21Z,4522947
3493,nextcloud/server,209725628,625148579,"a checkbox would be perfect!
",2020-05-07T09:46:17Z,37901988
3494,nextcloud/server,209725628,625151965,@fosple @joostvkempen please use [GitHub reactions](https://github.blog/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/). If you comment everyone subscribed to the issue get's a notification.,2020-05-07T09:53:01Z,3902676
3495,nextcloud/server,209725628,742740465,Any progress on this issue ? Really it could be so usefull to receive these mails,2020-12-10T19:23:14Z,54892453
3496,nextcloud/server,209725628,742761975,"or at least allow us to config a webhook which is triggered if an update is available, then we can send the emails by ourselfs",2020-12-10T19:59:26Z,649209
3497,nextcloud/server,209725628,759934491,I agree that an email notification is very important,2021-01-14T05:25:17Z,9902758
3498,nextcloud/server,209725628,772372770,"I would really like to see this feature, too. This is just missing IMHO. ",2021-02-03T09:40:43Z,11415964
3499,nextcloud/server,209725628,1071002733,Should be solved via https://github.com/nextcloud/notifications/issues/314 now,2022-03-17T15:49:44Z,213943
3500,nextcloud/server,708907357,708907357,"### How to use GitHub

* Please use the 👍 [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to show that you are affected by the same issue.
* Please don't comment if you have no relevant information to add. It's just extra noise for everyone subscribed to this issue.
* Subscribe to receive notifications on status change and new comments. 


### Steps to reproduce

I keep getting the following message in my logs:
```
Debug	files_sharing	/appinfo/app.php is deprecated, use \OCP\AppFramework\Bootstrap\IBootstrap on the application class instead.	
2020-09-25T14:36:30+0200
Debug	files_sharing	/appinfo/app.php is deprecated, use \OCP\AppFramework\Bootstrap\IBootstrap on the application class instead.	
2020-09-25T14:36:29+0200
Debug	files_sharing	/appinfo/app.php is deprecated, use \OCP\AppFramework\Bootstrap\IBootstrap on the application class instead.	
2020-09-25T14:36:29+0200
Debug	files_sharing	/appinfo/app.php is deprecated, use \OCP\AppFramework\Bootstrap\IBootstrap on the application class instead.	
2020-09-25T14:36:29+0200
Debug	files_sharing	/appinfo/app.php is deprecated, use \OCP\AppFramework\Bootstrap\IBootstrap on the application class instead.	
2020-09-25T14:36:29+0200
```

### Server configuration

**Database:**
phpMyAdmin

**PHP version:**
7.3.16

**Nextcloud version:** (see Nextcloud admin page)
20.0.0 RC2

App:
Files sharing
1.12.0",2020-09-25T12:45:59Z,47037905
3501,nextcloud/server,708907357,699461033,Increase the log level to 1 or higher. It's expected for level debug to log deprecated calls.,2020-09-26T08:41:07Z,3902676
3502,nextcloud/server,708907357,706713420,I had the same problem.,2020-10-11T14:31:49Z,12292644
3503,nextcloud/server,708907357,708420457,Debug message -> expected.,2020-10-14T13:56:10Z,1374172
3504,nextcloud/server,708907357,721773501,"Core apps really shouldn't trigger deprecation messages, but get fixed (arguably prior to a release).
Or other way round: if the implementation of files_sharing is considered stable and future proof, NC shouldn't mark it as deprecated.",2020-11-04T14:44:21Z,7279540
3505,nextcloud/server,708907357,721781620,"@andreas-p could you check if this green button is blocked for you?

![Bildschirmfoto von 2020-11-04 15-57-22](https://user-images.githubusercontent.com/1374172/98127040-876ed180-1eb6-11eb-8321-6f4ad2175135.png)

/s",2020-11-04T14:58:17Z,1374172
3506,nextcloud/server,708907357,721787882,"@ChristophWurst No it's not. If you're suggesting I should create a PR for this (officially closed!) issue: For my debugging purposes, I commented out the deprecation message in lib/private/legacy/OC_App.php. files_sharing/appinfo/app.php looks different from what I'm familiar with, so I refrained from digging there.",2020-11-04T15:08:48Z,7279540
3507,nextcloud/server,708907357,727630588,"As a user my pull request would simply be disabling the message altogether. But that might be useful for developers. Idea is.. if your ""expected debug message"" takes a dump on the UI which is user-facing then your ""feature"" is as useful as this guy's facial tattoo:
![useful facial tattoo](https://i.pinimg.com/736x/11/8f/e6/118fe6cec3fbfc8ead6eea47f7dc15cc--worlds-worst-tattoos-the-ojays.jpg)

Maybe another solution would be to keep deprecation messages away from the UI. Or give the user a way to use filters?

Otherwise this is how much fun you can have as a user trying to debug why the email app chokes on ssl connections:
[much fun - 37MB gif](https://bugout.regex.cf/dumbestdesignaward2020.gif)",2020-11-15T20:23:37Z,16505052
3508,nextcloud/server,708907357,727809726,"> As a user

So as a user you're complaining about a *debug log output*. Seriously, why do you worry about this so much? You enable debug mode for a few secs to gather some info, then you disable it again.

Now tell me: would you rather have a silent log where we, the maintainers, can't help you solve an issue due to the lack of information?",2020-11-16T08:05:17Z,1374172
3509,nextcloud/server,708907357,728119588,"> you're complaining about a _debug log output_

No, actually I'm not complaining about a debug log output, and I've already conceded that deprecation notices might be useful for developers. What I'm specifically stating is that **deprecation notices** at a rate of 3/second is a **useless thing in the UI**.

> Now tell me: would you rather have a silent log where we, the maintainers, can't help you solve an issue due to the lack of information?

I thought deprecation notices are more of a reminder to later tackle the phased out code. Am I wrong?",2020-11-16T15:04:51Z,16505052
3510,nextcloud/server,708907357,728126976,With the stateless nature of php it's hard to debounce this. Every request will trigger it.,2020-11-16T15:17:01Z,1374172
3511,nextcloud/server,708907357,728128145,"Okay, understood. Thank you.",2020-11-16T15:18:57Z,16505052
3512,nextcloud/server,708907357,728682934,"In particular, if you're using the Logging app with the auto-refresh option checked in the web UI, the requests generated by the auto-refresh itself will trigger this very frequently. I ran into this myself recently and if you turn that off and then tail the actual log file on the command line, you'll see it settles down.",2020-11-17T04:45:49Z,911174
3513,nextcloud/server,708907357,728694225,"TBH I saw no harm in commenting out that line and that's how I managed to get a usable UI. But it's a good tip, thank you.",2020-11-17T05:24:51Z,16505052
3514,nextcloud/server,708907357,733158163,FYI: Updated from NC 19.0.5.2 to 20.0.2.2 and loglevel was set from 1 to 0 in config.php during update. Machine went totally crazy having Nextcloud.log rotated (cut and moved at 100 MB size) automatically few times. I guess update isn´t reliable yet...,2020-11-24T18:30:48Z,13799156
3515,nextcloud/server,708907357,744443472,"I do not want to continue this hate-thread but I fear during development for the cookbook app this issue caused a regression as we were not able to test accordingly. Currently, we are recovering but the main issue can be brought down to this issue.
In fact, I was not able to see the live error logs during testing/debugging the app. So I missed the warning of PHP that a certain set of functions/fields are undefined (typos) and other similar effects. The log was just cluttered with these deprecation warnings (which are good in general). But as these are generated on each invocation (also the update of the logs) this ends in an infinite recursion.

For now, I am able to see some useful information by disabling the `files_sharing` app but this is no permanent solution, especially as we are planning to allow for the sharing of recipes in the app. At most that moment in time, we need to allow the sharing again, which will clutter the logs again.

I tried to create a patch in [this PR](https://github.com/nextcloud/server/pull/24689) but I am not able to get things right, I am missing the knowledge of the internal structure of the NC core.

I am asking @ChristophWurst to reconsider closing this issue as I can only second the general statement/issue here: *A component uses a deprecated method and that should be replaced by a newer structure.* The fact that it is a core component just makes it hard for other contributors to contribute a ""quick fix"".

If this is on the agenda for a near-future update, perfect, just give a short statement and I am going to keep waiting.",2020-12-14T13:34:20Z,8202922
3516,nextcloud/server,708907357,744466626,@ChristophWurst I am opening an issue to discuss the problem again.,2020-12-14T14:12:24Z,47037905
3517,nextcloud/server,708907357,748440202,"It would be nice to get a solution here fast. I just wanted to track some debug messages while continuing development and this situation still exist. 

The problem is not, that a lot of debugging messages are generated because of this deprecation message, the problem is that debugging is impossible ATM. There are something about 45 deprecations messages **per second**. See the log file snippet in #23351.",2020-12-19T08:17:34Z,26707476
3518,nextcloud/server,708907357,748966436,So what would you suggest as a solution?,2020-12-21T13:12:12Z,1374172
3519,nextcloud/server,708907357,749240524,"It seems, that the logger itself is responisble for the endless production of deprication warnings in the logfile. If this could be avoided the problem would be smaller and acceptable. 

As I am not familiar with the core development, I have no real proposal for a solution. ",2020-12-21T22:50:02Z,26707476
3520,nextcloud/server,708907357,749241489,Correction: seems that the files_sharing and contacts are my problem here. ,2020-12-21T22:53:26Z,26707476
3521,nextcloud/server,708907357,749270194,"@dartcafe I mentioned this up-thread; see https://github.com/nextcloud/server/issues/23046#issuecomment-728682934. You can get a vaguely reasonable log output with `tail -f data/nextcloud.log | jq '.time + "" "" + .reqId + "" "" + if (.message | type) == ""object"" then .message.Exception else .message end'` (though you will of course need to install `jq` on your system first). Then the Logging app won't create all the deprecation spam.",2020-12-22T00:27:09Z,911174
3522,nextcloud/server,708907357,765235684,"We have loglevel 1 and the log is spammed with deprecation warnings, 25GB log in the last days. ",2021-01-22T08:35:34Z,1674578
3523,nextcloud/server,708907357,765260416,"Wth are you talking about

On Fri, Jan 22, 2021, 1:36 AM Bjoern Franke <notifications@github.com>
wrote:

> We have loglevel 1 and the log is spammed with deprecation warnings, 25GB
> log in the last days.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/nextcloud/server/issues/23046#issuecomment-765235684>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ARULPN57G2EOV52LAFLOQFDS3E2HVANCNFSM4RZQC2IQ>
> .
>
",2021-01-22T09:18:17Z,73971639
3524,nextcloud/server,708907357,765267007,"> Wth are you talking about

Some entries above @ChristophWurst says users should not complain about debug output when setting loglevel to `debug`. I changed loglevel 0 to 1 expecting `info` messages, but no deprecation warnings. And the logfile grew from 2GB to 25 GB in 4 days, full of deprecation warnings. ",2021-01-22T09:27:31Z,1674578
3525,nextcloud/server,708907357,765269521,"Great now that we have all people combined that complained about the problem, here's the PR that fixes it: https://github.com/nextcloud/server/pull/25267

you're welcome",2021-01-22T09:31:12Z,1374172
3526,nextcloud/server,708907357,765481354,"Why aren't repeated duplicate log messages rate limited (the approach the Linux kernel takes is good where if a message is repeated a lot of times in `dmesg` it'll just stop showing it and print a message saying the previous message was repeated X times)?

![Screenshot_2021-01-22 Settings - Nextcloud](https://user-images.githubusercontent.com/3309784/105509550-85eac380-5cc5-11eb-8666-77f5d66a4039.png)
",2021-01-22T15:23:51Z,3309784
3527,nextcloud/server,708907357,765522129,@aidanharris read https://github.com/nextcloud/server/issues/23046#issuecomment-728119588 and the response.,2021-01-22T16:18:36Z,3902676
3528,nextcloud/server,708907357,766094879,"Same problem here with alot of reports and logs:

`[previewgenerator] Debug: /appinfo/app.php is deprecated, use \OCP\AppFramework\Bootstrap\IBootstrap on the application class instead.`

also
```
[no app in context] Info: Deprecated event type for OCA\Files::loadAdditionalScripts: Symfony\Component\EventDispatcher\GenericEvent is used
GET /index.php/apps/files/
```

like every few seconds those appear",2021-01-23T15:18:22Z,77236661
3529,nextcloud/server,708907357,778335050,One step closer to a more reasonable debug log: https://github.com/nextcloud/server/pull/25497 :v: ,2021-02-12T17:33:05Z,1374172
3530,nextcloud/server,708907357,778619212,@ChristophWurst Cool 👍. I hope it will be much clearer now and will not take up as much space.,2021-02-13T13:26:14Z,47037905
3531,nextcloud/server,708907357,875075144,"The logfile flooding still exists in NC22. 

Wouldn't it be a good idea to remove the deprecation at least for the core apps? 

I list the following apps in my NC22 installation: 
* files_sharing
* contacts
* suspicious_login
 ",2021-07-06T20:55:41Z,26707476
3532,nextcloud/server,708907357,875300205,"> files_sharing

Be my guest and help finish what I was unable to finish https://github.com/nextcloud/server/pull/25267 :wink: 

> contacts

https://github.com/nextcloud/contacts/issues/1831

> suspicious_login

https://github.com/nextcloud/suspicious_login/issues/499",2021-07-07T05:43:17Z,1374172
3533,nextcloud/server,708907357,875305451,"> > files_sharing
> 
> Be my guest and help finish what I was unable to finish #25267 😉

I would, if I would have the time and the knowledge. Polls alone takes more time, as I normaly could afford.",2021-07-07T05:54:41Z,26707476
3534,nextcloud/server,708907357,875307211,"No worries :v:

We'll have to wait for someone more knowledgeable then because the sharing stuff was also out of my area of expertise and thus got stuck.",2021-07-07T05:58:15Z,1374172
3535,nextcloud/server,708907357,1108626727,"File sharing is migrated, so closing this.
Other apps can be added here: https://github.com/nextcloud/server/issues/32132",2022-04-25T14:08:51Z,213943
3536,microsoft/vscode,1410698334,1410698334,"Some netizens need to scan the code to install vscode, I hope the official can stop this behavior 
![21276c6919c82457dcedd5587c5edc0](https://user-images.githubusercontent.com/13271663/196067460-dd5c6606-3ccc-4a04-a555-9d2f8add0636.jpg)
![07e482785c0bdd0986b798bbc86e7aa](https://user-images.githubusercontent.com/13271663/196067464-8323e6f6-5e56-4407-b629-965427faf557.jpg)
![5bcc8c2dbb4d94d26fd7b41448b0d63](https://user-images.githubusercontent.com/13271663/196067467-56f07382-364c-4c72-a92c-d6faed02d97f.jpg)
",2022-10-17T00:44:22Z,13271663
3537,microsoft/vscode,1410698334,1280784978,This is piracy and goes against the open source license ...,2022-10-17T12:32:02Z,73830635
3538,microsoft/vscode,1410698334,1280894062,This doesn't violate the open source license if they don't modify vscode. But it certainly violated trademark rights.,2022-10-17T13:50:09Z,2662758
3539,microsoft/vscode,1410698334,1282160660,Extremely interesting,2022-10-18T10:19:24Z,32984956
3540,microsoft/vscode,1410698334,1282226720,Need ￥39 to install? Selling free software?,2022-10-18T11:20:43Z,45794248
3541,microsoft/vscode,1410698334,1282268699,How can you find this...  Is just typing code.visualstudio.com too difficult for some people?,2022-10-18T11:56:46Z,6648049
3542,microsoft/vscode,1410698334,1282276362,"> How can you find this... Is just typing code.visualstudio.com too difficult for some people?

Use Baidu to search for vscode. Click Open in the first advertisement......",2022-10-18T12:03:24Z,29668663
3543,microsoft/vscode,1410698334,1282279114,hilarlious,2022-10-18T12:05:38Z,112148815
3544,microsoft/vscode,1410698334,1282292034,"Fun fact: MIT License allows you to ""sell copies of the Software"" with or without modification.",2022-10-18T12:16:55Z,12044683
3545,microsoft/vscode,1410698334,1282299306,"> How can you find this...  Is just typing code.visualstudio.com too difficult for some people?

In some countries, there is Internet interdict like Great Fire Wall in China. So people there cannot reach the servers of Microsoft.",2022-10-18T12:23:08Z,32984956
3546,microsoft/vscode,1410698334,1282303451,"Search from Baidu:

![image](https://user-images.githubusercontent.com/50246090/196424828-5ead4681-c783-442e-bbea-0b8e4c15fcc2.png)

The first one is https://qnw.shaid.top/vscode/index.html:

![image](https://user-images.githubusercontent.com/50246090/196424988-4523e2d7-4860-4631-876f-bfd4a2548359.png)

This website belongs to [Wuhan Hongge Yibai E-commerce Co. (武汉宏格亿佰电子商务有限公司)](http://www.shaid.top/).

If you do some searches on this domain, you can find their other ""products"":
- [Win10 Activation Tool (Win10 激活工具)](https://qnw.shaid.top/win10/index.html)
- Telegram Chinese version (Telegram 中文版) (removed)

  ![image](https://user-images.githubusercontent.com/50246090/196426820-f9fa3030-eb5e-430c-bdfc-026af7100576.png)
- visualcpp (removed)

  ![image](https://user-images.githubusercontent.com/50246090/196432416-34943cc9-de17-4e6b-a316-ca5a6ff3f5e7.png)

They even claim to be partners with China Mobile, China Unicom and China Telecom:
[![image](https://user-images.githubusercontent.com/50246090/196426325-f39cc8ab-8b66-40b7-8a1e-edc32ad7c08b.png)](http://www.shaid.top/)

Thanks to the Great Firewall, we have such a ""great"" Internet in China.

<hr />

Update:

Some people have found other similar companies:
- [Shangqiu Xuankangtai Network Technology Co. (商丘轩康泰网络科技有限公司)](http://www.sqiua.cn/index.html)
  - [Visual studio](http://office.xuank.top/install.php?m=visual)  
    ![image](https://user-images.githubusercontent.com/50246090/196571476-47c2e0e6-5465-4666-8889-ad34a1c4689a.png)
  - [Software Superstore (软件商超)](https://xkt.sqiua.cn/)  
    
    ![image](https://user-images.githubusercontent.com/50246090/196572556-a30b1388-e5f5-43f8-a94a-419e5501c2f9.png)
  - [System Home - Windows premium system download site (系统之家-Windows精品系统下载站)](http://windows.sqiua.cn/index.html)
  - [visualbasic](http://office.xuank.top/install.php?m=visualbasic)
  - [directx Repair Master (directx修复大师)](http://office.xuank.top/install.php?m=directx)
  - [Chrome](http://chrome.sqiua.cn/)  
  
     ![image](https://user-images.githubusercontent.com/50246090/196573087-e4bfb9f0-3060-4e72-a6bf-05a35238a151.png)
  - [Yixin Cat House (忆心猫舍)](http://yixinmaoshe.sqiua.cn/)  
    
     ![image](https://user-images.githubusercontent.com/50246090/196572912-95f01175-ad54-40d7-bfe7-2b10aaa767b8.png)
  - [AdobeCAD](https://autocad.sqiua.cn/)
  - [Map marking service center (地图标注服务中心)](http://mr.sqiua.cn/)
- [Yunnan Norforkang Network Technology Co. (云南诺福康网络科技有限公司)](http://www.cvbty.cn/)
  - [Xunjian Mind Map (寻简思维导图)](http://sw.xuank.top/)
",2022-10-18T12:26:28Z,50246090
3547,microsoft/vscode,1410698334,1282977561,"In fact, not only vscode, but also tools like dev-c++are used by the company for profit
However, in quite a short time, the company withdrew all advertisements on Baidu
This means that the content of this company is illegal",2022-10-18T20:39:56Z,61108540
3548,microsoft/vscode,1410698334,1283183571,"> How can you find this... Is just typing code.visualstudio.com too difficult for some people?

Yes, IT IS difficult for some people who just know vscode or just get started learning coding.

What makes this thing more hilarlious is that,
If you search `vscode` or `visual code` on Bing China, you will get a piracy link on the first search result.
![image](https://user-images.githubusercontent.com/8984680/196569964-6d9d19ab-cfb6-4eba-95c9-a792249cc511.png)
![image](https://user-images.githubusercontent.com/8984680/196569990-a81a1105-fbde-4cf0-a667-d0716cde20c6.png)",2022-10-19T00:37:30Z,8984680
3549,microsoft/vscode,1410698334,1283402491,Interesting...,2022-10-19T04:19:02Z,48862050
3550,microsoft/vscode,1410698334,1283409944,"Hilarious indeed, thanks to GFW.",2022-10-19T04:29:48Z,67035130
3551,microsoft/vscode,1410698334,1283410506,樂,2022-10-19T04:30:45Z,40854260
3552,microsoft/vscode,1410698334,1283412796,典,2022-10-19T04:34:11Z,14970892
3553,microsoft/vscode,1410698334,1283414858,"> Fun fact: MIT License allows you to ""sell copies of the Software"" with or without modification.

Not really true. The official version of Visual Studio Code contains the trademark and some modifications by Microsoft, and is therefore subjected to [Microsoft's terms](https://code.visualstudio.com/License/). The pirate version definetely goes against the terms and must be taken down.
However though, if they do recompile VSCode removing nonfree bits, and give it a different trademark (like what the libre VSCodium is doing), yes, they do have the right to sell it. But what do you expect from a commercial pirate website?",2022-10-19T04:37:08Z,25359455
3554,microsoft/vscode,1410698334,1283424282,"![image](https://user-images.githubusercontent.com/44344308/196600820-ca8d3cbc-3c07-4bb1-b49d-dbb642854a02.png)
VS Code ""Officially licensed"": https://www.google.com
lol",2022-10-19T04:50:34Z,44344308
3555,microsoft/vscode,1410698334,1283444619,"> ![image](https://user-images.githubusercontent.com/44344308/196600820-ca8d3cbc-3c07-4bb1-b49d-dbb642854a02.png) VS Code ""Officially licensed"": https://www.google.com lol

典中典",2022-10-19T05:19:39Z,45794248
3556,microsoft/vscode,1410698334,1283448682,"Do you know why there is a GFW in China? cause CCP sucks, these SOBs are afraid of their people knowning or telling the truth, all words published on China's social media/platform are reviewed strictly, anything CCP doesn't like will be banned, and on the other side, they claim that there is something called FREE-SPEECH is existed in China, so F*CK them",2022-10-19T05:25:44Z,48377190
3557,microsoft/vscode,1410698334,1283457064,The necessary process of localization in China,2022-10-19T05:38:03Z,10903843
3558,microsoft/vscode,1410698334,1283458623,哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈,2022-10-19T05:40:24Z,93422095
3559,microsoft/vscode,1410698334,1283467004,典,2022-10-19T05:52:40Z,4546175
3560,microsoft/vscode,1410698334,1283477475,"> ![image](https://user-images.githubusercontent.com/44344308/196600820-ca8d3cbc-3c07-4bb1-b49d-dbb642854a02.png) VS Code ""Officially licensed"": https://www.google.com lol

入典！！！
https://meme-libs.github.io/github/2",2022-10-19T06:06:26Z,51358815
3561,microsoft/vscode,1410698334,1283514759,It's hard for me to tell if you are joking...,2022-10-19T06:48:03Z,3437889
3562,microsoft/vscode,1410698334,1283516115,"> Fun fact: MIT License allows you to ""sell copies of the Software"" with or without modification.

Thanks for the TIP, I am going to sell VS Code at $114514/copy.",2022-10-19T06:49:34Z,3437889
3563,microsoft/vscode,1410698334,1283538184,mark LOL nice joke during work,2022-10-19T07:12:12Z,33801210
3564,microsoft/vscode,1410698334,1283540701,"> Fun fact: MIT License allows you to ""sell copies of the Software"" with or without modification.

but they‘re declaring ""officially licensed"", it does violate the MIT License.",2022-10-19T07:14:43Z,22633621
3565,microsoft/vscode,1410698334,1283552155,《 限 时 活 动 价 》《 39.7 》,2022-10-19T07:25:37Z,51546943
3566,microsoft/vscode,1410698334,1283566297,会上github发issue不会google一下官网是吧,2022-10-19T07:38:56Z,96194215
3567,microsoft/vscode,1410698334,1283567782,extremly hilarious🤣🤣🤣🤣,2022-10-19T07:40:12Z,46923757
3568,microsoft/vscode,1410698334,1283575731,难崩，程序员版steam管家是吧,2022-10-19T07:47:34Z,56433816
3569,microsoft/vscode,1410698334,1283576788,"（vsc微软管家 

",2022-10-19T07:48:30Z,45794248
3570,microsoft/vscode,1410698334,1283577261,笑死233,2022-10-19T07:48:56Z,26342994
3571,microsoft/vscode,1410698334,1283580698,"In China, it is normal
![GZB}9_~W30KO1P_QO0IIBV0](https://user-images.githubusercontent.com/104550011/196629946-8e48742c-5527-4d9c-bd9c-117f52dc095a.jpg)
",2022-10-19T07:52:05Z,104550011
3572,microsoft/vscode,1410698334,1283587988,"![image](https://user-images.githubusercontent.com/24404599/196631293-46c94e05-8281-409d-9cd3-cb79e72fec85.png)
Authorize to write to Google?",2022-10-19T07:58:14Z,24404599
3573,microsoft/vscode,1410698334,1283591860,LOL😂,2022-10-19T08:01:27Z,28563108
3574,microsoft/vscode,1410698334,1283592103,樂,2022-10-19T08:01:38Z,47449269
3575,microsoft/vscode,1410698334,1283595723,"If these vendors misappropriate the `Visual Studio code` trademark, I hope Microsoft sues them for copyright infringement.",2022-10-19T08:04:37Z,43780355
3576,microsoft/vscode,1410698334,1283620156,"If you do not wish to use the VSCode under M$, you may turn to VS Codium.",2022-10-19T08:24:01Z,15975785
3577,microsoft/vscode,1410698334,1283621170,I didn't find this ad in the search results of google or baidu. What happened to bing 🤣,2022-10-19T08:24:54Z,16476956
3578,microsoft/vscode,1410698334,1283624311,Developers who can't even find the official website should change careers ASAP,2022-10-19T08:27:33Z,45778220
3579,microsoft/vscode,1410698334,1283626515,LOL,2022-10-19T08:29:20Z,32829958
3580,microsoft/vscode,1410698334,1283630077,LOL,2022-10-19T08:32:18Z,71624062
3581,microsoft/vscode,1410698334,1283632363,crazy,2022-10-19T08:34:08Z,43086401
3582,microsoft/vscode,1410698334,1283632691,难绷，vscode管家是吧,2022-10-19T08:34:22Z,72327313
3583,microsoft/vscode,1410698334,1283637722,F**k Baidu,2022-10-19T08:37:12Z,38392315
3584,microsoft/vscode,1410698334,1283638794,VSCode need instant actions to avoid being authorized to Google. 😂,2022-10-19T08:38:08Z,107681877
3585,microsoft/vscode,1410698334,1283640223,die laughing,2022-10-19T08:39:19Z,30518686
3586,microsoft/vscode,1410698334,1283646222,彳亍，牛的,2022-10-19T08:44:05Z,17510330
3587,microsoft/vscode,1410698334,1283652677,u can alos see Steam downcenter、 manager....,2022-10-19T08:49:01Z,14959751
3588,microsoft/vscode,1410698334,1283653758,"I tried to translate the Chinese characters in these three pictures into English to make it easier for more readers to understand what happened.

![1](https://user-images.githubusercontent.com/20700283/196642847-1805b401-c91d-45b8-bbca-a3993bbc81f5.jpg)
![2](https://user-images.githubusercontent.com/20700283/196642873-ab54bae8-302e-49f0-97b4-1bbf639a179f.jpg)
![3](https://user-images.githubusercontent.com/20700283/196642892-3ffa3992-15aa-4888-8c93-2526a6eb2116.jpg)

-------
Appendix:
I don't know who was the math teacher of the author of this ""installer"", but I know 69.70 - 20 != 39.70.
lol",2022-10-19T08:49:46Z,20700283
3589,microsoft/vscode,1410698334,1283673205,难绷😂😂😂,2022-10-19T09:04:59Z,45305838
3590,microsoft/vscode,1410698334,1283685485,"I always thought it was a useless extension until I saw this issue...
It is recommended to pre-install this extension.

https://marketplace.visualstudio.com/items?itemName=lewistehminerz.unregistered-vscode

![image](https://user-images.githubusercontent.com/6159588/196648846-9bd63a4d-a75b-4dfb-abfc-499d0c76874f.png)
",2022-10-19T09:13:24Z,6159588
3591,microsoft/vscode,1410698334,1283686412,a lot of fun🤣,2022-10-19T09:14:11Z,37171891
3592,microsoft/vscode,1410698334,1283687010,XDDDDDDDDDDDDDD,2022-10-19T09:14:41Z,41638541
3593,microsoft/vscode,1410698334,1283692787,baidu makes great firewall great again.,2022-10-19T09:19:25Z,30151093
3594,microsoft/vscode,1410698334,1283697532,"真·程序员版steam
real steam for programmer",2022-10-19T09:23:09Z,7736239
3595,microsoft/vscode,1410698334,1283697944,"lol, why does this VSCode search and install on Baidu? Well, I admit that for those who are new to programming, they really don't know where to download it, but shouldn't the official website of VSCode be an English interface? Novice should be able to distinguish this.",2022-10-19T09:23:29Z,100057379
3596,microsoft/vscode,1410698334,1283699275,"More funny screenshots:

![image](https://user-images.githubusercontent.com/50246090/196645235-7f858aa9-5bda-4591-80bd-f23b93e6a9da.png)

![image](https://user-images.githubusercontent.com/50246090/196647834-e9e91993-a1cc-425b-985d-39c8cdaa95d7.png)

The original price is ￥69.7, but you will get a ""￥20"" (￥30 in fact) coupon when you close the window. Also, if you turn off the internet, it goes to ￥29.95. My guess is they did an online price increase because of the brisk sales. 🔥 

![image](https://user-images.githubusercontent.com/50246090/196646493-fdce2d63-c193-4161-8f18-c7ac9f5dbf4c.png)

The price for VS 2010 and 2012 is ￥49, 2013 is ￥59, 2017 is ￥69, 2019 is ￥79, and 2022 is ￥89. No discounts. 😭 

![image](https://user-images.githubusercontent.com/50246090/196646542-6bac2d4d-1737-4c5d-910f-7689f0dceb01.png)

Yeah, you can still buy VB 6.0 in 2022.
",2022-10-19T09:24:31Z,50246090
3597,microsoft/vscode,1410698334,1283701342,"@weimeng23 To add, China has a variety of software disguised as Steam, which is downloaded as a group number platform.",2022-10-19T09:26:14Z,100057379
3598,microsoft/vscode,1410698334,1283701664,"![image](https://user-images.githubusercontent.com/19525769/196652433-eb7b37b0-8cbd-4ecc-b9c1-34bda3c56f8f.png)
我英文不好，如果我没有理解错的话，这个issue的意思是：张朦朦你妈死了",2022-10-19T09:26:31Z,19525769
3599,microsoft/vscode,1410698334,1283703107,"Baidu seems to have already removed this ad in vscode search results. But the download link to microsoft website is ranked the 4th, even after tutorials on using vscode.
Bing removed the ad from ""vscode"" search results, but the ad is still there when you search ""visual code"".

IMHO, lawsuit against such piracy companies will barely work. The worst thing that will happen to them is going bankrupt.",2022-10-19T09:27:41Z,7239200
3600,microsoft/vscode,1410698334,1283705990,"笑尿😂
![苏喂苏喂苏喂](https://user-images.githubusercontent.com/21122615/196653570-72555758-610f-45ca-a3c0-ba270192c73a.gif)
",2022-10-19T09:30:03Z,21122615
3601,microsoft/vscode,1410698334,1283713146,"既然大家都是中国人，那我就直接打中文了
先说一下他们公司是有限公司，承担的责任是有限的。
注册资本是10W，意思就是如果出了事公司法人最多赔偿10W了事。
最后，这家公司是真的无耻啊",2022-10-19T09:35:58Z,24404599
3602,microsoft/vscode,1410698334,1283727955,牛的,2022-10-19T09:47:54Z,21031851
3603,microsoft/vscode,1410698334,1283728942,incomprehensible!😂,2022-10-19T09:48:40Z,42824008
3604,microsoft/vscode,1410698334,1283734817,"<img width=""858"" alt=""image"" src=""https://user-images.githubusercontent.com/10466275/196658413-3080fed8-dab9-4bc6-993d-bcd844065143.png"">
<img width=""970"" alt=""image"" src=""https://user-images.githubusercontent.com/10466275/196658545-1291c19f-72fc-4d91-97cb-6ad49afe5bd4.png"">
看起来已经下线了，百度一生黑",2022-10-19T09:53:23Z,10466275
3605,microsoft/vscode,1410698334,1283740168,FREEDOM !!! 你法我笑,2022-10-19T09:57:42Z,23721611
3606,microsoft/vscode,1410698334,1283751783,"> 先说一下他们公司是有限公司，承担的责任是有限的。

Limited liability company (LLC) don't imply the liability is always limited.",2022-10-19T10:07:05Z,3437889
3607,microsoft/vscode,1410698334,1283762013,非蠢即坏!!,2022-10-19T10:13:15Z,100825215
3608,microsoft/vscode,1410698334,1283763047,"很多开源软件许可证是允许你拿出去打包售卖的  https://www.gnu.org/licenses/gpl-faq.zh-cn.html#GPLCommercially 无论你卖什么价钱，唯一的要求就是你的顾客要求你出示源码，你得无条件提供；顾客拿你的东西二次售卖，你也别拦着。

开源软件不等于禁止商业化，只要许可证允许，你完全可以正大光明拿开源的东西打包换皮挣钱，比如 Redhat 就是这样一家大型企业。

这里问题只是侵犯了vscode的商标。


",2022-10-19T10:13:41Z,987524
3609,microsoft/vscode,1410698334,1283781361,"![image](https://user-images.githubusercontent.com/17821872/196665995-191f3f7d-e8fc-47ec-b6b3-3e0a0d4e82a2.png)
",2022-10-19T10:28:07Z,17821872
3610,microsoft/vscode,1410698334,1283803425,fake virus downloader with Chinese characteristic XD,2022-10-19T10:45:58Z,62735122
3611,microsoft/vscode,1410698334,1283820843,lol,2022-10-19T10:58:25Z,8097849
3612,microsoft/vscode,1410698334,1283827507,"特地前来查看
当场蚌埠住",2022-10-19T11:01:42Z,49244656
3613,microsoft/vscode,1410698334,1283832718,It has been a meme for a long time XD.Finally someone shared it here.good job.,2022-10-19T11:06:03Z,34237511
3614,microsoft/vscode,1410698334,1283834875,牛批，还是百度🐂,2022-10-19T11:07:56Z,42693892
3615,microsoft/vscode,1410698334,1283837970,LMAO,2022-10-19T11:10:41Z,43381113
3616,microsoft/vscode,1410698334,1283840781,"> 很多开源软件许可证是允许你拿出去打包售卖的 https://www.gnu.org/licenses/gpl-faq.zh-cn.html#GPLCommercially 无论你卖什么价钱，唯一的要求就是你的顾客要求你出示源码，你得无条件提供；顾客拿你的东西二次售卖，你也别拦着。
> 
> 开源软件不等于禁止商业化，只要许可证允许，你完全可以正大光明拿开源的东西打包换皮挣钱，比如 Redhat 就是这样一家大型企业。
> 
> 这里问题只是侵犯了vscode的商标。

官网下载的vscode不是完全开源版本的vscode，使用的license也不一样：https://code.visualstudio.com/License/。",2022-10-19T11:13:05Z,10379994
3617,microsoft/vscode,1410698334,1283851334,"百度无耻，Bing也不干净了。
Baidu is shameless, and Bing is no longer clean.",2022-10-19T11:22:20Z,2913058
3618,microsoft/vscode,1410698334,1283854056,lol,2022-10-19T11:24:21Z,37892712
3619,microsoft/vscode,1410698334,1283865799,"![image](https://user-images.githubusercontent.com/51020316/196678924-cdbd6a49-ee46-4804-9821-f31a39278bc7.png) 
- ""the graphics and software come from the Internet"" 
- ""respect the legitimate rights and interests of others"" 
- ""**if**(as if they didn't know) the content violates your legitimate rights"" 
- ""we will delete it as soon as possible""",2022-10-19T11:34:20Z,51020316
3620,microsoft/vscode,1410698334,1283872134,69.7-20=39.7,2022-10-19T11:40:08Z,18043671
3621,microsoft/vscode,1410698334,1283874072,Essentially they only infringe on trademarks and the quality of content on the Chinese Internet is very low.,2022-10-19T11:42:07Z,64189793
3622,microsoft/vscode,1410698334,1283876458,哈哈哈哈哈哈哈哈哈哈哈哈哈，love from china,2022-10-19T11:44:30Z,89195476
3623,microsoft/vscode,1410698334,1283876828,非常好作品，英雄联盟，爱来自瓷器,2022-10-19T11:44:51Z,64454529
3624,microsoft/vscode,1410698334,1283895397,"Chrome users in China: Hold my beer.

![1666180574023](https://user-images.githubusercontent.com/14220017/196684129-ec3634c4-b6f4-4e69-beca-583c2d44ee88.png)
",2022-10-19T11:57:19Z,14220017
3625,microsoft/vscode,1410698334,1283920005,"![niubi](https://user-images.githubusercontent.com/31164470/196688366-8fd99f82-c9ff-4c49-bdf8-4e87bc40f85b.png)
",2022-10-19T12:17:23Z,31164470
3626,microsoft/vscode,1410698334,1283949573,A good suggestion is not to use Baidu search engine,2022-10-19T12:37:50Z,37787014
3627,microsoft/vscode,1410698334,1283966857,我们百度是这样的，@baidu,2022-10-19T12:51:47Z,54094119
3628,microsoft/vscode,1410698334,1283969857,I’m thinking of popping up an alert that tells user when open for the first time. Hide alert’s code deep and dark!,2022-10-19T12:54:07Z,25259084
3629,microsoft/vscode,1410698334,1283996193,"It's very interesting that , if Microsoft prosecutes this company , it will say it just sells software installing service. But anyhow , I wish Microsoft can stop this swindle.",2022-10-19T13:14:37Z,50799666
3630,microsoft/vscode,1410698334,1283999802,中文：微软搞快点，直接起诉得了,2022-10-19T13:17:27Z,50799666
3631,microsoft/vscode,1410698334,1284020875,“The Fun They Had”,2022-10-19T13:29:15Z,80994270
3632,microsoft/vscode,1410698334,1284022633,The `Microsoft` editor authorized by `Google.com` to pay through `Alipay` searched from `Baidu.com`,2022-10-19T13:30:38Z,26023242
3633,microsoft/vscode,1410698334,1284035997,蚌埠住了,2022-10-19T13:40:26Z,56689775
3634,microsoft/vscode,1410698334,1284059981,"LOL. This is very common in China to pay for a FREE software with high price because some user has no ability to search on Google or international version of Bing or somewhere else. They just use Baidu with A LOT OF ADS, or some download station with Trojans or viruses. So some company will give money to Baidu to show their fake official website at the top of the searching results. You can see that the first page of searching result for ""Visual Studio Code"" is full of ads and there some more in the recommendation page. This is horrible. BTW, some briefcase company in China is shameless to use copyrighted trademarks, copyrighted software and software under license for profit. But you cannot give them a copyright strike because the consciousness of copyright is weak in China. Very sad to see this.",2022-10-19T13:56:53Z,59819050
3635,microsoft/vscode,1410698334,1284060057,"> The `Microsoft` editor authorized by `Google.com` to pay through `Alipay` searched from `Baidu.com`

hard to ""蚌""",2022-10-19T13:56:56Z,9190813
3636,microsoft/vscode,1410698334,1284088892,"> ![image](https://user-images.githubusercontent.com/17821872/196665995-191f3f7d-e8fc-47ec-b6b3-3e0a0d4e82a2.png)

haha",2022-10-19T14:15:03Z,37795442
3637,microsoft/vscode,1410698334,1284092436,"> 百度无耻，Bing也不干净了。 Baidu is shameless, and Bing is no longer clean.

只是奇怪的SEO罢了(",2022-10-19T14:17:31Z,86996742
3638,microsoft/vscode,1410698334,1284099817,They even cannot do a correct calculation because 69.7-20=39.7 (The result is 49.7),2022-10-19T14:22:34Z,59819050
3639,microsoft/vscode,1410698334,1284100489,"> ![niubi](https://user-images.githubusercontent.com/31164470/196688366-8fd99f82-c9ff-4c49-bdf8-4e87bc40f85b.png

Is this picture anything related to this issue?",2022-10-19T14:23:01Z,8068443
3640,microsoft/vscode,1410698334,1284346541,我更担心在国产Linux推广开以后，会不会有类似的无良商家把黑手伸向Linux平台的vscode或其他软件。,2022-10-19T17:27:28Z,17796830
3641,microsoft/vscode,1410698334,1284538957,"Apparently, the math is wrong. I am outrageous. :)",2022-10-19T20:31:31Z,16820635
3642,microsoft/vscode,1410698334,1284642936,能用上百度，你们就偷着乐吧,2022-10-19T22:36:27Z,19610256
3643,microsoft/vscode,1410698334,1284644526,樂,2022-10-19T22:39:04Z,79032826
3644,microsoft/vscode,1410698334,1284721876,![不行，这太后现代了](https://user-images.githubusercontent.com/30336566/196826890-99382c95-0f52-4e26-9310-78749d8b496c.png),2022-10-20T00:05:59Z,30336566
3645,microsoft/vscode,1410698334,1284748570,真是见识了,2022-10-20T00:52:17Z,26101369
3646,microsoft/vscode,1410698334,1284762387,绷,2022-10-20T01:14:52Z,10137
3647,microsoft/vscode,1410698334,1284789672,丢人丢到国外了属于是,2022-10-20T01:39:47Z,45785585
3648,microsoft/vscode,1410698334,1284790092,interesting ,2022-10-20T01:40:21Z,48322373
3649,microsoft/vscode,1410698334,1284790438,interesting,2022-10-20T01:40:54Z,48373109
3650,microsoft/vscode,1410698334,1284798900,Google Licensing? Fine 😅,2022-10-20T01:53:28Z,28031505
3651,microsoft/vscode,1410698334,1284807598,V2观光团,2022-10-20T02:06:03Z,29171710
3652,microsoft/vscode,1410698334,1284810898,@baidu,2022-10-20T02:11:13Z,20656708
3653,microsoft/vscode,1410698334,1284813133,感恩,2022-10-20T02:14:18Z,20613852
3654,microsoft/vscode,1410698334,1284817316,火钳流明,2022-10-20T02:19:35Z,7997593
3655,microsoft/vscode,1410698334,1284818317,V2观光团,2022-10-20T02:20:58Z,40257198
3656,microsoft/vscode,1410698334,1284818624,funny,2022-10-20T02:21:24Z,25605114
3657,microsoft/vscode,1410698334,1284819091,69.7 - 20 = 39.7   ???,2022-10-20T02:22:04Z,13534439
3658,microsoft/vscode,1410698334,1284820820,"> How can you find this... Is just typing code.visualstudio.com too difficult for some people?

Maybe student",2022-10-20T02:24:41Z,46060083
3659,microsoft/vscode,1410698334,1284821122,666,2022-10-20T02:25:06Z,40533565
3660,microsoft/vscode,1410698334,1284821136,"> Do you know why there is a GFW in China? cause CCP sucks, these SOBs are afraid of their people knowning or telling the truth, all words published on China's social media/platform are reviewed strictly, anything CCP doesn't like will be banned, and on the other side, they claim that there is something called FREE-SPEECH is existed in China, so F*CK them

omg,Do you have any sense in saying that？LOL.",2022-10-20T02:25:06Z,86960818
3661,microsoft/vscode,1410698334,1284821303,"> 我更担心在国产Linux推广开以后，会不会有类似的无良商家把黑手伸向Linux平台的vscode或其他软件。

已经有 Ubuntu 和 CentOS 了，收费80多，B站有学生上当",2022-10-20T02:25:19Z,46060083
3662,microsoft/vscode,1410698334,1284821932,"> > ![niubi]([user-images.githubusercontent.com/31164470/196688366-8fd99f82-c9ff-4c49-bdf8-4e87bc40f85b.png](https://user-images.githubusercontent.com/31164470/196688366-8fd99f82-c9ff-4c49-bdf8-4e87bc40f85b.png)
> 
> Is this picture anything related to this issue?

Yes It means unbelieve and awesome",2022-10-20T02:26:10Z,46060083
3663,microsoft/vscode,1410698334,1284825892,"> It's very interesting that , if Microsoft prosecutes this company , it will say it just sells software installing service. But anyhow , I wish Microsoft can stop this swindle.

There are too many such companies in China, you know? Adobe Flash Player, which is represented by a Chinese company, is still updated and added ads, does not allow you to use the international version (it will indicate that the software is damaged or not in the current area), and this ad is full of deceptive drugs and stock speculation content, some vulgar content, this adware program also comes with a virus (detected by Kaspersky), and the funniest thing is that the ads will crash when you watch the ads",2022-10-20T02:31:19Z,46060083
3664,microsoft/vscode,1410698334,1284825949,"If a beginner can not recognize that shit, it would be the first lesson they need to learn.  ^_^",2022-10-20T02:31:24Z,7044628
3665,microsoft/vscode,1410698334,1284827764,"> > 非常有趣的是，但如果微软这家公司，它会说它销售软件安装。只是，我希望微软能够阻止这种骗局。
> 
> 中国这样的公司有，你知道吗？药炒的内容，一些低俗的内容，这个广告软件程序还自带病毒（最搞笑的是看广告会崩溃）

The Internet environment in China is really bad now!",2022-10-20T02:33:46Z,86960818
3666,microsoft/vscode,1410698334,1284828320,lol,2022-10-20T02:34:23Z,38283893
3667,microsoft/vscode,1410698334,1284828889,"> 会上github发issue不会google一下官网是吧

不是的，他只是在这个地方发布这个话题引起关注，都上 GitHub 了，这个还是知道的",2022-10-20T02:35:11Z,46060083
3668,microsoft/vscode,1410698334,1284829738,雷碧，康帅傅,2022-10-20T02:36:25Z,22079509
3669,microsoft/vscode,1410698334,1284830043,未来可期,2022-10-20T02:36:53Z,59854674
3670,microsoft/vscode,1410698334,1284830184,"> Developers who can't even find the official website should change careers ASAP

There are too many such fake official websites in Chinese search engines, and he just discusses this topic too much, not that he can't find the official website",2022-10-20T02:37:08Z,46060083
3671,microsoft/vscode,1410698334,1284831941,"> 未来可期

依托Microsoft，快上市了😂",2022-10-20T02:39:18Z,22079509
3672,microsoft/vscode,1410698334,1284832408,amazing.,2022-10-20T02:39:51Z,14974790
3673,microsoft/vscode,1410698334,1284836804,unbelievable,2022-10-20T02:45:56Z,9898240
3674,microsoft/vscode,1410698334,1284837079,"> unbelievable

都在中国了，很普遍了吧",2022-10-20T02:46:20Z,46060083
3675,microsoft/vscode,1410698334,1284837153,6,2022-10-20T02:46:25Z,32817309
3676,microsoft/vscode,1410698334,1284844960,"> 

你是对的
",2022-10-20T02:58:07Z,67678038
3677,microsoft/vscode,1410698334,1284846918,可以啊，有前途。美滋滋的,2022-10-20T03:00:58Z,29566591
3678,microsoft/vscode,1410698334,1284847060,welcome to China xD,2022-10-20T03:01:09Z,49602236
3679,microsoft/vscode,1410698334,1284848172,太丢撵了,2022-10-20T03:02:46Z,35896236
3680,microsoft/vscode,1410698334,1284854469,"![image](https://user-images.githubusercontent.com/6455728/196847294-9b5f8d32-3201-4692-9b3f-d1c767958b8b.png)
Time to manually revoke certificates from TrustAsia Technologies

The downloaded exe file is signed by:
CN = DigiCert Trusted G4 Code Signing RSA4096 SHA384 2021 CA1",2022-10-20T03:11:36Z,6455728
3681,microsoft/vscode,1410698334,1284854587,"
![china_now](https://user-images.githubusercontent.com/24828354/196849315-5b48323d-da64-49b8-81e8-f534ff0f20ac.png)
这就是__",2022-10-20T03:11:44Z,24828354
3682,microsoft/vscode,1410698334,1284855549,一眼顶真：鉴定为假,2022-10-20T03:13:10Z,53813118
3683,microsoft/vscode,1410698334,1284856981,"[![29hEdS.gif](https://z3.ax1x.com/2021/05/26/29hEdS.gif)](https://imgse.com/i/29hEdS)

or this:
[![cashback](https://i1.hoopchina.com.cn/hupuapp/bbs/220/58393220/thread_58393220_20201024082704_s_1337599_o_w_498_h_267_60931.gif?x-oss-process=image/resize,w_800/format,webp)]",2022-10-20T03:15:11Z,4517807
3684,microsoft/vscode,1410698334,1284857257,信件已经收到啦~,2022-10-20T03:15:36Z,53813118
3685,microsoft/vscode,1410698334,1284865536,"Chinese programmer's first lesson: figuring out a way to access google (blocked in China) or bing international (blocked, and replaced by bing Chinese edition).

Chinese programmer's second lesson: learning English.

LOL

----

Anyway, I think google and Microsoft should take down those piracy and potentially harmful websites asap. Or, at least ask Nintendo and Disney about how to take down everything they don't like from the internet.",2022-10-20T03:25:23Z,8587572
3686,microsoft/vscode,1410698334,1284875441,We can't really stop people without information source from doing those. People will always buy free software (like Chrome browser or VS Community) on Taobao because they don't know they are free.,2022-10-20T03:39:47Z,5752560
3687,microsoft/vscode,1410698334,1284878498,这个有点厉害,2022-10-20T03:43:25Z,40444637
3688,microsoft/vscode,1410698334,1284885439,"> Search from Baidu:
> 
> ![image](https://user-images.githubusercontent.com/50246090/196424828-5ead4681-c783-442e-bbea-0b8e4c15fcc2.png)
> 
> The first one is https://qnw.shaid.top/vscode/index.html:
> 
> ![image](https://user-images.githubusercontent.com/50246090/196424988-4523e2d7-4860-4631-876f-bfd4a2548359.png)
> 
> This website belongs to [Wuhan Hongge Yibai E-commerce Co. (武汉宏格亿佰电子商务有限公司)](http://www.shaid.top/).
> 
> If you do some searches on this domain, you can find their other ""products"":
> 
> * [Win10 Activation Tool (Win10 激活工具)](https://qnw.shaid.top/win10/index.html)
> * Telegram Chinese version (Telegram 中文版) (removed)
>   ![image](https://user-images.githubusercontent.com/50246090/196426820-f9fa3030-eb5e-430c-bdfc-026af7100576.png)
> * visualcpp (removed)
>   ![image](https://user-images.githubusercontent.com/50246090/196432416-34943cc9-de17-4e6b-a316-ca5a6ff3f5e7.png)
> 
> They even claim to be partners with China Mobile, China Unicom and China Telecom: [![image](https://user-images.githubusercontent.com/50246090/196426325-f39cc8ab-8b66-40b7-8a1e-edc32ad7c08b.png)](http://www.shaid.top/)
> 
> Thanks to the Great Firewall, we have such a ""great"" Internet in China.
> 
> Update:
> 
> Some people have found other similar companies:
> 
> * [Shangqiu Xuankangtai Network Technology Co. (商丘轩康泰网络科技有限公司)](http://www.sqiua.cn/index.html)
>   
>   * [Visual studio](http://office.xuank.top/install.php?m=visual)
>     ![image](https://user-images.githubusercontent.com/50246090/196571476-47c2e0e6-5465-4666-8889-ad34a1c4689a.png)
>   * [Software Superstore (软件商超)](https://xkt.sqiua.cn/)
>     ![image](https://user-images.githubusercontent.com/50246090/196572556-a30b1388-e5f5-43f8-a94a-419e5501c2f9.png)
>   * [System Home - Windows premium system download site (系统之家-Windows精品系统下载站)](http://windows.sqiua.cn/index.html)
>   * [visualbasic](http://office.xuank.top/install.php?m=visualbasic)
>   * [directx Repair Master (directx修复大师)](http://office.xuank.top/install.php?m=directx)
>   * [Chrome](http://chrome.sqiua.cn/)
>     ![image](https://user-images.githubusercontent.com/50246090/196573087-e4bfb9f0-3060-4e72-a6bf-05a35238a151.png)
>   * [Yixin Cat House (忆心猫舍)](http://yixinmaoshe.sqiua.cn/)
>     ![image](https://user-images.githubusercontent.com/50246090/196572912-95f01175-ad54-40d7-bfe7-2b10aaa767b8.png)
>   * [AdobeCAD](https://autocad.sqiua.cn/)
>   * [Map marking service center (地图标注服务中心)](http://mr.sqiua.cn/)
> * [Yunnan Norforkang Network Technology Co. (云南诺福康网络科技有限公司)](http://www.cvbty.cn/)
>   
>   * [Xunjian Mind Map (寻简思维导图)](http://sw.xuank.top/)

宣称三大运营山是合作伙伴，怕是开通了三大运营商的宽带套餐吧哈哈哈哈哈哈",2022-10-20T03:53:26Z,13610847
3689,microsoft/vscode,1410698334,1284886712,"> > 我更担心在国产Linux推广开以后，会不会有类似的无良商家把黑手伸向Linux平台的vscode或其他软件。
> 
> 已经有 Ubuntu 和 CentOS 了，收费80多，B站有学生上当

要是卖实体安装介质收收本钱也不是不行(
要是提供怎么装的服务也不是不行(",2022-10-20T03:55:21Z,86996742
3690,microsoft/vscode,1410698334,1284893866,"In taobao, some merchants provide installation package download service too.",2022-10-20T04:06:20Z,37795442
3691,microsoft/vscode,1410698334,1284897582,真 智商税,2022-10-20T04:12:39Z,29097382
3692,microsoft/vscode,1410698334,1284906273,"Wait, you guys are installing VSCode for FREE?

![image](https://user-images.githubusercontent.com/6663691/196856413-fdde90d1-7d8e-41d9-bda4-123740017d27.png)
",2022-10-20T04:26:59Z,6663691
3693,microsoft/vscode,1410698334,1284931062,"![image](https://user-images.githubusercontent.com/54622682/196860848-1c74ace1-1270-4b33-a3c5-fc0ba03201db.png)
",2022-10-20T05:05:10Z,54622682
3694,microsoft/vscode,1410698334,1284935468,![QQ图片20221020130910](https://user-images.githubusercontent.com/24607145/196861392-e5eca5c2-923a-4dc3-a649-4c5253aa2e25.jpg),2022-10-20T05:09:43Z,24607145
3695,microsoft/vscode,1410698334,1284949631,"Wait, you guys are installing VSCode for FREE?",2022-10-20T05:21:55Z,24249584
3696,microsoft/vscode,1410698334,1284951416,建议入典,2022-10-20T05:23:27Z,86651333
3697,microsoft/vscode,1410698334,1284959633,"> Wait, you guys are installing VSCode for FREE?

Now that you have the ability to communicate on GitHub, I think you're kidding",2022-10-20T05:31:40Z,46060083
3698,microsoft/vscode,1410698334,1284960014,"> In taobao, some merchants provide installation package download service too.

这种是侵权，不一样的性质",2022-10-20T05:32:05Z,46060083
3699,microsoft/vscode,1410698334,1284967421,"This is absurd yet quite common in China, which is more embarrassing than funny. Sh*tposting the comment section is making it even worse. Someone has to put a stop to this.",2022-10-20T05:40:58Z,6663691
3700,microsoft/vscode,1410698334,1284975065,"> That's the most fabulous CHINA, most successful business model. Lots of similar business models on the Chineses Internet. It's a big pity for citizens(oh, we are just _""韭菜""_, never be a true citizens ever ) here. FUCK CCP!

你小子号没了",2022-10-20T05:50:27Z,24249584
3701,microsoft/vscode,1410698334,1284976295,"> That's the most fabulous CHINA, most successful business model. Lots of similar business models on the Chineses Internet. It's a big pity for citizens(oh, we are just _""韭菜""_, never be a true citizens ever ) here. FUCK CCP!

危",2022-10-20T05:51:54Z,55233292
3702,microsoft/vscode,1410698334,1284978885,"> That's the most fabulous CHINA, most successful business model. Lots of similar business models on the Chineses Internet. It's a big pity for citizens(oh, we are just _""韭菜""_, never be a true citizens ever ) here. FUCK CCP!

危",2022-10-20T05:55:12Z,26863131
3703,microsoft/vscode,1410698334,1284982361,So 69.70 - 20 = 39.70 ????,2022-10-20T05:59:17Z,69784329
3704,microsoft/vscode,1410698334,1284983427,"It's not that surprise for me these things happening in China.
As we all know there is a firewall call ""GFW"" protecting user using internet so...
The truth is that the man who paid a large amount of money can do anything he want ",2022-10-20T06:00:35Z,53432315
3705,microsoft/vscode,1410698334,1284988080,I think it is a trick,2022-10-20T06:06:20Z,51405033
3706,microsoft/vscode,1410698334,1284988692,"> That's the most fabulous CHINA, most successful business model. Lots of similar business models on the Chineses Internet. It's a big pity for citizens(oh, we are just _""韭菜""_, never be a true citizen ever ) here. FUCK CCP.



你是狗? 滚你妈的hi , 
这些东西是某些无良公司的作为,请你不要上升到这个层面. 
小心生孩子没屁眼. 出门被大货车压死",2022-10-20T06:07:00Z,54622682
3707,microsoft/vscode,1410698334,1284988742,"Even if the officials could notice, there is no way to make such people disappear. Such companies are all shell companies, a company may have just two employees, a packager and a legal officer. That's what's so pathetic.

![23EBEDCA3763335B3422895935B7BDB0](https://user-images.githubusercontent.com/46388610/196868970-476f8d72-891d-40e5-82a7-16372fd807b8.jpg)
",2022-10-20T06:07:04Z,46388610
3708,microsoft/vscode,1410698334,1284996180,百度果然向钱看齐,2022-10-20T06:15:32Z,36690576
3709,microsoft/vscode,1410698334,1285002573,不要核酸要吃饭，不要封控要自由，不要谎言要尊严，不要文革要改革，不要领袖要选票，不做奴才做公民,2022-10-20T06:22:17Z,44705105
3710,microsoft/vscode,1410698334,1285008922,"> 发起要吃饭，不要封控要自由，不要礼让，不要文革要改革，不要带头选票，不要做奴才才做

好你小子，逮住你了",2022-10-20T06:29:22Z,4987317
3711,microsoft/vscode,1410698334,1285009191,智商税，挺好的,2022-10-20T06:29:38Z,4987317
3712,microsoft/vscode,1410698334,1285011955,楼上有个小粉红急了,2022-10-20T06:32:38Z,45278810
3713,microsoft/vscode,1410698334,1285014030,"国内网站很多都喜欢给人喂狗屎，毕竟只有你吃屎了它们才能吃上饭
ps：我不是做慈善的，我希望它们早日全部饿死
pps：不是说国外就很好的意思",2022-10-20T06:35:01Z,57350104
3714,microsoft/vscode,1410698334,1285034942,> 安利个 google extension: 百度去广告,2022-10-20T06:55:10Z,25813927
3715,microsoft/vscode,1410698334,1285040933,"> 69.7 - 20 = 39.7 ???

You know mathematics.",2022-10-20T07:01:19Z,80948381
3716,microsoft/vscode,1410698334,1285045144,"WHY USE Baidu? In China, [Bing](https://bing.com) works just as well.",2022-10-20T07:05:20Z,99539499
3717,microsoft/vscode,1410698334,1285046995,"We call somebody who paid 'IQ tax'（智商稅） in Chinese if he has downloaded and paid for it. :-D

Long years ago, I also saw some online shops(mainly on Taobao) sell discs with burned Ubuntu ISO images, I reported these behaviors to the official community and some software engineer forums but some people replied to me that these behaviors are OK and are not illegal.

I guess that because some Chinese noob programmers are not familiar with English, so they don't know how to find and download the official **Visual Studio Code** from the official website in English.",2022-10-20T07:07:11Z,11588555
3718,microsoft/vscode,1410698334,1285050832,"> ![image](https://user-images.githubusercontent.com/44344308/196600820-ca8d3cbc-3c07-4bb1-b49d-dbb642854a02.png) VS Code ""Officially licensed"": https://www.google.com lol

#NTR",2022-10-20T07:10:57Z,80948381
3719,microsoft/vscode,1410698334,1285057218,"> 非常好作品，英雄联盟，爱来自瓷器

君中国语当上手",2022-10-20T07:17:18Z,80948381
3720,microsoft/vscode,294122809,294122809,"There are a few major problems when searching by an extension's name. First of all, if I remember correctly, the marketplace had this thing on every extension's page:

![](https://cloud.githubusercontent.com/assets/2685357/25901135/734e34fc-359e-11e7-8a0b-a4ee9171ef4c.png)

So I've added an analogous message to all of my extensions' readme files, and I think other extension authors have done this too.

I have an extension called [Todo+](https://marketplace.visualstudio.com/items?itemName=fabiospampinato.vscode-todo-plus) (full id: `fabiospampinato.vscode-todo-plus`). This is what happens when the command `ext install vscode-todo-plus` is executed:

![](https://user-images.githubusercontent.com/17926167/35692049-868ae976-077a-11e8-8ecc-debf7c71c558.gif)

- `vscode-todo-plus` in an exact match of the id minus my username, and the extension is not even in the top 10 results.
- Searching for `fabiospampinato.vscode-todo-plus`, which is an exact match of the full id, leads to the same results.
- Searching for `@id:fabiospampinato.vscode-todo-plus` works, **but only inside VSC**, it doesn't work on [marketplace.visualstudio.com](https://marketplace.visualstudio.com)
- Where are these `ext install` and `@id:` commands documented anyway?

Let's try to search for another extension of mine, [Open in Browsers](https://marketplace.visualstudio.com/items?itemName=fabiospampinato.vscode-open-in-browsers) (full id: `fabiospampinato.vscode-open-in-browsers`):

<img width=""1102"" alt=""screen shot 2018-02-03 at 16 51 16"" src=""https://user-images.githubusercontent.com/1812093/35768894-f918c7e0-0902-11e8-8876-8386a2c52078.png"">

- Not only `vscode-open-in-browsers` is an exact match of the extention id minus my username, but ""Open in Browsers"" is the extension's title, and ""vscode"" is practically a meaningless keyword, as is ""in"". What's the first result for this search? [vscode-ins-support](https://marketplace.visualstudio.com/items?itemName=wk-j.vscode-ins-support) 😕

I hope we do agree there's a problem here.

If I may suggest a few improvements: 

1. Always check if an extension's id (minus the username) is an exact match of the current query.
2. Implement [stop words](https://en.wikipedia.org/wiki/Stop_words).

_Please don't close this issue as ""non actionable""._",2018-02-03T16:12:05Z,1812093
3721,microsoft/vscode,294122809,362861010,"This has been a long standing issue -- it is the reason I had to rename my extension to *Git Lens* rather than its actual name *GitLens* (no space), because when searching for git it would never show up. And even with that rename -- I had to pack in other words into the title (SEO hacking style) to even get it to show up on some other keywords -- even if those keywords were in the package.json keywords list.",2018-02-03T22:36:47Z,641685
3722,microsoft/vscode,294122809,362868288,"So those keywords I've been adding are actually useless you say? Nice...

I'm not sure if it's worse that the search engine works as badly as it does or that it hasn't been fixed already.",2018-02-04T00:15:47Z,1812093
3723,microsoft/vscode,294122809,363009048,"Not useless, but just ranked quite a lot below what is in the name -- at least that used to be the case",2018-02-05T08:07:34Z,641685
3724,microsoft/vscode,294122809,363022419,cc @viradhamMS @pkuma-msft,2018-02-05T09:08:12Z,22350
3725,microsoft/vscode,294122809,363354353,"@fabiospampinato 

Disregarding the fact that the Marketplace doesn't return any results when searching for an extension's id, we have fixed the Marketplace instructions since a while. `ext install` was our way of guiding users to install extensions inside VS Code. Since we now have URL handlers for Mac and Windows, those instructions became almost irrelevant. We now only show them for Linux users:

![image](https://user-images.githubusercontent.com/22350/35850126-1510cbaa-0b24-11e8-88da-f21d798b53b3.png)

Running this in VS Code will correctly trigger the search for `@id.name`. The `name` format should've never been there. Only the`id.name` is unequivocal in order to find an extension in the Marketplace.

One idea I have for alleviating this pain is to improve the `ext install name` (without `id`) experience: what if we showed in quick open a list of extensions which match that name? This would simply mitigate the fact that there are instructions for `ext install name` out there. But in any case, I recommend switching your instructions to `ext install id.name`.

We've notified the Marketplace of the search issues.",2018-02-06T08:58:09Z,22350
3726,microsoft/vscode,294122809,363467905,"> we have fixed the Marketplace instructions since a while

I've only recently noticed the change (from `ext-name` to `owner.ext-name`) because somebody opened an issue about that in one of my repositories, has this change been mentioned in any changelog in the past?

> Since we now have URL handlers for Mac and Windows, those instructions became almost irrelevant

I would have replaced those `ext install (...)` instructions with URL handlers, but I've tried them once and after that, for a few days/weeks, a ""Do you want to install (...)"" message kept popping up every time I opened a new window. By the time that was fixed I guess I had forgotten about them.

> what if we showed in quick open a list of extensions which match that name? 

Why providing 2 different interfaces for discovering extensions?

Shouldn't searching ""just work""?",2018-02-06T15:58:05Z,1812093
3727,microsoft/vscode,294122809,363829662,"> I've only recently noticed the change (from ext-name to owner.ext-name) because somebody opened an issue about that in one of my repositories, has this change been mentioned in any changelog in the past?

I guess it wasn't, since it was a Markeplace change... Sorry about that.

> Shouldn't searching ""just work""?

It should. We've notified the Marketplace of the search issues.",2018-02-07T16:39:34Z,22350
3728,microsoft/vscode,294122809,392340703,"Just to reiterate on how bad the search engine works:

- I've just published an extension named `Open in node_modules`
- Searching for `open in node modules` (without the underscore) won't even show said extension within the first 60 results 🤷‍♂️",2018-05-27T15:41:03Z,1812093
3729,microsoft/vscode,294122809,392549459,"@fabiospampinato, regarding the issue of searching your new extension:

While searching an extension, we also take into account the community inputs like number of downloads, number of ratings and average rating along with the string matching. Among the string matching, exact string matches carry a higher weight than prefix match. 

Since `node_modules` is one word in your extension name, word `node` in search text gets prefix match here. However, there are lot of extensions on Marketplace which have exact word 'node' in their extension name. So they carry a higher weight for matching the word 'node'. Additionally, since your extension is new, download count and ratings of other extensions are way higher, pushing their total score to the top.

One way to improve your extension rank is to break word `node_modules` into separate words `node modules`. This will increase the string matching score and push your extension up in the result.",2018-05-28T15:02:16Z,13743203
3730,microsoft/vscode,294122809,392557158,"@gaurav42 Well, I would still consider whatever algorithm you guys are using broken because if I search for ""open in node modules"" I get these search results (some samples ~~randomly picked):

| Rank    | Ext. Name    | Ext. Description | My comments  |
|--------|-------------|-----------------|----------------|
| `#6`  | [Node Exec](https://marketplace.visualstudio.com/items?itemName=miramac.vscode-exec-node) | Execute the current file or your selected code with node.js. | 80k downloads, but no mention of ""open"" or ""module(s)"" anywhere.
| `#10` | [CSS Modules](https://marketplace.visualstudio.com/items?itemName=clinyong.vscode-css-modules) | Visual Studio Code extension for CSS Modules | 22k downloads, but no mention of ""open"" or ""node"" anywhere.
| `#17` | [Node TDD](https://marketplace.visualstudio.com/items?itemName=prashaantt.node-tdd) | Ease test-driven development in Node and JavaScript | 9k downloads, no mention of ""module(s)"", but at least it has a few ""opened"" in its readme.
| `#64` | [Open in Vim](https://marketplace.visualstudio.com/items?itemName=jonsmithers.open-in-vim) | Opens current file in vim | 2k downloads, this is not even mentioning ""node"" anywhere, let alone ""module(s)"". But at least it has ""open"" and ""in"" in it's title.
| `#65` | [Open in node_modules](https://marketplace.visualstudio.com/items?itemName=fabiospampinato.vscode-open-in-node-modules) | Open the current selection or arbitrary string in node_modules. | 2 downloads, it matches all the provided keywords, **in its title**.

If I had to guess what's wrong with your approach I'd say:

- Lack of stopwords, kind of meaningless keywords like ""vscode"" or ""in"" are given too much weight.
- Poor query parsing, if keywords are joined in some sort or another the whole thing falls apart (vscode-foo-bar, foo_bar, GitLens etc.)
- Maybe you're giving too much weight to downloads and ratings, the first thing to sort for is relevancy.

> One way to improve your extension rank is to break word node_modules into separate words node modules. This will increase the string matching score and push your extension up in the result.

I'm not going to rename the extension to something wrong (`node_modules` is a folder, I'm not talking about `node modules`, as in ""NPM packages"", here) just to work around this.",2018-05-28T15:38:02Z,1812093
3731,microsoft/vscode,294122809,396254024,"I agree, the search ranking seems wierd.

Some examples (when searching for ""python""):

Py Files generator is above autoDocString, despite having about 50k less installs and 0 reviews
Same thing with ladieratheme - it is above autoDocString despite having about 49k less installs and 3 less reviews

Trustcode odo snippets and kvlang is above docker linter, despite having 47k less installs

Python paste and indent has 3 stars but apparently it's 2k extra downloads trumps Python (pydev)'s 5 star rating.  Seems like download count is weighted more heavily than rating in the ranking (or maybe python paste and indent has better keywords)

Python Coding Conventions has 773 downloads and is unrated, yet somehow is above magicpython (with 742 _thousand_ downloads and 3.5 stars) and several other extensions with far more downloads / good ratings.  So you could have a very popular or well-rated extension but if you don't have the right keywords you will still be ranked down.

Wait..,. but looking at the Python Coding Conventions package.json, it doesn't even have any keywords!

https://github.com/harip/python-coding-conventions/blob/master/package.json  🤔🤔🤔

> While searching an extension, we also take into account the community inputs like number of downloads, 

You mean the download count that is _also_ the update count?  This has been a outstanding issue ever since 2016 - when you release an update the marketplace shows your downloads as having increased.
The update count should not effect the search ranking.

> number of ratings and average rating 

That's good, but what algorithim are you using to calculate the weighted rating?  Not sure if that is open source but hopefully it isn't something like RatingA - RatingB or RatingA/RatingB

http://www.evanmiller.org/how-not-to-sort-by-average-rating.html",2018-06-11T14:02:25Z,13080965
3732,microsoft/vscode,294122809,396260945,"> Python Coding Conventions has 773 downloads and is unrated, yet somehow is above magicpython 

This probably happens because of this:

> Poor query parsing, if keywords are joined in some sort of another the whole thing falls apart (vscode-foo-bar, foo_bar, GitLens etc.)

> You mean the download count that is also the update count? This has been a outstanding issue ever since 2016 - when you release an update the marketplace shows your downloads as having increased.
The update count should not effect the search ranking.

Yeah that's another major problem, one could automatically push a new update every day and downloads will go through the roof even though the same number of people are using it.

I guess technically those users are _downloading_ the update, but once this things are counted the downloads counter becomes less meaningful. 

The download counter for extensions on Chrome's store decreases when somebody uninstalls your extension and I think it doesn't increase just because the extension gets updated.",2018-06-11T14:22:03Z,1812093
3733,microsoft/vscode,294122809,397171332,"Hey guys, I want to thank you for all the feedback. Please keep them coming. We are discussing this issue internally and I'll update this thread as soon as we have something to share.",2018-06-14T05:02:55Z,13727302
3734,microsoft/vscode,294122809,405897918,"Here there's another weird one, searching for ""monokai"" and ordering by downloads returns those language packs at the top. The only place where those language packs mention ""monokai"" is inside their `package.json` as the value of some `contributes.localizations[0].translations[x].id` keys, why are you guys even indexing those fields?

<img width=""1193"" alt=""screen shot 2018-07-18 at 13 09 36"" src=""https://user-images.githubusercontent.com/1812093/42878075-112f643c-8a8c-11e8-93a2-1c5f54f57589.png"">",2018-07-18T11:22:07Z,1812093
3735,microsoft/vscode,294122809,423249854,"I've just realized that the readmes aren't indexed at all. 

I searched for ""cyclomatic"" in the marketplace, and I've got this result:

<img width=""674"" alt=""screen shot 2018-09-20 at 18 27 56"" src=""https://user-images.githubusercontent.com/1812093/45832685-ebe46c00-bd02-11e8-85ec-3cf9dc21ad82.png"">

Then I tried searching for ""marketplace cyclomatic"" on Google, and this is the result:

<img width=""846"" alt=""screen shot 2018-09-20 at 18 25 18"" src=""https://user-images.githubusercontent.com/1812093/45832583-95772d80-bd02-11e8-875b-473a56d814a1.png"">

This is a bit ridiculous. 

I'm not saying that the search functionalities in the marketplace should be as good as Google's under **all** circumstances, but that's a full-word match in the readme, this query just can't return 0 results.

@pkuma-msft do you have any updates for us?",2018-09-20T16:34:48Z,1812093
3736,microsoft/vscode,294122809,423671808,"@fabiospampinato Yes, unfortunately we do not index readme.md.. sorry about that. We use SQL FTS in our backend today and that's kind of the limiting factor. There's only so much custom logic we can run over the results returned by FTS to make it 'more' relevant - and more importantly it doesn't scale in the long run.

We are exploring moving our search platform to Azure Search or Bing which are techologies that are being actively developed and should provide us with more features and capabilities.

I'm afraid we are not going to invest more in trying to optimize search with FTS.",2018-09-21T21:11:09Z,13727302
3737,microsoft/vscode,294122809,423693320,@pkuma-msft thanks for the update.  Any timeframe for when the move will happen?,2018-09-21T23:06:22Z,13080965
3738,microsoft/vscode,294122809,423753315,"Holy crap changing my extension name to ""AREPL for python"" moved it to the *third* position in the search rankings when searching for python.  Thanks for the tip @eamodio and @gaurav42  !",2018-09-22T15:50:45Z,13080965
3739,microsoft/vscode,294122809,424617852,"@Almenon Sorry we are still in early exploration stages, cannot comment on a timeline yet.",2018-09-26T07:42:24Z,13727302
3740,microsoft/vscode,294122809,451374134,"From @octref 

> Previously: https://github.com/Microsoft/vscode/issues/24511
> 
> Now if you search for Vue, Vetur doesn't even show up in the first page despite being the most popular Vue extension. The install count of the 24 Vue extensions in first page combined is not even half of Vetur's install count.
> 
> ![image](https://user-images.githubusercontent.com/4033249/50652857-a6a81b00-0f3c-11e9-95e2-e2838082acfc.png)
> 
> A new Vue user coming to VS Code have a hard time finding Vetur by organic search. He might be misled to install a lot of the random extensions in Marketplace and believe VS Code has poor support for Vue files.",2019-01-04T07:52:29Z,22350
3741,microsoft/vscode,294122809,451536672,"~~@kesane-msft~~

~~Which~~

Oops, misclick.",2019-01-04T19:02:08Z,4033249
3742,microsoft/vscode,294122809,451538992,"> our search algorithm gives more weightage to matches in the extension's display name. If you add the ""vue"" term in your display name your extension would start showing as the top result.

@kesane-msft Which I would refuse. What do I call it? `Vetur Vue`?

Another example: Searching for ""Golang""

![image](https://user-images.githubusercontent.com/4033249/50705744-a2e5c880-1010-11e9-941e-5a1d68cc6848.png)

What should [Go extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode.Go) do? Renaming it to Golang? After which the query `Go` wouldn't return it as top result?

Maybe it's better to start a community curation of good extensions for each language/framework. http://howistart.org/ is a good example. At least it wouldn't be impossible for new users to find which extension to use, because the search results are not ranked helpfully.
",2019-01-04T19:10:53Z,4033249
3743,microsoft/vscode,294122809,451542588,"Maybe there could be a keyword (maybe the first one?), that is ranked as if it was part of the title. So that way GitLens could have _git_, Vetur could have _vue_, Go could have _golang_, etc.",2019-01-04T19:24:19Z,641685
3744,microsoft/vscode,294122809,451543703,"@eamodio Already did that, just need marketplace to index the `keywords` in package.json:

![image](https://user-images.githubusercontent.com/4033249/50706819-cc542380-1013-11e9-8786-e286bb014c2a.png)
",2019-01-04T19:28:11Z,4033249
3745,microsoft/vscode,294122809,451544111,"@octref yup, exactly",2019-01-04T19:29:36Z,641685
3746,microsoft/vscode,294122809,451546077,"> Maybe there could be a keyword (maybe the first one?), that is ranked as if it was part of the title. So that way GitLens could have git, Vetur could have vue, Go could have golang, etc.

This sounds too specific to me, IMHO the solution is transitioning from full-text search to a real search engine. Readmes, titles, descriptions and keywords should be properly indexed and the search query should be properly parsed too. ",2019-01-04T19:36:24Z,1812093
3747,microsoft/vscode,294122809,451562565,@fabiospampinato While I don't disagree -- I was just trying to offer an alternative that would hopefully be easy to implement but improves things somewhat and therefore likely to happen 😉 ,2019-01-04T20:42:34Z,641685
3748,microsoft/vscode,294122809,468030791,"Just wanted to open this as an issue. I often read `search for [xxx] and install the first result`.
I search the exact name of an extension and the one I'm looking for never is the first result :(

![image](https://user-images.githubusercontent.com/802702/53523036-073a7b00-3adc-11e9-8000-2b2b802ba1e5.png)
",2019-02-27T21:07:26Z,802702
3749,microsoft/vscode,294122809,478329705,"I can confirm this is still a bad issue. 
### This isn't a 'bad' search algorithm, something is broken here.

When searching for ""line endings"" (picture below)<br>
The following 4 circles are all similar extensions. The red circles have MORE downloads, installs, and better ratings, and I checked the `package.json` all of them have ""line endings"" in the title and as keywords.<br>

How can 'Line Note' (4th result) which
- doesn't have 'endings' in the title
- has 203 downloads
- no reviews
- doesn't even have 'endings' in the package.json or readme 

Beat out both 'line-endings' and 'code-eol (2019) Line Endings'
- both contain the full search term in the title
- they have 4K and 6K downloads respectively
- they have 4 stars and 5 stars respectively
-  and have 'line endings', 'line', 'endings' in the keywords of the package.json and readme

And it's not like 'Line Note' is some anomaly, there are 25 just as bad results that come before the relevant one. 

<img width=""1221"" alt=""Screen Shot 2019-03-31 at 5 02 32 AM"" src=""https://user-images.githubusercontent.com/17692058/55287759-c059d280-5372-11e9-8c99-21549a98d33f.png"">",2019-03-31T10:31:42Z,17692058
3750,microsoft/vscode,294122809,478330152,"My only theory to how these are being ranked, is that the marketplace has grabbed 200 results and accidentally sorted them by trending-ness instead of search-query relevance.

Since this is an upstream problem, is there a more direct repo we can reach out to? @auchenberg. 

If not then I agree with @octref, its been 2 months, it's probably time for the community to start building their own solution. I don't understand how something this critical can fail without tests catching it and a rollback being issued.",2019-03-31T10:39:16Z,17692058
3751,microsoft/vscode,294122809,478556746,"@jeff-hykin , I am from the Marketplace team. Thanks for bringing this issue to our notice. We are looking into it and will get it fixed.",2019-04-01T12:21:47Z,28951219
3752,microsoft/vscode,294122809,484345011,"Any updates after half a month?

If it hasn't been looked into, an estimate on when it will be looked into would be nice.
If it has been looked into, an update on what was found/not found would also be nice.

If the Marketplace team has higher priority tasks that's fine, I completely understand, I think we would just like to know that.",2019-04-18T03:45:13Z,17692058
3753,microsoft/vscode,294122809,484379754,"@jeff-hykin , we would have the fix for the issue you reported rolled out by next week or so.",2019-04-18T06:56:33Z,28951219
3754,microsoft/vscode,294122809,484382367,@kesane-msft any details on what changes are being made exactly?,2019-04-18T07:06:35Z,13080965
3755,microsoft/vscode,294122809,484406703,"@Almenon , we are tweaking our index analyzers.",2019-04-18T08:31:13Z,28951219
3756,microsoft/vscode,294122809,486682037,"@jeff-hykin we have rolled out the change. You should see the better search results.
Thanks for bringing it to our notice and keep the feedback coming in!",2019-04-25T13:52:41Z,28951219
3757,microsoft/vscode,294122809,486698811,"It seems that some issues have been addressed, but some things got even worse:

- Now searching for both `vscode-todo-plus` and `fabiospampinato.vscode-todo-plus` returns 0 results, IMHO that's unacceptable (and this was the original problem reported in this issue).

- Sometimes if I type `open in browsers` and then hit enter the query becomes `open in browser`, deleting the last character 🤷‍♂️.

- The marketplace doesn't search automatically until I hit enter, _unless_ I delete the query, IMHO that's pretty confusing (and that's also not what's happening in the homepage).

- If I clear the input field then the cursor disappears for a while until the results are returned.
  - There's probably no need to wait for those results to be returned in the first place, the top 55 extensions by downloads won't change very often, I'd just cache them (refreshing the cache once a day or so) and just ship them along with the rest of the page to the browser.",2019-04-25T14:35:09Z,1812093
3758,microsoft/vscode,294122809,486720939,"Yep things still aren't working right. Earlier gaurav42 said that ""While searching an extension, we also take into account the community inputs like number of downloads, number of ratings and average rating along with the string matching"". However, when I search for ""python"" ""python extended"" is above my extension despite having literally 0 reviews. In terms of download count we are very close (130k vs. 150k) so if the reviews were taken into account I would expect AREPL to be ranked higher. 

I would even go so far as to say that reviews should be _more_ important than download count in influencing search ranking (assuming you have enough reviews to be statistically valid). As a user when I search for things I want to find the best extensions, not the most downloaded extensions, but my extension has good reviews so I am biased on that front :P

@kesane-msft can you explain the change in more detail please? I don't know what ""tweaking the index analyzers"" means.",2019-04-25T15:25:45Z,13080965
3759,microsoft/vscode,294122809,486723634,"Oh and here's a really weird one - python (pydev) is ranked above python test explorer, despite having the same number of reviews, python mentioned in description of both, worse reviews on average, and ~45k less downloads.",2019-04-25T15:32:32Z,13080965
3760,microsoft/vscode,294122809,486724463,"I'm not sure what algorithm is being used but there's some existing ones that may be helpful:

http://www.evanmiller.org/how-not-to-sort-by-average-rating.html
https://stackoverflow.com/questions/1411199/what-is-a-better-way-to-sort-by-a-5-star-rating",2019-04-25T15:34:50Z,13080965
3761,microsoft/vscode,294122809,486834451,"Thanks @kesane-msft, I appreciate it.
The ""Line endings"" search seems to be fixed so I'm pretty happy about that.

After a few searches I do agree with @Almenon the search is sadly still pretty broken.

**For me, the biggest issue is its unclear why it is broken (both three months ago, and now)**
Is there some kind of AI returning the results? A handcrafted weighting system? Some transparency about how the search works would go a long way.

I know the code probably can't be made open source, but if the marketplace team made an API that would allow for exact-name matches and regex matches I'm pretty confident the VS Code community could fix this permanently within 48 hours.

Here's an example of the search still being pretty messed up. It's an improvement from last time; at least the relevant results are somewhere in the top 10, and the other results are popular. But on the other hand it is pretty sad that most students in my undergrad Info Storage and Retrieval class wrote better ranking systems in less than a month.

<img width=""382"" alt=""Screen Shot 2019-04-25 at 3 36 20 PM"" src=""https://user-images.githubusercontent.com/17692058/56767016-64ffe200-6770-11e9-970e-2efb90a2e104.png"">",2019-04-25T20:54:59Z,17692058
3762,microsoft/vscode,294122809,487600553,"Marketplace team has recently updated their search strategy to use Azure based search and which will expect to fix quiet a number of search related issues. There could be still some gaps which might exist and can be fixed. Since this issue is overwhelmed with lot of feedback and it is getting hard for us to understand what is fixed and what is not, I am locking this issue and ask users to create separate issues if they see any new bugs. At the same time, I would go through this complete issue and extract those which are not yet fixed into new issues which will help us and Marketplace team to better track and fix them. Thanks for the feedback provided here.",2019-04-29T14:24:45Z,10746682
3763,microsoft/vscode,294122809,487698891,"Took me quite a while to test, but nothing reported from this issue was fixed and search by author ID, extension ID, or combination of both are broken.

| Query | Marketplace Search | Expected | Actual | Regression |
|---|---|---|---|---|
|`fabiospampinato.vscode-todo-plus`| [Link](https://marketplace.visualstudio.com/search?term=fabiospampinato.vscode-todo-plus&target=VSCode&category=All%20categories&sortBy=Relevance) | Hit [Todo+](https://marketplace.visualstudio.com/items?itemName=fabiospampinato.vscode-todo-plus) and top-ranked | 0 result | N/A |
|`vscode-todo-plus` | [Link](https://marketplace.visualstudio.com/search?term=vscode-todo-plus&target=VSCode&category=All%20categories&sortBy=Relevance) | Hit [Todo+](https://marketplace.visualstudio.com/items?itemName=fabiospampinato.vscode-todo-plus) and top-ranked | 0 result | Used to at least hit but not top-ranked |
|`vscode-open-in-browsers` | [Link](https://marketplace.visualstudio.com/search?term=vscode-open-in-browsers&target=VSCode&category=All%20categories&sortBy=Relevance) | Hit and top-ranked | 0 result | Used to at least hit but not top-ranked |
|`vue`| [Link](https://marketplace.visualstudio.com/search?term=vue&target=VSCode&category=All%20categories&sortBy=Relevance) | Hit [Vetur](https://marketplace.visualstudio.com/items?itemName=octref.vetur) and top-ranked | Hit but not top-ranked | N/A |
|`golang`| [Link](https://marketplace.visualstudio.com/search?term=golang&target=VSCode&category=All%20categories&sortBy=Relevance) | Hit [Go](https://marketplace.visualstudio.com/items?itemName=ms-vscode.Go) and top-ranked | NOT in result | N/A |
| `octref` (publisher id) | [Link](https://marketplace.visualstudio.com/search?term=octref&target=VSCode&category=All%20categories&sortBy=Relevance) | Hit all extensions published by [octref](https://marketplace.visualstudio.com/publishers/octref) | 0 result | Used to hit all extensions published by [octref](https://marketplace.visualstudio.com/publishers/octref) |
",2019-04-29T18:49:52Z,4033249
3764,microsoft/vscode,294122809,487866155,"@octref Thanks for consolidating already (I was planning to do it this week). These issues were already raised with Marketplace team and here is their explanation

### Searching by id: `GitHub.vscode-pull-request-github-insiders`

We don’t support search on id of the extension(Not supported in SqlFTS too). When we search for GitHub.vscode-pull-request-github-insiders, then we search for two words “GitHub” and “vscode-pull-request-github-inside”.  That’s why all the extension’s containing Github are coming at the TOP and results are looking better than SqlFTS.

**Tip**: You can search by extension id using the phrase `@id:GitHub.vscode-pull-request-github-insiders`. Issue: https://github.com/Microsoft/vscode/issues/69350. Let's track it there.

### Searching by publisher id: `dias`

Azure search provides us custom analyzer to control search relevance. We had done stemming during indexing. So when user searches for “dias”, it returns all the extensions containing “dia” or “dias” both. Here DialogScript is coming at the top, because it contains “dia” , while “dias” is matched in publisher display name of “Open Folder Context Menus for VS Code” extension.  Extension display name prefix match has more weight than publisher display name. That’s why “DialogScript” is  coming at the top.
We have fixed this issue. We are not doing stemming now. So if you will search for “dias”, it will return the extension’s containing “dias” only.

**Tip**: You can search by publisher using the phrase `publisher:""Pine Wu""`. You can also click on Publisher name in the extension editor. I think this is a good solution for searching by publisher.

### Vue

We do have weights on tags too, but it’s very less (1/100)  in compare to extension’s display name. That’s why Vetur extension’s is not coming at the top, when user searches for ‘Vue’. ‘Vue’ search term is a  special case, where we expect Vetur extension as top result, because it’s a popular extension of ‘Vue’ language. We were unbale to solve this problem using SqlFTS, but we can solve this using AzureSearch. We can add Synonym Maps property to the index field definition in AzureSearch.

@kesane-msft Please add any details if missing. After consolidating, one of the improvements we would like to see from you is with respect to `vue` phrase in which case giving weightage to tags might help.
",2019-04-30T08:43:30Z,10746682
3765,microsoft/vscode,294122809,902002775,Upstream issue https://github.com/microsoft/vsmarketplace/issues/154,2021-08-19T15:17:42Z,1926584
3766,microsoft/vscode,294122809,1297137304,"This is fixed in latest [VS Code Insiders](https://code.visualstudio.com/insiders/). Please try it out and let us know if there are still examples where you think that search is not working.
We expect the fixed search to be in VS Code Stable start of December.
Looking forward to feedback - thanks!",2022-10-31T14:00:24Z,1926584
3767,ansible/ansible,284999235,284999235,"##### ISSUE TYPE
 - Feature Idea

##### COMPONENT NAME
import_playbook

##### ANSIBLE VERSION
<!--- Paste verbatim output from ""ansible --version"" between quotes below -->
```
2.4.0.0
```

##### CONFIGURATION
<!---
If using Ansible 2.4 or above, paste the results of ""ansible-config dump --only-changed""
Otherwise, mention any settings you have changed/added/removed in ansible.cfg
(or using the ANSIBLE_* environment variables).
-->

n/a

##### OS / ENVIRONMENT

CentOS 6

##### SUMMARY

I need to conditionally import a playbook, which isn't possible.  The keyword I'm missing is ""include_playbook"" which would allow a ""when"" to apply to it.  Why?

I have a playbook that performs some maintenance, OS, Kernel, package, and firmware upgrades.  For Major upgrades, we track progress in a ticketing system.  So, during those runs, I'd like to conditionally import a playbook that updates the ticket information.  In order for that to work, I need to pass the user's password to the ticketing system, so there's a `vars_prompt` in the ticket update playbook.  If the user specifies which ticket they're working to the maintenance playbook, I'd like the ticket update playbook to be called after the maintenance is performed.

If there's another way for this to work, I'm open to alternate ideas, but I think conditionally playbook imports would be generically useful.  I'm not understanding why tasks could be included dynamically, but a playbook wouldn't be.

##### STEPS TO REPRODUCE
<!---
For bugs, show exactly how to reproduce the problem, using a minimal test-case.
For new features, show how the feature would be used.
-->

<!--- Paste example playbooks or commands between quotes below -->
```yaml
# Maintenance Play Runs first, then conditionally import a second playbook
- import_playbook: update-ticket.yaml
  when: ticket_id is defined
```

<!--- You can also paste gist.github.com links for larger files -->

##### EXPECTED RESULTS
<!--- What did you expect to happen when running the steps above? -->

I expect `update-ticket.yaml` to only import if the `ticket_id` is defined.

##### ACTUAL RESULTS
<!--- What actually happened? If possible run with extra verbosity (-vvvv) -->

no parse error, and playbook `update-ticket.yaml` is imported 100% of the time.
",2017-12-28T22:12:37Z,99365
3768,ansible/ansible,284999235,354366745,"Files identified in the description:
* [lib/ansible/modules/utilities/logic/import_playbook.py](https://github.com/ansible/ansible/blob/devel/lib/ansible/modules/utilities/logic/import_playbook.py)

If these files are inaccurate, please update the `component name` section of the description or use the `!component` bot command.

[click here for bot help](https://github.com/ansible/ansibullbot/blob/master/ISSUE_HELP.md)
<!--- boilerplate: components_banner --->",2017-12-28T22:18:52Z,6585283
3769,ansible/ansible,284999235,361277446,"We have similar need. We have a main playbook that prepares the system, but we allow the user to provide some extra steps by creating a playbook files in a well known directories. We do not know what files will be present there in advance. For this we would really like to have include_playbook that supports with_fileglob (or with_items).",2018-01-29T15:17:35Z,171090
3770,ansible/ansible,284999235,361278852,"Just to explain why include_tasks is not enough. The system is clustered and the tasks talk to localhost, other physical hosts and virtual machines. We could in theory use delegate_to if it supported host groups.",2018-01-29T15:21:43Z,171090
3771,ansible/ansible,284999235,375157219,"I also have this need in order to import different playbooks to configure Vagrant guests differently depending on the active hypervisor.

Alternate to `when` is something like (this is also not supported):
```YAML
- import_playbook: ""install_{{ 'virtualbox' if ansible_product_name == 'VirtualBox' else 'vmware' if ansible_product_name = 'VMware Virtual Platform' else 'noop' }}_extensions.yml""
```
",2018-03-22T02:21:37Z,341037
3772,ansible/ansible,284999235,383512601,"Is this going to be fixed soon? I find it as a serious source of problems because the optional playbook code ca be huge when it comes to number of tasks, causing over **extensive console/log verbosity** of tasks that are never supposed to be loaded.

The `skip_reason"": ""Conditional result was False""` is not of much help either because the user will not see any condition on those tasks, the condition being few nested includes/imports away in another file.

It helps nobody that Ansible will list hundreds of lines of files that were never supposed to be run, making much harder to investigate them.",2018-04-23T09:27:31Z,102495
3773,ansible/ansible,284999235,386834350,"As a workaround until this is possible, if the use case is to support different host groups it is possible to use an include_task with a conditional on group names, i.e. a task include for a group `foo` is included when a file `tasks_directory/foo.yml` is provided:

```
- include_tasks: ""{{item}}""
  with_fileglob: ""tasks_directory/*.yml""
  vars:
    file_host_group: ""{{ (item | basename | splitext)[0]}}""
  when: ""file_host_group in group_names""
```

The playbook including this construct has to run for all hosts.
",2018-05-05T20:55:47Z,22767209
3774,ansible/ansible,284999235,417762220,"Is this still being worked at all?  I'm kind of implementing something like an `ansible-galaxy` style method of ""installing"" playbooks into a playbooks sub-directory and then I want to `include_playbook` a playbook that was just ""installed"".  

Here is what I've got so far:

```yaml
- name: PLAY | Install other required playbooks
  hosts: localhost
  connection: local
  tasks:

    - name: INCLUDE_VARS | include variables to discover other needed playbooks
      include_vars:
        dir: playbooks/
        files_matching: requirements.yml
        depth: 1

    - name: GIT | Clone playbooks
      git:
        repo: ""{{ item.src }}""
        dest: ""playbooks/{{ item.src.split('/')[-1] }}""
        version: ""{{ item.version }}""
      loop: ""{{ elk_required_playbooks }}""

    - name: SHELL | Install included playbooks roles
      shell: ansible-galaxy install -r roles/requirements.yml -p roles/
      args:
        chdir: ""playbooks/{{ item.src.split('/')[-1] }}""
      loop: ""{{ elk_required_playbooks }}""

- name: PLAY | Run the installed helloWorld playbook
  import_playbook: ""playbooks/ap_hello_world/helloWorld.yml""
```
If I run this as-is, I get an import error because the playbook to be imported isn't there yet.

```bash
ERROR! Unable to retrieve file contents
Could not find or access '/path/to/playbooks/ap_hello_world/helloWorld.yml'
```
If I comment out the `import_playbook` play, the functionality above that works nearly like `ansible-galaxy` and ""installs"" the playbooks I need (and the roles they need).  

```bash
PLAY [PLAY | Install other required playbooks] *************************************************************************************************************************************

TASK [Gathering Facts] *************************************************************************************************************************************************************
Friday 31 August 2018  13:54:13 -0500 (0:00:00.239)       0:00:00.240 ********* 
Friday 31 August 2018  13:54:13 -0500 (0:00:00.237)       0:00:00.237 ********* 
ok: [localhost]

TASK [INCLUDE_VARS | include variables to discover other needed playbooks] *********************************************************************************************************
Friday 31 August 2018  13:54:15 -0500 (0:00:01.366)       0:00:01.606 ********* 
Friday 31 August 2018  13:54:15 -0500 (0:00:01.366)       0:00:01.604 ********* 
ok: [localhost]

TASK [GIT | Clone playbooks] *******************************************************************************************************************************************************
Friday 31 August 2018  13:54:15 -0500 (0:00:00.124)       0:00:01.730 ********* 
Friday 31 August 2018  13:54:15 -0500 (0:00:00.124)       0:00:01.728 ********* 
changed: [localhost] => (item={u'src': u'<gir_url>/ap_hello_world', u'version': u'v0.3.0'})

TASK [SHELL | Install included playbooks roles] ************************************************************************************************************************************
Friday 31 August 2018  13:54:17 -0500 (0:00:02.591)       0:00:04.322 ********* 
Friday 31 August 2018  13:54:17 -0500 (0:00:02.591)       0:00:04.319 ********* 
changed: [localhost] => (item={u'src': u'<git_url>/ap_hello_world', u'version': u'v0.3.0'})

PLAY RECAP *************************************************************************************************************************************************************************
localhost                  : ok=4    changed=2    unreachable=0    failed=0 
```
And now I can run the same playbook again, this time with the `import_playbook` play not commented and it works as I desire:

```bash
PLAY [PLAY | Install other required playbooks] *************************************************************************************************************************************

TASK [Gathering Facts] *************************************************************************************************************************************************************
Friday 31 August 2018  14:00:30 -0500 (0:00:00.244)       0:00:00.244 ********* 
Friday 31 August 2018  14:00:30 -0500 (0:00:00.241)       0:00:00.241 ********* 
ok: [localhost]

TASK [INCLUDE_VARS | include variables to discover other needed playbooks] *********************************************************************************************************
Friday 31 August 2018  14:00:32 -0500 (0:00:01.471)       0:00:01.716 ********* 
Friday 31 August 2018  14:00:32 -0500 (0:00:01.471)       0:00:01.713 ********* 
ok: [localhost]

TASK [GIT | Clone playbooks] *******************************************************************************************************************************************************
Friday 31 August 2018  14:00:32 -0500 (0:00:00.138)       0:00:01.854 ********* 
Friday 31 August 2018  14:00:32 -0500 (0:00:00.138)       0:00:01.852 ********* 
ok: [localhost] => (item={u'src': u'<git_url>/ap_hello_world', u'version': u'v0.3.0'})

TASK [SHELL | Install included playbooks roles] ************************************************************************************************************************************
Friday 31 August 2018  14:00:35 -0500 (0:00:02.821)       0:00:04.675 ********* 
Friday 31 August 2018  14:00:35 -0500 (0:00:02.821)       0:00:04.673 ********* 
changed: [localhost] => (item={u'src': u'<git_url>/ap_hello_world', u'version': u'v0.3.0'})

PLAY [PLAY | BEGIN Setup & Timing] *************************************************************************************************************************************************

TASK [set_fact] ********************************************************************************************************************************************************************
Friday 31 August 2018  14:00:36 -0500 (0:00:01.323)       0:00:05.999 ********* 
Friday 31 August 2018  14:00:36 -0500 (0:00:01.323)       0:00:05.997 ********* 
ok: [localhost]

TASK [debug] ***********************************************************************************************************************************************************************
Friday 31 August 2018  14:00:36 -0500 (0:00:00.155)       0:00:06.154 ********* 
Friday 31 August 2018  14:00:36 -0500 (0:00:00.155)       0:00:06.152 ********* 
ok: [localhost] => {
    ""msg"": ""Start Time - 2018-08-31 14:00:36""
}

PLAY [PLAY | Say Hello to My Little Friend] ****************************************************************************************************************************************

TASK [SHELL | echo something] ******************************************************************************************************************************************************
Friday 31 August 2018  14:00:36 -0500 (0:00:00.096)       0:00:06.251 ********* 
Friday 31 August 2018  14:00:36 -0500 (0:00:00.096)       0:00:06.249 ********* 
ok: [knebawils001]

TASK [DEBUG | debug host's standard output] ****************************************************************************************************************************************
Friday 31 August 2018  14:00:37 -0500 (0:00:00.714)       0:00:06.965 ********* 
Friday 31 August 2018  14:00:37 -0500 (0:00:00.714)       0:00:06.963 ********* 
skipping: [knebawils001]

PLAY [PLAY | Say Hello via an Ansible Role] ****************************************************************************************************************************************

TASK [ar_hello_world : SHELL | echo role's message on host] ************************************************************************************************************************
Friday 31 August 2018  14:00:37 -0500 (0:00:00.151)       0:00:07.117 ********* 
Friday 31 August 2018  14:00:37 -0500 (0:00:00.151)       0:00:07.115 ********* 
ok: [knebawils001]

TASK [ar_hello_world : DEBUG | debug host shell standard output] *******************************************************************************************************************
Friday 31 August 2018  14:00:37 -0500 (0:00:00.342)       0:00:07.459 ********* 
Friday 31 August 2018  14:00:37 -0500 (0:00:00.342)       0:00:07.457 ********* 
skipping: [knebawils001]

PLAY [PLAYBOOK | END Setup & Timing] ***********************************************************************************************************************************************

TASK [set_fact] ********************************************************************************************************************************************************************
Friday 31 August 2018  14:00:38 -0500 (0:00:00.212)       0:00:07.672 ********* 
Friday 31 August 2018  14:00:38 -0500 (0:00:00.212)       0:00:07.669 ********* 
ok: [localhost]

TASK [debug] ***********************************************************************************************************************************************************************
Friday 31 August 2018  14:00:38 -0500 (0:00:00.179)       0:00:07.851 ********* 
Friday 31 August 2018  14:00:38 -0500 (0:00:00.178)       0:00:07.848 ********* 
ok: [localhost] => {
    ""msg"": ""Start Time - 2018-08-31 14:00:36, End Time - 2018-08-31 14:00:38, Elapsed Time - 0:00:02""
}

PLAY RECAP *************************************************************************************************************************************************************************
knebawils001          : ok=2    changed=0    unreachable=0    failed=0   
localhost                  : ok=8    changed=1    unreachable=0    failed=0
```
I suppose this could be split up into two (2) separate playbooks in the same Git repo.  The first would be called `prepare.yml` or maybe `prerequisites.yml` to ""install"" the other needed playbooks and the second main playbook (`playbook.yml`) will do the necessary imports, etc.  I was really wanting to make this a ""one-shot"" playbook.",2018-08-31T19:06:38Z,1863643
3775,ansible/ansible,284999235,420314552,"Include conditionals are very useful for creating branches in our playbooks.  Will it be ensured that import_playbook will support conditionals in the next release citing this issue?

If not, you are losing a lot of power and will end up creating a lot of hacks.  Not to mention breaking a ton of include playbook conditionals in end user plays.",2018-09-11T15:27:00Z,23485244
3776,ansible/ansible,284999235,423873463,"+1 on implementing a ""when"" conditional.",2018-09-24T04:10:28Z,1874225
3777,ansible/ansible,284999235,426810904,soon-ish we approaching 1 y since this request was open and no progress so far ...  any chance this get some attention @bcoca ? much thanks !,2018-10-03T21:31:39Z,7383422
3778,ansible/ansible,284999235,428493588,I would like to vote a '+1'as well.,2018-10-10T08:58:25Z,390197
3779,ansible/ansible,284999235,428535951,+1,2018-10-10T11:24:10Z,17454073
3780,ansible/ansible,284999235,428584701,"+1 as well, would love to see this feature",2018-10-10T14:04:16Z,6311058
3781,ansible/ansible,284999235,428587115,Please use the 👍  button to let the maintainers know you need this feature. Getting a ton of emails from these +1s.,2018-10-10T14:10:29Z,711940
3782,ansible/ansible,284999235,429295401,+1,2018-10-12T11:32:02Z,30902622
3783,ansible/ansible,284999235,431035752,"If someone is interested how to use play-level variables for conditional playbook-import:
1. Set up that variable as a fact in the play
2. Use `when` with `import_playbook` to check this variable (with full path, `hostvars.hostname.a_variable`)

If someone is interested, I managed to make import_playbook be conditional on `--limit` in the command line:

https://medium.com/opsops/import-playbook-with-play-level-condition-775122fe78ff

An example:

```
- hosts: all,localhost
  gather_facts: no
  run_once: True
  tasks:
   - set_fact:
        full_run: '{{ play_hosts == groups.all }}'
     delegate_to: localhost
     delegate_facts: yes

- import_playbook: test.yaml
  when: hostvars.localhost.full_run
```",2018-10-18T14:41:38Z,652496
3784,ansible/ansible,284999235,478921094,is this issue/request still up to date in a more current version of ansible? Iam using ansible 2.6.1 and the when condition doesn't seem to work when I use the import_playbook function.,2019-04-02T09:38:34Z,25477935
3785,ansible/ansible,284999235,478928583,"@brotaxt  You need to initialize variables before doing `when`. Just add some random task to random host (before doing first 'import_playbook'). F.e., do set_fact on localhost, as in example above.",2019-04-02T10:00:11Z,652496
3786,ansible/ansible,284999235,479003604,"@amarao 

many thanks for the quick response. :+1:  Unfortunately it doesn't seem to work for me. Even if answer the prompt with ""no"" the playbook ""vmware_createsnap.yml"" gets invoked. The other tasks are working as expected. What I am doing wrong? 


My playbook: 

```
---
-
  hosts: all
  gather_facts: true
  vars_prompt:
   - name: ""snapshots_required""
     prompt: ""Do you want to automatically create VMWare Snapshots? [yes/no]""
     private: no
  name: ""Install all available Updates""
  tasks:

   - name: Check for Updates
     include: checkforupdates.yml

   - name: setting fact for hosts which have outstanding updates
     set_fact:
       updates_available: ""yes""
     when: ""yumoutput.changed or zypperoutput.changed""


   - name: setting fact for hosts which have no outstanding updates
     set_fact:
       updates_available: ""false""
     when: updates_available is not defined


- import_playbook: vmware_createsnap.yml
  when: snapshots_required = ""yes""
```",2019-04-02T13:47:30Z,25477935
3787,ansible/ansible,284999235,479013767,"@brotaxt  this is a feature request, you currently CANNOT conditionally import playbooks, the conditions above happen to skip all the tasks in one, but this is not a supported behaviour and not guaranteed to work across versions of Ansible.",2019-04-02T14:09:58Z,836171
3788,ansible/ansible,284999235,479015890,"just to be clear:
import or include? because I think the current implementation or naming is actually wrong, based on the definition in https://docs.ansible.com/ansible/latest/user_guide/playbooks_reuse_includes.html

Does import_playbook actually ""lazy load"" or does it get loaded & parsed with the yaml file?",2019-04-02T14:14:33Z,114300
3789,ansible/ansible,284999235,479022512,"@Kriechi neither, it gets loaded at 'playbook compile time' which is before execution but not on file load

there is no include_playbook, that is the whole purpose of this feature request, to add one",2019-04-02T14:28:04Z,836171
3790,ansible/ansible,284999235,479024069,"@bcoca mhm that sounds even more wrong - or am I missing the big picture here?
I would have expected that `import_task` and `include_task` have an `*_playbook` sibling...",2019-04-02T14:31:08Z,114300
3791,ansible/ansible,284999235,479024936,"@Kriechi the engine never supported that, why `include:` was very misleading and we had to separate it into the different include_X/import_X options and make each behaviour explicit. So include_X is dynamic aka runtime, while import_X is 'static' aka 'compile time'.",2019-04-02T14:32:46Z,836171
3792,ansible/ansible,284999235,479026636,"ok - so `include_playbook` would be a feature request? Or can we track it here?

E.g., I'm running a git-checkout task on localhost, and then want to `include_playbook: some/repo/foo.yml`
This should include (lazy-load) the updated playbook from that repository, AFTER pulling the latest commit from the remote. Currently, `import_playbook` imports the ""old"" playbook, then pulls, and then runs the outdated playbook.",2019-04-02T14:36:14Z,114300
3793,ansible/ansible,284999235,479048865,"@Kriechi  ... please read the subject of this ticket, that is EXACTLY what we are tracking here",2019-04-02T15:23:28Z,836171
3794,ansible/ansible,284999235,479056103,"true - the part the confused me is ""conditionally import..."".
`import_playbook` and `include_playbook` are the feature we want.",2019-04-02T15:37:22Z,114300
3795,ansible/ansible,284999235,495311805,"I'm trying to do what [watsonb](https://github.com/ansible/ansible/issues/34281#issuecomment-417762220) was, using ansible-galaxy to install roles, and then using the roles. 

I found a decent workaround for the all-in-one playbook, which was to place the playbooks, in the order that you want them to execute, on the command line. In Watsonb's case, that would look like

ansible-playbook ... prepare.yml playbook.yml

Any variables that you set on the command line are passed to the playbooks, sequentially.

This might also solve [MarSik's](https://github.com/ansible/ansible/issues/34281#issuecomment-361277446) problem, also, using file globbing on the command line instead of in the playbook.",2019-05-23T17:30:07Z,45837141
3796,ansible/ansible,284999235,495339159,"@decet It could, but that would basically mean using a top level bash script as the entrypoint and spliting the main ansible playbook into multiple stage files. Not too horrible, just ugly.",2019-05-23T18:47:20Z,171090
3797,ansible/ansible,284999235,495431953,"I've overcome this in my own way as follows.  First, my typical playbook directory structure:

```bash
.
├── .ansible-lint
├── .gitignore
├── .yamllint
├── ansible.cfg
├── callback_plugins
│   ├── junit.py
│   ├── log_plays.py
│   ├── profile_roles.py
│   ├── profile_tasks.py
│   ├── timer.py
├── check_ansible_lint.sh
├── check_syntax.sh
├── check_yaml_lint.sh
├── create.yml
├── destroy.yml
├── Jenkinsfile
├── localhost_inventory.yml
├── playbooks
│   ├── ap_linux_instance
│   └── requirements.yml
├── prerequisites.yml
├── README.md
├── reports
├── requirements.txt
├── roles
│   ├── ar_linux_ansible_venv
│   ├── ar_linux_cname
│   ├── config_encoder_filters
│   └── requirements.yml
└── VERSION.md
```

My .gitignore ignores most sane OS/language/IDE things, but also ignores everything in the `roles/` and `playbooks/` folders except for the `requirements.yml` files in each folder.  The requirements.yml file within the playbooks folder is similar to your Galaxy-style requirements.yml, but rather than calling out dependent playbooks by Galaxy owner.name, I specify the full Git source (Galaxy supports this of course).  The requirements.yml within the roles/ folder is just your traditional Galaxy-style requirements.

I have this `prerequisites.yml` playbook, that looks like this:

```yaml
---

- name: PLAY | Install other required playbooks
  hosts: localhost
  connection: local
  tasks:

    - name: INCLUDE_VARS | include variables to discover other needed playbooks
      include_vars:
        dir: playbooks/
        files_matching: requirements.yml
        depth: 1

    - name: GIT | Clone playbooks
      git:
        repo: ""{{ item.src }}""
        dest: ""playbooks/{{ item.src.split('/')[-1] }}""
        version: ""{{ item.version }}""
      loop: ""{{ required_playbooks }}""

    - name: SHELL | Install included playbooks roles
      shell: ansible-galaxy install -r roles/requirements.yml -p roles/ --force
      args:
        chdir: ""playbooks/{{ item.src.split('/')[-1] }}""
      loop: ""{{ required_playbooks }}""
      when: item.galaxy
      changed_when: false
      tags: [ skip_ansible_lint ]
```
And, assuming that my ""big bang"" create.yml depends on a playbook and its roles from another playbook project, I import it like this:

```yaml
# ~~~~~~~~~~
# Ensure that all of the host VMs in the inventory are up and running
# either on-prem or in Azure as specified in the inventory
#
- name: Ensure inventory hosts are present
  import_playbook: ""playbooks/ap_linux_instance/create.yml""
  tags: [ base_server, hosts ]
```

And so, the work-flow to run my ""big bang"" (e.g. create.yml) is a 3-liner:

```bash
ansible-playbook prerequisites.yml
ansible-galaxy install -r roles/requirements -p roles/
ansible-playbook create.yml -i <path_to_inventory>
```

You could, of course, wrap the above 3-liner in a `create.sh` shell script for convenience.  This method has served me well for some fairly complex playbook projects that depend on other playbook projects.  This forces us to keep roles and playbooks fairly self-contained and re-usable and factor variables out into their own inventory projects.  When performed with discipline, it makes it really easy to migrate unaltered roles/playbooks to other environments, then just update inventory variables that are unique to that environment.

**This doesn't solve the conditional import problem**, mind you, but does help me use the `import_playbook` statement for something that may not exist just yet.  It kind of gets around a conditional in my very specific use-case.  I make use of tagging on the `import_playbook` to leverage the command-line `--tags` and `--skip-tags` features if I need scalpel-like precision at run-time. But if you had to make an import decision based on some other conditional logic (e.g., OS family), well, we still need that as a language feature I think.  For now, I just handle those cases with sub-playbooks and chain them together ensuring I target the appropriate hosts/groups that should or should not be targeted based on how I've setup my inventory (yes, it can get messy).

This is all pretty wild and requires a high degree if what I commonly refer to as ""4th dimensional thinking"", especially when you consider branches/versions of things and running them from CI/CD platforms like Jenkins or even AWX.  But I still find Ansible fascinating and use it daily.

HTH,

Ben",2019-05-24T00:37:55Z,1863643
3798,ansible/ansible,284999235,517325711,"I see that this issue now has a ""has_pr"" label. I searched a lot, but I can't find the PR. Can someone link it here?",2019-08-01T15:01:04Z,19341644
3799,ansible/ansible,284999235,517328598,"The bot added that label, and it looks like it was a false positive.  No work is being done on this feature, nor are there any plans to work on it currently.",2019-08-01T15:07:47Z,39340
3800,ansible/ansible,284999235,517684810,any plans for this one ?,2019-08-02T12:40:44Z,45048425
3801,ansible/ansible,284999235,533077631,"I'd just like to mention that there is a bit in the documentation that insinuates that the feature requested already exists. See:
https://docs.ansible.com/ansible/latest/user_guide/playbooks_conditionals.html#applying-when-to-roles-imports-and-includes

Specifically, the note mentions that Ansible allows `when` to work with playbook includes since version 2.0. Perhaps this documentation should also be amended.",2019-09-19T10:54:22Z,105533
3802,ansible/ansible,284999235,575351947,"If only unfixed issues aged like fine wine instead of like bait fish....

But seriously is this ever going to either be fixed or closed as won't fix? The `has_pr` label typically indicates at fix is being gestated, but in this case there is no hint as to where we should be looking for the fix.",2020-01-16T21:20:04Z,410276
3803,ansible/ansible,284999235,575366695,"My previous comment still stands as an answer to these questions. 

https://github.com/ansible/ansible/issues/34281#issuecomment-517328598",2020-01-16T21:59:55Z,39340
3804,ansible/ansible,284999235,601657097,+1,2020-03-20T11:40:01Z,15322260
3805,ansible/ansible,284999235,605519148,"+1
This is a frustrating ""missing feature"" 
Since there is no plan to allow for conditional vars_prompt, the only other way to conditionally get user input at start of playbook would be with conditional ""import_playbook"" based on whether a tag was sent in on commandline.
`- import_playbook: playbook_choice_1.yml
  when: ""'specific_tag' in ansible_run_tags""`

I understand that we want a playbook to require no user input, but this is just not a realistic scenario. When you have several users of a given playbook, then you need to ask for passwords. The other option is having dozens of ""ansible vault"" files (nightmare), or by implementing hashicorp vault (first get everything working as desired then implement another level of integration)... ",2020-03-28T21:02:34Z,637090
3806,ansible/ansible,284999235,605739059,"I (still) agree with @subcan that this feature should exist. This would be a great thing to get implemented while we're all on COVID-19 lockdown! ;)

As to options for managing multiple configurations, there is another option in between Ansible vault files and full-blown Hashicorp Vault deployment. I wrote a tool I named ""python_secrets"" and it works well with Ansible to manage multiple sets of variables outside of a Git repository (including one with Ansible playbooks). I document how to use it this way and have described it in several public talks listed on my home page (my talk at WSLConf from earlier this month will be added as soon as the videos are released).

https://pypi.org/project/python-secrets/
https://youtu.be/WD2Oqy2oc3A ",2020-03-30T01:40:34Z,1874225
3807,ansible/ansible,284999235,629691500,"Files identified in the description:
* [`lib/ansible/modules/import_playbook.py`](https://github.com/ansible/ansible/blob/devel/lib/ansible/modules/import_playbook.py)

If these files are incorrect, please update the `component name` section of the description or use the `!component` bot command.

[click here for bot help](https://github.com/ansible/ansibullbot/blob/master/ISSUE_HELP.md)
<!--- boilerplate: components_banner --->",2020-05-16T18:58:06Z,6585283
3808,ansible/ansible,284999235,646603277,+1,2020-06-19T12:14:45Z,1803926
3809,ansible/ansible,284999235,665843390,+1,2020-07-29T18:59:09Z,4961166
3810,ansible/ansible,284999235,665846985,"I've locked this to contributors for now.  Adding `+1` comments is too noisy.  For future reference, add a reaction to the issue body, and don't comment.",2020-07-29T19:06:23Z,39340
3811,ansible/ansible,284999235,1079722773,"Thank you very much for your submission to Ansible. It means a lot to us that you've taken time to contribute.

Unfortunately, this issue has been open for some time while waiting for a contributor to take it up but there does not seem to have been anyone that did so. So we are going to close this issue to clear up the queues and make it easier for contributors to browse possible implementation targets.

However, we're absolutely always up for discussion. Because this project is very active, we're unlikely to see comments made on closed tickets and we lock them after some time. If you or anyone else has any further questions, please let us know by using any of the communication methods listed in the page below:

   * https://docs.ansible.com/ansible/latest/community/communication.html

In the future, sometimes starting a discussion on the development list prior to proposing or implementing a feature can make getting things included a little easier, but it's not always necessary.

Thank you once again for this and your interest in Ansible!

[click here for bot help](https://github.com/ansible/ansibullbot/blob/devel/ISSUE_HELP.md)
<!--- boilerplate: waiting_on_contributor_close --->",2022-03-26T16:03:14Z,6585283
3812,ansible/ansible,1159293958,1159293958,"### Summary

I see 404s in your docs - it looks like somebody **removed** URLs to important doc entries like e.g. 
http://docs.ansible.com/ansible/playbooks.html
but then **did not introduce a redirect** from the old doc entry to some new one, so that people are seeing 404s.

This is a very bad documentation practice and should never be done in any project. Yes, **REDIRECTS** exist for exactly this purpose. For an important infrastructure project it is expected that documentation writers understand these concepts. 

The underlying issue here is not just a simple 404 - it seems like the documentation writing person is not informed about the existence of redirects and believes that it is OK to just remove URLs from the internet - this should be made clear on day one for anybody doing this kind of work: it is NOT!

Also we see here a complete lack of understanding and empathy for the readers situation, what is another very important skill for documentation writers that must be required for such a job position. The person who introduced this problem did not think for one second about how annoying and disctracting it is to find a broken URL while the reader is trying to figure out some technical details. 

This all can be fixed on a technical level with simple redirects - but it needs to be fixed on a human level, the person who did this needs to learn about the importance of respecting the reading experience, this is of course true for the project manager. 

This is not acceptable for such a top level project.


### Issue Type

Documentation Report

### Component Name

http://docs.ansible.com/ansible/playbooks.html

### Ansible Version

```console
$ ansible --version
```


### Configuration

```console
$ ansible-config dump --only-changed
```


### OS / Environment

INTERNET

### Additional Information

NO MORE INFO

### Code of Conduct

- [X] I agree to follow the Ansible Code of Conduct",2022-03-04T06:25:47Z,89897153
3813,ansible/ansible,1159293958,1058875113,"Files identified in the description:
None

If these files are incorrect, please update the `component name` section of the description or use the `!component` bot command.

[click here for bot help](https://github.com/ansible/ansibullbot/blob/devel/ISSUE_HELP.md)
<!--- boilerplate: components_banner --->",2022-03-04T06:29:44Z,6585283
3814,ansible/ansible,1159293958,1059190159,"@labeldevops thanks for reporting this. The redirects were there, though from a quick test of a few pages they seem to all be missing.

Please be assured we do try and make sure that we don't break search results or old bookmarks. For example, a lot of work went it to ensuring that old module urls still work in the new world of collections.

Once this has been fixed we will add some tests against the production webserver to ensure these continue to function. ",2022-03-04T14:04:25Z,940557
3815,ansible/ansible,1159293958,1060992298,"@labeldevops please keep in mind that Ansible has a Community Code of Conduct. Your original issue filing verbiage violates the spirit of the [code of conduct](https://docs.ansible.com/ansible/latest/community/code_of_conduct.html#code-of-conduct). 

In addition, I personally know the docs individuals who work very hard on their work and can attest this is not due to a lack of empathy to the readers experience, the docs team cares a lot about the experience of reading their documentation. 
These docs, as you pointed out, are written by humans and humans make mistakes. There is a way to bring issues up about the experience you're having as a reader without attacking people who are just doing their job the best they can.

Be kind. ",2022-03-07T18:23:33Z,11764497
3816,ansible/ansible,1159293958,1211171106,Closing as there are redirects already in place. If there are specific problems please open an issue for the specific redirect problem and the docs team will work on implementing it. The collection docs restructure was a pretty big split and while the docs team tried to cover everything things did slip through the cracks.,2022-08-10T19:27:08Z,8462645
3817,ansible/ansible,239227953,239227953,"##### ISSUE TYPE
 - Feature Idea

##### COMPONENT NAME
ansible-vault

##### ANSIBLE VERSION
```
ansible 2.3.1.0
  config file = 
  configured module search path = Default w/o overrides
  python version = 2.7.13 (default, Apr 23 2017, 16:50:35) [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)]
```

##### CONFIGURATION
n/a

##### OS / ENVIRONMENT
n/a

##### SUMMARY
ansible-vault decrypt allows the decryption of completely encrypted yaml files, but it will not decrypt vaulted variables in an unencrypted yaml file with encrypted variables.

It would be nice, for CLI purposes, to have decrypt take a partially encrypted file, and give us the decrypted text.

##### STEPS TO REPRODUCE
* create `test.yml` file with single encrypted variable encrypted by `~/.vault_pass.txt`
* ansible-vault decrypt file

```
ansible-vault decrypt test.yml --vault-password-file ~/.vault_pass.txt
```

##### EXPECTED RESULTS
* Expected plain text output with encrypted variable decrypted.

##### ACTUAL RESULTS
```
ERROR! input is not vault encrypted data for test.yml
```
",2017-06-28T16:50:58Z,2132602
3818,ansible/ansible,239227953,311905076,@alikins I believe you look after vault,2017-06-29T08:55:23Z,940557
3819,ansible/ansible,239227953,313492245,Would be also good if 'ansible-vault view' worked for such files.,2017-07-06T19:14:57Z,6112411
3820,ansible/ansible,239227953,317448528,"This might be something that will get covered in https://github.com/ansible/ansible/blob/devel/docs/docsite/rst/roadmap/ROADMAP_2_4.rst#id25

As a user, what would you expect the decrypted file to look like?

First thought is just to replace the !vault yaml scalar with the decrypted text. That probably makes the most sense for 'view'.

For 'decrypt' and especially 'edit', I'm not sure that will be sufficient. For 'edit', the re-encrypt phrase is going to need to be able to figure out which variable values originally came from a vaulted value. Especially if the file is edited significantly (reordering lines for example, or changing the variable name). 

So the file presented for editing would need to include some markers indicating the text that was decrypted/should be re-encrypted. A couple of ways to do that:

   1) Add comments to mark the text, and doing some text manipulation/regexes to replace it with encrypted text in place. Something like:

``` yaml
# START VAULT PLAINTEXT - my_var
my_var: my text goes here
# END VAULT PLAINTEXT - my_var
some_plain_var: blippy
```

  2) Add a new yaml type indicating text to be encrypted. Something like:

``` yaml
my_var: !vault-plaintext |
    my text goes here
some_plain_var: blippy
```

It would be best if we could yaml parse the input, decrypt the value, serialize the yaml to a file for editing, let user edit it, then yaml parse the results, encrypt the value, and serialize to yaml and save.

But... doing that with the available yaml parser would lose comments and ordering of maps.

So likely some in place string/text manipulations will be required.",2017-07-24T14:53:42Z,15162
3821,ansible/ansible,239227953,329495428,"Not going to happen for 2.4, so bumped to 2.5.",2017-09-14T14:14:22Z,15162
3822,ansible/ansible,239227953,361973797,"@jhkrischel This issue is waiting for your response. Please respond or the issue will be closed.

[click here for bot help](https://github.com/ansible/ansibullbot/blob/master/ISSUE_HELP.md)
<!--- boilerplate: needs_info_base --->",2018-01-31T15:47:45Z,6585283
3823,ansible/ansible,239227953,362034126,"Any news when this is planned to be implemented in ansible?
We have lots of passwords as vaulted variables, hence updating\viewing them is troublesome.
I did some script (based on solution, from alikins last post) to at least parse such yml and decrypt every variable to stdout\file to see a decrypted file at once, but this is just a script that is not a complete solution (and it is decrypting only).
UPD: I ended up going thru ansible code to understand how it works with encrypted variables and wrote some tiny script that I can use in my automation jobs with Jenkins. I hope it would be useful for anyone who is waiting for this issue to be fixed.
https://github.com/andrunah/ansible-vault-variable-updater
It would be nice to have this functionality in ansible out of the box.",2018-01-31T18:57:34Z,17752048
3824,ansible/ansible,239227953,370435473,"I do see this in 2.5.

```
root@ubuntu-xenial:~# ansible  --version
ansible 2.5.0rc1 (stable-2.5 36566e62a7) last updated 2018/03/05 13:46:00 (GMT +000)
  config file = /etc/ansible/ansible.cfg
  configured module search path = [u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules']
  ansible python module location = /root/git/ansible/lib/ansible
  executable location = /root/git/ansible/bin/ansible
  python version = 2.7.12 (default, Nov 20 2017, 18:23:56) [GCC 5.4.0 20160609]

root@ubuntu-xenial:~# cat vars.yaml
ansible_ssh_pass: !vault |
          $ANSIBLE_VAULT;1.2;AES256;my_user
          31313064366365626535323066613234626234336664333266663161366233396365633063303539
          3066363333666236666335656631666663373037643338630a303763363031373337663733326134
          38336566366535373561373830386638663635363438333633313536333731646331366138383961
          3331346163623661340a663862323337313562376338386539326438323562383136383832376266
          31306663393532323761353761353435373432633632626365633734303335633436
nonpass: pass
root@ubuntu-xenial:~# ansible-vault view vars.yaml
Vault password:
ERROR! input is not vault encrypted datavars.yaml is not a vault encrypted file for vars.yaml
```",2018-03-05T14:27:09Z,19914339
3825,ansible/ansible,239227953,370438572,"@jhkrischel This issue is waiting for your response. Please respond or the issue will be closed.

[click here for bot help](https://github.com/ansible/ansibullbot/blob/master/ISSUE_HELP.md)
<!--- boilerplate: needs_info_base --->",2018-03-05T14:37:12Z,6585283
3826,ansible/ansible,239227953,371898299,"I poked at this a little yesterday and braindumped some thoughts in code comments at https://github.com/alikins/ansible/commit/603cac4a041a10ec8186617c95ef539a9ece787a

(copied/paraphrased here for discussion)

>         Open a file, figure out if it is all vault or yaml with vault strings, edit.
> 
>         if yaml with vault strings, parse the yaml with AnsibleYaml 
>         and secret. Replace with '!vault-plaintext vault-id' and plaintext. 
>         Save, open editor. 
>         On save/reencrypt, reparse the file with AnsibleYaml, get the
>         plaintext of the to be reencrypted vaulted string, encrypt it
>         (!vault-plaintext -> !vault,
>          AnsibleVaultUnencryptedUnicode -> AnsibleVaultEncryptedUnicode).
> 
>         And then, things get complicated... we can't just AnsibleYaml.dumps()
>        the data structure out:
>             1. Comments and comment placement is not preserved which 
>                 is kind of annoying
>             2. AnsibleYaml can loads things into data structures
>                 that it can not `dumps()` out.
>                 Ie, we can't serialize a bunch of stuff we can deserialize.
> 
>             So just AnsibleYaml.dumps'ing the datastructure back
>             to a file will usually either fail or do the wrong thing.
> 
>             #2 above is unlikely to get fixed soon if ever.
>             #1 is mostly a limitiation of the PyYaml yaml module ansible uses. 
>                Other implementation like Rueyaml can do this, but it is unlikely for 
>                ansible to change this any time soon.
> 
>         So, since we can't just serialize to yaml, we likely need to do some 
>         string manipulation to replace the '!vault ' blob.
> 
>         We would need to know exactly what the before string looked like
>         and where in the file it is, and what the new !vault will look like.
>         But we don't really know what the new  !vault-plaintext 
>         string will look like.
> 
>         For that  matter, we don't know if it will be in the same place,
>         or if it will exist at all, or if it will be at the same path in the 
>         datastructure after the edit. 
> 
>         We could limit edit to only try to work in cases where
>         those aren't changed. We also have no idea what the
>         plaintext will look like.
>         
>        ideas:
>            - !vault-plaintext is a compound yaml type, with fields for
>               the vault id to use, and for the plaintext. Could also
>              possibly include some identifying info for what the !vault
>              it replaced looked like. An example:
> 
>         some_var: !vault-plaintext:
>                     vault_id: 'dev'
>                     decrypted_from: |
>                                     $ANSIBLE_VAULT;1.1;AES256
>                                     66393964663765613335633461643334393234346231666665306635323635333137306339356232
>                     plaintext: |
>                                 The new plaintext to replace decrypted_from with
> 
>         That would give vault-edit enough info to do a reliable job 
>         of replacing the previous content.
> 

       

The downside to that approach is that it points out the limitations of the current !vault format. It may also be useful to extend !vault to support getting a data structure with info in it instead of just the plain text scalar. At the moment, I'm not sure if it could do both but it seems possible.

Or could just call the extended info version of !vault  !vault-extended or similar.  At that point it might be possible to make !vault-extended the default vault blob format for vaulted files as well. ie, instead of
a vaulted file being:

```
$ANSIBLE_VAULT;1.1;AES256
66393964663765613335633461643334393234346231666665306635323635333137306339356232
3533306631646431663239623762366365663137383435380a393139303161383561303336623962
35373663663036333863373666326634616532376335333133326163376136353636633763623739
3736343064326662390a306438356239386665306437646665323836393032393565666136643362
3663
```

It would be yaml something like

``` yaml
--- 
- !vault-extended:
    vault_id: 'dev'
    cipher: AES256
    encrypted_on: 2018-03-14
    ciphertext: |
        $ANSIBLE_VAULT;1.2;AES256;dev
        66393964663765613335633461643334393234346231666665306635323635333137306339356232
                  
```

ie, more or less like https://github.com/voxpupuli/hiera-eyaml",2018-03-09T18:13:35Z,15162
3827,ansible/ansible,239227953,373044736,Would it be possible for the user to tell you which scalars to decrypt - not try to do the whole file?,2018-03-14T14:39:50Z,7386929
3828,ansible/ansible,239227953,380822610,"Right now the usability of encrypted variables compared to whole encrypted files is rather poor unfortunately. Especially in cases where I quickly need to access an encrypted variable (e.g. a password) I really don't want to google for solutions like https://stackoverflow.com/questions/43467180/how-to-decrypt-string-with-ansible-vault-2-3-0.

It is also a problem for `git diff` use cases (https://stackoverflow.com/questions/29937195/how-to-diff-ansible-vault-changes). Is improving this state still on the roadmap? I didn't find it neither for 2.5 nor 2.6...",2018-04-12T14:21:15Z,950561
3829,ansible/ansible,239227953,389723552,"Hi, so I've figured out a way to do this for checking individual values, using a yaml parser, **yq** https://github.com/mikefarah/yq (there's more than one yq project, but I used this). This works with ansible 2.5.2

I have a vars file, with encrypted and unencrypted values, `all.yml`

```
unencrypted_value: 1234
encrypted_value: !vault |
          $ANSIBLE_VAULT;1.1;AES256
          37316535353565313063353530353539666634363834626664366263666538346131653332353932
          3637363030613037316336306466656432353463383230370a396530323164353563363434663238
          30336436396264656663663837346162323762333063376631326633356533376566633563386637
          6531383261396366640a363339616164333630373730613564646434386364396534653063666238
          6131
```

I have a password file, `vault-password`
```
password
```

Using `yq`, I'm able to decrypt the value pretty easily, by selecting the encrypted value and passing it to the decrypt function

```
$ yq read all.yml encrypted_value | ansible-vault --vault-id vault-password decrypt
Decryption successful
secretsecret
```

Hope this helps!",2018-05-17T02:20:56Z,10137
3830,ansible/ansible,239227953,389833783,"Thanks, this helps if it is possible to install additional software. I would argue that ansible-vault should also have this functionality built-in.",2018-05-17T11:24:52Z,950561
3831,ansible/ansible,239227953,390048688,Yeah definitely that'd be the best option :),2018-05-17T23:48:43Z,10137
3832,ansible/ansible,239227953,411567862,"Also related, let rekey work on all encrypted variables in a file. There doesn't seem to be a good way to rekey all the encrypted variables, which makes encrypted variables super cumbersome now that we have to rekey (will end up having script this). Even if it just spits it back out to stdout that'd be a huge help instead of modifying the variables in the file directly.",2018-08-08T22:04:22Z,5423285
3833,ansible/ansible,239227953,415326258,"Why is this issue still assigned to the `2.5 milestone` when `ansible 2.5` is already release a long time ago ? See #44556 for outdated milestones.

Please reassign to a current milestone, this is a really missing feature imo (especially the lack of rekeying functionality).",2018-08-23T07:55:52Z,488213
3834,ansible/ansible,239227953,425570600,It would be nice if rekey worked this way as well.  Updating only the encrypted values in a mixed variable file.,2018-09-28T21:25:28Z,112187
3835,ansible/ansible,239227953,454476078,"Just giving another thumbs up on this; something like the `yq` solution above works okay and can be scripted, but having the functionality be part of `ansible-vault` itself would make management and re-keys so much simpler, and require one fewer dependency.",2019-01-15T17:21:38Z,481677
3836,ansible/ansible,239227953,476400031,"A simple but effective solution would be to keep the existing symantecs of ansible vault encrypt, decrypt and view commands, to detect and encrypt and decrypt values of complete files.

For existing users of encrypted files, it would be trivial to convert to the enrypted values.  It could even be considered best practice is to keep encrypted values in files named such as secrets.yml, to make it easier to spot accidently unencrypted secrets.

During the encrypt phase, it would convert any unencrypted values to encrypted values.  This would allow users to very simply add new values just by editing the ""secrets.yml"", test as required, then run the encypt command.   Users would be able to enforce or check encryption by git hooks or similar.
",2019-03-25T22:21:46Z,1361780
3837,ansible/ansible,239227953,476406076,"That solution would be simple, but likely not enough. For example every variable can be encrypted with a different secret/vault identifier. Also encrypted and unencrypted variables can be mixed.

I'd still like to have a way at least to decrypt all variables belonging to a vault ID transparently using `ansible-vault`. Seriously, this is a usability problem since Ansible 2.3! This makes it nearly impossible for me to use vaulted variables, since being able to run `git diff` on changes is important.",2019-03-25T22:44:19Z,950561
3838,ansible/ansible,239227953,499032715,This is still a problem with Ansible 2.8... A solution would be really appreciated!,2019-06-05T10:42:20Z,509689
3839,ansible/ansible,239227953,499063784,"For others looking for a quick solution I created this script: https://gist.github.com/steffann/240d4170e45aa3cf7cf0df5e9beaf0ba

It uses [ruamel.yaml](https://yaml.readthedocs.io/), which preserves ordering, comments etc in the YAML file. Great when depending on decent git diffs etc :)",2019-06-05T12:33:33Z,509689
3840,ansible/ansible,239227953,499095867,maybe a bit unrelated but I like how [sops](https://github.com/mozilla/sops) does it.,2019-06-05T14:01:47Z,488213
3841,ansible/ansible,239227953,500969741,"Running into this issue again and it sucks. Please guys, this issue has been open for almost 2 years now and for people who really use ansible-vault, this is a major pain the butt.",2019-06-11T18:36:23Z,36764136
3842,ansible/ansible,239227953,555283144,"same issue here, we need to unencrypt all values and it is a nightmare, this must be common function",2019-11-19T01:04:53Z,12090299
3843,ansible/ansible,239227953,562088907,+1 for this functionality.,2019-12-05T11:25:00Z,41154665
3844,ansible/ansible,239227953,562146057,"+1, really need it!",2019-12-05T14:13:48Z,50200552
3845,ansible/ansible,239227953,571555780,"I solved this using debug mode. E.g.

`ansible localhost -m debug -a var='myVariable' -e ""@myFile.yml"" --ask-vault-pass`",2020-01-07T11:51:24Z,740323
3846,ansible/ansible,239227953,581893524,+1 it would be very handy!!!,2020-02-04T12:45:01Z,57633461
3847,ansible/ansible,239227953,582865382,"> I solved this using debug mode. E.g.
> 
> `ansible localhost -m debug -a var='myVariable' -e ""@myFile.yml"" --ask-vault-pass`

Works beautifully! No need for `--ask-vault-pass` if you have the password in a file identified by the `ANSIBLE_VAULT_PASSWORD_FILE` environment variable:

https://docs.ansible.com/ansible/latest/reference_appendices/config.html#envvar-ANSIBLE_VAULT_PASSWORD_FILE",2020-02-06T11:39:12Z,4614943
3848,ansible/ansible,239227953,583154947,+1 would be super helpful especially as we transition away from fully encrypted ansible-vault files to just files with encrypted variables (for ease of use/readability/ability to modify etc),2020-02-06T23:06:22Z,14334928
3849,ansible/ansible,239227953,584305782,"A few expansions to @whirlwin's clever workaround:

Instead of a specific var, you can have ansible dump all vars (encrypted and plain) by specifying `-a var=""vars""`. There will be some noise in the result with a few stock vars, but it's a nice way to see everything at once.

You can also specify multiple source files by passing multiple `-e ""@...""` args.

Also, I haven't tested this personally but you can apparently avoid the implicit `localhost` warnings by setting `ANSIBLE_LOCALHOST_WARNING=false` on Ansible 2.6+.

All together:

```
ANSIBLE_LOCALHOST_WARNING=false ansible localhost -m debug -a var=""vars"" \
  -e ""@file1.yml"" -e ""@path/to/file2.yml"" -e ""@path/to/file3.yml""
```

plus `--ask-vault-pass` if needed.

Obviously a built-in solution would be much better, but this makes mixed plain/encrypted var files somewhat workable.",2020-02-10T19:22:57Z,1138892
3850,ansible/ansible,239227953,586923211,+1 for this functionality.,2020-02-17T10:27:30Z,2369602
3851,ansible/ansible,239227953,627925652,"If you just need decryption instead of rekeying, I've written https://github.com/theblazehen/ansible_vault_decrypt_strings",2020-05-13T11:38:46Z,3276410
3852,ansible/ansible,239227953,724188474,+1 for this feature,2020-11-09T18:21:11Z,748448
3853,ansible/ansible,239227953,724191363,"I've locked this to contributors for now. Adding +1 comments is too noisy. For future reference, add a reaction to the issue body, and don't comment.",2020-11-09T18:25:49Z,39340
3854,ansible/ansible,239227953,1163518912,"Take a look at the `vault` and `unvault` Jinja filters that were added recently to core, might get what you need...",2022-06-22T19:28:12Z,6775756
3855,flatpak/xdg-desktop-portal,1027933041,1027933041,"This may seem dangerous but for screenshot applications, this sounds necessary from a design POV. Having a screenshot application which asks twice to screenshot would be quite awkward, I think.

Related : https://gitlab.gnome.org/GNOME/gnome-shell/-/merge_requests/1970 , https://github.com/flameshot-org/flameshot/issues/1910",2021-10-16T02:58:26Z,39467792
3856,flatpak/xdg-desktop-portal,1027933041,944898938,"Is your request something like this?

* Invent some special metadata that can be set on a Flatpak (or Snap?) app, with the semantics ""this app can take screenshots without prompting"". Flatpak (and Snap?) would not do anything differently in setting up sandboxes based on this metadata, it's just a marker.
* Make the screenshot portal look for that metadata in `/.flatpak-info`. If it's present, just take the screenshot without prompting, with parameters (window/whole screen/etc.) controlled by the request.
* Maybe the portal backend is still responsible for visual/audio feedback (screen flash/shutter sound) to make sure the user is aware that a screenshot was taken?
* Ideally, app stores like Flathub limit access to that metadata (more review required), in the same way they ideally would for other ""dangerous"" permissions like `host` filesystem access.

Or a possible alternative would be to do something a bit like #638:

* Have a flag that the screenshot app can set, to say ""I'm always going to need this permission""
* The first time that flag is used, have a prompt with some sort of ""remember this"" option
* Make a note in the permission store that this app is OK to take screenshots any time, or give it a token that can be looked up in the permission store later, or similar

Or a mixture of the two: ignore or reject the flag from the second approach if it's set by an app that doesn't have the permission metadata from the first?",2021-10-16T11:09:12Z,1561141
3857,flatpak/xdg-desktop-portal,1027933041,944926777,"I think it should be strictly easy to control so it shouldn't be metadata (and require something like Flatseal to change).

I think it should be a one-time prompt like there is for the background or camera permission. I suppose it wouldn't be a bad idea to make the permanency of it optional as to make the use case for it more flexible though.",2021-10-16T14:51:26Z,39467792
3858,flatpak/xdg-desktop-portal,1027933041,952953545,"> I think it should be a one-time prompt like there is for the background or camera permission. I suppose it wouldn't be a bad idea to make the permanency of it optional as to make the use case for it more flexible though.

This is my proposal as well, similar to permissions on a mobile OS. 

(I am the maintainer for the screenshot program Flameshot)",2021-10-27T13:52:30Z,46930769
3859,flatpak/xdg-desktop-portal,1027933041,952957260,"> I think it should be a one-time prompt like there is for the background or camera permission. I suppose it wouldn't be a bad idea to make the permanency of it optional as to make the use case for it more flexible though.

Agree with this one. The user experience with pop up dialogs that require additional clicks every time you take a screenshot is just not a way to go.

I'm the developer and maintainer of ksnip.   ",2021-10-27T13:56:35Z,12796236
3860,flatpak/xdg-desktop-portal,1027933041,952980950,"> I think it should be a one-time prompt like there is for the background or camera permission. I suppose it wouldn't be a bad idea to make the permanency of it optional as to make the use case for it more flexible though.

Giving the user the _option_ to grant the application permission to take future screenshots/screencasts without further user permission would be OK. We just don't want the application to be able to give itself this permission, or to be able to force the user to grant this permission in order to use it just once.

So let's forget about additional metadata. I would retitle this issue from ""Screenshot portal without prompt"" to ""Screenshot portal should have toggle for user to disable future prompts for this application.""",2021-10-27T14:21:24Z,1424966
3861,flatpak/xdg-desktop-portal,1027933041,952999493,"As part of such change it would be useful to pass the type of screenshot that should be taken. The prompt that opens up is not just a permission thing but also a selection of type of screenshot (and if the cursor should be included eventually). 
If the user for example first asks for Active Window screenshot and confirms that he don't want to be asked again, what happens when the user requests another screenshot and expects to get a fullscreen screenshot now. Currently there is just an option to ask for scrrenshot but not what kind of screenshot. ",2021-10-27T14:41:32Z,12796236
3862,flatpak/xdg-desktop-portal,1027933041,953019834,"OK, makes sense. So: add API for application to choose the type of screenshot that should be taken, only show the option to permanently grant permission to take screenshots if the new API is used.

That wouldn't apply the same to screencasts, but those are a separate portal.",2021-10-27T15:03:08Z,1424966
3863,flatpak/xdg-desktop-portal,1027933041,953032059,"Yeah, I think just a dialog that asks for that permissions with an option to make it permanent would in that case be the best, without any other selection option. Bonus points for additional parameter in the API call like ""Include cursor"" and ""Include window decoration"".

> That wouldn't apply the same to screencasts, but those are a separate portal.

Yes, I think it was a different API but I can imagine that they have similar issues if they haven't been fixed already. ",2021-10-27T15:15:52Z,12796236
3864,flatpak/xdg-desktop-portal,1027933041,972386355,"I hope you only have to give permission one time, and then from there it can do it every time. This would be more secure than the old method, but less annoying then the current.",2021-11-18T00:56:01Z,702330
3865,flatpak/xdg-desktop-portal,1027933041,974775081,"Can I express how powerless this issue makes me feel in relation to Gnome development. The tone from the Gnome folks here: https://gitlab.gnome.org/GNOME/gnome-shell/-/merge_requests/1970 indicates to me this issue will not be resolved/is not considered an issue. Creating a Gnome related bug report is akin to shouting into the void, isn't it?",2021-11-21T08:28:16Z,4091936
3866,flatpak/xdg-desktop-portal,1027933041,974794674,"They don't see the issue there but it's probably the same folks that doesn't take much screenshots so they don't feel the pain. As long as they're not made aware about the user frustration coming from this, they won't fix it I'm afraid. ",2021-11-21T10:59:02Z,12796236
3867,flatpak/xdg-desktop-portal,1027933041,974830861,"> Can I express how powerless this issue makes me feel in relation to Gnome development. The tone from the Gnome folks here: https://gitlab.gnome.org/GNOME/gnome-shell/-/merge_requests/1970 indicates to me this issue will not be resolved/is not considered an issue. Creating a Gnome related bug report is akin to shouting into the void, isn't it?

Honestly, your expectations are misplaced. Bypassing the screenshot portal is unacceptable, as it defeats the point of having the portal in the first place. Applications should not be able to screenshot your desktop without your permission. Removing the backdoor should not be controversial.

If you read up in this issue, we already have agreement on the path forward to enhance the screenshot portal. It's just waiting for a motivated developer to tackle it.",2021-11-21T14:51:41Z,1424966
3868,flatpak/xdg-desktop-portal,1027933041,974832739,"@kurobeats @DamirPorobic
The Gnome developers made this decision (don't allow external apps to use gnome's private API) for protecting the privacy of users and forbidding API abuse. They are also doing a redesign for the screenshot UI of gnome: https://gitlab.gnome.org/GNOME/gnome-shell/-/merge_requests/1954 

The Words accusing the developers of not caring about something are not true, and totally unconstructive.",2021-11-21T15:03:14Z,24508452
3869,flatpak/xdg-desktop-portal,1027933041,974836178,"@VitalyAnkh I've spoken with Gnome developers (also same with KDE developers but they haven't disabled the private dbus yet) on few occasions about this topic, on tickets and on IRC and my impression was that this is for them a minor inconvenience. One of them told me even that this is a fair trad off, few clicks more but you get more security. So I don't see it as ""not true"" and even the ""totally unconstructive"" is debatable, user requirement sets the prio for features, if no one speaks up the developers that don't use this feature won't get the user pain. 

Not sure what the screenshot UI redesign to do with this issue, our problem is that we need to give permission for every screenshot instead of once like most people are used to like from mobile phone apps. 

@mcatanzaro is right, there is a suggestions that, when implemented, would fix our issues, I don't see any further discussion here. ",2021-11-21T15:19:54Z,12796236
3870,flatpak/xdg-desktop-portal,1027933041,974918665,"> Honestly, your expectations are misplaced. 

Maybe so but the action taken has now created a negative user experience. Forgetting my comments, I'm a voice in the crowd, think about the ""regular user"", they are going to see this behaviour and be confused by it because it's not something they have come to expect.

>Bypassing the screenshot portal is unacceptable, as it defeats the point of having the portal in the first place. Applications should not be able to screenshot your desktop without your permission. Removing the backdoor should not be controversial.

Let's keep in mind we are talking about a screenshotting tool here not a trojan (which I imagine we are trying to defend against here). Flameshot, for example, doesn't have to overcome hurdles to screenshot on Windows or MacOS.

> If you read up in this issue, we already have agreement on the path forward to enhance the screenshot portal. It's just waiting for a motivated developer to tackle it.

I'm very happy to hear it, but wouldn't it have been pragmatic to implement the solution before we do unexpected things to the user base?
",2021-11-21T23:24:24Z,4091936
3871,flatpak/xdg-desktop-portal,1027933041,974923201,"Author of flameshot here, actually on MacOS you do have to grant a one time permission to record the the screen.

While I was annoyed this changed in gnome before we adjust the portal to only ask a single time, overall this will be a great compromise between ease of use and security. It also will be exactly the same as MacOS. I think my users will also find the one time prompt acceptable.",2021-11-21T23:51:50Z,46930769
3872,flatpak/xdg-desktop-portal,1027933041,975460881,"> Yeah, I think just a dialog that asks for that permissions with an option to make it permanent would in that case be the best, without any other selection option. Bonus points for additional parameter in the API call like ""Include cursor"" and ""Include window decoration"".
> 
> > That wouldn't apply the same to screencasts, but those are a separate portal.
> 
> Yes, I think it was a different API but I can imagine that they have similar issues if they haven't been fixed already.

Yeah I fully agree!!
I understand the security concerns from the Gnome team, but it's also about giving users options. Give users the ability to give certain applications permission to do this and remember their choice.

Certain programs like Screenshot apps and screen capture/recording programs need this to work. And giving those programs permission every time gets tiring and not user friendly. (from an UX point of view)",2021-11-22T12:14:23Z,980950
3873,flatpak/xdg-desktop-portal,1027933041,985498575,"I'm rather amazed that people invoke ""security"" as a reason while undermining security by not thinking things through.

When a security feature is very annoying or even breaks software, it becomes an anti-feature. For example, when Telegram Desktop was failing to read files I was drag and dropping into it because the portal wasn't smart enough to see that drag and drop should give permissions for that file, I installed Flatseal and gave Telegram Desktop permissions to all files. Where's the security in that?

Any security features that are sufficiently irritating become just yet another annoying thing to turn off. They not only provide zero security, but are also an added annoyance.

While I appreciate the concern and I also appreciate that a decision was reached to implement the ""remember this obvious choice"" feature, I am disappointed that usability and actual security are not priorities and they are just an afterthought after users complain.

I would hope this is implemented and more care is taken in the future when ""security"" features are implemented, because for now I've gotten used to avoid Flatpaks in order to have actually functioning applications, and I'd love for that to change in the future.",2021-12-03T12:59:19Z,9626761
3874,flatpak/xdg-desktop-portal,1027933041,985564408,"@dancojocaru2000 IIRC that is not related to a security feature, it's missing functionality in the application and it needs to add support for the file transfer portal. Please follow the discussion here: https://github.com/flatpak/xdg-desktop-portal/issues/99

Edit: According to this issue the missing functionality was in Electron: https://github.com/flathub/org.telegram.desktop/issues/23",2021-12-03T14:28:33Z,93952137
3875,flatpak/xdg-desktop-portal,1027933041,988331863,"Hi Team,
Can I request if this issue is being addressed? Totally fine if it isn't deemed an issue, I can revert to Xorg.",2021-12-07T23:16:01Z,4091936
3876,flatpak/xdg-desktop-portal,1027933041,1000877073,Any news? This is not normal. Workflow can't be damaged this way..,2021-12-24T15:33:32Z,20903913
3877,flatpak/xdg-desktop-portal,1027933041,1000910900,"Very frustrating, please give a user an option not to see the window every time, the great thing about Linux is flexibility and I would not like to see it less flexible than e.g. macOS.",2021-12-24T18:09:40Z,7964831
3878,flatpak/xdg-desktop-portal,1027933041,1001132853,"I reported again to gnome upstream: https://gitlab.gnome.org/GNOME/gnome-shell/-/merge_requests/1970#note_1341047
and here: https://gitlab.gnome.org/GNOME/gnome-shell/-/issues/4895",2021-12-26T09:03:11Z,20903913
3879,flatpak/xdg-desktop-portal,1027933041,1001295912,"@All3xJ members of the team are claiming criticism of the bug they introduced as harassment (or something close to: https://gitlab.gnome.org/GNOME/gnome-shell/-/issues/4895#note_1341442). I think it'll be best if people affected by this issue move to Xorg (it still works perfectly well!) or to a user friendly desktop environment (I've moved to Plasma), this bug isn't actively being worked on.",2021-12-27T02:24:13Z,4091936
3880,flatpak/xdg-desktop-portal,1027933041,1001311899,"There are **no objections** to having a one-time prompt for the portal; the other issues being linked are related to internal GNOME APIs being used for this, which was never correct in the first place and is an entirely separate API.

Please avoid the ""any news on this"" comments, they do nothing but clutter the thread. The issue has also been open for less than three months and will likely end up involving volunteer work contributed in developers' free time...so pardon the bluntness, but I'm pretty sure more time and energy has already been spent on this thread than saved by not having to perform a click.",2021-12-27T03:24:38Z,1690697
3881,flatpak/xdg-desktop-portal,1027933041,1001517600,"Temporarily locking due to too low signal-to-noise ratio. I'd encourage anyone who wishes to see this feature to go ahead and implement it in xdg-desktop-portal, and at least one frontend (GNOME, KDE, or wlroots).",2021-12-27T11:20:17Z,3518204
3882,flatpak/xdg-desktop-portal,1027933041,1209495486,"https://github.com/flatpak/xdg-desktop-portal/pull/853 exists, now someone with proper authority have to review and eventually merge that",2022-08-09T14:58:59Z,3518204
3883,flatpak/xdg-desktop-portal,1027933041,1249045592,any news? please give user a chance to setup this feature,2022-09-16T08:00:02Z,6695246
3884,flatpak/xdg-desktop-portal,1027933041,1249064370,"> any news? please give user a chance to setup this feature

This has already been added.",2022-09-16T08:19:27Z,46741
3885,flatpak/xdg-desktop-portal,1027933041,1249081784,"> > any news? please give user a chance to setup this feature
> 
> This has already been added.

ok, how to keep permission to share screenshots to some application?

now I can not see this option (flatseal for managing permissions in flatpak apps)
![image](https://user-images.githubusercontent.com/6695246/190594258-7c50337b-8548-4778-99dc-dda4809f698c.png)
",2022-09-16T08:33:44Z,6695246
3886,flatpak/xdg-desktop-portal,1027933041,1260049291,"Edit: I understand what was meant in the comments above now. (Stupid me!) The fix for this has already been processed and was pushed through GNOME 43. You can find out which version of GNOME you're using with `gnome-shell --version`. BTW, if you are running Fedora, it will be available with Fedora 37, release date is quoted ""around 25 OCT 2022"".

@jadahl ~~If you happen to have info on where/how we can set this, please reply.~~ I have run into this issue recently as well, and had initially assumed there was an issue with flameshot, since I had never used it before. Realizing that this is a permissions thing, that I need to permit each time I launch the program, is getting very tedious to deal with even if the program itself is _considerably_ better than the default screenshot program.

@axel-n How did you go about getting all of those permissions listed in Settings? My own listing looks like this
![application_list](https://user-images.githubusercontent.com/5470143/192634605-c47ecbdd-10bf-40b4-8aa7-e35bfb1b66af.png)
",2022-09-27T21:01:29Z,5470143
3887,flatpak/xdg-desktop-portal,1027933041,1260053104,"> @axel-n How did you go about getting all of those permissions listed in Settings? My own listing looks like this
> ![application_list](https://user-images.githubusercontent.com/5470143/192634605-c47ecbdd-10bf-40b4-8aa7-e35bfb1b66af.png)
> 

@michael-hart-github 
That's [Flatseal](https://flathub.org/apps/details/com.github.tchx84.Flatseal), not Gnome Settings.",2022-09-27T21:05:21Z,1786609
3888,flatpak/xdg-desktop-portal,1027933041,1286656450,"I'm sorry if I missed something, but I wasn't able to find any guidence on how to update xdg-desktop-portal in order for gnome not to ask if I want to share my screenshot with Flameshot every time. Could you please point me to how I can do this? I'm using ubuntu 22.04 with gnome 42.",2022-10-21T08:47:18Z,56092540
3889,flatpak/xdg-desktop-portal,1027933041,1286936725,"@andreyizrailev I think the easiest solution would be for you to upgrade to 22.10, which has this feature out of the box. And the new Gnome in that version is quite nice too, so I'd say it's worth the upgrade.",2022-10-21T13:05:47Z,629806
3890,flatpak/xdg-desktop-portal,1027933041,1287127317,@gpothier that is very unfortunate that I have to choose between the long term support and a convenient way of making screenshots. But thank you for the answer!,2022-10-21T15:32:16Z,56092540
3891,flatpak/xdg-desktop-portal,1027933041,1289257495,"btw, how fast is the portal api? Can it do 30+FPS? Currently I have X11 python application using mss.grab() which takes screenshots on desktop regions 30fps and calculates averages which controls ambient led-strip around the monitor in real time. So I would need to be able to do the same in Wayland. Or is there no way to use Wayland because of security restrictions?",2022-10-24T16:02:42Z,229321
3892,flatpak/xdg-desktop-portal,1027933041,1289262582,"> btw, how fast is the portal api? Can it do 30+FPS?

You shouldn't use the screenshot portal to make screencasts, there is the screenast portal for that.",2022-10-24T16:06:30Z,46741
3893,flatpak/xdg-desktop-portal,1027933041,1289277288,"> > btw, how fast is the portal api? Can it do 30+FPS?
> 
> You shouldn't use the screenshot portal to make screencasts, there is the screenast portal for that.

I'm currently capturing bbox regions from 4k desktop (for performance reasons). Is screencast portal able to provide casts from partial desktop regions? Have to start searching for API documentation. Does screencast portal api have setting for not asking end user permissions (as target machine is running in kiosk mode without keyboard and mouse).",2022-10-24T16:18:18Z,229321
3894,flatpak/xdg-desktop-portal,1027933041,1289307667,"> Is screencast portal able to provide casts from partial desktop regions?

No, but can in theory be added.

> Does screencast portal api have setting for not asking end user permissions

Yes.",2022-10-24T16:43:01Z,46741
3895,flatpak/xdg-desktop-portal,1027933041,1300955742,"Thanks a lot, devs! :D expecially @GeorgesStavracas ",2022-11-02T17:18:02Z,20903913
3896,flatpak/xdg-desktop-portal,1027933041,1302728664,Why is this closed? The issue is still there.,2022-11-03T22:06:47Z,9994511
3897,flatpak/xdg-desktop-portal,1027933041,1302730680,This issue was fixed by c8274173f7d127f1d7e39e8b5bcaf7f0ee751f48.,2022-11-03T22:09:12Z,1424966
3898,flatpak/xdg-desktop-portal,1027933041,1518119807,"For people who are a bit newer to Linux - 

Note that Ubuntu releases are the year and date, so Ubuntu 22.04 was released 2022 on the 4th month.

Ubuntu 22.04 LTS comes with GNOME 42.5 (you can verify this yourself by running `gnome-shell --version`), and it's extremely unlikely to receive an official update to GNOME 43. LTS releases happen every two years, so the next one will be Ubuntu 24.04 LTS, which will include GNOME 43 or a newer version. So right now, for Ubuntu LTS you can expect this to be fixed in about a year from this posting, in 2024 on the 4th month.

If you want GNOME 43 sooner, you can upgrade to Ubuntu 22.10 now or newer versions when they're released. None of these will be considered LTS releases though and non-LTS releases have a shorter support period of 9 months. This means you'll need to upgrade more frequently to stay on a supported version.

Another option is to install GNOME 43 from a third-party repository (like a PPA) or build it from source. This method can be risky, as it might cause compatibility issues or system instability. If you choose to try to do this, you should really consider backing up your data and be ready to troubleshoot potential issues. Although, if you had to read this to understand the situation, I don't really suggest this option.",2023-04-21T17:16:32Z,85081861
3899,opnsense/core,1092238495,1092238495,"I upgraded to 22.1 and radvd no longer starts up.

The web UI mentioned an error..

PHP Warning:  Invalid argument supplied for foreach() in /usr/local/www/services_router_advertisements.php on line 334

OPNsense 22.1.b_141-amd64",2022-01-03T05:31:54Z,1129902
3900,opnsense/core,1092238495,1003886480,"
Thank you for creating an issue.
Since the ticket doesn't seem to be using one of our templates, we're marking this issue as low priority until further notice.

For more information about the policies for this repository,
please read https://github.com/opnsense/core/blob/master/CONTRIBUTING.md for further details.

The easiest option to gain traction is to close this ticket and open a new one using one of our templates.
",2022-01-03T06:05:06Z,76789482
3901,opnsense/core,1092238495,1003913193,"@brad0 the error should be in the system log:

    # opnsense-log | grep radvd

I don't think the PHP warning is related just yet.


Cheers,
Franco",2022-01-03T07:33:56Z,1915288
3902,opnsense/core,1092238495,1004480741,It doesn't show anything. Anything else to check?,2022-01-04T02:20:19Z,1129902
3903,opnsense/core,1092238495,1004527036,@brad0 Can you try: `opnsense-log routing`,2022-01-04T04:50:11Z,7823088
3904,opnsense/core,1092238495,1005258633,"```
version 2.19 started
warning: AdvRDNSSLifetime <= 2*MaxRtrAdvInterval would allow stale DNS servers to be deleted faster
warning: (/usr/local/etc/radvd.conf:111) AdvRDNSSLifetime <= 2*MaxRtrAdvInterval would allow stale DNS servers to be deleted faster
warning: AdvDNSSLLifetime <= 2*MaxRtrAdvInterval would allow stale DNS suffixes to be deleted faster
lo not found: Device not configured
lo not found: Device not configured
exiting, 1 sigterm(s) received
sending stop adverts
lo not found: Device not configured
removing /var/run/radvd.pid
returning from radvd main
```",2022-01-04T23:48:23Z,1129902
3905,opnsense/core,1092238495,1005429162,Tracking WAN on a manual loopback device? I've seen before this doesn't work.,2022-01-05T06:57:28Z,1915288
3906,opnsense/core,1092238495,1006001008,"> Tracking WAN on a manual loopback device? I've seen before this doesn't work.

I have not knowingly configured anything regarding the loopback interface. All I did configure was the VLAN interfaces and that's it.",2022-01-05T19:07:44Z,1129902
3907,opnsense/core,1092238495,1006004542,"Well, in any case this is not normal so at least for community support I’d take one more look at the following:

    # ifconfig
    # cat /var/etc/radvd.conf


Cheers,
Franco",2022-01-05T19:13:01Z,1915288
3908,opnsense/core,1092238495,1006162537,"```
root@inet-fw:~ # ifconfig -a
em0: flags=8822<BROADCAST,SIMPLEX,MULTICAST> metric 0 mtu 1500
        options=481249b<RXCSUM,TXCSUM,VLAN_MTU,VLAN_HWTAGGING,VLAN_HWCSUM,LRO,WOL_MAGIC,VLAN_HWFILTER,NOMAP>
        ether 18:03:73:31:54:49
        media: Ethernet autoselect
        status: no carrier
        nd6 options=29<PERFORMNUD,IFDISABLED,AUTO_LINKLOCAL>
ix0: flags=8863<UP,BROADCAST,RUNNING,SIMPLEX,MULTICAST> metric 0 mtu 1500
        description: IX
        options=4e538bb<RXCSUM,TXCSUM,VLAN_MTU,VLAN_HWTAGGING,JUMBO_MTU,VLAN_HWCSUM,WOL_UCAST,WOL_MCAST,WOL_MAGIC,VLAN_HWFILTER,VLAN_HWTSO,RXCSUM_IPV6,TXCSUM_IPV6,NOMAP>
        ether a0:36:9f:b3:2a:14
        media: Ethernet autoselect (10Gbase-T <full-duplex>)
        status: active
        nd6 options=29<PERFORMNUD,IFDISABLED,AUTO_LINKLOCAL>
ix1: flags=8963<UP,BROADCAST,RUNNING,PROMISC,SIMPLEX,MULTICAST> metric 0 mtu 1500
        options=4e53fbb<RXCSUM,TXCSUM,VLAN_MTU,VLAN_HWTAGGING,JUMBO_MTU,VLAN_HWCSUM,TSO4,TSO6,LRO,WOL_UCAST,WOL_MCAST,WOL_MAGIC,VLAN_HWFILTER,VLAN_HWTSO,RXCSUM_IPV6,TXCSUM_IPV6,NOMAP>
        ether a0:36:9f:b3:2a:15
        media: Ethernet autoselect (10Gbase-T <full-duplex>)
        status: active
        nd6 options=29<PERFORMNUD,IFDISABLED,AUTO_LINKLOCAL>
lo0: flags=8049<UP,LOOPBACK,RUNNING,MULTICAST> metric 0 mtu 16384
        options=680003<RXCSUM,TXCSUM,LINKSTATE,RXCSUM_IPV6,TXCSUM_IPV6>
        inet6 ::1 prefixlen 128
        inet6 fe80::1%lo0 prefixlen 64 scopeid 0x4
        inet 127.0.0.1 netmask 0xff000000
        groups: lo
        nd6 options=21<PERFORMNUD,AUTO_LINKLOCAL>
enc0: flags=41<UP,RUNNING> metric 0 mtu 1536
        groups: enc
        nd6 options=29<PERFORMNUD,IFDISABLED,AUTO_LINKLOCAL>
pflog0: flags=20100<PROMISC,PPROMISC> metric 0 mtu 33160
        groups: pflog
pfsync0: flags=0<> metric 0 mtu 1500
        groups: pfsync
ix1_vlan3: flags=8943<UP,BROADCAST,RUNNING,PROMISC,SIMPLEX,MULTICAST> metric 0 mtu 1500
        description: MAIN
        options=4600703<RXCSUM,TXCSUM,TSO4,TSO6,LRO,RXCSUM_IPV6,TXCSUM_IPV6,NOMAP>
        ether a0:36:9f:b3:2a:15
        inet 192.168.3.2 netmask 0xffffff00 broadcast 192.168.3.255
        inet 192.168.3.1 netmask 0xffffffff broadcast 192.168.3.1 vhid 3
        inet6 2001:470:b050:3::2 prefixlen 64
        inet6 fe80::a236:9fff:feb3:2a15%ix1_vlan3 prefixlen 64 scopeid 0x8
        groups: vlan
        carp: MASTER vhid 3 advbase 1 advskew 0
        vlan: 3 vlanproto: 802.1q vlanpcp: 0 parent interface: ix1
        media: Ethernet autoselect (10Gbase-T <full-duplex>)
        status: active
        nd6 options=21<PERFORMNUD,AUTO_LINKLOCAL>
ix1_vlan4: flags=8943<UP,BROADCAST,RUNNING,PROMISC,SIMPLEX,MULTICAST> metric 0 mtu 1500
        description: JUMBO
        options=4600703<RXCSUM,TXCSUM,TSO4,TSO6,LRO,RXCSUM_IPV6,TXCSUM_IPV6,NOMAP>
        ether a0:36:9f:b3:2a:15
        inet 192.168.4.2 netmask 0xffffff00 broadcast 192.168.4.255
        inet 192.168.4.1 netmask 0xffffffff broadcast 192.168.4.1 vhid 4
        inet6 2001:470:b050:4::2 prefixlen 64
        inet6 fe80::a236:9fff:feb3:2a15%ix1_vlan4 prefixlen 64 scopeid 0x9
        groups: vlan
        carp: MASTER vhid 4 advbase 1 advskew 0
        vlan: 4 vlanproto: 802.1q vlanpcp: 0 parent interface: ix1
        media: Ethernet autoselect (10Gbase-T <full-duplex>)
        status: active
        nd6 options=21<PERFORMNUD,AUTO_LINKLOCAL>
ix1_vlan5: flags=8943<UP,BROADCAST,RUNNING,PROMISC,SIMPLEX,MULTICAST> metric 0 mtu 1500
        description: PUBLICWIFI
        options=4600703<RXCSUM,TXCSUM,TSO4,TSO6,LRO,RXCSUM_IPV6,TXCSUM_IPV6,NOMAP>
        ether a0:36:9f:b3:2a:15
        inet 192.168.5.2 netmask 0xffffff00 broadcast 192.168.5.255
        inet 192.168.5.1 netmask 0xffffffff broadcast 192.168.5.1 vhid 5
        inet6 2001:470:b050:5::2 prefixlen 64
        inet6 fe80::a236:9fff:feb3:2a15%ix1_vlan5 prefixlen 64 scopeid 0xa
        groups: vlan
        carp: MASTER vhid 5 advbase 1 advskew 0
        vlan: 5 vlanproto: 802.1q vlanpcp: 0 parent interface: ix1
        media: Ethernet autoselect (10Gbase-T <full-duplex>)
        status: active
        nd6 options=21<PERFORMNUD,AUTO_LINKLOCAL>
ix1_vlan6: flags=8943<UP,BROADCAST,RUNNING,PROMISC,SIMPLEX,MULTICAST> metric 0 mtu 1500
        description: DYNACERT
        options=4600703<RXCSUM,TXCSUM,TSO4,TSO6,LRO,RXCSUM_IPV6,TXCSUM_IPV6,NOMAP>
        ether a0:36:9f:b3:2a:15
        inet 192.168.6.2 netmask 0xffffff00 broadcast 192.168.6.255
        inet 192.168.6.1 netmask 0xffffffff broadcast 192.168.6.1 vhid 6
        inet6 2001:470:b050:6::2 prefixlen 64
        inet6 fe80::a236:9fff:feb3:2a15%ix1_vlan6 prefixlen 64 scopeid 0xb
        groups: vlan
        carp: MASTER vhid 6 advbase 1 advskew 0
        vlan: 6 vlanproto: 802.1q vlanpcp: 0 parent interface: ix1
        media: Ethernet autoselect (10Gbase-T <full-duplex>)
        status: active
        nd6 options=21<PERFORMNUD,AUTO_LINKLOCAL>
ovpns2: flags=8010<POINTOPOINT,MULTICAST> metric 0 mtu 1500
        options=80000<LINKSTATE>
        groups: tun openvpn
        nd6 options=29<PERFORMNUD,IFDISABLED,AUTO_LINKLOCAL>
ovpnc1: flags=8051<UP,POINTOPOINT,RUNNING,MULTICAST> metric 0 mtu 1500
        options=80000<LINKSTATE>
        inet 192.168.180.2 --> 192.168.180.1 netmask 0xffffff00
        inet6 2001:470:b0db:180::1000 prefixlen 64
        inet6 fe80::1a03:73ff:fe31:5449%ovpnc1 prefixlen 64 scopeid 0xd
        groups: tun openvpn
        nd6 options=21<PERFORMNUD,AUTO_LINKLOCAL>
        Opened by PID 63830
pppoe0: flags=88d1<UP,POINTOPOINT,RUNNING,NOARP,SIMPLEX,MULTICAST> metric 0 mtu 1492
        description: WAN
        inet 142.114.5.252 --> 10.11.5.161 netmask 0xffffffff
        nd6 options=29<PERFORMNUD,IFDISABLED,AUTO_LINKLOCAL>
gif0: flags=8051<UP,POINTOPOINT,RUNNING,MULTICAST> metric 0 mtu 1440
        description: HENETV6
        options=80000<LINKSTATE>
        tunnel inet 142.114.5.252 --> 216.66.38.58
        inet6 2001:470:1c:70::2 --> 2001:470:1c:70::1 prefixlen 128
        inet6 fe80::1a03:73ff:fe31:5449%gif0 prefixlen 64 scopeid 0xf
        groups: gif
        nd6 options=21<PERFORMNUD,AUTO_LINKLOCAL>
```
```
root@inet-fw:~ # cat /var/etc/radvd.conf
# Automatically generated, do not edit
```",2022-01-05T23:39:32Z,1129902
3909,opnsense/core,1092238495,1006485254,"Not sure what is going on, but radvd is empty so maybe no tracking enabled? It should have said in the initial post.",2022-01-06T11:09:02Z,1915288
3910,opnsense/core,1092238495,1006565001,"I find it strange that the log messages reference the example configuration file with line number: `/usr/local/etc/radvd.conf:111`

Which, this file includes exclusively a setting for loopback:
```
interface lo
{
        AdvSendAdvert on;
```
I'm not sure why radvd would be using that file over the one in /var/etc/ though, as it's hard coded to use the /var/etc conf:

https://github.com/opnsense/core/blob/master/src/etc/inc/plugins.inc.d/dhcpd.inc#L567
`        mwexec('/usr/local/sbin/radvd -p /var/run/radvd.pid -C /var/etc/radvd.conf -m syslog');`

Maybe a red herring, unless there is another means to starting it somewhere else.",2022-01-06T12:53:06Z,7823088
3911,opnsense/core,1092238495,1006587518,"If it goes to the default if the given config is empty that could explain it, but it’s really not that relevant in the grand scheme of things. I can’t see what is broken that should work (configuration issue likely).",2022-01-06T13:21:21Z,1915288
3912,opnsense/core,1092238495,1006590077,"I was able to reproduce those messages by manually starting radvd with `/usr/local/etc/rc.d/radvd onestart`

```
radvd[5628]: version 2.19 started 
radvd[5628]: warning: AdvRDNSSLifetime <= 2*MaxRtrAdvInterval would allow stale DNS servers to be deleted faster 
radvd[5628]: warning: (/usr/local/etc/radvd.conf:111) AdvRDNSSLifetime <= 2*MaxRtrAdvInterval would allow stale DNS servers to be deleted faster 
radvd[5628]: warning: AdvDNSSLLifetime <= 2*MaxRtrAdvInterval would allow stale DNS suffixes to be deleted faster 
radvd[19904]: lo not found: Device not configured 
```

`/usr/local/etc/rc.d/radvd` references that config file:
```
load_rc_config $name
: ${radvd_enable=""NO""}
: ${radvd_config=""/usr/local/etc/${name}.conf""}
```

I think somehow the `radvd_enable=""YES""` got added to the rc conf, and it's trying to start through rc.

I did a quick grep through the source and I don't see a provision for setting that to ""YES"", and on my system where radvd is in use, radvd_enable isn't put into the rc.conf system.",2022-01-06T13:24:55Z,7823088
3913,opnsense/core,1092238495,1006598535,"Yep, legacy service integration does not use rc.d… we only started using it after forking.",2022-01-06T13:37:27Z,1915288
3914,opnsense/core,1092238495,1008217928,So what do I need to do to fix this?,2022-01-09T02:52:56Z,1129902
3915,opnsense/core,1092238495,1008245837,"I’m unsure what the goalpost is. You keep giving no further information on what you actually expect other than radvd starting, which is irrelevant without configuration and ISP considerations.",2022-01-09T07:37:19Z,1915288
3916,opnsense/core,1092238495,1008254971,"radvd starts and runs, like it did (without changing anything) before updating from 21.7 to 22.1.

My setup is very simple and straightforward. It's using a 6in4 tunnel with static IPs on each VLAN interface.

![Screenshot_4](https://user-images.githubusercontent.com/1129902/148674407-b8e6a79d-f5a8-4593-9c01-2fb7f260023c.jpg)

![Screenshot_5](https://user-images.githubusercontent.com/1129902/148674411-52439f3f-a1a2-4ed0-be1d-aad973435d86.jpg)

```
root@inet-fw:~ # pluginctl -s radvd start
Service `radvd' has been started.
root@inet-fw:~ # ps -auxwww | grep radvd
root    25618   0.0  0.1  12740  2204  0  S+   03:37       0:00.00 grep radvd
```

I don't see a verbose flag for pluginctl. How do you see what is going on?",2022-01-09T08:44:12Z,1129902
3917,opnsense/core,1092238495,1008262032,"Radvd only starts when you configure tracking Interfaces, but then you need a prefix lager than /64 anyway on your tunnel. Despite radvd not starting what sort of functionality have you lost from radvd not starting?",2022-01-09T09:29:08Z,1915288
3918,opnsense/core,1092238495,1008269974,"That makes absolutely no sense. Who broke things?

A network that doesn't work. Even if I didn't care about SLAAC (which I do) RA is required.",2022-01-09T10:22:45Z,1129902
3919,opnsense/core,1092238495,1008273422,I’m sorry to say this is a waste of both of our time.,2022-01-09T10:47:25Z,1915288
3920,opnsense/core,1092238495,1024960967,"I had the same issue of radvd not starting up after the update from 21.7.8 to 22.1.
I have multiple interfaces configured with static IPv6 + DHCP6 + managed RA, so in my understanding it definitely needs to be run.
On the old version it was still running, after the update not running anymore.
On the old version, DHCP6 was still working, after the update DHCP6 and therefore IPv6 on these managed networks was completely broken (apart from the statically configured IP6s).
Can't say anything about SLAAC since we don't use it on any network.
Trying to start radvd manually didn't help.
Log entries nowhere to be found unfortunately.
Curiously, `/var/etc/radvd.conf` was completely empty which can't be correct.

I've found a workaround though, which I wanted to share here:
If you go to **Services -> Router Advertisements -> Some Interface** and just click on save, the config will be rewritten and radvd will start up again.
In my case, this fixed the DHCP6 problems.
I did the save thing for any interface that was shown in the RA menu there to be sure.
The fix seems to persist across reboots as well, so I think it only needs to be applied once after the update to fix things.

So I guess this is some kind of problem where the config gets lost during the upgrade and doesn't get rewritten afterwards.
And btw:

> Radvd only starts when you configure tracking Interfaces

Either I'm misunderstanding something or this claim is false, since I don't have any tracking interfaces (or is DHCP6 only managed interface also tracking?) and still radvd is needed, so it needs to be started (and did start in prior versions, and now starts again after applying the workaround).",2022-01-29T18:15:09Z,70420366
3921,opnsense/core,1092238495,1028607075,"> I’m sorry to say this is a waste of both of our time.

@fichtner: I’m sorry to say this is a waste of my time... and many more people who have broken IPv6 after update, because you released broken version, while this was known issue for a month. Sorry, but I simply don't understand your dismissive attitude towards @brad0. It is not his job to debug and fix the issue.

Anyway, back to the issue. `/var/etc/radvd.conf` is cleared during the update, even tho in the web gui configuration looks ok, one need to change something, save and change back to regenerate `radvd.conf`. And @megmug seems to confirm that. Once you start digging it is easy to fix, but I guess you can agree that desynchronized settings in gui/system is not something that is immediately seen as a solution. And not everyone like @brad0 have to know how to fix that.

> That makes absolutely no sense. Who broke things?

@brad0: They did... You did nothing wrong. Upgrade process broke your settings.",2022-02-03T05:06:19Z,1126053
3922,opnsense/core,1092238495,1028611336,Closing for heated discussion.,2022-02-03T05:16:32Z,1915288
3923,nodejs/node,638202113,638202113,"**Is your feature request related to a problem? Please describe.**
Lot of software developers are discussing on twitter to rename default branches for their projects from ""master"" to ""main"" or equivalent https://twitter.com/search?q=master%20branch&src=typed_query

The primary reason being master-slave an oppressive metaphor.

**Describe the solution you'd like**
Node.js core follows the trend to change the industry standard, and renames default branch from ""master"" to ""main"" or similar

**Describe alternatives you've considered**
Sticking with existing master branch name for the default

EDIT: Updated ""renaming master to main"" to ""renaming default branch name from 'master' to 'main'""",2020-06-13T17:41:50Z,16024985
3924,nodejs/node,638202113,643656288,This is not a technically difficult task (https://www.hanselman.com/blog/EasilyRenameYourGitDefaultBranchFromMasterToMain.aspx) but it might break some things. I definitely think we should try to change it.,2020-06-13T17:52:45Z,5952481
3925,nodejs/node,638202113,643661780,Does this mean we would have to change the cluster API (which includes master in its vocabulary) as well in a semver-major?,2020-06-13T18:41:38Z,3065230
3926,nodejs/node,638202113,643663090,"This is getting silly already. No way it makes any sense.

UPD: How is this offtopic? Please stop bringing politics into development world. That's just mindblowingly silly.",2020-06-13T18:52:36Z,7792608
3927,nodejs/node,638202113,643663108,"That is hilarious. From defacing nodejs.org with blm propaganda, to renaming master branch to avoid similarity with master-slave metaphor. What is the next requirement OpenJS will ask for? To remove the **test** directory to avoid similarity with **test**icles? Please keep technical aspects of the software free of politics and globalistic propaganda.",2020-06-13T18:52:43Z,10137
3928,nodejs/node,638202113,643664189,"@ronag i don't think that cluster was brought up in this thread, though it has been discussed on other occasions.",2020-06-13T19:01:09Z,5952481
3929,nodejs/node,638202113,643664819,"I'm -0 to the change itself. I don't think that changing it because it is ""industry standard"" is a valid argument, at least not yet. If we are changing it because we think it is a loaded/inappropriate word, then I think we should remove all occurrences to remain consistent with that decision.",2020-06-13T19:06:17Z,3065230
3930,nodejs/node,638202113,643666482,"I don't feel like this is something people have actually complained about. When we've made these changes in the past (for example in child_worker.suicide) it was guided by someone acting in good faith feeling strongly about the terminology. 

If someone from the project's base does feel strongly about it - I suggest we rename it, though `master` is the default git branch name so I would caution we pick our battles.

I vote we:
 - Don't change the name from master to something else currently.
 - Change it when a contributor/collaborator feels strongly enough about this.

Of course, if you @trivikr personally feel strongly about this then I support you :]",2020-06-13T19:20:15Z,1315533
3931,nodejs/node,638202113,643667302,"> Of course, if you @trivikr personally feel strongly about this then I support you :]

I don't feel strongly against using master branch name. I proposed it as I plan to follow it in my personal projects and work projects, and wanted to ask Node.js community.

Should we add this to tsc-agenda?

> though `master` is the default git branch name so I would caution we pick our battles.

Is there an equivalent ask in git repo?
If not, we should create one.
",2020-06-13T19:28:22Z,16024985
3932,nodejs/node,638202113,643667337,"Also, to all the people making the drive by comments: please keep discussion civil and remember Node.js has a [code of conduct](https://github.com/nodejs/admin/blob/master/CODE_OF_CONDUCT.md). If you can't be polite here you really don't have to comment. It's fine to either support or object to the proposed change (or any proposed change) as long as you are [civil](https://github.com/nodejs/admin/blob/master/CODE_OF_CONDUCT.md#our-standards) - **we do not tolerate abuse** towards the project and its members here.

To collaborators: kind reminder that you are [allowed to ban users](https://github.com/nodejs/admin/blob/master/Moderation-Policy.md#non-collaborator-posts) that make these sort of comments and to hide said comments - but please update the project (as explained there) with what actions you took so that any moderation actions taken are done in full transparency.",2020-06-13T19:28:42Z,1315533
3933,nodejs/node,638202113,643667917,"> Is there an equivalent ask in git repo?

I'm not sure where the git repo is - but I recommend trying the mailing list and asking there https://git-scm.com/community 

I think ""doing whatever git does and bringing the issue to their attention"" is a viable strategy - but again, I don't feel particularly strongly about the use of ""master"" and if someone else does - sure.

> Should we add this to tsc-agenda?

I'm not entirely sure why? ",2020-06-13T19:33:23Z,1315533
3934,nodejs/node,638202113,643668315,"> > Should we add this to tsc-agenda?
>
> I'm not entirely sure why?

To let tsc make a call on this request.

Other option would be to keep this issue open for a week or so, and see if it gathers more feedback or support.",2020-06-13T19:36:54Z,16024985
3935,nodejs/node,638202113,643669163,"> To let tsc make a call on this request.

As far as I understand it that's not how our governance works. Any collaborator may add the `tsc-agenda` label for an issue so it gets TSC eyes on it - but that should only be done if consensus seeking fails. It's an escape hatch for when we need to _force a vote_ which is pretty rare. At least that is my understanding of [the process](https://github.com/nodejs/node/blob/master/GOVERNANCE.md).

So far everyone here seems to be pretty in sync (no one is opposed but no one is particularly in favor). We're not even in disagreement 😅",2020-06-13T19:44:39Z,1315533
3936,nodejs/node,638202113,643671351,"> I'm not sure where the git repo is - but I recommend trying the mailing list and asking there git-scm.com/community

I've sent an email to Git Community mailing list, and will update here once they come up with a decision.",2020-06-13T20:04:38Z,16024985
3937,nodejs/node,638202113,643672588,"Bear in mind that URLs linking to `master` will not redirect to the new default branch name (and for repos where it matters, which isn't this one, github-pages only works on the default branch when it's named `master`). It may be worth waiting for Github to fix these discrepancies before making the switch.

(to be clear; i'm in favor of making the change, and ""main"" seems as good as anything else, but the disruption caused by Github's incomplete support for a non-master default branch are significant)",2020-06-13T20:14:47Z,45469
3938,nodejs/node,638202113,643674041,+1 to main but I do want to see what lead GitHub takes in making this easier,2020-06-13T20:27:02Z,439929
3939,nodejs/node,638202113,643674094,"I personally feel very strongly that we should change this.

+1 to `main`",2020-06-13T20:27:30Z,498775
3940,nodejs/node,638202113,643674646,I would be -1 untill a plan is drawn for the changes. This could cause a lot of issues with our build ci which would need to be accounted for.,2020-06-13T20:32:39Z,26796102
3941,nodejs/node,638202113,643675178,"Ok, it looks like there are people in favor in the org who feel strongly that we should change this. So far no objections and everyone in the conversation is +0 -0 or +1.

Does anyone object to changing this (just the main branch name from `master` to `main`)?

It looks like it's [not particularly hard](https://github.com/nodejs/node/issues/33864#issuecomment-643656288) technically (+ an update to the collaborator guide and policy). We probably need to address [the links](https://github.com/nodejs/node/issues/33864#issuecomment-643672588) as well.

",2020-06-13T20:37:21Z,1315533
3942,nodejs/node,638202113,643675410,"> So far no objections and everyone in the conversation is +0 -0 or +1.

What about https://github.com/nodejs/node/issues/33864#issuecomment-643674646?

Also, I think GitHub might pick `trunk` instead of `main`. https://github.com/cli/cli/issues/929...

> Does anyone object to changing this (just the main branch name from master to main)?

I don't object... but I don't think it's a good idea to rush this...",2020-06-13T20:39:17Z,3065230
3943,nodejs/node,638202113,643675605,"Fwiw there is quite a lot of work for us to do in order to make sure we do this in a way that is not disruptive.

To @AshCripps point, we definitely need to do a large audit and preparation before moving forward.

I was putting together some notes yesterday outlining steps to take and what to consider before making a change like this

I'd like to suggest that we pause discussion until Monday and I can come back with a suggestion of what we should audit and steps to follow to do this in a way that would minimize disruption",2020-06-13T20:40:58Z,498775
3944,nodejs/node,638202113,643675746,@ronag that message was posted after I started writing mine so I did not see it. Sorry for the (timing) confusion. Fwiw https://github.com/nodejs/node/issues/33864#issuecomment-643674646 isn't a conceptual -1 it's a -1 until a plan is drawn for how we make the changes. I thought that not making the change quickly without discussing this or laying out how (clearly) is a given.,2020-06-13T20:42:13Z,1315533
3945,nodejs/node,638202113,643686866,"Needlessly censoring words does nothing to resolve the social issues surrounding them. The reasoning behind this is the same used when changing the gun emoji to a squirt gun. But in the end, such efforts only take away words and symbols we can use to easily describe concepts, such as the hierarchical and control structures we work with every day. It's destructive at worst and pointless virtue signalling at best.",2020-06-13T22:38:46Z,5233816
3946,nodejs/node,638202113,643687426,"Anyway, -1 to this. Not worth the effort.",2020-06-13T22:44:46Z,5233816
3947,nodejs/node,638202113,643688427,I am -1 on renaming/changing the branch.,2020-06-13T22:52:06Z,54666
3948,nodejs/node,638202113,643692754,-1 on this.,2020-06-13T23:32:41Z,1985555
3949,nodejs/node,638202113,643699648,Putting this on the tsc agenda for discussion,2020-06-14T00:26:59Z,439929
3950,nodejs/node,638202113,643747657,-1 on renaming the branch,2020-06-14T10:28:52Z,6447530
3951,nodejs/node,638202113,643749207,"I would recommend we escalate this (to a vote for example) when:
 - We have a clear plan on how to make this change addressing the raised issues (GH links, build infrastructure etc).
 - We have an individual or group willing to champion those changes.

Can either of the collaborators -1ing (@gireeshpunathil / @mscdex ) speak up regarding what in particular they are objecting to? (The process of changing it? the name `main` itself?)",2020-06-14T10:43:43Z,1315533
3952,nodejs/node,638202113,643749616,"Also, this issue is locked because it has received a large number of abuse comments. Like the website change - changes with this flavor tend to get a lot of attention.",2020-06-14T10:47:35Z,1315533
3953,nodejs/node,638202113,643751038,"> I can come back with a suggestion of what we should audit and steps to follow to do this in a way that would minimize disruption

@MylesBorins Also think about how to roll back when things go wrong. Auditing Jenkins jobs is the kind of mind-numbing tedium that makes human error more likely than not.",2020-06-14T11:02:20Z,275871
3954,nodejs/node,638202113,643766588,"@benjamingr I don't think such a change is necessary and by making such a change it would be creating a lot of technical issues. I've never heard anyone seriously say they found the branch name offensive since git's inception, much like I've never heard anyone seriously find similar terminology offensive elsewhere in computing in the many decades it's been in use.

I believe attaching a single context to such technical terminology (that involves inanimate things) is not useful. Instead, everyone should be focusing their time and effort on things in the world that are *actually* and *obviously* offensive.",2020-06-14T13:27:02Z,54666
3955,nodejs/node,638202113,643769288,"@mscdex
> much like I've never heard anyone seriously find similar terminology offensive...

That's literally what you're seeing here. Keep in mind that what you might consider ""*actually*"" and ""*obviously*"" offensive will differ significantly from what others may find ""*actually*"" and ""*obviously*"" offensive.
",2020-06-14T13:50:03Z,439929
3956,nodejs/node,638202113,643770750,There are definitely some very real and difficult technical challenges to address here. My suggestion would be to make this a strategic initiative with the first step being to establish precisely how to make this change with the least amount of disruption. ,2020-06-14T14:02:11Z,439929
3957,nodejs/node,638202113,643773398,"As a community that has pledged for inclusivity, it is natural to be sensitive and sympathetic to the current situations. However, I guess taking a step back and looking at things from a more wider perspective, I would ask these questions myself, in order:

 - Is Node.js community well represented by race, gender, ...? any process change required for the membership criteria for better inclusion of under-represented groups?
 - Is Node.js leadership well represented by race, gender, ...? again, any process change required?
 - Is Node.js process and practices designed to be inclusive, and is it working well? a retrospective session with commitment to acting on the results?
 - Is Node.js nomenclature, words used etc. free from offensive terms? (items such as this)

Doing the naming change first - easy, but causes turbulence in the eco-system, plus gives an impression that we are good with the rest (it is so possible that we are doing good there, but we haven't inspected). Addressing the other things - difficult and time consuming, but addresses the root of the issue from systemic perspectives.

I am willing, and eager to taking part, and / or driving such initiatives, if there is a consensus on `Let us do these things first, and attack this later!!!`",2020-06-14T14:23:09Z,6447530
3958,nodejs/node,638202113,643775615,"> That's literally what you're seeing here.

Are you referring to the OP? If so, that didn't exactly strike me as someone saying they personally find the term offensive. To me the issue text read more like ""hey, other people are making this change in other projects, should we do it too?"".

> Keep in mind that what you might consider ""_actually_"" and ""_obviously_"" offensive will differ significantly from what others may find ""_actually_"" and ""_obviously_"" offensive.

Here ""actually"" and ""obviously"" was meant to describe things that a greater majority of people can agree are offensive.",2020-06-14T14:38:53Z,54666
3959,nodejs/node,638202113,643778024,"To make it explicit, I am strongly +1 on doing this. This is not mutually exclusive with other initiatives in any way. 

As for the technical issues, we can either wait to see if something comes from Github, or implement tooling ourselves that would keep the current branch name synced/as an alias, for example.",2020-06-14T14:57:09Z,899444
3960,nodejs/node,638202113,643787510,"I'd like to echo the sentiment from @addaleax that changing the branch name need not be mutually exclusive from other changes which our project likely needs to do to be more welcoming / inclusive. We should always be examining our process and thinking about what we can do to improve and make small iterative changes towards where we want to be.

The term `master` for the main tracking branch is something that has always bothered me. To push back against any narrative that might claim that changing the name of the branch is a fad I'd like to point to https://github.com/mylesborins/node-osc which is one of my more popular modules. I have been using the name `current` rather than `master` for over a year. The only reason why I had not brought up making this change at the project was because I didn't think it would have the momentum to be successful, not because I didn't believe that this change should be made.

@mscdex do you have specific technical failures you are concerned will happen? Would an in depth transfer that speaks to all of those changes put your mind at ease? If there was a way to ensure that anyone attempting to push or work with `master` in the future would be gracefully redirected by GitHub help to ease those concerns (this is a feature I'm poking at internally to see if it can be offered). 

> I believe attaching a single context to such technical terminology (that involves inanimate things) is not useful. Instead, everyone should be focusing their time and effort on things in the world that are actually and obviously offensive.

We debated extensively about naming API surfaces, why should this be any different? We accepted a default and those helping to create that default are saying ""hmm, maybe we should reconsider this"". Even if you don't find it useful to attach that context does that fact that others do, and in turn it makes them less productive at what they do, not resonate with you?

> Doing the naming change first - easy, but causes turbulence in the eco-system, plus gives an impression that we are good with the rest (it is so possible that we are doing good there, but we haven't inspected). Addressing the other things - difficult and time consuming, but addresses the root of the issue from systemic perspectives.

@gireeshpunathil I'm having a really hard time parsing the logic here. It seems like you are saying that because making this change will be easier than other changes will imply there is no other work to be done? I don't think anyone is saying that at all. What I do see people saying is that this is a small incremental change, that will take quite a bit of work to do properly... but one that I think will be meaningful to various members of our community.

FWIW I've been thinking quite a bit about our governance model, and more specifically consensus seeking, to examine if it is the best governance model for an inclusive environment. I proposed a [openjs world summit session](https://github.com/openjs-foundation/summit/issues/297) to discuss it as well!

I truly think that making positive social change is an ongoing effort that requires constant small change in the right direction. In fact I would argue that choosing to not make smaller, more obvious, iterative changes (such as changing our default branch) would have the inverse effect of telling people that Node.js is an environment that does not care about this.",2020-06-14T16:10:15Z,498775
3961,nodejs/node,638202113,643788471,"Also some interesting references for this discussion

[Original Discussion on git mailing list](https://public-inbox.org/git/7265e8cd-6166-1da6-c1b9-85d5d591bfd7@gmail.com/T/#ma9a4da070ddf8ff87729960bb31ce2a8776c73c1)

[Proposed change to git to allow overriding the default branch](https://public-inbox.org/git/nycvar.QRO.7.76.6.2006111610000.56@tvgsbejvaqbjf.bet/T/#mb6bb1a9ea6133b2829d9d47a6ecfa3c909e15b05)

[A contributor from bitbucket stating the github + bitbucket + git are all working on this together](https://twitter.com/seanjregan/status/1271515685353426946)

[A great twitter thread explaining why making this change is the right choice](https://twitter.com/mislav/status/1270388510684598272)

[A statement from a GitHub employee that `main` is likely to be the new default](https://twitter.com/billygriffin22/status/1271274332396773378)

[A checklist used by the github cli to change default branch](https://github.com/desktop/desktop/issues/6478)

[A tool by @gr2m to automate the process of changing the branch](https://github.com/gr2m/octokit-plugin-rename-branch)

[Discussion about changing defaults in GitLab](https://gitlab.com/gitlab-org/gitlab/-/issues/221164)",2020-06-14T16:17:44Z,498775
3962,nodejs/node,638202113,643793816,"> It seems like you are saying that because making this change will be easier than other changes will imply there is no other work to be done?

@MylesBorins - that was an addendum. My main point is that the other items that I listed are more tangible, personal and direct to the people / group who are subjected to the theme in question, and hence present a natural order to address.

I now see your submission in collab summit, and acknowledge it as part of a constant attempt towards improving our community, in the context of diversity and inclusivity! thank you!!",2020-06-14T17:01:43Z,6447530
3963,nodejs/node,638202113,644169683,"Not a TSC member, but I am a +1 to this change, echoing @addaleax and @MylesBorins sentiments that these do not need to be synchronous changes but can be done simultaneously as we do more intensive work.

For full transparency, I've also brought up the possibility of doing this with all repos under the @nodejs/community-committee.",2020-06-15T14:28:41Z,502396
3964,nodejs/node,638202113,645268065,"I'm -0 to this change, Many open source project are considering this change but I'm very fearful of all the changes to scripts/automation. As @MylesBorins mentioned, we will need to do a full audit before considering this change. I'd also suggest pushing out a blog post/tweets detailing the changes several weeks before we make the switch in order to give developers a chance to migrate any tooling that they're written.",2020-06-17T09:38:18Z,20224954
3965,nodejs/node,638202113,645451151, I do want to see what lead GitHub takes in making this easier,2020-06-17T15:37:29Z,9373002
3966,nodejs/node,638202113,645516766,"I found a great guide on ways to handle the branch-rename as well as paths towards gradual migration. Will likely be experimenting with some of this stuff on a personal repo over the next two weeks.

https://github.com/chancancode/branch-rename/#gradual-migration",2020-06-17T17:33:12Z,498775
3967,nodejs/node,638202113,645554210,"Just a note with some tooling I'm seeing come up around this:

- @gr2m (former CommComm member and long-term ecosystem ecosystem member) is working on a GitHub bot. He's talked a bit about it [on Twitter](https://twitter.com/gr2m/status/1273297040231948288?s=20) and has [a repo](https://github.com/rename-master-branch/github-app/pull/1).
- [github-default-branch](https://www.npmjs.com/package/github-default-branch) allows you to change the default branch for one or many repos via CLI.
- [retarget_prs](https://www.npmjs.com/package/retarget_prs) allows you to retarget PRs from one branch to another. It was built by one of the PMs on npm / maintainers of libgit2.",2020-06-17T18:45:57Z,502396
3968,nodejs/node,638202113,645557374,"> Just a note with some tooling I'm seeing come up around this:

What would really help is a way to have `master` automatically mirror (as in both read and write) `main` for a transitional and possibly long amount of time and point new users and tools towards `main` while gradually and at a very comfortable pace pace migrating all the existing infrastructure, docs, guides and tooling.

I'm not sure if that's possible (or hard) as I'm far from an expert or tooling - but such a tool doesn't sound particularly hard to make conceptually. I think the [guide](https://github.com/chancancode/branch-rename/#gradual-migration) Myles linked to provides one such way (via GitHub actions) that might be appropriate.

Links on GitHub would also probably need to automatically redirect somehow.

",2020-06-17T18:52:17Z,1315533
3969,nodejs/node,638202113,645627743,"So we can change any links referring to master to refer to HEAD and they will ""just work"" with whatever our default branch is. The branch-rename repo I pointed at has an action for mirroring master / main

https://github.com/chancancode/branch-rename/#phase-1-mirror-master-and-main",2020-06-17T21:12:42Z,498775
3970,nodejs/node,638202113,660292849,"GitHub has released a repo with official guidance

https://github.com/github/renaming

It mentions a new redirect feature that launched today

> Now: supporting early movers 🚚
Some projects on GitHub have already renamed their default branch. As a result, any links to those projects that contained the old branch name would previously have been broken.
>
>So, our first change, shipped on July 17th, updates GitHub.com to redirect links that contain a deleted branch name to the corresponding link in the repository's default branch.
>
>This change supports projects that have already moved. If you haven’t moved yet, we recommend not moving right now, and waiting until later this year. We’re investing in tools to make the renaming the default branch of an existing repository a seamless experience for both maintainers and contributors.
",2020-07-17T19:16:07Z,498775
3971,nodejs/node,638202113,660319004,"That's good news, but also means we have to wait ""until later this year"".",2020-07-17T20:18:19Z,3109072
3972,nodejs/node,638202113,661038614,"We can run a test, but when I changed a repo on Friday the redirect was working.

We could test this on another repo in the org first.

I can also follow up with that team internally and see if we can get it turned on for us.",2020-07-20T13:26:31Z,498775
3973,nodejs/node,638202113,661255619,"I've seen comments that indicate that blob links redirect but tree links don't (or the reverse); so it may not be a complete feature yet.

Given the large number of open PRs on this repo, it seems like ""silently retargeting open PRs"" would be a pretty helpful feature to have too.",2020-07-20T18:22:02Z,45469
3974,nodejs/node,638202113,661277783,"@MylesBorins I doubt CommComm folks would have any issue changing some of our repos, if you'd like to go with that.",2020-07-20T19:06:16Z,502396
3975,nodejs/node,638202113,661319082,"@bnb if ya'll wanna pick a repo we can work from there.

Maybe nodejs.dev?
",2020-07-20T20:34:44Z,498775
3976,nodejs/node,638202113,661372672,nodejs/community-committee or nodejs/examples are both low-risk repos that we could try. nodejs.dev is a more complex repo that could also work. I'm up for helping out with any.,2020-07-20T22:13:41Z,502396
3977,nodejs/node,638202113,674274901,"I've just finished renaming CITGM. Steps included.

* Create main branch
  - `git checkout -b main`
* Push main branch
  - `git push upstream main`
* changing default branch in https://github.com/nodejs/citgm/settings/branches
* Using the [retarget_prs](https://github.com/ethomson/retarget_prs) utility to update all open pull-requests targeting master
  - `npx retarget_prs --token LOLNOPE https://github.com/nodejs/citgm master main`
  - I generated a temporary personal access token with all repo permissions for this
* Delete master branch
  - `git push upstream :master`
* [Open a PR](https://github.com/nodejs/citgm/pull/819) to update documentation

So far there do not seem to be issues. Some highlights

- All old URLs are automatically getting redirected.
  - e.g. https://github.com/nodejs/citgm/blob/master/.gitattributes redirects to https://github.com/nodejs/citgm/blob/main/.gitattributes
- retargeting prs appears to have worked without issue
- all existing integrations with actions + codecov seem to still work

[Tracking issue](https://github.com/nodejs/citgm/issues/812)",2020-08-14T21:14:13Z,498775
3978,nodejs/node,638202113,676540767,"I also updated nodejs/examples. Here's the steps - they're slightly different than the ones Myles took since I just used the web UI:

- Create `main` from `master` in web UI (dropdown on the repo's main page)
- Change default branch from `master` to `main` in the repo's settings
- Manually retargeted PRs (there were only two, pretty easy to just do this through the PRs in the web UI)
- Delete `master` (can be restored if needed)",2020-08-19T16:50:19Z,502396
3979,nodejs/node,638202113,676571082,see also: https://github.com/nodejs/admin/issues/543 regarding changing the default for new repos,2020-08-19T17:51:11Z,45469
3980,nodejs/node,638202113,762301015,Related: https://github.com/github/renaming#renaming-existing-branches,2021-01-18T14:56:09Z,14309773
3981,nodejs/node,638202113,773576351,"List of non-archived repos, along with whether they need to be updated or not.

Generated with:

```JavaScript
const repos = require('./noderepos.json');
for (let repo of repos) {
  if (!repo.archived) {
    let done = ' ';
    if (repo.default_branch !== 'master') {
      done = 'x';
    }
    console.log('- [' + done + '] ' + repo.name + ':' + repo.default_branch);
  }
}
```
on output from

`npx repos nodejs noderepos.json`

- [x] Gzemnid:master
- [x] Release:master
- [x] TSC:main
- [x] abi-stable-node:doc
- [x] admin:master - PR - https://github.com/nodejs/admin/issues/589
- [x] badges:master
- [x] bot-love:master
- [x] branch-diff:master
- [ ] build:master - https://github.com/nodejs/build/issues/2761
- [x] build-toolchain-next:main
- [x] changelog-maker:master
- [x] ci-config-github-actions:master
- [x] ci-config-travis:master
- [x] citgm:main
- [x] code-and-learn:master
- [x] commit-stream:master
- [x] community-committee:main
- [x] core-validate-commit:master
- [x] corepack:main
- [x] create-node-meeting-artifacts:master
- [x] diagnostics:main
- [x] docker-node:master
- [x] education:master
- [x] email:master
- [x] examples:main
- [x] getting-started:master
- [x] github-bot:master
- [x] gyp-next:master
- [x] hardware:master
- [x] help:master
- [x] http-next:main
- [x] http-parser:master
- [x] i18n:master - https://github.com/nodejs/i18n/issues/502
- [x] installer:master
- [x] llhttp:master - https://github.com/nodejs/llhttp/issues/115
- [x] llnode:master
- [x] llparse:master
- [x] llparse-test-fixture:master
- [x] lts-schedule:master
- [x] make-node-meeting:master
- [x] meeting-picker:master
- [x] mentorship:master
- [x] modules:main
- [x] nan:master - https://github.com/nodejs/nan/issues/920
- [x] napi-test-suite:master
- [x] next-10:master
- [ ] node:master
- [x] node-addon-api:main
- [x] node-addon-examples:main
- [x] node-api-headers:main
- [x] node-auto-test:master
- [x] node-code-ide-configs:master
- [x] node-core-utils:master
- [ ] node-gyp:master - https://github.com/nodejs/node-gyp/issues/2495
- [x] node-inspect:master
- [x] node-meeting-agenda:master
- [x] node-report:master
- [x] node-review:master
- [x] node-v8:canary
- [x] node-version-jenkins-plugin:master
- [x] node.js.org:gh-pages
- [x] nodejs-ar:master
- [x] nodejs-bg:master
- [x] nodejs-bn:master
- [x] nodejs-collection:master
- [x] nodejs-cs:master
- [x] nodejs-da:master
- [x] nodejs-de:master
- [x] nodejs-dist-indexer:master
- [x] nodejs-el:master
- [x] nodejs-es:master
- [x] nodejs-fa:master
- [x] nodejs-fi:master (archived)
- [x] nodejs-fr:master
- [x] nodejs-he:master (archived)
- [x] nodejs-hi:master
- [x] nodejs-hu:master
- [x] nodejs-id:master
- [x] nodejs-it:master
- [x] nodejs-ja:master
- [x] nodejs-ka:master
- [x] nodejs-ko:master
- [x] nodejs-latest-linker:master
- [x] nodejs-nightly-builder:master
- [x] nodejs-nl:master
- [x] nodejs-pl:master
- [x] nodejs-pt:master
- [x] nodejs-ru:master
- [x] nodejs-sv:master
- [x] nodejs-sw:master
- [x] nodejs-ta:master
- [x] nodejs-tr:master
- [x] nodejs-uk:master
- [x] nodejs-vi:master
- [x] nodejs-zh-CN:gh-pages
- [x] nodejs-zh-TW:master
- [x] nodejs.dev:master
- [X] nodejs.org:master - https://github.com/nodejs/nodejs.org/issues/3761
- [x] nodetogether:master
- [X] official-images:master - https://github.com/nodejs/docker-node/issues/1563
- [x] outreach:master
- [x] package-compliant:master
- [x] package-maintenance:main
- [x] post-mortem:master
- [x] promises-debugging:master
- [x] readable-stream:master - https://github.com/nodejs/readable-stream/issues/461
- [x] reliability:master
- [x] remark-preset-lint-node:master
- [x] repl:master
- [x] security-advisories:master
- [x] security-wg:master
- [x] snap:master - https://github.com/nodejs/snap/pull/17
- [x] social-media-delegates:master
- [x] string_decoder:master
- [x] tap2junit:master
- [x] tooling:master
- [x] tweet:main
- [x] undici:master
- [x] unofficial-builds:master - https://github.com/nodejs/unofficial-builds/issues/35
- [x] uvwasi:master
- [x] version-management:master
- [x] web-server-frameworks:master
- [x] webidl-napi:main
- [x] whatwg-stream:master
- [X] ~~worker:master~~ archived instead
",2021-02-04T20:14:25Z,9373002
3982,nodejs/node,638202113,773648776,">  post-mortem:master

Suprised this one hasn't been archived.",2021-02-04T22:36:41Z,5445507
3983,nodejs/node,638202113,786043290,"@richardlau it should've been, good call",2021-02-25T16:48:10Z,4048656
3984,nodejs/node,638202113,812870891,Today I renamed the branch on 22 repositories and opened a pull requests to update workflows where necessary.,2021-04-03T14:13:26Z,2352663
3985,nodejs/node,638202113,816940682,"@targos, did you update - https://github.com/nodejs/node/issues/33864#issuecomment-773576351 ? just want to make sure we have a good view on what's left.",2021-04-09T20:11:21Z,9373002
3986,nodejs/node,638202113,817295840,"@mhdawson yes, I did.",2021-04-11T12:00:35Z,2352663
3987,nodejs/node,638202113,837180465,"Went through an opened issues in a number of internationalization repos to give people a heads up/ask if there are any concerns.

nodejs-ar through    nodejs-ko:master.  Unless there are concerns by Friday I think we should be ok to go ahead and rename those.  Any help for doing that after Friday would be good :)",2021-05-10T19:16:47Z,9373002
3988,nodejs/node,638202113,839916907,Updated a few of the inactive/historical CommComm initiative repos and checked them off.,2021-05-12T16:23:44Z,502396
3989,nodejs/node,638202113,842513599,"When through an updated the internationalization ones that I'd opened issues on last time.

Created issues to FYI/ask for concerns to rename master->main in the remaining internationalization related ones.
",2021-05-17T17:46:22Z,9373002
3990,nodejs/node,638202113,842521384,"Also opened issue in 

https://github.com/nodejs/education
https://github.com/nodejs/Gzemnid
https://github.com/nodejs/ci-config-github-actions
https://github.com/nodejs/ci-config-travis
https://github.com/nodejs/github-bot/
https://github.com/nodejs/security-advisories
https://github.com/nodejs/security-wg/
",2021-05-17T17:59:14Z,9373002
3991,nodejs/node,638202113,842607626,"@mhdawson I already did education, apparently clicking the checkbox didn't take",2021-05-17T20:20:49Z,502396
3992,nodejs/node,638202113,842642041,"@bnb, not sure how I missed that it had already been done, thanks.",2021-05-17T21:11:05Z,9373002
3993,nodejs/node,638202113,863513971,"Rename about  8 more today, opened a few more FYI issues in advance of rename",2021-06-17T19:41:54Z,9373002
3994,nodejs/node,638202113,872411163,"For the remaining repositories, could it be enough to rename the branch and then create a new `master` branch and set up a simple bot that copies commits from `main` to `master`? I believe we are only concerned about tools reading from the old `master` branch, not writing to it.",2021-07-01T17:05:35Z,3109072
3995,nodejs/node,638202113,872441501,@tniessen that would prevent github from setting up the very helpful redirects.,2021-07-01T17:56:56Z,45469
3996,nodejs/node,638202113,872452849,"Oh, right, good point. Thanks.",2021-07-01T18:15:33Z,3109072
3997,nodejs/node,638202113,879989633,"In the TSC meeting, @gireeshpunathil asked if it's possible to create a master branch after it has been renamed to main. I tested and the answer is ""yes"" even with branch protection rules. @targos suggested that if we create a blank/minimal branch and then protect it, that might work.",2021-07-14T15:27:34Z,718899
3998,nodejs/node,638202113,879989848,"There are concerns about repos like `nodejs/node` where we land PRs manually, that after the rename, collaborators can still push to `master` by mistake and recreate the branch by doing so.

We can mitigate this with a branch protection rule that prevents pushes, but according to @Trott's tests, the rules do not prevent branch creation (first push after the rename).
Maybe it would work if we push an empty `master` branch and protect it? I don't know what would happen with GitHub automatic redirects in this case...",2021-07-14T15:27:51Z,2352663
3999,nodejs/node,638202113,879992730,"Definitely do not create a blank master branch, that will break the redirects from master to main that GitHub sets up.",2021-07-14T15:31:29Z,45469
4000,nodejs/node,638202113,879994553,@ljharb do you have an idea to prevent mistakes? Node.js collaborators are used to `git push upstream master` to land pull request and I'm pretty sure a mistake will happen if we don't have something in place to block it.,2021-07-14T15:33:47Z,2352663
4001,nodejs/node,638202113,880018943,"The options I see are:
1. no change, deal with it as it happens
1. create a branch and protect it, but this breaks usability for everyone who's not a collaborator
1. make a bot that listens for master branch creations and deletes them
1. wait for github.com to support serverside git hooks and reject any branch named master (no idea how long this will take)
1. teach collaborators a different workflow",2021-07-14T16:04:31Z,45469
4002,nodejs/node,638202113,882696374,"> teach collaborators a different workflow

perhaps it's time we begin merging rather than  `git push upstream master`? I've heard people mention this for years - perhaps this could be a catalyst.",2021-07-19T16:42:52Z,502396
4003,nodejs/node,638202113,918193961,Renamed in another 4 repos,2021-09-13T13:30:46Z,9373002
4004,nodejs/node,638202113,918348557,Opened some  more issues on the remaining repos. There are now issues open for all remaining repos except for nodejs/node.  Once we get the others done we can tackle that one.,2021-09-13T16:09:51Z,9373002
4005,nodejs/node,638202113,926885661,"A few more done, down to 9 left.",2021-09-24T19:58:04Z,9373002
4006,nodejs/node,638202113,933708067,"2 more, down to 7 left.",2021-10-04T17:41:38Z,9373002
4007,nodejs/node,638202113,933728358,"> There are concerns about repos like nodejs/node where we land PRs manually, that after the rename, collaborators can still push to master by mistake and recreate the branch by doing so.
>
> We can mitigate this with a branch protection rule that prevents pushes, but according to @Trott's tests, the rules do not prevent branch creation (first push after the rename).
Maybe it would work if we push an empty master branch and protect it? I don't know what would happen with GitHub automatic redirects in this case...

IIUC the concern is only until the migration happens (i.e. until when both `master` and `main` exist and are in sync in nodejs/node), correct? We can plan a time for the migration (similarly to what we do with security releases and CI upgrades), communicate it, and then during that time we restrict push to this repo to those involved in the migration. Once the migration is completed, we restore push to `@nodejs/collaborators`, and ensure the branch protections for `master` only let the bot push to it. I _think_ we can even add a git hook for push to give collaborators a message telling them to use `main` instead of `master` when pushing (not entirely sure).

Another idea I have (which I've been thinking for a different repository/organization, is to use the `post-checkout` git hook to communicate every time someone checks out to master (and try to identify if it was ran on our CI etc). That would help us identify when usage of master branch drops enough for us to safely discontinue it.",2021-10-04T18:08:36Z,4048656
4008,nodejs/node,638202113,933730999,"If both `master` and `main` exist, though, then github won't redirect people from the former to the latter - it'd be better to keep `master` entirely deleted (altho i think branch protections could still prevent it being pushed to)",2021-10-04T18:11:57Z,45469
4009,nodejs/node,638202113,933733260,Are there any cases where GitHub redirect wouldn't work? Can people (or Jenkins jobs or other forms of automation) still checkout to it etc?,2021-10-04T18:14:58Z,4048656
4010,nodejs/node,638202113,933745527,"@mmarchini I'm not sure, but I've not heard of any issues after switching over any of the other repos.

I do think treating it along the lines of a security release make makes sense. A pre-planned block of time where we restrict access makes sense.",2021-10-04T18:31:02Z,9373002
4011,nodejs/node,638202113,933746541,And I'm along the same thought as @ljharb that we should avoid a period where both are used and just do a rename and try to prevent a push of master after that.,2021-10-04T18:32:13Z,9373002
4012,nodejs/node,638202113,933752336,One of the things we could do is to prepare `node-core-utils` so that it refuses to land a commit if the branch is `master`.,2021-10-04T18:39:50Z,2352663
4013,nodejs/node,638202113,933754995,@targos and I guess a loud FYI for people to update as well.,2021-10-04T18:43:40Z,9373002
4014,nodejs/node,638202113,933781862,"This list is slightly outdated btw: It's missing `full-icu-npm`, `icu4c-data-npm`, and `full-icu-test`. @srl295 offered to update those <3",2021-10-04T19:19:50Z,502396
4015,nodejs/node,638202113,933782044,... and @srl295 finished before I could even comment lmao,2021-10-04T19:20:04Z,502396
4016,nodejs/node,638202113,933802871,@bnb good that they are already addressed. Once we believe we are complete we can re-run the commands I used to double check we are finished.,2021-10-04T19:49:57Z,9373002
4017,nodejs/node,638202113,955745618,"> One of the things we could do is to prepare `node-core-utils` so that it refuses to land a commit if the branch is `master`.

https://github.com/nodejs/node-core-utils/pull/586",2021-10-31T16:30:41Z,2352663
4018,nodejs/node,638202113,1041439413,"What is missing to rename the nodejs/node repo? Are we tracking down what needs to be done?
Maybe @bnb could you take a stab at this?",2022-02-16T12:26:10Z,52195
4019,nodejs/node,638202113,1041708614,"One thing to check is - in the prompt to do the renaming, it tells you how many PRs it will migrate. If that number doesn’t match the number of open ones, then those not included will be irrecoverably closed. This may be fine, maybe not, but it’s something to be cautious of. (it usually applies when the PR author has not checked “allow edits”, or, when they’ve deleted their fork). The person doing the migration won’t get notifications of the PR closures, but other watchers of them will.",2022-02-16T15:32:58Z,45469
4020,nodejs/node,638202113,1041803690,"![image](https://user-images.githubusercontent.com/2352663/154302447-0b4eda97-71c4-4f37-819f-77d339004712.png)
",2022-02-16T15:50:31Z,2352663
4021,nodejs/node,638202113,1041830539,That's not bad! Only 3 will end up being closed. Sadly there's no easy way to find those without looking through every open PR.,2022-02-16T16:11:28Z,45469
4022,nodejs/node,638202113,1041842550,"> Maybe @bnb could you take a stab at this?

any specific ideas on what needs to be done here? Happy to help of course.

I think the big thing is that we'll all want to be ready to help out when _something_ inevitably breaks because `master` is hardcoded. Looking through a [very unsophisticated GitHub Search](https://github.com/nodejs/node/search?q=master), we do have a lot of `master` in the codebase and some of them definitely seem to be referring to branches, including in python and Markdown. I think Markdown should be an easy fix in the future, but it's something that will need to be done.",2022-02-16T16:23:07Z,502396
4023,nodejs/node,638202113,1042053017,"> any specific ideas on what needs to be done here? Happy to help of course.

No idea tbh :(. We really need a champion for this.",2022-02-16T19:09:17Z,52195
4024,nodejs/node,638202113,1042312074,"I worked through most of the repositories so that we now have all but 6 out of over 100 moved over - see https://github.com/nodejs/node/issues/33864#issuecomment-773576351 for the checklist.

The remaining ones are more complicated in that there are external dependencies. I know that @richardlau has volunteered to look at some of them.

I've not had any cycles lately to loop back and see where we have made process/try to unblock. @bnb that is where a champion who can take a look at the overall picture, do want's need for the remaining repos or find volunteers for the remaining 6 could really help to close it out.",2022-02-16T21:08:10Z,9373002
4025,nodejs/node,638202113,1064176831,"These are the repos that still need a rename:

- [x] build:master - https://github.com/nodejs/build/issues/2761
- [x] i18n:master - https://github.com/nodejs/i18n/issues/502
- [x] node:master
- [x] node-gyp:master - https://github.com/nodejs/node-gyp/issues/2495
- [x] snap:master - https://github.com/nodejs/snap/pull/17
- [x] unofficial-builds:master - https://github.com/nodejs/unofficial-builds/issues/35
",2022-03-10T15:21:28Z,9373002
4026,nodejs/node,638202113,1064187631,"Poked the issues for i18n, node-gyp, and unofficial-builds.",2022-03-10T15:31:28Z,9373002
4027,nodejs/node,638202113,1119287028,"https://github.blog/changelog/2022-05-05-block-creation-of-branches-that-have-matching-names/

🎉 (2)
",2022-05-06T06:05:19Z,2352663
4028,nodejs/node,638202113,1123709787,"While https://github.blog/changelog/2022-05-05-block-creation-of-branches-that-have-matching-names/ doesn't restrict admins/owners, requiring signed commits should block accidental branch recreation, as not all of the commits in the history are signed. I checked that it works on https://github.com/nodejs/Gzemnid/settings/branch_protection_rules/25690377

![Screenshot_20220511_163952](https://user-images.githubusercontent.com/291301/167852075-6e78b50e-85ae-4ad8-94c0-d4a3528355e4.png)

",2022-05-11T12:42:03Z,291301
4029,nodejs/node,638202113,1123718305,Nice workaround!,2022-05-11T12:49:03Z,2352663
4030,nodejs/node,638202113,1139645911,"@nodejs/node FYI we are planning to rename the primary branch for nodejs/node to main on Wednesday June 15 ~ 10 ET.  This will require that people do the following with respect to local clones. The info from GitHub:

```
Your members will have to manually update their local environments. We'll let them know when they visit the repository, or you can share the following commands.
git branch -m master main
git fetch origin
git branch -u origin/main main
git remote set-head origin -a
```",2022-05-27T13:58:22Z,9373002
4031,nodejs/node,638202113,1139652291,"> @nodejs/node FYI we are planning to rename the primary branch for nodejs/node to main on Wednesday June 15 ~ 10 ET. This will require that people do the following with respect to local clones. The info from GitHub:
> 
> ```
> Your members will have to manually update their local environments. We'll let them know when they visit the repository, or you can share the following commands.
> git branch -m master main
> git fetch origin
> git branch -u origin/main main
> git remote set-head origin -a
> ```

cc @nodejs/collaborators ",2022-05-27T14:05:19Z,5445507
4032,nodejs/node,638202113,1139655253,"wow, this has been a long time coming, congrats on the perseverance!",2022-05-27T14:08:48Z,52195
4033,nodejs/node,638202113,1139668672,@richardlau thanks for adding the at mention :),2022-05-27T14:23:48Z,9373002
4034,nodejs/node,638202113,1152534081,Just noticed that we don't  have a calendar event for the renaming (was trying to find the exact time in the calendar). It probably would be somewhat useful to create an event in the project calendar for this.,2022-06-10T16:26:23Z,4299420
4035,nodejs/node,638202113,1152535376,"BTW for those who uses node-core-utils, you can use `ncu-config` to update your local config once the renaming happens:

```
$ ncu-config set branch main
```",2022-06-10T16:28:01Z,4299420
4036,nodejs/node,638202113,1154414218,@joyeecheung added to the project calendar. For everybody it's this Wednesday June 15th.,2022-06-13T20:38:23Z,9373002
4037,nodejs/node,638202113,1156558863,About to do the rename ,2022-06-15T14:38:15Z,9373002
4038,nodejs/node,638202113,1156630643,"We went through and updated ~100 jobs, now testing a few key ones.",2022-06-15T15:39:14Z,9373002
4039,nodejs/node,638202113,1156661233,"Re-ran steps to check all repos in the node.js org above (https://github.com/nodejs/node/issues/33864#issuecomment-773576351) and they look good:

[midawson@midawson rename]$ node doit.js 
- [x] .github:main
- [x] Gzemnid:main
- [x] Release:main
- [x] TSC:main
- [x] abi-stable-node:doc
- [x] admin:main
- [x] bot-love:main
- [x] branch-diff:main
- [x] build:main
- [x] build-toolchain-next:main
- [x] changelog-maker:main
- [x] ci-config-github-actions:main
- [x] ci-config-travis:main
- [x] citgm:main
- [x] cjs-module-lexer:main
- [x] code-and-learn:main
- [x] commit-stream:main
- [x] core-validate-commit:main
- [x] corepack:main
- [x] create-node-meeting-artifacts:main
- [x] devcontainer:main
- [x] diagnostics:main
- [x] docker-node:main
- [x] email:main
- [x] eslint-plugin-nodejs-internal:main
- [x] examples:main
- [x] full-icu-npm:main
- [x] full-icu-test:main
- [x] getting-started:main
- [x] github-bot:main
- [x] gyp-next:main
- [x] hardware:main
- [x] help:main
- [x] http-next:main
- [x] http-parser:main
- [x] i18n:main
- [x] icu4c-data-npm:main
- [x] js-native-api-test:main
- [x] llhttp:main
- [x] llnode:main
- [x] llparse:main
- [x] llparse-test-fixture:main
- [x] loaders:main
- [x] loaders-test:main
- [x] lts-schedule:main
- [x] make-node-meeting:main
- [x] meeting-picker:main
- [x] modules:main
- [x] nan:main
- [x] next-10:main
- [x] node:main
- [x] node-addon-api:main
- [x] node-addon-examples:main
- [x] node-api-headers:main
- [x] node-auto-test:main
- [x] node-code-ide-configs:main
- [x] node-core-test:main
- [x] node-core-utils:main
- [x] node-gyp:main
- [x] node-meeting-agenda:main
- [x] node-pr-labeler:main
- [x] node-review:main
- [x] node-v8:main
- [x] node-version-jenkins-plugin:main
- [x] node.js.org:gh-pages
- [x] nodejs-collection:main
- [x] nodejs-dist-indexer:main
- [x] nodejs-ko:main
- [x] nodejs-latest-linker:main
- [x] nodejs-nightly-builder:main
- [x] nodejs.dev:main
- [x] nodejs.org:main
- [x] official-images:main
- [x] package-maintenance:main
- [x] post-mortem:main
- [x] promises-debugging:main
- [x] readable-stream:main
- [x] release-keys:main
- [x] reliability:main
- [x] remark-preset-lint-node:main
- [x] repl:main
- [x] security-wg:main
- [x] snap:main
- [x] social-media-delegates:main
- [x] social-team:main
- [x] string_decoder:main
- [x] tap2junit:main
- [x] tooling:main
- [x] tweet:main
- [x] undici:main
- [x] unofficial-builds:main
- [x] uvwasi:main
- [x] version-management:main
- [x] web-server-frameworks:main
- [x] webidl-napi:main
[midawson@midawson rename]$ 
",2022-06-15T16:06:38Z,9373002
4040,nodejs/node,638202113,1156670832,"Good on the pkgjs branch as well

midawson@midawson rename]$ node doit.js 
- [x] .github:main
- [x] action:main
- [x] action-import-blocklist:main
- [x] create:main
- [x] create-package-json:main
- [x] dependents:main
- [x] design:main
- [x] detect-node-support:main
- [x] meet:main
- [x] membership-updater:main
- [x] nv:main
- [x] parseargs:main
- [x] statusboard:main
- [x] support:main
- [x] support-separate-repo:main
- [x] triagebot:main
- [x] wiby:main
[midawson@midawson rename]$",2022-06-15T16:15:20Z,9373002
4041,nodejs/node,638202113,1156672099,"AMAZING!!!
<img width=""487"" alt=""Screen Shot 2022-06-15 at 12 14 01 PM"" src=""https://user-images.githubusercontent.com/498775/173876093-16209cbf-25d9-4f47-92a8-e8c7eccd84c1.png"">

",2022-06-15T16:16:37Z,498775
4042,nodejs/node,638202113,1156672491,🎉 🎉 🎉 🎉 🎉 🎉 🎉 🎉 ,2022-06-15T16:17:01Z,502396
4043,nodejs/node,638202113,1156747748,"🙌 🙌 🙌

Can we unlock this issue?

@mhdawson can provide a public update and close it!",2022-06-15T17:32:31Z,16024985
4044,nodejs/node,638202113,1156789288,"I don’t think unlocking the issue is necessary at this point. If the branch rename is causing an issue, I’d recommend opening a new issue instead. 
it feels good to click on the “Close as completed” button for this one, thanks to everyone who worked on making it happen 🚀",2022-06-15T18:21:39Z,14309773
4045,nodejs/node,638202113,1156791566,"We've completed running some tests and all looks ok at this point. We are not aware of any issues at this point.

This was a long an incremental effort (almost 2 years) and I'd like to thank @richardlau and @sxa who helped me do some of the tricker/more complicated repos.  It was easier to do those as a group than it would have been for any one person on their own.  Also thank to all of the other collaborators who supported in ways from finding a way to prevent accidental pushes of the old branch to quickly responding to issues suggesting the rename in repos across the nodejs org.

I'd also like to thank Red Hat management who early in the larger discussion on renaming primary repos across  the GitHub ecosystem not only supported but asked teams to help open source communities make the change.

",2022-06-15T18:24:20Z,9373002
4046,nodejs/node,638202113,1173688210,Added fix to [node-stress-single-stress](https://ci.nodejs.org/view/Stress/job/node-stress-single-test/) today which had a reference to the old name hiding in an advanced panel in the jenkins UI :-),2022-07-04T11:09:28Z,6487691
4047,nodejs/node,638202113,1174075576,Have to add that given the change size and my initial skepticism about it being a smooth transition - ±20 days later it has been a pretty smooth transition from a collaborator's PoV (at least mine) so props to whomever ensured that and made sure that this change could happen without interruptions.,2022-07-04T18:49:12Z,1315533
4048,nodejs/node,877864027,877864027,"<!--
Thank you for suggesting an idea to make Node.js better.

Please fill in as much of the template below as you're able.
-->

**Is your feature request related to a problem? Please describe.**
There was recently a decision to rename N-API to Node-API. I believe this is a poor decision that will result in actively harmful results for both users of the API and general Node.js users.

Specifically, there are multiple problems with this naming:

- Building APIs are a common use case for Node.js. This naming can lead to confusing information or misleading search results.
- Products often refer to the way to access their services with JavaScript or Node.js as their ""JavaScript API"". If people want to use this from Node.js, there is a non-trivial chance they will look for ""Node.js API"" which will lead to confusing results.
- Node.js itself has an API, which theoretically includes this API. Naming a part of the whole the same thing as the whole is immensely confusing from an education perspective.
- This API is far less likely to be used than other parts of the Ndoe.js API, which leads to an exacerbation of challenge presented by the problems above.

**Describe the solution you'd like**

Rename Node-API to something else.

**Describe alternatives you've considered**
- Undoing Node-API rename, moving it back to N-API. 
  -  There is a reason a rename was done initially, and that reason is valid.
- Leave it as is.
  - This is going to be actively harmful to communication and education n the long-run.",2021-05-06T18:48:22Z,502396
4049,nodejs/node,877864027,833781199,"For context the rename/work is complete. The blog post which explains the changes is https://nodejs.medium.com/renaming-n-api-to-node-api-27aa8ca30ed8.

The node-api team was asked to change the name in: https://github.com/nodejs/abi-stable-node/issues/420 . Being sensitive to the concern the team took on this extra work.


",2021-05-06T18:57:27Z,9373002
4050,nodejs/node,877864027,836143936,"Repeating [what I wrote 3 months ago](https://github.com/nodejs/TSC/issues/967#issuecomment-773031602):

> I support changing the name N-API to something else, but the new term does have an obvious downside. The term ""Node API"" already has a straightforward meaning. Applying it to a specific API is more vague than descriptive.
> 
> That said, I don't know that I have better ideas. (Native Bindings API? Addon API? ABI Stability API?)

I'll also add that `Node API` is somewhat contrary to our years-long efforts to have the name of the runtime spelled `Node.js` and not `Node` (or `NodeJS` or a number of other variatns). 

Of course, now that the name change has already happened, there are significant costs/downsides to changing the name a second time in such a short period of time. I'd still support it, though, if the name was more descriptive and not subject to misinterpretation. A better name is better for our users.",2021-05-10T04:13:30Z,718899
4051,nodejs/node,877864027,845532857,I'm -1 on changing the name again and I appreciate the effort the node-api team put into making the changes. It's a good change. ,2021-05-20T22:52:51Z,439929
4052,nodejs/node,877864027,1146086717,This was discussed in the Node-API team meeting today and and consensus was we don't think we want/will change at this point. Some of the team members could not comment directly since it was locked.,2022-06-03T15:38:29Z,9373002
4053,Cog-Creators/Red-DiscordBot,1336666723,1336666723,"The music side of RED was working just fine, then all of a sudden, audio will not play back. I think the upstream Lavalink server needs to be updated.


2022-08-12 01:20:26.950  INFO 146 --- [XNIO-1 I/O-16] lavalink.server.io.SocketServer          : {""op"": ""stop"", ""guildId"": ""[REDACTED]""}
2022-08-12 01:32:15.440  INFO 146 --- [XNIO-1 I/O-16] lavalink.server.io.SocketServer          : {""op"": ""play"", ""guildId"": ""[REDACTED]"", ""track"": ""QAAAhgIAIUljZSBQb3NlaWRvbiAtIEN4IHNvbmcgKFJldXBsb2FkKQALWW91bmdKaWxrZXIAAAAAAASMEAALVzM5LWdyX1VQN2cAAQAraHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g/dj1XMzktZ3JfVVA3ZwAHeW91dHViZQAAAAAAAAAA"", ""noReplace"": false, ""startTime"": ""0"", ""pause"": false}
2022-08-12 01:32:16.412 ERROR 146 --- [lava-daemon-pool-playback-1-thread-1] c.s.d.l.t.p.LocalAudioTrackExecutor      : Error in playback of W39-gr_UP7g

com.sedmelluq.discord.lavaplayer.tools.FriendlyException: Something broke when playing the track.
    at com.sedmelluq.discord.lavaplayer.tools.ExceptionTools.wrapUnfriendlyExceptions(ExceptionTools.java:44) ~[walkyst-lavaplayer-85f4877.jar!/:na]
    at com.sedmelluq.discord.lavaplayer.track.playback.LocalAudioTrackExecutor.execute(LocalAudioTrackExecutor.java:116) ~[walkyst-lavaplayer-85f4877.jar!/:na]
    at com.sedmelluq.discord.lavaplayer.player.DefaultAudioPlayerManager.lambda$executeTrack$1(DefaultAudioPlayerManager.java:348) ~[walkyst-lavaplayer-85f4877.jar!/:na]
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[na:na]
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[na:na]
    at java.base/java.lang.Thread.run(Thread.java:829) ~[na:na]
    Suppressed: com.sedmelluq.discord.lavaplayer.tools.exception.EnvironmentInformation: 
  lavaplayer.version: 1.3.98-devoxin
  os.arch: amd64
  os.name: Linux
  os.version: 5.15.0-41-generic
  java.vendor: Debian
  java.version: 11.0.15
  java.runtime.version: 11.0.15+10-post-Debian-1deb11u1
  java.vm.version: 11.0.15+10-post-Debian-1deb11u1
Caused by: java.lang.IllegalStateException: No match found
    at java.base/java.util.regex.Matcher.group(Matcher.java:645) ~[na:na]
    at com.sedmelluq.discord.lavaplayer.source.youtube.YoutubeSignatureCipherManager.extractFromScript(YoutubeSignatureCipherManager.java:243) ~[walkyst-lavaplayer-85f4877.jar!/:na]
    at com.sedmelluq.discord.lavaplayer.source.youtube.YoutubeSignatureCipherManager.getExtractedScript(YoutubeSignatureCipherManager.java:160) ~[walkyst-lavaplayer-85f4877.jar!/:na]
    at com.sedmelluq.discord.lavaplayer.source.youtube.DefaultYoutubeTrackDetailsLoader.loadTrackInfoFromInnertube(DefaultYoutubeTrackDetailsLoader.java:197) ~[walkyst-lavaplayer-85f4877.jar!/:na]
    at com.sedmelluq.discord.lavaplayer.source.youtube.DefaultYoutubeTrackDetailsLoader.load(DefaultYoutubeTrackDetailsLoader.java:46) ~[walkyst-lavaplayer-85f4877.jar!/:na]
    at com.sedmelluq.discord.lavaplayer.source.youtube.DefaultYoutubeTrackDetailsLoader.loadDetails(DefaultYoutubeTrackDetailsLoader.java:34) ~[walkyst-lavaplayer-85f4877.jar!/:na]
    at com.sedmelluq.discord.lavaplayer.source.youtube.YoutubeAudioTrack.loadBestFormatWithUrl(YoutubeAudioTrack.java:76) ~[walkyst-lavaplayer-85f4877.jar!/:na]
    at com.sedmelluq.discord.lavaplayer.source.youtube.YoutubeAudioTrack.process(YoutubeAudioTrack.java:42) ~[walkyst-lavaplayer-85f4877.jar!/:na]
    at com.sedmelluq.discord.lavaplayer.track.playback.LocalAudioTrackExecutor.execute(LocalAudioTrackExecutor.java:104) ~[walkyst-lavaplayer-85f4877.jar!/:na]
    ... 4 common frames omitted",2022-08-12T01:57:57Z,16472317
4054,Cog-Creators/Red-DiscordBot,1336666723,1212659169,"Thanks, we are aware and have a Lavalink.jar prepped but there are other PRs to address first.",2022-08-12T01:59:49Z,20862007
4055,Cog-Creators/Red-DiscordBot,1336666723,1212659172,Sounds good. Will wait for a fix to be pushed. Appreciate the swift response.,2022-08-12T02:01:25Z,16472317
4056,Cog-Creators/Red-DiscordBot,1336666723,1212659173,I just noticed we're on the Red-Lavalink repo and not the Red-DiscordBot repo so I'm going to move this issue over there as this library is not affected and it's the audio backend side of things that is broken at the moment.,2022-08-12T02:03:08Z,20862007
4057,Cog-Creators/Red-DiscordBot,1336666723,1212662423,"Yeah, I put it there since it seemed to be an issue with Lavalink directly. No worries.",2022-08-12T02:10:45Z,16472317
4058,Cog-Creators/Red-DiscordBot,1336666723,1213548156,"PR's tied to this issue/needed for updating:

* #5821 
* #5822 
* @jack1142 , I can't find your PR that ""reverts"" #5751 to change the startup line detection to your ready check, that would go in this slot as needing to be included
* https://github.com/Cog-Creators/Red-Lavalink/pull/129
",2022-08-12T21:56:37Z,20862007
4059,Cog-Creators/Red-DiscordBot,1336666723,1214290325,"quick fix, stop bot first:

`export RedBotAudioCogLocation=***/you/should/probably/change/this/*** && cp $RedBotAudioCogLocation/Lavalink{.jar,.jar.old} && wget -O$RedBotAudioCogLocation/Lavalink.jar https://github.com/Cog-Creators/Lavalink-Jars/releases/download/3.4.0_1350/Lavalink.jar `


the audio cog was at `~/.local/share/Red-DiscordBot/data/RedBot/cogs/Audio/`  for me  (Linux, venv)

this also shouldn't need to be reverted when the update comes out, the update should replace lavalink.jar if I understand correctly
",2022-08-14T05:46:06Z,105951210
4060,Cog-Creators/Red-DiscordBot,1336666723,1214403713,"
how was that off topic at all? 
",2022-08-14T15:45:36Z,105951210
4061,Cog-Creators/Red-DiscordBot,1336666723,1214406898,"By adding a comment that provided an unsupported fix that will break a lot of current users if they try to do what you suggested.

Leave the fixes and workaround for the people who are aware of the wider user base and repercussions for a given approach.",2022-08-14T16:05:37Z,27962761
4062,Cog-Creators/Red-DiscordBot,1336666723,1214411009,"what? name a SINGLE repercussion?  all the rest of the pr's are waiting on this jar, you telling me your code cant handle a file being replaced, when its literally going to replace that file? sorry, i may have a new github account, but i am by no means a noob, and i have four bs detectors attached to my head; my eyes and ears. i even added a command to keep the old jar, in case your code checksummed it for some reason, if you're checking the file creation date of a jar you're replacing or some crap, i concede, your dumb code will cause problems with this jar being replaced.

FURTHERMORE, I have redbot set up exactly as the docs instruct, you telling me you support environments that are not set up according to the recommendation of the docs? i know that's complete bs from being a part of the discord server. 


> people who are aware of the wider user base and repercussions for a given approach.

are these people aware their audio cog hasn't been working for days lmao, that not a repercussion? ",2022-08-14T16:29:44Z,105951210
4063,Cog-Creators/Red-DiscordBot,1336666723,1214412621,"Please refrain from acting like this in our issues. If you want to go ahead and fix this for your self by the path you suggested that's fine with us. However this fix doesn't roll for us for an general production environment. We have an open PR (#5823) ready for this that we're planning on handling today.

> sorry, i may have a new github account, but i am by no means a noob, and i have four bs detectors attached to my head; my eyes and ears.

We do not filter content based on GitHub account age, experience or the amount of sensory elements one has. We do however filter based on our audience, the end user. If we feel like the fix suggested here may proof dangerous or harmful, as seen above, we will take action on it.

>  if you're checking the file creation date of a jar you're replacing or some crap, i concede, your dumb code will cause problems with this jar being replaced.

Now I'll be honest, I may not know our ""dumb code"" like some of our other contributors such as Draper here, but I fully entrust them that its working as intended. And that the above fix you've provided will not be compatible with it.

> are these people aware their audio cog hasn't been working for days lmao, that not a repercussion?

Yes, we are aware that we've received comments from people that Audio isn't working on an daily basis, and I'd like to remind you that there is an human on the other side of the argument here. We all have our own jobs to do, and our own live to life. Red for us isn't what brings in the money to pay the bills. It's an open source software project that we do in our spare time for fun.

I'm sorry that you think that our handling of your fix isn't the right thing to do. But I'm gonna stand with my team here and uphold it.

Have a wonderful rest of your day.",2022-08-14T16:41:47Z,10947836
4064,Cog-Creators/Red-DiscordBot,1336666723,1214933136,This has been fixed in Red 3.4.18.,2022-08-15T12:00:26Z,6032823
4065,PCSX2/pcsx2,914128991,914128991,"God of War's FMV PSS and PSW files are rednered with a black bar at the bottom, which is handeled by naturally the game engine somehow.

When running the game on the 1.6.0 release this bar is present and I cant get rid of it at all.
I tried the 1.4.0 release and it worked correctly on the default settings without changing anything

this issue exists in 1.7.0 latest dev build as well

Describe the bug
Black bar at the bottom of FMVs. 

To Reproduce
Run God of War 1 with a version of pcsx2 later than 1.4.0

Expected behavior
It shouldnt appear

GS Settings
Default settings

Emulation Settings
Default settings

GS Window Screenshots

System Info (please complete the following information):

PCSX2 Revision: 1.6.0 / 1.7.0 dev rev 1284
OS:  Win 10
CPU: Ryze 3600
GPU: RX470
Logs and Dumps",2021-06-08T01:13:24Z,35445029
4066,PCSX2/pcsx2,914128991,856365754,"There is a provided issue template when you create an issue, please follow it.
Does this issue happen in software mode? (Pressing F9 or changing the renderer in gsdx)

Issue template:

**Describe the bug**
<!-- A clear and concise description of what the bug is. -->

**To Reproduce**
<!-- Steps to reproduce the behavior. -->

**Expected behavior**
<!-- A clear and concise description of what you expected to happen. -->

**GS Settings**
<!-- Any non-default settings for GS. -->
<!-- If you don't want to list them out, please provide screenshots of your configuration window (including hw hacks if enabled). -->

**Emulation Settings**
<!-- Any non-default core settings. -->
<!-- If you don't want to list them out, please provide screenshots of your configuration window. -->
<!-- Please note that the safe preset works for most games. -->
<!-- MTVU can have some compatibility issues so please disable it before making a report. -->
<!-- If you need to modify the settings manually because a game requires you to do so to work, please state that explicitly. -->

**GS Window Screenshots**
<!-- If your issue is graphical in nature and you think screenshots will help illustrate your issue, you may do that here. -->

**System Info (please complete the following information):**
 -   PCSX2 Revision: <!-- e.g. dev-525 -->
 -   OS: <!-- e.g. Windows 10 -->
 -   CPU: <!-- e.g. i5-7600 -->
 -   GPU: <!-- e.g. GTX 1070 -->

**Logs and Dumps**",2021-06-08T01:15:50Z,29295048
4067,PCSX2/pcsx2,914128991,856371105,"Software mode doesnt change a thing. I tried pressing F9 and tried turning on the option to switch to SW automatically during FMV, doesnt solve it.

Also I did my best with the template. Sorry I rarely post to github.",2021-06-08T01:32:16Z,35445029
4068,PCSX2/pcsx2,914128991,856431782,"Experiencing the exact same thing, works perfectly on version 1.4.0 and 1.5.0. 1.6.0 also has bugs with effects like fog, while previous versions work fine.",2021-06-08T04:34:53Z,71799959
4069,PCSX2/pcsx2,914128991,856432217,Do you know which version of 1.5 it doesn't work on?,2021-06-08T04:36:04Z,22896793
4070,PCSX2/pcsx2,914128991,856432910,"> Do you know which version of 1.5 it doesn't work on?

Sorry, I don't. I'm using v1.5.0-dev-2143-g1d983a681.",2021-06-08T04:38:06Z,71799959
4071,PCSX2/pcsx2,914128991,856560647,"Looks fine in software mode for me.  That black bar at the bottom, there doesn't happen to be one at the top too, is there?",2021-06-08T08:15:37Z,6278726
4072,PCSX2/pcsx2,914128991,856790251,"> Looks fine in software mode for me. That black bar at the bottom, there doesn't happen to be one at the top too, is there?

What version are you using? And can you share your settings?

This is how the game looks.
![gsdx_20210608154847](https://user-images.githubusercontent.com/35445029/121196928-2c002a00-c871-11eb-8103-3c10dfeedf16.png)

This is how FMV's are supposed to look and how they actually do look like using 1.4.0
![gsdx_20210608155307](https://user-images.githubusercontent.com/35445029/121197774-e132e200-c871-11eb-9eba-5f93918139b2.png)

This is the black bar when using 1.6.0
![gsdx_20210608154917](https://user-images.githubusercontent.com/35445029/121197111-54882400-c871-11eb-9d04-d3bb946f4e9f.png)


",2021-06-08T13:55:32Z,35445029
4073,PCSX2/pcsx2,914128991,856793358,"I'm just using 1.7 build 1286, the bars I get are because my window isn't exactly 4:3, but if I correct it I get the below.

![image](https://user-images.githubusercontent.com/6278726/121198659-366af580-c86a-11eb-96be-be2fc05d6163.png)

Just to note, mine is the PAL version
",2021-06-08T13:59:04Z,6278726
4074,PCSX2/pcsx2,914128991,857125735,"> I'm just using 1.7 build 1286, the bars I get are because my window isn't exactly 4:3, but if I correct it I get the below.
> 
> ![image](https://user-images.githubusercontent.com/6278726/121198659-366af580-c86a-11eb-96be-be2fc05d6163.png)
> 
> Just to note, mine is the PAL version

@refractionpcsx2 
I have only the NTSC version of both GoW1 and 2.  And from your screenshot I can see the internal resolution is different. 
iirc Sony Santa Monica rendered the FMV's in ddifferent resolutions for each region, 480 for NTSC and 576i for PAL. Which is why I believe you dont get the same bug I get.

This is what observed so far: on the my NTSC version
1.6.0 / 1.7.0 r1286:

GoW1 Has the Black border bug
GoW1 game runs at 512x448 inerlaced and 640x448 w/ progressive scan on
GoW1 FMV's run at 640x480 regardless
GoW1 FMV files are PSS files rendered at 640x480

on 1.4.0 the resolutions are the same, except that the bug happenes when I enable progressive scan.

GoW2 doesnt have this issue at all because the FMVs are at 640x448

Ibelieve thats the reason you dont experience the same bug because PAL FMV files are rendered at a higher res than NTSC

I extracted the files and examined them for both games to exmine them



",2021-06-08T20:46:32Z,35445029
4075,PCSX2/pcsx2,914128991,858138204,"Check your Zoom setting. Config > Emulation Settings > GS Window. If it is not 100%, set it to 100% exactly and see if the issue persists. I have seen before that starting the emu with zoom enabled and certain combinations of either full screen or windowed modes can result in this jarring offset in other games.",2021-06-09T22:16:54Z,6377490
4076,PCSX2/pcsx2,914128991,859123433,"> Check your Zoom setting. Config > Emulation Settings > GS Window. If it is not 100%, set it to 100% exactly and see if the issue persists. I have seen before that starting the emu with zoom enabled and certain combinations of either full screen or windowed modes can result in this jarring offset in other games.

zomm is 100%
As I said, I'm using defualt settings and fresh installs of every version of the emulator.
And rhis is an *FMV* issue not a game issue. Meaning it only happenes when prerendered cutscenes are played, not during gameplay..

And the previous comments the resolutions I mentiond are internal resolutions.",2021-06-10T22:28:50Z,35445029
4077,PCSX2/pcsx2,914128991,869145342,"Here is the list of all 1.5 builds, see if you can track down which build introduced the issue.
https://gist.github.com/turtleli/a7de466bdf0aac4d028be5fa82a31de2",2021-06-27T11:21:32Z,18107717
4078,PCSX2/pcsx2,914128991,869395680,"> Here is the list of all 1.5 builds, see if you can track down which build introduced the issue.
> https://gist.github.com/turtleli/a7de466bdf0aac4d028be5fa82a31de2

I did some testing and I believe it was introduced on build 1729 (v1.5.0-dev-102-g5bf12519d).",2021-06-28T06:23:50Z,71799959
4079,PCSX2/pcsx2,914128991,869471239,"https://github.com/PCSX2/pcsx2/commit/5bf12519dae05a5adf0bbd69ecc3671ce54a7b0c

Just to link directly to the commit which you say broke it, it also makes sense that this changed the behaviour. Maybe Greg meant w <= 640

Edit: maybe not, maybe something else hasn't been taken in to account...",2021-06-28T08:11:30Z,6278726
4080,PCSX2/pcsx2,914128991,997705907,@refractionpcsx2 why is pcsx2 wiki saying this is fixed when it is not because still dev build of pcsx2 1.7.0 so can find a real fix to this bug because been broken mouths now.,2021-12-20T08:31:56Z,10137
4081,PCSX2/pcsx2,914128991,997709682,"> @refractionpcsx2 why is pcsx2 wiki saying this is fixed when it is not because still dev build of pcsx2 1.7.0 so can find a real fix to this bug because been broken mouths now.

I don't see that on the wiki? But we don't maintain the Wiki we just host it, it's for users to update, it's a wiki.

we need to think about what to do with this. We could remove the NTSC Saturation modification that's in there, but then people would bitch there's huge black lines either side,  so need to see if there's anything based on the clock settings we can detect to use the saturation instead of arbitrarily basing it on the set width/height",2021-12-20T08:36:56Z,6278726
4082,PCSX2/pcsx2,914128991,997719667,@refractionpcsx2 then Add a gamedb hack for gow1 to fix black bars in fmv movies. Gamedb hacks are magic blutts that will fixed this bug without adding more bugs to outher games into pcsx2.,2021-12-20T08:51:31Z,10137
4083,PCSX2/pcsx2,914128991,997720692,"the GS isn't rigged up to the GameDB at the moment, though doing so is something we're considering (more of a pain in the ass because it's over a thread).  But we'll get around to it.

Anyway, it's not the end of the world, you aren't missing anything, there's just some extra black pixels at the bottom, the whole FMV is there, you'll survive.  If it's so much of a problem, you're welcome to play GoW on 1.4.",2021-12-20T08:53:00Z,6278726
4084,PCSX2/pcsx2,914128991,997723201,Thier other problems besides this like green and problems line with upscale bug no one report yet thier is fixed need a lot of settings changed for it to work.,2021-12-20T08:56:50Z,10137
4085,PCSX2/pcsx2,914128991,997724945,"Which upscaling bug? It isn't related to this issue, but instead of complaining about bugs which haven't been reported yet, maybe consider reporting them, then we can look in to them.

However the only upscaling bug I know is the green/purple fringing, which we probably can't do a lot about easily, it's due to how the PS2 renders effects and it expects the original resolution, so upscaling it causes problems.",2021-12-20T08:59:34Z,6278726
4086,PCSX2/pcsx2,914128991,997731220,"Well, that's just the nature of upscaling PS2 games, I'm afraid.  The console kinda sucks for that and our life is a lot harder than say the Gamecube (and most definitely the PS3)",2021-12-20T09:07:21Z,6278726
4087,PCSX2/pcsx2,914128991,997920621,"We are still discussing this? Wasn't this fixed with memory wrapping? 

Edit: oh alright, that was a separate issue.",2021-12-20T13:21:55Z,12788199
4088,PCSX2/pcsx2,914128991,998385216,"> Well, that's just the nature of upscaling PS2 games, I'm afraid. The console kinda sucks for that and our life is a lot harder than say the Gamecube (and most definitely the PS3)

At guys trying fix this bug unlike asshole at rpcs3 when report bug happened on psn ver of this game because does but they just closed the problem so yeah guys should fixed bug because think once done pcsx2 will batter then rpcs3 in this game.",2021-12-21T01:05:23Z,10137
4089,PCSX2/pcsx2,914128991,998385862,I'm sorry but you're absolutely clueless... You seem to think we can just magic a fix for these things or if thin air. ,2021-12-21T01:07:06Z,6278726
4090,PCSX2/pcsx2,914128991,998386552,Mean guys trying to get this bug fixed unlike rpcs3 team so just close problem up and mark invalid even bug happened on Rpcs3 god of war hd so hope guys do get bug fixed soon.,2021-12-21T01:08:55Z,10137
4091,PCSX2/pcsx2,914128991,998387170,"> Mean guys trying to get this bug fixed unlike rpcs3 team so just close problem up and mark invalid even bug happened on Rpcs3 god of war hd so hope guys do get bug fixed soon.

If it's so easy to fix why haven't you done it yet? Or are you just so brain dead that all you can do is clutter up our GitHub with incoherent garbage?

Please, either have something meaningful to contribute, or go away.",2021-12-21T01:10:33Z,6377490
4092,kubernetes/website,1242792738,1242792738,"**This is a Feature Request**

<!-- Please only use this template for submitting feature/enhancement requests -->
<!-- See https://kubernetes.io/docs/contribute/start/ for guidance on writing an actionable issue description. -->

**What would you like to be added**
Update https://kubernetes.io/docs/contribute/ so that early in the page there is a link to
https://contribute.cncf.io/contributors/projects/#kubernetes

You should decide on some suitable text for the hyperlink. Read https://contribute.cncf.io/contributors/ to get ideas about what to write, if you're not sure.

:information_source:  Don't remove any of the existing links; this is an addition.

**Why is this needed**
This change will help signpost readers to the CNCF contributor site.

**Comments**
/kind feature
/language en
/triage accepted
/priority backlog
/help
",2022-05-20T08:14:16Z,22591623
4093,kubernetes/website,1242792738,1132614133,"@sftim: 
	This request has been marked as needing help from a contributor.

### Guidelines
Please ensure that the issue body includes answers to the following questions:
- Why are we solving this issue?
- To address this issue, are there any code changes? If there are code changes, what needs to be done in the code and what places can the assignee treat as reference points?
- Does this issue have zero to low barrier of entry?
- How can the assignee reach out to you for help?


For more details on the requirements of such an issue, please see [here](https://git.k8s.io/community/contributors/guide/help-wanted.md) and ensure that they are met.

If this request no longer meets these requirements, the label can be removed
by commenting with the `/remove-help` command.


<details>

In response to [this](https://github.com/kubernetes/website/issues/33848):

>**This is a Feature Request**
>
><!-- Please only use this template for submitting feature/enhancement requests -->
><!-- See https://kubernetes.io/docs/contribute/start/ for guidance on writing an actionable issue description. -->
>
>**What would you like to be added**
>Update https://kubernetes.io/docs/contribute/ so that early in the page there is a link to
>https://contribute.cncf.io/contributors/projects/#kubernetes
>
>You should decide on some suitable text for the hyperlink. Read https://contribute.cncf.io/contributors/ to get ideas about what to write, if you're not sure.
>
>:information_source:  Don't remove any of the existing links; this is an addition.
>
>**Why is this needed**
>This change will help signpost readers to the CNCF contributor site.
>
>**Comments**
>/kind feature
>/language en
>/triage accepted
>/priority backlog
>/help
>


Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>",2022-05-20T08:14:19Z,20407524
4094,kubernetes/website,1242792738,1132621650,/assign,2022-05-20T08:21:58Z,88731979
4095,kubernetes/website,1242792738,1133114248,"/assign
",2022-05-20T16:46:17Z,25511583
4096,kubernetes/website,1242792738,1133549900,"Hello @sftim, I have gone through the issue mentioned, so I think the following changes can be suitable in the note section.

Note: To learn more about contributing to Kubernetes, [one of the CNCF hosted projects](https://contribute.cncf.io/contributors/projects/#kubernetes), see the [contributor documentation](https://www.kubernetes.dev/docs/).

the text for the hyperlink ( https://contribute.cncf.io/contributors/projects/#kubernetes ) should be one of the CNCF hosted projects.",2022-05-21T06:51:35Z,25511583
4097,kubernetes/website,1242792738,1134446220,"/assign
",2022-05-23T09:49:04Z,86051118
4098,kubernetes/website,1242792738,1134450972,Hi @NitishKumar06 We are working on this issue. please work on other issue.,2022-05-23T09:51:31Z,88731979
4099,kubernetes/website,1242792738,1134452410,/unassign @NitishKumar06 ,2022-05-23T09:52:18Z,88731979
4100,kubernetes/website,1242792738,1134513692,"I've already made changes to this issue now. Please let it be reviewed.
",2022-05-23T10:45:39Z,86051118
4101,kubernetes/website,1242792738,1134513781,"/assign
",2022-05-23T10:45:45Z,86051118
4102,kubernetes/website,1242792738,1134518812,"No you can't work on this issue if someone is already working . this is against the policy of kubernetes. tonight I will open PR for this issue. There are many issues just work on that.
/unassign @NitishKumar06 
",2022-05-23T10:51:31Z,88731979
4103,kubernetes/website,1242792738,1134523403,"See this issue has been opened from three days and as far I've observed the another contributor also raised the assign option for the same. Let my changes be reviewed and if it doesn't hold good, definitely you can also raise PR. The best one will be merged. /assign",2022-05-23T10:57:28Z,86051118
4104,kubernetes/website,1242792738,1134523791,"/assign
",2022-05-23T10:57:57Z,86051118
4105,kubernetes/website,1242792738,1134530570,/unassign @NitishKumar06 ,2022-05-23T11:05:12Z,88731979
4106,kubernetes/website,1242792738,1134533449,"Why are you doing this again and again? @sftim Please have a look. I've raised the PR for this issue which was not raised by anyone for 3 days. Now, he is unassigning me.",2022-05-23T11:08:08Z,86051118
4107,kubernetes/website,1242792738,1134533561,"/assign
",2022-05-23T11:08:14Z,86051118
4108,kubernetes/website,1242792738,1134535006,"Infact there's @kadtendulkar as well. Why are you not removing her from assignes? Please be respectful.
",2022-05-23T11:09:33Z,86051118
4109,kubernetes/website,1242792738,1134536099,/unassign @NitishKumar06 ,2022-05-23T11:10:35Z,88731979
4110,kubernetes/website,1242792738,1134537282,cool. Do this I've raised the PR anyways.,2022-05-23T11:11:50Z,86051118
4111,kubernetes/website,1242792738,1134537484,/assign,2022-05-23T11:12:02Z,86051118
4112,kubernetes/website,1242792738,1134539047,"@NitishKumar06 this is against the policy of kubernetes. 
/unassign @NitishKumar06 ",2022-05-23T11:13:32Z,88731979
4113,kubernetes/website,1242792738,1134539378,"Hey @NitishKumar06 please don't work on this issue, I'm new to community and this going to be my first issue. @ashish-jaiswar is helping me with that. 
This is community to help and work with unity. Please give chance to new contributers to work as well.

/unassign @NitishKumar06 ",2022-05-23T11:13:41Z,25511583
4114,kubernetes/website,1242792738,1134541662,"@kadtendulkar Actually this is infact my first contribution as well. The fact that @ashish-jaiswar has more PRs merged shows he isn't a newbie here. My last PR was not merged because someone else merged it through my changes . Please notice that!
",2022-05-23T11:15:45Z,86051118
4115,kubernetes/website,1242792738,1134541808,"/assign
",2022-05-23T11:15:53Z,86051118
4116,kubernetes/website,1242792738,1134544428,/unassign @NitishKumar06 ,2022-05-23T11:18:27Z,25511583
4117,kubernetes/website,1242792738,1134544975,"@kadtendulkar Why are you doing this?
",2022-05-23T11:18:57Z,86051118
4118,kubernetes/website,1242792738,1134552247,"Hey @NitishKumar06 this is my first issue to work on, please let me solve this. This is not about competition, its only about community work, wouldn't it bother you if i do the same with you. 
Please don't go against kubernetes policies.  if someone is assigned on the issue first you must ask if they are still working or not. 
You are not doing the right thing @NitishKumar06 
And in this case I'm working on it. Today i will raise a PR. Please be kind and look for other issues you will find one definitely. 😀


/unassign @NitishKumar06 ",2022-05-23T11:26:37Z,25511583
4119,kubernetes/website,1242792738,1134556206,Hey @NitishKumar06  you are not following the policy of kubernetes. I will report you to Kubernetes community.,2022-05-23T11:30:50Z,88731979
4120,kubernetes/website,1242792738,1134557192,"Okay @ashish-jaiswar , Let my PR be reviewed. Sounds good now? ",2022-05-23T11:31:53Z,86051118
4121,kubernetes/website,1242792738,1134561440,"hey @NitishKumar06, kubernetes community will not accept your PR. Because you are working against the policy of kubernetes.",2022-05-23T11:35:55Z,88731979
4122,kubernetes/website,1242792738,1134563752,"@ashish-jaiswar My last PR was not merged because although I was working on this issue as well but someone after me started working on it and his PR got merged.
",2022-05-23T11:38:29Z,86051118
4123,kubernetes/website,1242792738,1134575413,"_I'm writing this as a [tech lead](https://github.com/kubernetes/community/tree/49ecc500205884984a2174422f7715e7f9556b28/sig-docs#leadership) for SIG Docs_

:white_circle:  **TL;DR;** calm it down and be excellent to each other. :white_circle: 

Folks, it's best if people co-ordinate work. **_Anyone_** is welcome to open a PR to work on an issue provided they sign the CLA and agree to abide by our [code of conduct](https://kubernetes.io/community/code-of-conduct/).

If reviewers see multiple PRs opened for the same issue, they will usually:
- sigh inwardly
- look at the PR that was opened first, see if it's good to merge, and if so, merge that
- if not, look at the other PRs in chronological order
and finally
- close the other PRs with an explanation


So there's three further things here as I see it:
1. https://github.com/kubernetes/website/issues/33848#issuecomment-1134518812 mentions that
   >  you can't work on this issue if someone is already working . this is against the policy of kubernetes.
  
   I'm afraid that's not true. For SIG Docs, we actually have a policy of no [cookie licking](https://www.redhat.com/en/blog/dont-lick-cookie): nobody should assert that they own the implementation of an issue. 

1. However, it's unhelpful to work on an issue where someone else has started work, without first discussing with that person how to move forward. The polite and compassionate thing to do if someone has said they're working on an issue, but you don't see progress after a week or so is to get in touch and ask if they would like help. If they step back or you don't hear anything at all, you remain welcome to open your own PR.
   - https://github.com/kubernetes/website/issues/33848#issuecomment-1134446220
   - https://github.com/kubernetes/website/issues/33848#issuecomment-1134539047
   
   are both examples of behavior I'd prefer not to see more of.

1. It's best if people who have already worked on an issue avoid picking up Good First Issue issues. This issue is _not_ a “good first issue”; I deliberately chose not to add the Good First Issue label because I left the implementer some work to do around writing the hyperlink text and deciding what message would be suitable.
   That means it is OK for someone who has already submitted a PR to work on this. However, I would encourage people who are already contributing to hold off on Help Wanted issues (this one included) and only start work on a Help Wanted issue if you don't see any other work that you can reasonably pick up.

This discussion is getting more heated than it should be. If folks want to discuss how SIG Docs works, take it to Slack (https://kubernetes.slack.com/messages/sig-docs - invitations available from https://slack.k8s.io/) or come to the community meeting, 5:30 AM UTC on Wednesday 2022-05-25. A Zoom link for that call is in the Slack channel.

GitHub issues are not the place for [ad hominem](https://en.wikipedia.org/wiki/Ad_hominem) criticisms etc and I hope all concerned will take that on board.

I don't see grounds to involve the code of conduct folks at this point, and I'm hoping things stay that way. @ashish-jaiswar if you do have a concern about someone not following the code of conduct, please raise that by following the documented process.

_PS_ I edited this a few times for accuracy; sorry about the noise from updates.",2022-05-23T11:51:47Z,22591623
4124,kubernetes/website,1242792738,1134584378,"Thanks @sftim . This was needed! @ashish-jaiswar Please go through it especially the first point.
",2022-05-23T12:02:10Z,86051118
4125,kubernetes/website,1242792738,1134586040,"/assign
",2022-05-23T12:03:47Z,86051118
4126,kubernetes/website,1242792738,1134587903,"Thanks @sftim  

@NitishKumar06 I'm requesting since I'm new contributer please allow me to do. According to the 2nd and 3rd point its appropriate to check if assigned people are working or not. And i have already mentioned I'm working on it. please know that. 

/unassign @NitishKumar06 ",2022-05-23T12:05:43Z,25511583
4127,kubernetes/website,1242792738,1134596032,"But why didn't @ashish-jaiswar raise this point at that time when you made yourself assigned to this issue? He was working on that issue before you . But when I assigned it to myself, you know .",2022-05-23T12:13:07Z,86051118
4128,kubernetes/website,1242792738,1134596785,@sftim Please tell @kadtendulkar not to misuse this function of unassign. /assign,2022-05-23T12:13:46Z,86051118
4129,kubernetes/website,1242792738,1134602868,"/assign
",2022-05-23T12:18:55Z,86051118
4130,kubernetes/website,1242792738,1134603274,/@unassign @NitishKumar06 ,2022-05-23T12:19:17Z,88731979
4131,kubernetes/website,1242792738,1134605209,"/assign
",2022-05-23T12:21:07Z,86051118
4132,kubernetes/website,1242792738,1134605551,/@unassign @NitishKumar06,2022-05-23T12:21:28Z,88731979
4133,kubernetes/website,1242792738,1134606223,Work on issue . See ya!,2022-05-23T12:22:09Z,86051118
4134,kubernetes/website,1242792738,1134607095,/unassign @NitishKumar06 ,2022-05-23T12:22:58Z,88731979
4135,kubernetes/website,1242792738,1134607684,"Hey @NitishKumar06 , @ashish-jaiswar is helping me to raise my first PR that's why he assigned himself first. Please understand, I'm new to kubernetes community and I'm finding such conflicts.
It is always good to ask if the assigned people are working or not and since beginning i an mentioning that I'm working on it. But seems like you are not coordinating with new contributers .",2022-05-23T12:23:29Z,25511583
4136,kubernetes/website,1242792738,1134612894,"Natali Vlatko here, co-chair of SIG Docs. This conversation has become too heated and isn't in line with our community guidelines with regards to politeness and respect. Our tech lead, @sftim, had given great clarity and guidance for those involved to move forward, but unfortunately, this hasn't taken place. I'll be messaging all involved separately to resolve this disagreement via the Kubernetes Slack.",2022-05-23T12:28:31Z,15304481
4137,HandBrake/HandBrake,348797250,348797250,"I have been using Handbrake for a long time and after the h265 codec appeared, I started using it including HE-AAC for sound. Would it be possible to add its support directly to the Windows and Linux installation file just like it is for macOS? Thank you.",2018-08-08T16:19:15Z,42215440
4138,HandBrake/HandBrake,348797250,411468753,"Unfortunately, no. There are currently no viable encoders for this that can be included within HandBrake releases for Windows and Linux.

If you compile from source, you can enable the fdk-aac encoder which supports it, but sadly this encoder is not compatible with HandBrakes GPL license so we can not include this in official releases. 

If the situation changes, we'll look into it.",2018-08-08T16:29:16Z,628593
4139,HandBrake/HandBrake,348797250,412226504,For what it's worth Opus> HE-AAC in most ABX listening tests. 96kbps approaches transparency with 128kbps practically  transparent with a few edge cases when it comes to music. Hydrogenaud.io has good material on opus if curious. ,2018-08-10T22:53:18Z,37335427
4140,HandBrake/HandBrake,348797250,412231318,"> For what it's worth Opus> HE-AAC in most ABX listening tests. 96kbps approaches transparency with 128kbps practically transparent with a few edge cases when it comes to music. Hydrogenaud.io has good material on opus if curious.

With non-consumer equipment, Opus has a distinct sonic signature, even where the sampling rate change on encode has been avoided. Plain AAC-LC sounds better to me, and I can differentiate AAC at any bit rate compared to lossless even with most consumer equipment, including mobile devices. Do the highest possible quality 44.1->48 kHz conversion to music I'm familiar with, in hardware or software, and I will still be able to identify it. That Opus does this right off the bat makes it a no-go in my book, and I disagree with their reasoning for it.

ABX listening tests are pointless unless they take into account the intimate familiarity many people have with their music; they rarely do. That's a much longer discussion.

Anyway, use what works for you. Just remember that perceptual coding is generally subjective by nature and not everyone will agree with your likes and dislikes. Especially people that can hear an artifact, identify it, and verify what they're hearing with proper analysis tools, in that order. 😸 ",2018-08-10T23:29:18Z,70239
4141,HandBrake/HandBrake,348797250,442419935,so that's how it went in FDK. i just recently read it. i was wondering if Nero AAC Encoder will work on handbrake. but is Nero AAC Encoder license can be use in this project? i just wonder..,2018-11-28T11:49:30Z,45423837
4142,HandBrake/HandBrake,348797250,485288445,"you can dlopen/loadlibrary fdk-aac so/dll, if you do not distribuite it along your software, it does not violate the the gpl license",2019-04-21T23:00:34Z,120074
4143,HandBrake/HandBrake,348797250,485304431,"No, actually you can't. See: https://www.gnu.org/licenses/gpl-faq.en.html#GPLPlugins
I'm sure you've seen applications that dynamically link to non-GPL libs in an effort to circumvent GPL restrictions.  They are not following the guidelines provided by GNU.",2019-04-22T02:17:07Z,709720
4144,HandBrake/HandBrake,348797250,485532382,"@jstebbins handbrake dynloads libdvdcss on my system (debian), it's almost the same thing",2019-04-22T19:59:39Z,120074
4145,HandBrake/HandBrake,348797250,485534752,libdvdcss is GPL,2019-04-22T20:07:29Z,709720
4146,HandBrake/HandBrake,348797250,485538082,"but not shipped in many distributions due to possible patent problems, still compatible?",2019-04-22T20:18:11Z,120074
4147,HandBrake/HandBrake,348797250,485540966,"There are a couple reasons distributions don't ship dvdcss.  In the US, the big one would be aiding piracy.  They can't afford to be sued by the MPAA. Second would be patents, but I'm not sure those patents haven't expired by now.  They are quite old at this point. Regardless, patents have nothing to do with copyright.  The software license is compatible so there is no copyright issue that prevents using it.  ",2019-04-22T20:27:55Z,709720
4148,HandBrake/HandBrake,348797250,485545371,"fdk-aac license is incompatible with gpl because of patents, otherwise it would be a simple BSD, even all mp3 encoding libraries have same problem, not having explicitly stated free patent licensing does not mean you have the rights.
Anyway it's not the point, if you dynload fdk-aac and you do not distribuite the dll, you are violating nothing, the code you compiled in is the wrapper that is still compatible with gpl.
I suspect even coreaudio on osx may have a licensing problem a this point.",2019-04-22T20:42:32Z,120074
4149,HandBrake/HandBrake,348797250,485546181,"@sherpya No offense intended, but it sounds like you're new to some of this stuff. I suggest you do more reading and leave our project out of your hypotheses. You're not helping.",2019-04-22T20:45:08Z,70239
4150,HandBrake/HandBrake,348797250,485547188,"perhaps the problem can be avoided at all by running aac-enc sample program piping audio data, megui does it this way",2019-04-22T20:48:24Z,120074
4151,HandBrake/HandBrake,348797250,485547873,"@sebastinas fdk aac license is a bsd with explicit no patent grant, every aac encoder around has no patent grant (but is not explicit), sorry I don't understand the difference, gnu does not like fdk aac license because in their opinion they promote buying a license for the patent",2019-04-22T20:50:38Z,120074
4152,HandBrake/HandBrake,348797250,485547874,"To your previous point, we believe Core Audio AAC is not in violation of any license, as it is a system library. We also have periodic contact with Apple, such as recently when they suggested some threading changes to improve compatibility with future macOS versions, and they have never expressed any of the concerns you have.",2019-04-22T20:50:38Z,70239
4153,HandBrake/HandBrake,348797250,485548130,"> sorry I don't understand the difference

This is not an appropriate forum for this kind of research, sorry.",2019-04-22T20:51:29Z,70239
4154,HandBrake/HandBrake,348797250,485551823,"I've already made my researches:

```
Fraunhofer FDK AAC license (#fdk)

    This is a free software license as far as it goes. It is incompatible with any version of the GNU GPL.

    It has a special danger in the form of a term expressly stating it does not grant you any patent licenses, with an invitation to buy some. Because of this, and because the license author is a known patent aggressor, we encourage you to be careful about using or redistributing software under this license: you should first consider whether the licensor might aim to lure you into patent infringement. If you conclude that the program is bait for a patent trap, it would be wise to avoid the program.

    It is possible that the pertinent patents have expired. Depending on whether Fraunhofer still has active patents covering the work, the software might be a trap now, or not. (Of course, any program is potentially threatened by patents, and the only way to end that is to change patent law to make software safe from patents.)
```

The fdk aac license: https://directory.fsf.org/wiki/License:Fdk

look at point 3, even ffmpeg aac encoder that is lgpl does not explicit says that has no patent grant, but it does not means that it has, the only difference is that GNU FSF does not like Fraunhofer:

```
It has a special danger in the form of a term expressly stating it does not grant you any patent
licenses, with an invitation to buy some. Because of this, and because the license author is a known 
patent aggressor, we encourage you to be careful about using or redistributing software under this
license: you should first consider whether the licensor might aim to lure you into patent infringement
If you conclude that the program is bait for a patent trap, it would be wise to avoid the program.
```

about coreaudio, https://github.com/nu774/qaac
uses coreaudio dll on windows, it needs application support library included in itunes, I suspect they are not system libraries, at least on windows, but the api is the same on osx

My idea is the incompatibily is bogus, at least with gplv2, v3 has explicit denial",2019-04-22T21:03:18Z,120074
4155,HandBrake/HandBrake,348797250,485552643,This very issue has been debated at length on many occasions.  We will not debate it again here.  I suggest you find some of those other debates and read them.,2019-04-22T21:06:00Z,709720
4156,HandBrake/HandBrake,348797250,485555175,"HandBrake does not use Core Audio AAC on Windows.

Again, please avoid dragging HandBrake into your hypotheses. It only creates doubt for the uninformed.",2019-04-22T21:14:45Z,70239
4157,HandBrake/HandBrake,348797250,485558044,"Topic locked from further conversation. This isn't open for debate. 

Nero, FDK, Apple Core Audio are not license compatible for Windows. 
Thus, per my original feedback, if a license compatible encoder becomes available, we will look into adding it.  
",2019-04-22T21:25:13Z,628593
4158,HandBrake/HandBrake,348797250,1341580997,"Sadly it seems that a viable HE-AAC encoder is not forthcoming. As such, I'm closing this for now.

If this changes and a GPL compatible encoder becomes available in the future we can re-visit this as it would be good for feature parity.  For now this is unfortunately a macOS exclusive feature in HandBrake. ",2022-12-07T20:51:59Z,628593
4159,hashicorp/nomad,498868764,498868764,"When a client is stopped, the tasks on that client are left running. When the client restarts, it goes through a restore process to get handles to all its tasks again. If a task fails or is removed while the client is shutdown, the client should be able to garbage collect any of its dangling resources (like alloc dirs) and restart the task. This is not happening with iptables.

Fortunately we ""tag"" all the iptables rules in one of two ways:
- Placing them in a chain named `CNI-xxxx`/`CNI-DN-xxxx`. I don't know what that `xxxx` is but it's not the alloc ID, container ID, or network namespace ID.
- Adding a comment in the form ` /* name: ""nomad"" id: ""<alloc ID>"" *`.

So it should be possible to identify ""Nomad owned"" rules and clean them up if they don't belong to an allocation we know about if we can figure out the naming for the CNI chains.

### Nomad version

```
Nomad v0.10.0-dev (e2761807a346c5e3afd577b7994cfc788700bb15+CHANGES)
```

(But probably any recent version.)

### Reproduction steps

1. Run Nomad under systemd.
2. Run our Consul Connect demo job: `nomad job run ./e2e/connect/input/demo.nomad`
3. Stop the job: `nomad job stop countdash`
4. Observe that the tasks and iptables are cleaned up properly.
   - `docker ps`
   - `sudo iptables -t nat -L -v -n`
5. Run the job again: `nomad job run ./e2e/connect/input/demo.nomad`
6. Stop the Nomad client with `sudo systemctl stop nomad`.
7. Observe that the tasks and iptables are still in place.
   - `docker ps`
   - `sudo iptables -t nat -L -v -n`
8. Remove the tasks: `docker rm -f $(docker ps -a)`
9. Restart Nomad: `sudo systemctl start nomad`
10. Observe that the tasks are started: `docker ps`
11. Stop the job cleanly: `nomad job stop countdash`
12. Observe that iptables are left behind: `sudo iptables -t nat -L -v -n`

### Logs

<details><summary>iptables after repro steps</summary>

```
vagrant@linux:/opt/gopath/src/github.com/hashicorp/nomad$ sudo iptables -t nat -L -v --line-numbers -n
Chain PREROUTING (policy ACCEPT 0 packets, 0 bytes)
num   pkts bytes target     prot opt in     out     source               destination
1       29  1276 DOCKER     all  --  *      *       0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL
2       20   880 CNI-HOSTPORT-DNAT  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL

Chain INPUT (policy ACCEPT 0 packets, 0 bytes)
num   pkts bytes target     prot opt in     out     source               destination

Chain OUTPUT (policy ACCEPT 6 packets, 360 bytes)
num   pkts bytes target     prot opt in     out     source               destination
1        5   300 DOCKER     all  --  *      *       0.0.0.0/0           !127.0.0.0/8          ADDRTYPE match dst-type LOCAL
2      279 16740 CNI-HOSTPORT-DNAT  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL

Chain POSTROUTING (policy ACCEPT 6 packets, 360 bytes)
num   pkts bytes target     prot opt in     out     source               destination
1      348 20568 CNI-HOSTPORT-MASQ  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* CNI portfwd requiring masquerade */
2        0     0 MASQUERADE  all  --  *      !docker0  172.17.0.0/16        0.0.0.0/0
3        0     0 CNI-6fcd2f53d5f720ec4eb5f04d  all  --  *      *       172.26.64.102        0.0.0.0/0            /* name: ""nomad"" id: ""3e803d29-4f9d-ad8b-adb6-31456a39db69"" */
4        0     0 CNI-06d73cb6cdf7130196e2018a  all  --  *      *       172.26.64.101        0.0.0.0/0            /* name: ""nomad"" id: ""ee25f5d7-dcb9-b336-fe3e-27e365aa5cd0"" */

Chain CNI-06d73cb6cdf7130196e2018a (1 references)
num   pkts bytes target     prot opt in     out     source               destination
1        0     0 ACCEPT     all  --  *      *       0.0.0.0/0            172.26.64.0/20       /* name: ""nomad"" id: ""ee25f5d7-dcb9-b336-fe3e-27e365aa5cd0"" */
2        0     0 MASQUERADE  all  --  *      *       0.0.0.0/0           !224.0.0.0/4          /* name: ""nomad"" id: ""ee25f5d7-dcb9-b336-fe3e-27e365aa5cd0"" */

Chain CNI-6fcd2f53d5f720ec4eb5f04d (1 references)
num   pkts bytes target     prot opt in     out     source               destination
1        0     0 ACCEPT     all  --  *      *       0.0.0.0/0            172.26.64.0/20       /* name: ""nomad"" id: ""3e803d29-4f9d-ad8b-adb6-31456a39db69"" */
2        0     0 MASQUERADE  all  --  *      *       0.0.0.0/0           !224.0.0.0/4          /* name: ""nomad"" id: ""3e803d29-4f9d-ad8b-adb6-31456a39db69"" */

Chain CNI-HOSTPORT-DNAT (2 references)
num   pkts bytes target     prot opt in     out     source               destination

Chain CNI-HOSTPORT-MASQ (1 references)
num   pkts bytes target     prot opt in     out     source               destination
1       59  3540 MASQUERADE  all  --  *      *       0.0.0.0/0            0.0.0.0/0            mark match 0x2000/0x2000

Chain CNI-HOSTPORT-SETMARK (0 references)
num   pkts bytes target     prot opt in     out     source               destination
1       59  3540 MARK       all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* CNI portfwd masquerade mark */ MARK or 0x2000

Chain DOCKER (2 references)
num   pkts bytes target     prot opt in     out     source               destination
1        0     0 RETURN     all  --  docker0 *       0.0.0.0/0            0.0.0.0/0
```

</details>

cc @davemay99 @angrycub as a heads up",2019-09-26T12:58:51Z,1409219
4160,hashicorp/nomad,498868764,540137667,"Summary of the investigation at this point:
* When the client restarts, the network hook's `Prerun` fires and tries to recreate the network and setup the iptables via CNI.
* This fails because the netns already exists, as expected. So we tear down the task and start over.
* But in the next pass when we setup the iptables via CNI, we collide with the iptables left behind.
* To fix this we need the network namespace path (which is used by CNI as part of the handle for [`go-cni#Network.Remove`](https://godoc.org/github.com/containerd/go-cni#Network.Remove)).
* In the non-Docker case, Nomad controls the path to the netns and it's derived from the alloc ID, but in the Docker case (which includes all cases with Connect integration b/c of the Envoy container), Docker owns that path and derives it from the pause container name. So we can't use a deterministic name as a handle to clean up. But we can't get it from Docker either because at the point we need it that container has already been removed.


I've verified the following more common failure modes are handled correctly:
* Tasks recover fully when the client restarts (after a few PRs we landed in the current 0.10.0 release branch)
* There's no resource leak when the client restarts if the containers aren't removed.
* There's no resource leak when the client restarts as part of a node (machine) reboot.

Status:
* We could try to fix this by threading state about the network namespace from the allocation runner back into the state store, similar to how we deal with deployment health state. But this will always be subject to races between client failures and state syncs.
* We already have a PR open for 0.10.x to reconcile and GC Docker containers. Because all the rules we're creating are tagged with the string ""nomad"" and Nomad's alloc IDs, we can make a similar loop for iptable GC.
* Because I've verified that this leak doesn't happen in the common failure modes of a client or node reboot, we're not going to block the 0.10.0 release on this. We'll work up a PR for an out-of-band reconcile loop for 0.10.x.

Moving this issue out of the 0.10.0 milestone.",2019-10-09T18:48:20Z,1409219
4161,hashicorp/nomad,498868764,610551615,"Noting this _isn't_ the same as #7537 - the repro steps here still leak rules, e.g.

<details>
<summary>after.txt</summary>

```
Chain PREROUTING (policy ACCEPT 12 packets, 5781 bytes)
 pkts bytes target     prot opt in     out     source               destination         
  154 74057 DOCKER     all  --  *      *       0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL
  118 56714 CNI-HOSTPORT-DNAT  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL
  118 56714 CNI-HOSTPORT-DNAT  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL

Chain INPUT (policy ACCEPT 12 packets, 5781 bytes)
 pkts bytes target     prot opt in     out     source               destination         

Chain OUTPUT (policy ACCEPT 32 packets, 3515 bytes)
 pkts bytes target     prot opt in     out     source               destination         
  463 31262 DOCKER     all  --  *      *       0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL
  264 17528 CNI-HOSTPORT-DNAT  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL
  245 16388 CNI-HOSTPORT-DNAT  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL

Chain POSTROUTING (policy ACCEPT 32 packets, 3515 bytes)
 pkts bytes target     prot opt in     out     source               destination         
  448 40004 CNI-HOSTPORT-MASQ  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* CNI portfwd requiring masquerade */
  429 38864 CNI-HOSTPORT-MASQ  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* CNI portfwd requiring masquerade */
   10   720 MASQUERADE  all  --  *      docker0  0.0.0.0/0            0.0.0.0/0            ADDRTYPE match src-type LOCAL
    0     0 MASQUERADE  all  --  *      !docker0  172.30.254.0/24      0.0.0.0/0           
    0     0 CNI-11bedb6da1593ed3af43ef13  all  --  *      *       172.26.65.117        0.0.0.0/0            /* name: ""nomad"" id: ""2c0c32b5-2ea3-471f-378f-a740391bea60"" */
    0     0 CNI-b5b13e7bdc4638b22a8a6e73  all  --  *      *       172.26.65.118        0.0.0.0/0            /* name: ""nomad"" id: ""6eb897ef-4de8-9a2c-22cc-5965fe282b19"" */

Chain CNI-11bedb6da1593ed3af43ef13 (1 references)
 pkts bytes target     prot opt in     out     source               destination         
    0     0 ACCEPT     all  --  *      *       0.0.0.0/0            172.26.64.0/20       /* name: ""nomad"" id: ""2c0c32b5-2ea3-471f-378f-a740391bea60"" */
    0     0 MASQUERADE  all  --  *      *       0.0.0.0/0           !224.0.0.0/4          /* name: ""nomad"" id: ""2c0c32b5-2ea3-471f-378f-a740391bea60"" */

Chain CNI-HOSTPORT-DNAT (4 references)
 pkts bytes target     prot opt in     out     source               destination         

Chain CNI-HOSTPORT-MASQ (2 references)
 pkts bytes target     prot opt in     out     source               destination         
   19  1140 MASQUERADE  all  --  *      *       0.0.0.0/0            0.0.0.0/0            mark match 0x2000/0x2000
    0     0 MASQUERADE  all  --  *      *       0.0.0.0/0            0.0.0.0/0            mark match 0x2000/0x2000

Chain CNI-HOSTPORT-SETMARK (0 references)
 pkts bytes target     prot opt in     out     source               destination         
   19  1140 MARK       all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* CNI portfwd masquerade mark */ MARK or 0x2000
   19  1140 MARK       all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* CNI portfwd masquerade mark */ MARK or 0x2000

Chain CNI-b5b13e7bdc4638b22a8a6e73 (1 references)
 pkts bytes target     prot opt in     out     source               destination         
    0     0 ACCEPT     all  --  *      *       0.0.0.0/0            172.26.64.0/20       /* name: ""nomad"" id: ""6eb897ef-4de8-9a2c-22cc-5965fe282b19"" */
    0     0 MASQUERADE  all  --  *      *       0.0.0.0/0           !224.0.0.0/4          /* name: ""nomad"" id: ""6eb897ef-4de8-9a2c-22cc-5965fe282b19"" */

Chain DOCKER (2 references)
 pkts bytes target     prot opt in     out     source               destination         
```
</details>",2020-04-07T18:34:01Z,394887
4162,hashicorp/nomad,498868764,832057480,"We have a similar issue - iptables is a mess after some Nomad restarts.

We're on Nomad 1.0.4",2021-05-04T16:03:20Z,4280480
4163,hashicorp/nomad,498868764,895749486,"We have hit this too. Having a few stale iptables rules is fine until new allocations are assigned with ports used by the stale rules.

```
CNI-DN-b545c28573a241d01dadd  tcp  --  0.0.0.0/0            0.0.0.0/0            /* dnat name: ""nomad"" id: ""416a88b0-76fa-e270-6c80-7f102216ca13"" */ multiport dports 23674,23642
CNI-DN-b545c28573a241d01dadd  udp  --  0.0.0.0/0            0.0.0.0/0            /* dnat name: ""nomad"" id: ""416a88b0-76fa-e270-6c80-7f102216ca13"" */ multiport dports 23674,23642
CNI-DN-7833d1be7094c0db0c99d  tcp  --  0.0.0.0/0            0.0.0.0/0            /* dnat name: ""nomad"" id: ""aace5b7e-8e97-43d4-5705-06a94f326aee"" */ multiport dports 23674,22463,23641,21075
CNI-DN-7833d1be7094c0db0c99d  udp  --  0.0.0.0/0            0.0.0.0/0            /* dnat name: ""nomad"" id: ""aace5b7e-8e97-43d4-5705-06a94f326aee"" */ multiport dports 23674,22463,23641,21075
```
In the above example, `416a88b0-76fa-e270-6c80-7f102216ca13` was removed while `aace5b7e-8e97-43d4-5705-06a94f326aee` is a new allocation. Clients of the service will get an error `Unable to establish connection to 10.133.67.138:23674`  while trying to send requests to the running allocation. 

Even though we almost always drain nodes before manual nomad restart, sometimes nomad gets restarted automatically by systemd due to consul failure or other reasons, in which case we'd end up having stale iptables rules. So it'd be great if the iptables rule reconciliation can be implemented.  🙏 🤞 ",2021-08-10T05:55:34Z,357672
4164,hashicorp/nomad,498868764,934436245,"I'm seeing a slightly different trigger of this issue, but with the same root cause and end results. We use puppet to manage some iptables rules on the host[0]. When puppet makes a ruleset change, it triggers the new ruleset to be persisted (on EL, that's via iptables-save to `/etc/sysconfig/iptables`). This saved ruleset includes all the permanent rules we're managing via puppet, but also all the ""transient"" rules installed by nomad/cni plugins. Therefore the next time the ruleset is loaded (e.g. after host reboot), the iptables chains are pre-filled with stale rules from historic tasks. Currently nothing is cleaning those up, and since nomad is appending new rules, the saved rules are higher up in the NAT chains. This is causing particular pain where our ingress containers which listen on static ports 80, 443 get caught in the cached ruleset, because then after a reboot, the NAT rules redirect the traffic to a blackhole instead of the running ingress container. It also affects tasks that don't use static port numbers, but then the pain is deferred to when the port eventually gets reused and is harder to track down.

This is on nomad 1.1.5.

[0] One of the rules we're managing is `-A FORWARD -m conntrack --ctstate INVALID -j DROP` to drop packets which the conntrack module thinks are not valid, will not apply NAT. The kernel treats these as unsolicited packets, returning a TCP-RST. This tears down the connection between the container and the external service, causing disruption. There seem to be quite a few bug reports about this relating to docker, kubernetes etc, and this rule is the widely accepted workaround.",2021-10-05T13:53:21Z,1030831
4165,hashicorp/nomad,498868764,974046409,"I've just re-read the upgrade guide for (in preparation for 1.2.0), and I think the changes in that 1.1.0 to append the CNI rules rather than insert at the top of the chain is what made this issue more noticeable (https://github.com/hashicorp/nomad/pull/10181). Previously, had transient rules been persisted, the next time an alloc was started, the new iptables rules would be inserted above the stale ones and thus take precedence. Now they are added below the stale rules, so traffic is matched and blackholed by the stale rules.",2021-11-19T12:53:07Z,1030831
4166,hashicorp/nomad,498868764,1019846399,"As noted in #11901, this affects us quite badly right now (though we're using nftables as opposed to iptables, the issue and result is the same). Whenever an unclean stop or agent restart has occurred for a job with static ports, those ports will (silently, no errors, local checks seem to succeed) fail to bind again until reboot or stale rules are manually removed.


While at first glance it looked like this was a regression caused by the priority-inversion in  #10181 (as noted by @optiz0r ), that PR looks concerned only with the with `NOMAD-ADMIN` chain while in our case the issue is with stale rules blackholing dports under the `CNI-HOSTPORT-DNAT` chain (or maybe they're indeed the same after CNI does its magic?).
",2022-01-24T08:38:43Z,74199244
4167,hashicorp/nomad,498868764,1145133872,"This is becoming a major security and stability issue as we are seeing allocations try to forward from ports that already have rules in iptables, and requests bound for them are getting forwarded based on the stale iptables rule.
Is there anything we can do to ensure this gets prioritized?  Or can someone share a cleanup script they have been using?

Here is the error log around the time it fails to cleanup the stale allocation:

<details>
    <summary>Logs</summary>

```
containerd[3148]: time=""2022-05-24T07:05:43.326040572Z"" level=info msg=""shim disconnected"" id=d13e150a783b3a72482e859901590f7d002b71c43c36ffe2f0d46aecca64e794
containerd[3148]: time=""2022-05-24T07:05:43.326107477Z"" level=error msg=""copy shim log"" error=""read /proc/self/fd/17: file already closed""
dockerd[3364]: time=""2022-05-24T07:05:43.326090953Z"" level=info msg=""ignoring event"" container=d13e150a783b3a72482e859901590f7d002b71c43c36ffe2f0d46aecca64e794 module=libcontainerd namespace=moby topic=/tasks/delete type=""*events.TaskDelete""
consul[3318]: 2022-05-24T07:05:43.342Z [WARN]  agent.cache: handling error in Cache.Notify: cache-type=service-http-checks error=""Internal cache failure: service '_nomad-task-4984d1ec-c7ab-09ad-883f-3c66cc9d1cb7-group-<redacted>-8083' not in agent state"" index
consul[3318]: 2022-05-24T07:05:43.342Z [WARN]  agent.cache: handling error in Cache.Notify: cache-type=service-http-checks error=""Internal cache failure: service '_nomad-task-4984d1ec-c7ab-09ad-883f-3c66cc9d1cb7-group-<redacted>-8083' not in agent state"" index
consul[3318]: 2022-05-24T07:05:43.349Z [WARN]  agent.cache: handling error in Cache.Notify: cache-type=service-http-checks error=""Internal cache failure: service '_nomad-task-4984d1ec-c7ab-09ad-883f-3c66cc9d1cb7-group-<redacted>-8083' not in agent state"" index
consul[3318]: 2022-05-24T07:05:43.349Z [WARN]  agent.cache: handling error in Cache.Notify: cache-type=service-http-checks error=""Internal cache failure: service '_nomad-task-4984d1ec-c7ab-09ad-883f-3c66cc9d1cb7-group-<redacted>-8083' not in agent state"" index
consul[3318]: 2022-05-24T07:05:43.380Z [WARN]  agent: Failed to deregister service: service=_nomad-task-4984d1ec-c7ab-09ad-883f-3c66cc9d1cb7-group-<redacted>-8083-sidecar-proxy error=""Service ""_nomad-task-4984d1ec-c7ab-09ad-883f-3c66cc9d1cb7-group-<redacted>
consul[3318]: 2022-05-24T07:05:43.380Z [WARN]  agent: Failed to deregister service: service=_nomad-task-4984d1ec-c7ab-09ad-883f-3c66cc9d1cb7-group-<redacted>-8083-sidecar-proxy error=""Service ""_nomad-task-4984d1ec-c7ab-09ad-883f-3c66cc9d1cb7-group-<redacted>
kernel: docker0: port 1(vethc6029f5) entered disabled state
kernel: vethb5b69c3: renamed from eth0
kernel: docker0: port 1(vethc6029f5) entered disabled state
kernel: device vethc6029f5 left promiscuous mode
kernel: docker0: port 1(vethc6029f5) entered disabled state
systemd[1]: Stopping Nomad...
```
</details>
",2022-06-02T17:44:06Z,2483388
4168,hashicorp/nomad,498868764,1165846957,"Hit this same issue today.  Performed some manual iptables clean up on a problem client.  Here are my notes in case this helps anyone else:

* Seems likely it was caused by some combination of quick nomad job allocation stop and re-deployment and/or nomad systemd service restarts, possibly before cleanup of a stopped allocation could be cleaned up.
* Symptom first noticed indicitive of a problem was of failed consul healthcheck, where the port that should be the listener on the host level and is properly bridged into the container as seen from the Nomad UI and nomad job config just isn't working.
* Tcpdump on the client machine to the port listener which doesn't appear to work shows SYN packet sent to a nomad bridge IP (`[S]`) and a RESET packet immediately returned (`[R.]`)
* Looking at the tcpdump output shows that the packet is actually being sent to the wrong nomad bridge IP, and further looking at the IP tables, shows that there are duplicate (or more) rules set up for port forwarding the listener into the Nomad bridge due to unclean allocation handling of an old allocation.
* On our OS, cleaning up the iptables can be done with a client reboot, but cleaning up by hand in these occurrences can be also be done.
* General procedure -- check iptables (command refs below).  There will be iptables entries which show comments of allocation ID association.  Check for non-existent allocation IDs being present and/or conflicting with existing allocation iptables rules.  If such allocation IDs are seen, they will also be seen to be associated with user defined iptables chains starting with CNI-[a-f0-9] and CNI-DN-[a-f0-9].  These can all be purged with the example cmds below:

```
# In this case, old rules from a nomad bridge IP with no active allocation superseded the correct rules to a nomad bridge IP with an active allocation listener.

# Find references to obsolete iptables rules with missing allocation IDs (first cmds contains all info, addnl cmds are a bit more verbose)
iptables-save
iptables -t filter -L -v -n
iptables -t nat -L -v -n

# Delete rules specific to the bad allocation IP from the filter and NAT tables
iptables -t filter -D CNI-FORWARD -d 172.26.66.2/32 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
iptables -t filter -D CNI-FORWARD -s 172.26.66.2/32 -j ACCEPT

# Delete references to the user defined chains of the non-existent allocation from the filter and NAT tables
iptables -t nat -D POSTROUTING -s 172.26.66.2/32 -m comment --comment ""name: \""nomad\"" id: \""8145e50e-b164-693e-2136-8055fde5ad10\"""" -j CNI-fe647aec064bf60036a312df
iptables -t nat -D CNI-HOSTPORT-DNAT -p tcp -m comment --comment ""dnat name: \""nomad\"" id: \""8145e50e-b164-693e-2136-8055fde5ad10\"""" -m multiport --dports 8008,5432 -j CNI-DN-fe647aec064bf60036a31
iptables -t nat -D CNI-HOSTPORT-DNAT -p udp -m comment --comment ""dnat name: \""nomad\"" id: \""8145e50e-b164-693e-2136-8055fde5ad10\"""" -m multiport --dports 8008,5432 -j CNI-DN-fe647aec064bf60036a31

# Delete rules from user defined chains of the non-existent allocation
iptables -t nat -F CNI-DN-fe647aec064bf60036a31
iptables -t nat -F CNI-fe647aec064bf60036a312df

# Delete the user defined chains from the non-existent allocation
iptables -t nat -X CNI-DN-fe647aec064bf60036a31
iptables -t nat -X CNI-fe647aec064bf60036a312df
```",2022-06-24T18:54:57Z,39752197
4169,hashicorp/nomad,498868764,1229108947,Any update on this issue? Facing it and it's causing very annoying stability issues on a select few hosts.,2022-08-27T03:02:15Z,1811263
4170,hashicorp/nomad,498868764,1264727324,"Please fix this or offer a proper solution, I don't care if we have to run a script to do it, but something that can be automated would be nice. We've positioned our whole infrastructure on Nomad, and this is killing us. We would prefer not to jump ship, but I'm still concerned how this isn't affecting other users?",2022-10-02T20:32:11Z,1811263
4171,hashicorp/nomad,498868764,1264730209,Affects us as well,2022-10-02T20:48:47Z,4280480
4172,hashicorp/nomad,498868764,1265425696,"Hey folks, we update issues when we're working on them. I can say this is on our roadmap but I can't really give a timeline.",2022-10-03T13:17:00Z,1409219
4173,hashicorp/nomad,498868764,1270581232,What are we supposed to do in the meantime? ,2022-10-06T19:28:35Z,1811263
4174,hashicorp/nomad,498868764,1271071212,This issue is pretty devastating for the use case at my PoB. Is there a workaround that can be implemented until an official fix comes out? At the moment we have to do full reboots of Nomad and take our whole network offline when we run into it.,2022-10-07T03:31:22Z,5481250
4175,hashicorp/nomad,498868764,1271533987,@johnalotoski has posted a process above; if you were to run that as a periodic task (or just a cron job) that'd clean up the iptables.,2022-10-07T12:35:37Z,1409219
4176,hashicorp/nomad,498868764,1272221015,"Little bit difficult to script around that honestly, since you're also having to compare to what allocations exist, and what host they are on",2022-10-08T04:18:40Z,1811263
4177,hashicorp/nomad,498868764,1280242535,"Okay, we're moving on from this, we can't support our org with this",2022-10-17T03:51:46Z,1811263
4178,citizenfx/fivem,1186681963,1186681963,"Hi, we have a problem. A new ""exploit"" has appeared, if it can be called that, a simple execution of the PlaySound function crashes certain players in range of the one executing the command ( using executors )
On one server 5 crashed, on another even 130 crashed at once.

Warning: I leave here the code in order to receive solutions.

```
for i=1, 100 do
           PlaySound(-1, 'Checkpoint_Hit', 'GTAO_FM_Events_Soundset', true)
           PlaySound(-1, 'Boss_Blipped', 'GTAO_Magnate_Hunt_Boss_SoundSet', true)
           PlaySound(-1, 'All', 'SHORT_PLAYER_SWITCH_SOUND_SET', true)
end
```",2022-03-30T16:07:46Z,78696476
4179,citizenfx/fivem,1186681963,1083371890,"Do you have any reproduction steps other than this code? Where to place this code for a test, any crash details, anything?",2022-03-30T16:40:00Z,24576130
4180,citizenfx/fivem,1186681963,1083382819,"> Do you have any reproduction steps other than this code? Where to place this code for a test, any crash details, anything?

You can use an executor to run that code from client-side, i tested it using a cheat that have this feature.
The crash happens instantly as the code is executed, game closes without giving any error.",2022-03-30T16:51:49Z,78696476
4181,citizenfx/fivem,1186681963,1083383867,"What is 'an executor' and where to find such? Again, providing more info would help. :/

Other than that, it's unlikely you're going to see this issue resolved if you don't provide info.",2022-03-30T16:53:00Z,24576130
4182,citizenfx/fivem,1186681963,1083387034,"> What is 'an executor' and where to find such? Again, providing more info would help. :/
> 
> Other than that, it's unlikely you're going to see this issue resolved if you don't provide info.

I have used Eulen.
![image](https://user-images.githubusercontent.com/78696476/160890101-772080c9-719f-4407-8427-8b519af8961a.png)
",2022-03-30T16:56:29Z,78696476
4183,citizenfx/fivem,1186681963,1083388343,Do you have a link to this 'Eulen' program at all? Or any other repro method?,2022-03-30T16:57:56Z,24576130
4184,citizenfx/fivem,1186681963,1083389296,"> Do you have a link to this 'Eulen' program at all? Or any other repro method?

Eulen is a paid-cheat, atm i don't really know a free executor.
There's a bunch of it but they are all paid-soft.",2022-03-30T16:58:57Z,78696476
4185,citizenfx/fivem,1186681963,1083390139,Do you have a link or a contact method for the author so they can provide more info?,2022-03-30T16:59:48Z,24576130
4186,citizenfx/fivem,1186681963,1083390689,I think that if you put the snippet just in a regular client.lua script (without cheating) your game would also crash?,2022-03-30T17:00:23Z,42589271
4187,citizenfx/fivem,1186681963,1083392045,"The only thing you should do is make it some way that it is only executed by one player (to immitate a cheat menu, to check if the other players that arent executing the code are also crashing.",2022-03-30T17:01:49Z,42589271
4188,citizenfx/fivem,1186681963,1083392827,"> Do you have a link or a contact method for the author so they can provide more info?

No, several people have joined my server and crashed players.
My Anticheat took screenshots of them, I saw the Lua menu they were using, got hold of it and identified the code they were using to crash players.
The code i sent here.",2022-03-30T17:02:36Z,78696476
4189,citizenfx/fivem,1186681963,1083393246,"> Do you have a link or a contact method for the author so they can provide more info?

I also think that a cheat provider whose goal is to irritate people on fivem, won't give any info about making the cheat menu less irritating.",2022-03-30T17:03:07Z,42589271
4190,citizenfx/fivem,1186681963,1083393481,"> > Do you have a link or a contact method for the author so they can provide more info?
> 
> No, several people have joined my server and crashed players. My Anticheat took screenshots of them, I saw the Lua menu they were using, got hold of it and identified the code they were using to crash players. The code i sent here.

Have you verified and confirmed that the code is the culprit by testing it by yourself with some participants?",2022-03-30T17:03:26Z,25645606
4191,citizenfx/fivem,1186681963,1083394398,"> 

Yes. I have tested it with my staff-team, they are all crashing the way i said, without any error.",2022-03-30T17:04:21Z,78696476
4192,citizenfx/fivem,1186681963,1083395632,"Can you send a screenshot of the crash dialog that's presented to them?
Any information about the crash is helpful, and so far you haven't provided anything other than the snippet of code that's supposedly causing the crash.",2022-03-30T17:05:35Z,25645606
4193,citizenfx/fivem,1186681963,1083395972,"Locking this issue report for now, too many people trying to involve themselves here.",2022-03-30T17:06:00Z,24576130
4194,citizenfx/fivem,1186681963,1083531913,"To people trying to make new issue reports about this: **don't.** You're not helping by doing such.

This is already tracked internally as of ~20 days ago, but it didn't get picked up for some reason. I'm also unable to pick this up myself at this time, so instead internal parties have been reminded again.",2022-03-30T19:19:16Z,24576130
4195,zammad/zammad,1500084349,1500084349,"### Used Zammad Version

5.3

### Environment

- Installation method: package
- Operating system (if you're unsure: `cat /etc/os-release` ): [e.g. debian 10.4, ubuntu 20.04]
- Database + version:  mariadb
- Elasticsearch version: [e.g. 7.17]
- Browser + version: [e.g. chrome 83, safari 14, firefox 105]


### Actual behaviour

- Migration does not convert the json values to the array structure of postgres. I think there is a rake task missing which will look up all values in the database and convert the json structures to array.

I attached a PoC how it could look like:

[fix_maria_migration.zip](https://github.com/zammad/zammad/files/10245529/fix_maria_migration.zip)


### Expected behaviour

- Migration should work.

https://docs.zammad.org/en/latest/appendix/migrate-to-postgresql.html

### Steps to reproduce the behaviour

Migrate from mysql/mariadb to postgres, but before create multi tree selects and fill up some tickets:

https://docs.zammad.org/en/latest/appendix/migrate-to-postgresql.html

### Support Ticket

Ticket#10115329

### I'm sure this is a bug and no feature request or a general question.

yes

### ToDo:

- [x] Investigate a post or pre pgloader solution
- [x] Decide and implement a solution
- [x] Update the current documentation
- [x] Adding a integration test of the database migration from mysql/mariadb to postgres",2022-12-16T11:53:58Z,5445062
4196,zammad/zammad,1500084349,1354630399,@MrGeneration Can you add a note to the documentation that the migration path is currently broken linked to this issue?,2022-12-16T11:54:24Z,5445062
4197,zammad/zammad,1500084349,1354637945,"That's entirely the wrong repository.
The PoC is not living here.",2022-12-16T11:57:30Z,6549061
4198,zammad/zammad,1500084349,1354673441,After an internal heads up by @rolfschmidt I learned that this is something the migration script can't handle and thus needs a rake task.,2022-12-16T12:13:33Z,6549061
4199,zammad/zammad,1500084349,1357557285,"Alternative solution idea: maybe we could create a rake task that reads the DB before migration, and outputs [pgloader casting rules](https://pgloader.readthedocs.io/en/latest/ref/mysql.html?highlight=cast#mysql-database-casting-rules) which would perform the migration correctly in the first place.",2022-12-19T12:05:34Z,211281
4200,zammad/zammad,1500084349,1359343433,"There are currently concerns regarding the performance of the PoC attached to this issue, since it uses `ActiveRecord::Migration` instances, creates new columns, migrates the data, drop, then rename the columns... Also, it's not clear if it would have additional side-effects, since it is supposed to be run in the application/Rails context. A benchmark with a large data set and both solutions in place (PoC vs `pgloader`) might try dispel these concerns, but it hasn't been tried yet.

Initial research into `pgloader` casting rules resulted in some findings:

* It doesn't look like `pgloader` provides a suitable transformation function for a JSON array into a PostgreSQL array which can be used as part of its `CAST` command (on the column type level.) We could not find an example either, and trying some stuff including expected DSL like `""text array""` didn't lead to meaningful results. This doesn't mean it's not supported, just that we could not identify it at this time.

* [An issue for `pgloader`](https://github.com/dimitri/pgloader/issues/881) with a similar problem provided a possible workaround via `AFTER LOAD DO` block, which can transform specific columns on specific tables only, but it does use a `translate` function which seems to work for our JSON => array use-case. (Note that this same function doesn't seem to be provided in the `CAST` context.) 

Here is a working example for `pgloader` post-processing block for one `multi_tree_select` column on the `tickets` table:

```
...
AFTER LOAD DO
    $$ alter table tickets alter column multi_tree_select type text[] using translate(multi_tree_select::text, '[]', '{}')::text[] $$
...
```

A proposed rake pre-task could try to prepare the complete `pgloader` script and include one line per affected column.",2022-12-20T13:23:16Z,6049445
4201,zammad/zammad,1500084349,1359348022,"@dvuckovic that's kindof what I hoped for. So I'm all for pursuing this further. It should be best to get the migration right in the first place.

Please note that @rolfschmidt included at least one column for this transformation which does not come from object attributes (IIRC `PublicLink`). We should try to include all candidates, of course.",2022-12-20T13:27:29Z,211281
4202,zammad/zammad,1500084349,1361049031,"Another idea that just came to mind: it's possible to get all JSON columns in a MySQL database with one query.

```
mysql> select table_name, column_name from information_schema.columns where table_schema = 'zammad_development' and data_type = 'json';
+--------------+-------------------+
| table_name   | column_name       |
+--------------+-------------------+
| public_links | screen            |
| tickets      | multi_tree_select |
| tickets      | multi_select      |
+--------------+-------------------+
3 rows in set (0.00 sec)
```

However, it's still safer to go with the PoC approach since we are absolutely positive these columns are used only as simple JSON arrays.",2022-12-21T09:13:43Z,6049445
4203,zammad/zammad,1500084349,1361102781,"@dvuckovic plus, this would not work with MariaDB, as it does not have native json types.

Perhaps it would be a good idea to verify your solution also for MariaDB -> PG migration.",2022-12-21T09:59:58Z,211281
4204,zammad/zammad,1500084349,1361235079,"@mgruner Correct, in MariaDB these columns are of `longtext` type.

I just verified that the same script works on MariaDB as well, columns are properly migrated to `array` type on PostgreSQL.",2022-12-21T12:08:35Z,6049445
4205,zammad/zammad,1500084349,1362795200,"For posterity: we did some reproduction of the original issue, by skipping `AFTER LOAD DO` statements in our test job. And the results are rather surprising!

On MySQL, the column will be left in `json` type on PostgreSQL (it still supports them!). While everything will still work as expected (the stored data is decoded/encoded correctly), the schema is definitely different than a regular PostgreSQL instance that was initialized from scratch.

On MariaDB, the column will be migrated to `text` type on PostgreSQL. This breaks the decoding of stored values, and while the system will still happily continue chugging on, the data does look corrupted. Just take a look how the frontend shows some of these multiselects:

![Image](https://user-images.githubusercontent.com/6049445/209136432-bfe873ab-85e6-4b77-8004-3e503f549b46.png)
![Image](https://user-images.githubusercontent.com/6049445/209136431-265ae272-c120-46f5-8689-4ba1e5cd091f.png)

Strictly speaking, this means the original issue is reproducible only on MariaDB backends. However, it's not clear if there would be some issues down the line, so we would still prefer the migration to `array` columns.",2022-12-22T12:44:06Z,6049445
4206,zammad/zammad,1500084349,1362798907,"> Strictly speaking, this means the original issue is reproducible only on MariaDB backends. However, it's not clear if there 
> would be some issues down the line, so we would still prefer the migration to array columns.

I am confused as I received this PR: 
https://github.com/zammad/zammad-documentation/pull/263

What is needed what is not?
Can we please clarify that before hand? I feel like I'm touching things without any use.",2022-12-22T12:48:37Z,6549061
4207,zammad/zammad,1500084349,1362812644,"@MrGeneration Please ignore my previous comment, it was just a minor finding during testing.

The documentation PR you linked to looks correct and the new command applies to both MySQL and MariaDB backends. With the fix in place, the migrated columns are always of the same data type (`ARRAY`), which is what you would expect.",2022-12-22T13:03:38Z,6049445
4208,EbookFoundation/free-programming-books,1043368759,1043368759,"- Homogenize markdown format across files. Solve some linter faults
   - [x] #6625
   - [x] #6724
- [x] #6698
- [x] Sort links according to english text seen in README.md. **Addressed in #6164**

### Local branch

- [x] Reintegrate #5837
- [x] ~Reintegrate stalled conflictive #5752 to complete translation~. Addressed by: #6719
- [x] Reintegrate #6429
- [x] Reintegrate #5831
- [x] Reintegrate #6567 ",2021-11-03T10:28:56Z,3125580
4209,EbookFoundation/free-programming-books,1043368759,969096629,"I'll prepare a PR that
- moves all the instructions files into  a docs directory (is that the best name?)
- removes crosslinks from each translated file and adds them to a docs index page so new files only need to add a link in one place.",2021-11-15T16:42:01Z,926513
4210,EbookFoundation/free-programming-books,1043368759,969367481,"> I'll prepare a PR that
> 
> * moves all the instructions files into  a docs directory (is that the best name?)
> * removes crosslinks from each translated file and adds them to a docs index page so new files only need to add a link in one place.

See this note in chore posted by you: https://github.com/EbookFoundation/free-programming-books/issues/6164#issuecomment-940222595",2021-11-15T22:08:11Z,3125580
4211,EbookFoundation/free-programming-books,1043368759,970457367,"so using ""docs"" works?",2021-11-16T16:42:55Z,926513
4212,EbookFoundation/free-programming-books,1043368759,1039943890,"@EbookFoundation/reviewers last step. What do you think about this HowTo's homogeneization?

Performed Tasks:
- Add H1 header
- `Read this ...` right alignmements
- Split a bit first paragraph to highlight welcome.
- Introduce Pull Request acronym.
- Trademark typos: GitHub, YouTube, Pull Request
- Update GitHub links urls. `help`/`guides` subdomain was moved to `docs` so a 301 Redirect is skipped.
- Details block added, with center align. When clicked, the contributor graphs are toggled.
- Some bolds to highlight common issues (like in CONTRIBUTING)
- Repo name in monospace font
- Re-worded last part of 4th paragraph to explain that no new PR is needed to open if there are linter errors or changes are requested. Now it's more clear, I thought.

![image](https://user-images.githubusercontent.com/3125580/154009817-8d08ec9d-2d2d-4ba7-930b-4d01e31bcfcf.png)

It's ok? Aproved to continue apply into the other files?",2022-02-15T07:31:09Z,3125580
4213,EbookFoundation/free-programming-books,1043368759,1040046410,"I think this looks pretty nice, but _unfortunately_, you may want to test it in the mobile app.

I don't think things like the `<details>` tag work there. (Bear in mind when I tested was over a year ago.)

We could always ignore that technically, since I'd consider that a bug on GitHub's side, though. (Markdown is Markdown, and should work regardless of client.)",2022-02-15T09:27:35Z,22801583
4214,EbookFoundation/free-programming-books,1043368759,1040258059,"> I think this looks pretty nice, but _unfortunately_, you may want to test it in the mobile app.
> 
> I don't think things like the `<details>` tag work there. (Bear in mind when I tested was over a year ago.)
> 
> We could always ignore that technically, since I'd consider that a bug on GitHub's side, though. (Markdown is Markdown, and should work regardless of client.)

https://davorpa.github.io/free-programming-books/ Tested on a Kindle Fire EReader and a Samsumg S5 (Android 6.1). It works as expected 4x4 in both environments: GitHub Preview, GitHub Pages. In anyway, the previous link ""large, growing"" points to same graph
",2022-02-15T13:14:09Z,3125580
4215,EbookFoundation/free-programming-books,1043368759,1040286981,"Just checking, you tested in the GitHub Mobile App?
https://play.google.com/store/apps/details?id=com.github.android&hl=en_US&gl=US",2022-02-15T13:40:09Z,22801583
4216,EbookFoundation/free-programming-books,1043368759,1040337013,"> Just checking, you tested in the GitHub Mobile App?
> https://play.google.com/store/apps/details?id=com.github.android&hl=en_US&gl=US

yes. it works as well.

| Closed | Open |
|---|---|
| ![Screenshot_20220215-152254.png](https://user-images.githubusercontent.com/3125580/154081554-0255f8dc-97fa-4f83-97fb-276be29d739f.png) | ![Screenshot_20220215-152337.png](https://user-images.githubusercontent.com/3125580/154081630-f5667cfd-8dd2-4da4-9a45-c538ba1fcc11.png) |",2022-02-15T14:26:45Z,3125580
4217,EbookFoundation/free-programming-books,1043368759,1040490811,"on the whole, I think this looks great. 
I'm ambivalent about monospacing ""Free-Programming-Books"". The name travels outside of the repo to places where it's not possible (or just more work for me) to monospace it - twitter, the Free Ebook Foundation website, and most particularly, the HTML presentation on github. I've managed to keep the dashes intact, mostly. So while monospacing looks good here, I prefer to use less code-y version.
Have commented in the PR on the text that reveals the usage graph.",2022-02-15T16:31:34Z,926513
4218,EbookFoundation/free-programming-books,747855745,747855745,"It seems that the | in the link breaks the rendering
![C#](https://user-images.githubusercontent.com/464876/99858549-b73ff980-2b85-11eb-81bc-0cd2dcb04e77.png)

Renders correctly when viewing the page within GitHub editing tools, but renders incorrectly on the GH Pages site.",2020-11-20T23:16:43Z,464876
4219,EbookFoundation/free-programming-books,747855745,731469274,not surprising I guess. Want to fix it?,2020-11-21T00:08:04Z,926513
4220,EbookFoundation/free-programming-books,747855745,731490030,hi @eshellman - I'm looking into it now...  ,2020-11-21T02:03:16Z,464876
4221,EbookFoundation/free-programming-books,747855745,731892873,"For more information - I have developed a linter plugin for this issue, just trying to figure out how to contribute it.

https://github.com/computamike/remark-link-escape

Do I close this issue, or is that something that the team needs to do?",2020-11-23T02:09:27Z,464876
4222,EbookFoundation/free-programming-books,747855745,732206068,Let's leave it open; hopefully it will be addressed together with  #5031 ,2020-11-23T14:44:06Z,926513
4223,EbookFoundation/free-programming-books,747855745,890391432,"Hi team

Although the nice @computamike's plugin was added as dependency in v2.2.0, this improvement was reverted in 2.3.0 (2020 Dec)

https://github.com/vhf/free-programming-books-lint/compare/v2.2.0...v2.3.0

@vhf CI action is now using last released version (v3.0.0) with last alphabetize requested features #5031 but not with vhf/free-programming-books-lint#12

CI logs: https://github.com/EbookFoundation/free-programming-books/runs/3195091515?check_suite_focus=true#step:4:8 (see install lint step)

I don't know what is their status because I'm newbie, but it would be a nice point to reintegrate again before it rots into oblivion, I thought @eshellman 🤗

/labelthis: resummit pr",2021-07-31T19:04:42Z,3125580
4224,EbookFoundation/free-programming-books,747855745,932840330,"Took me awhile to find the relevant page: https://ebookfoundation.github.io/free-programming-books/casts/free-podcasts-screencasts-en.html

Appears to be rendering fine now
![image](https://user-images.githubusercontent.com/4824630/135735312-eeef87da-7acd-4ece-9c97-83bc9ac380b8.png)
",2021-10-03T00:34:44Z,4824630
4225,EbookFoundation/free-programming-books,747855745,932845566,"> Took me awhile to find the relevant page: https://ebookfoundation.github.io/free-programming-books/casts/free-podcasts-screencasts-en.html
> 
> Appears to be rendering fine now

We know it. See the fix #5177. The vertical bar is escaped

",2021-10-03T01:28:11Z,3125580
4226,EbookFoundation/free-programming-books,747855745,932846482,Oh nice thanks! What is the remaining work on this issue? Did you want to close this issue?,2021-10-03T01:39:46Z,4824630
4227,EbookFoundation/free-programming-books,747855745,1035946731,"It also happens in Arabic courses file #6715 

| Environment | Screenshot |
|---|---|
| Github File Preview :+1: | ![image](https://user-images.githubusercontent.com/3125580/153564450-5db31da1-beb6-458f-b783-734f06e277f8.png) |
| GitHub Pages :-1:           | ![image](https://user-images.githubusercontent.com/3125580/153564725-14fdd447-9871-4d26-b2e4-ed0213fb1f57.png) ![image](https://user-images.githubusercontent.com/3125580/153564985-80c9f555-6218-44d0-bf89-27a7371274fd.png) |

Is wellknown that pipes is the table markdown token. Seems that Kramdown don't deal well with it",2022-02-11T07:36:52Z,3125580
4228,EbookFoundation/free-programming-books,747855745,1039391336,"Needs linter replug in to definitelly solve it. See https://github.com/EbookFoundation/free-programming-books/issues/5176#issuecomment-890391432

Notified to kramdown engine at https://github.com/kramdown/parser-gfm/issues/35",2022-02-14T18:05:23Z,3125580
4229,strapi/strapi,453440611,453440611,"<!--
Hello 👋 Thank you for submitting an issue.

Before you start, please make sure your issue is understandable and reproducible.
To make your issue readable make sure you use valid Markdown syntax.

https://guides.github.com/features/mastering-markdown/
-->

**Describe the bug**
Currently it seems it's not possible to create relations with the Administrator model (one-way).

An example use case:
We have a plugin in which we want to create new entities of a certain type, but we also want to know which administrator created them, so we would like to save a relationship between this entity and administrator. This can be a one-way relationship, but it seems even that is not possible.

**Steps to reproduce the behavior**
1. Create a model
2. Create a relationship with administrator:
```
""created_by"": {
  ""model"": ""administrator"",
  ""plugin"": ""admin""
}
```
3. Run project
4. See error

**Expected behavior**
Ability to create relationships (even if one-way) to administrator

**Screenshots**
N/A

**Code snippets**
N/A

**System**
- Node.js version: v10.2.1
- NPM version: 5.6.0
- Strapi version: 3.0.0-beta.2
- Database: PostgreSQL
- Operating system: Archlinux

**Additional context**
N/A
",2019-06-07T10:17:02Z,2807772
4230,strapi/strapi,453440611,499841948,"Agreed, @lauriejim this is also related to what I mentioned to you in slack about overriding the admin model file",2019-06-07T10:50:51Z,8593673
4231,strapi/strapi,453440611,499884619,"Hi,

I agree with you it would be a nice feature to have. 

Right now you can implement your usecase by using the user from the users-permissions plugin (I know it means having duplicates of your admins in the users) but this will at least allow you to implement until this feature is implemented somehow.

@derrickmehaffy @mdibaiee What kind of relations would you like to have ?",2019-06-07T13:28:26Z,6065744
4232,strapi/strapi,453440611,500028218,"I have a similar issue with the new way the admin is configured.

## My use case

We have an in-house, agency calendar. Users must sign in to view the calendar. The sign in authentication is handled by Strapi.

Until recently, the calendar user's roles were either Strapi 'Administrators' or Strapi 'Authenticated'. There is no 'Public' access to the calendar. Users registered via a registration link (the registration process handled by Strapi) -- and a successful, email verified registration meant you were automatically set as 'Authenticated'.

If your Strapi user role is 'authenticated', you're only able to view the calendar. If, however, your Strapi user role is 'Administrator', you're able to view the calendar as well as make edits to the calendar via the Strapi admin back-end. A unit manager would say, hey, so-and-so needs to edit the calendar as well as view the calendar. When I got this request, I manually bumped the user's role from 'Authenticated' to 'Administrator' -- and then I'd send that new 'Administrator' user a link to the Strapi admin.

The point here is that both 'Authenticated' and 'Administrator' users used the same login process -- go to the registration link on the front-end, register, verify e-mail, and then login via the ```/auth/local```. The best part of this is that the user -- no matter if she were 'Authenticated' or 'Administrator' could use the same username and password for access to the back-end Strapi admin and the front-end calendar app (written in Vue)

## What's happening now

Now, 'Administrator' roles have to be authenticated via ```/admin/auth/local``` on the front-end whereas 'Authenticated` users authenticate via ```/auth/local```.

## The issue

So the issue right now is that those admin users can't simply login to the calendar anymore. The ```/auth/local``` route won't authenticate them. They need a seperate username/password to access the Strapi backend. So they'll have two username/password combos -- one to access the Strapi back-end, another to access the 'front-end'.

## Confusion

So, I'm confused now. I'd been using the single route to login. Now there's two routes. So either I need to create sepeate usernames and passwords -- or I ask the user up front: are you an admin? Are you a user? ... and then set the auth route accordingly.



My questions:

- How can I set an 'administrator' role so that she's able to log in to the Strapi back-end and my app front-end with the same username and password?

- I no longer see the 'administrator' role in my content type user relation. So, if say, a person is a Strapi admin and an authenticated Strapi user -- their username needs to be in both the 'Admin' role as well as the 'authenticated' role in order to see it in the user-relation dropdown on a content-type (and, say, add that user to a blog post or article or whatever.)

- Even if I create a brand new custom role -- and click every box for the permissions -- that new role isn't able to access the Strapi admin. I want a simple checkbox to say, okay, can this user access the Strapi admin? If yes -- click it and grant access. That's it. That seems like the simplest solution.

I understand the desire to seperate the strapi roles from the app roles -- but in my case, and (I suspect) many others -- the roles are identical. An 'admin' is an 'admin', 'root' is 'root', 'superAdmin' is 'superAdmin' -- so she should be able to log in to the strapi backend and login to the app front-end -- all with the same username and password.

The alternative, of course, is to create a brand new back-end UI -- essentially duplicating Strapi's admin -- and granting that custom user (or 'authenticated' user) access to my custom back-end UI -- and then just using Strapi as a way to handle GET/POST requests. 

But this backend UI and baked-in authentication was exactly the reason why I started using Strapi in the first place. :)",2019-06-07T20:29:27Z,612753
4233,strapi/strapi,453440611,500113939,"@alexandrebodin currently, our use case is just a one-way relationship to the administrator model.

We want our local plugins to be able to create models, which, as a metadata, include the Administrator which created them in their database record. So our table looks like this:

```
{
  ""connection"": ""default"",
  ""collectionName"": ""sms"",
  ""info"": {
    ""name"": ""sms"",
    ""description"": """"
  },
  ""options"": {
    ""increments"": true,
    ""timestamps"": true,
    ""comment"": """"
  },
  ""attributes"": {
    ""created_by"": {
      ""model"": ""administrator"",
      ""plugin"": ""admin""
    },
    ""body"": {
      ""type"": ""string"",
      ""required"": true
    },
    ""retailer"": {
      ""model"": ""retailer""
    },
    ""users_count"": {
      ""type"": ""integer"",
      ""min"": 0
    },
    ""created_at"": {
      ""required"": true,
      ""type"": ""date""
    },
    ""updated_at"": {
      ""required"": true,
      ""type"": ""date""
    },
    ""deleted_at"": {
      ""required"": false,
      ""type"": ""date""
    }
  }
}
```

If I understand correctly, the only change I'm looking for right now is the ability to specify `admin` as the `plugin` (or any other type of change) to use the model.

If you can point me to where the code for this resides, I might be able to create a pull-request myself.

As a next step (and I can foresee this requirement), I think we potentially will want to customize the administrator model as well.",2019-06-08T10:34:24Z,2807772
4234,strapi/strapi,453440611,501163858,"Hi guys. 

Here is how I think the admin / user-permissions should work in the future so we can discuss the short term wins we can have:

## User-permissions

The idea is to make the `user-permissions` plugin a real `oauth` / `open id connect` server and use passportjs to allow for a lot of providers. 

## Admin connection

In the same way I want the admin to use passport so people can configure it to connect to the users-permissions plugin if you want to. Use a ldap or any other passport providers.


What do you guys think about it ? 

",2019-06-12T08:00:02Z,6065744
4235,strapi/strapi,453440611,501164197,"For the fields/columns customization it is a much simpler usecase. You can already custmize it by make the updates in the model directly in your bootstrap function. 
",2019-06-12T08:01:01Z,6065744
4236,strapi/strapi,453440611,501192117,"> For the fields/columns customization it is a much simpler usecase. You can already custmize it by make the updates in the model directly in your bootstrap function.

@alexandrebodin Probably should elaborate on this for over-riding the model.",2019-06-12T09:21:57Z,8593673
4237,strapi/strapi,453440611,501200923,"You can add custom fields attributes (⚠️ not relations ⚠️ ) by creating a custom hook.

Make sure to put run it first by updating the config in 

`config/hook.json`

```json
{
  ""timeout"": 3000,
  ""load"": {
    ""before"": [""custom-hook""],
    ""order"": [
      ""Define the hooks' load order by putting their names in this array in the right order""
    ],
    ""after"": []
  }
}
```
`hooks/custom-hook/index.js`
```js
const _ = require('lodash');

module.exports = () => {
  return {
    initialize(cb) {
      _.merge(strapi.admin.models.administrator, {
        attributes: {
          randomColum: {
            type: 'string',
          },
        },
      });

      cb();
    },
  };
};
```

We either can allow modifiy the model via json like the other and in the interface but this will make the possbile errors too important in my opinion.

I would like to provide and fairly simple api to customize a model on startup.

example:
```js
strapi.models('administrator').extend({
  atttributes: {
    newField: {},
    newRelation: {},
  },
});
```

",2019-06-12T09:47:10Z,6065744
4238,strapi/strapi,453440611,501206093,"@alexandrebodin what about modifying existing values? Such as these: 

https://github.com/strapi/strapi/blob/15daa6212c1323c1ccb12099df92d8a51a704ee1/packages/strapi-admin/models/Administrator.settings.json#L2-L6",2019-06-12T10:02:33Z,8593673
4239,strapi/strapi,453440611,501207096,"And for example what if someone wants to modify their services/controllers of the admin package?

IE: https://github.com/strapi/strapi/blob/master/packages/strapi-admin/services/auth.js

I'm more for the opinion that the admin package needs to be split, one for the UI, the other should be an authentication plugin similar to `users-permissions`",2019-06-12T10:05:32Z,8593673
4240,strapi/strapi,453440611,501207199,"(or convert the whole admin package itself into a plugin)
",2019-06-12T10:05:51Z,8593673
4241,strapi/strapi,453440611,501211672,"you can change any meta with the example I provided. At your own risks.

I really don't think people should modify the controllers or services. Extending the model makes sense. Providing some programtic extension points too but just changing the files is really the best way to allo people to break everything at anytime. It doesn't really scale.
If it is missing features I would rather have them start a conversation and make a PR so it helps not only them but the community. Alllow people to just overwritte any files kinda break the idea of making Strapi open source as no one would need to contribute back to it 🤔 I guess it is a matter of opinion anyway.

",2019-06-12T10:19:56Z,6065744
4242,strapi/strapi,453440611,501217942,"@alexandrebodin 

> Alllow people to just overwritte any files kinda break the idea of making Strapi open source as no one would need to contribute back to it

I would like to point out that realistically Strapi should for easy customization in all aspects, this should include options for all packages as not everyone's use-case is going to follow ""the heard"", and there is going to be some niche cases. As an example extending the admin authentication to add certain password, username, email restrictions. Maybe specific password polices, the options are limitless and hence the reason I think that admin package needs to allow for this. 

If we are allowing users to modify the ""frontend"" of the adminUI, the backend should have the same respect.

![image](https://user-images.githubusercontent.com/8593673/59344633-57bc9980-8cc3-11e9-93a4-4235657eb9ab.png)
",2019-06-12T10:40:23Z,8593673
4243,strapi/strapi,453440611,501218955,"^^^ ""The API"" in the above image should be including the Admin Authentication API along with it's services, controllers, and routes.",2019-06-12T10:43:44Z,8593673
4244,strapi/strapi,453440611,501235122,"As long as we can create apis that respect the Open/closed principle sure. 

Otherwise Strapi becomes responsible for the bugs people create in the codebase. In this situation it's better to fork say a plugin than just change half of the codebase locally and still ask strapi to make sure everything works after upgrading. 

It's would be unrealistic to think that would be a maintainable ecosystem",2019-06-12T11:37:54Z,6065744
4245,strapi/strapi,453440611,502951492,@alexandrebodin I would like to work on this since our product has a strict requirement for customizing the admin model. Can you please point me in a direction that matches your vision on this issue and I can get started on this?,2019-06-18T05:20:55Z,2807772
4246,strapi/strapi,453440611,502989435,"Hi @mdibaiee before starting we need to figure out what is going to be allowed to be modified and how. 
Does having the Model in the UI is necessary for example. Or can we just give a way to extend it programatically.",2019-06-18T07:48:54Z,6065744
4247,strapi/strapi,453440611,502991580,"@alexandrebodin I agree. So, the way we think about it is that we want it to be a model just like any other model, very similar to how plugins are handled, we should be able to extend it and create relations with it. We would also like to manage administrators from the admin panel.

I think having it in the UI makes sense, if I try to think of a CMS, I can always imagine it having a panel for managing admin users (WordPress does that).

I also don't see a reason not to allow _extensions_ to the admin model, services and controllers. Though one may argue that we should not be able to remove anything or modify the default attributes from the model to avoid breaking the application, and I think that makes sense.

I'm interested to hear your opinions on this as well.",2019-06-18T07:55:34Z,2807772
4248,strapi/strapi,453440611,503504895,"Any thoughts on this, @alexandrebodin ?",2019-06-19T10:33:45Z,2807772
4249,strapi/strapi,453440611,520900930,I'd love a way to be able to associate an administrator with one of my content types.,2019-08-13T16:09:27Z,158645
4250,strapi/strapi,453440611,618498068,"> You can add custom fields attributes (⚠️ not relations ⚠️ ) by creating a custom hook.

@alexandrebodin I've just tried your suggestion and unfortunately the field does not get saved in mongo. The field correctly appears in the UI and the value I set for it is send as part of the PUT request to update an existing administrator (`/content-manager/explorer/strapi::administrator/:id`). But the value never reaches mongo. Any idea what could be wrong?

I'm on 3.0.0-beta.19.3

",2020-04-23T16:24:03Z,8288413
4251,strapi/strapi,453440611,621326415,"> > You can add custom fields attributes (⚠️ not relations ⚠️ ) by creating a custom hook.
> 
> @alexandrebodin I've just tried your suggestion and unfortunately the field does not get saved in mongo. The field correctly appears in the UI and the value I set for it is send as part of the PUT request to update an existing administrator (`/content-manager/explorer/strapi::administrator/:id`). But the value never reaches mongo. Any idea what could be wrong?
> 
> I'm on 3.0.0-beta.19.3

Hi @aefox
I follow this way override the strapi-admin package for adding the relation field 'role' to the administrator model.

 First, I added one field to this file:
https://github.com/strapi/strapi/blob/master/packages/strapi-admin/models/Administrator.settings.json
  ```
 ""role"": {
      ""plugin"": ""users-permissions"",
      ""model"": ""role""
    }
```
Then check the create and update functions on https://github.com/strapi/strapi/blob/master/packages/strapi-admin/controllers/Admin.js

```
const { email, username, password, blocked, ...otherProps } = ctx.request.body;
...
const user = {
      email: email,
      username: username,
      blocked: blocked === true ? true : false,
      password: await strapi.admin.services.auth.hashPassword(password),
      ...otherProps
    };
```
And it worked!
Hope it helps,

",2020-04-29T16:37:09Z,61815769
4252,strapi/strapi,453440611,641129122,"Can i use the following approach. Im new to strapi

// helper/admin-overrides.js
const fs = require('fs');
 let dir = './extensions/admin/models/Administrator.settings.json';
 let dirDist = './node_modules/strapi-admin/models/Administrator.settings.json'
 fs.copyFileSync(dir, dirDist);

 dir = './extensions/admin/controllersx/Admin.js';
 dirDist = './node_modules/strapi-admin/controllers/Admin.js'
 fs.copyFileSync(dir, dirDist);

And use this in package.json
""scripts"": {
    ""develop"": ""npm run override-admin && strapi develop"",
    ""start"": ""npm run override-admin && strapi start"",
    ""override-admin"": ""node helper/admin-overrides.js"",",2020-06-09T08:41:03Z,10430751
4253,strapi/strapi,453440611,716595222,"This issue has been mentioned on **Strapi Community**. There might be relevant details there:

https://forum.strapi.io/t/extend-user-admin-model/713/1
",2020-10-26T14:46:40Z,5716596
4254,strapi/strapi,453440611,1078326108,"Due to the nature of how the admin plugin now works in v4, we currently do not plan to allow for this type of customization by default and there are other methods such as patch-package that do make it possible without needing support from us on this.

Marking as closed as discussed with @Aurelsicoko ",2022-03-24T21:11:50Z,8593673
4255,strapi/strapi,393724050,393724050,"<!-- ⚠️ If you do not respect this template your issue will be closed. -->

<!-- ⚠️ Make sure to browse the opened and closed issues before submitting your issue. -->

<!-- 
Please also submit your idea on the Strapi Product Board:
https://portal.productboard.com/strapi/tabs/2-under-consideration/submit-idea

If your request on the product board is accepted this feature request issue will be closed,
but will still accept public discussion.
-->

- [ ] **I have created my request on the Product Board before I submitted this issue**
- [ ] **I have looked at all the other requests on the Product Board before I submitted this issue**

**Please describe your feature request:**

### Background

**Context**
I have been working over the last few days with the strapi code-base add support for geographical data types (GeoJSON, to be more precise). I was pleased to see that strapi already had support for JSON, so I repurposed the code for `InputJSON` and `InputJSONWithErrors` components to create the GeoJSON content-type. To do so, I've followed the instructions posted on [this issue](https://github.com/strapi/strapi/issues/483#issuecomment-358971985) 

 **Why a separate GeoJSON content-type? (A real world use-case)**
Our organization primarily works with local governments in Nepal to build official portals that help them connect with their citizens. In doing so, some offices have realized a need to include a feature that allows them to visualize all of capital-projects for the fiscal year. See image below:

![image](https://user-images.githubusercontent.com/24402285/50380691-535ddf00-0697-11e9-9e77-2cad4e1674d7.png)

We want government officials to be able to upload geographic information on their own. Due to their limited technical knowledge, we found that the best way to get this done is by providing them with a user-interface to draw out the project.

I've gone ahead and created a component to facilitate the same. 

### Steps taken:

**Steps to display new input in the content type builder**

- To display my new field I have added it it in the `/plugins/content-type-builder/admin/src/containers/Form/forms.json`file.

- I've also populated the corresponding translations in the `/plugins/content-type-builder/admin/src/translations` folder. However, since I don't know language other than English, I've used english everywhere.

- I added another `switch-case` statement inside  `strapi/plugins/strapi-hook-mongoose/lib/utils/index.js` file for my new content-type (geojson), simply copy pasting the one for json. The plugin didn't run until I did that.

- I have also added images for my new content type in the `AttributeRow` and `AttributeCard` components.

- I was thus able to display my new attribute in the plugin (this might have an impact on the plugin's component)

**Using my new type in the content manager**

- I had to add a few new packages npm packages `(mapbox gl-js, @mapbox/mapbox-gl-js-draw, @mapbox/geojson-extent etc.)` and css files, I did not know how to add CSS files so i directly used the index.html file in  the `admin/admin/src/index.html` I know this is not optimal, what are my options?

  - When adding `@mapbox/mapbox-gl-js-draw` I ran into issues as I rebuilt the  `content-manager` plugin. So for my present purpose, I have served the CDN through my public directory `/public`, I’m aware this is not the best way to do things. Looking forward to your inputs here too.

- I also had to update the `containers/saga.js` and update the following line :
`const cleanedData = (attrType === 'json' || attrType==='geojson') ? record[current] : cleanData(record[current], 'value', 'id');`

- In the content manager plugin I modified the `Edit` container and its children in order to display my new field type.

- I created the `InpuGJSON` and `InputGJSONWithErrors` component and served them through the `Edit` component as suggested. 

### How it looks
![image](https://user-images.githubusercontent.com/24402285/50380851-b05c9380-069d-11e9-8525-9452bbb743fa.png)

### Features:
The GeoJSON input component allows a user to:
- Upload a `.geojson` file.
- Preview and modify uploaded `.geojson` in the browser.
- Draw custom points, lines and polygons (multiple points, lines and polygons also allowed) directly through the tool.
- Search for a place using a search box (using `mapbox-generic-geocoder` with OSM Nominatim), and zoom in to the location.

### Questions/Comments:
- What is the right way to use CDNs for JS and CSS in strapi? Im not sure if i followed standard practices for the same.
- Currently I've used [Maptiler Cloud](https://cloud.maptiler.com/) for hosting my vector tiles. This service allows upto 100000 free views in a month. If needed I can set up a map tile server for the same.
- All of my changes can be found at [my fork of strapi here](https://github.com/arkoblog/strapi).
- I'm looking forward to someone from strapi going through my code and pointing out blunders, mistakes so that I can work on the same. I've also submitted a pull request to facilitate the same. 



 

",2018-12-23T05:03:40Z,24402285
4256,strapi/strapi,393724050,449695588,"Hi @arogyakoirala thanks for your inputs here are some answers:

> What is the right way to use CDNs for JS and CSS in strapi? Im not sure if i followed standard practices for the same.

Maybe you can try something like this in the `content-manager/admin/src/components/InputGJSONWithErrors`:
```
import '<my-package/dist/<stylesheet>.css';
// ... rest of the code
```

If it doesn't work, well you can try to create the tag manually and insert it into the dom or just hardcode it in the index.html like you did. I prefer the first option which is to import the stylesheet.

> Currently I've used Maptiler Cloud for hosting my vector tiles. This service allows upto 100000 free views in a month. If needed I can set up a map tile server for the same.

I think that @lauriejim or @Aurelsicoko are the one who could answer this question


> All of my changes can be found at my fork of strapi here.


That's awesome thanks for providing your fork! I'll take a look into it!

> I'm looking forward to someone from strapi going through my code and pointing out blunders, mistakes so that I can work on the same. I've also submitted a pull request to facilitate the same.

You can DM on Slack (@soupette) I answer most of the time!


Thanks again for providing a such detailed issue!",2018-12-24T07:12:41Z,13311463
4257,strapi/strapi,393724050,451853950,"
finding a way to search geo locations within my frontend (nuxt), from the strapi backend.
https://mongoosejs.com/docs/geojson.html

I believe I need a point schema, is it possible to add something like this to the model? 
Since point is currently not available as a strapi type (from the mongoose docs):

```
location: {
    type: {
      type: String,
      enum: ['Point'], 
      required: true
    },
    coordinates: {
      type: [Number],
      required: true
    }
  }
```
It would be awesome if a resolver for finding GeoJSON values would be baked in within strapi,
for both API and graphql.. or some clear documentation where to implement:

```
db.locations.find({
   location: {
     $nearSphere: {
       $geometry: {
         type: 'Point',
         coordinates: [-122.5, 37.1]
       },
       $maxDistance: 900 * 1609.34
     }
   }
 })
```
thanks

",2019-01-07T08:14:05Z,23075300
4258,strapi/strapi,393724050,453519654,"The GeoJSON field wasn't one of priority. The GeoPoint (aka Map) is a priority for example. However, the work done here seems pretty good. I would love to see it merged to the project.

I'm not sure to understand the questions:

- Where do you expect to store the vectors tiles?
- Can you submit the PR so we'll be able to review the code and continue the work together,",2019-01-11T13:40:25Z,4144726
4259,strapi/strapi,393724050,453548711,"

> The GeoJSON field wasn't one of priority. The GeoPoint (aka Map) is a priority for example. 
> However, the work done here seems pretty good. I would love to see it merged to the project.

Thanks! I would be more than happy to help in any way.
 
> I'm not sure to understand the questions:
> 
>     Where do you expect to store the vectors tiles?

The vector tiles that I'm currently using comes from MapTiler Cloud ([see pricing here](https://www.maptiler.com/cloud/plans/)). I'm currently using the free plan which allows upto 100,000 requests a month. I guess what I'm asking is if that would be enough. If not, we will have to look for other free options (which I'm sure are available, I just haven't spent enough time researching on the same).

>     Can you submit the PR so we'll be able to review the code and continue the work together,

I did this the same time I opened this issue. You can find it [here.](https://github.com/strapi/strapi/pull/2526)
",2019-01-11T15:13:37Z,24402285
4260,strapi/strapi,393724050,453551747,"> The GeoJSON field wasn't one of priority. The GeoPoint (aka Map) is a priority for example.

@Aurelsicoko I'm not sure if I understand what a GeoPoint is. Can you elaborate?    ",2019-01-11T15:22:29Z,24402285
4261,strapi/strapi,393724050,453571709,"Should I open a different feature request?

I 'just' :-) need a field within contenttypes to add points (geojson)

So I can add markers to my strapi model
```
""marker"":{
      ""type"": ""Point"",
      ""default"":[],
      ""coordinates"": [0,0]
    },
```
These markers can be found with something like

```
Message.find({
  marker: {
   $near: {
    $maxDistance: 1000,
    $geometry: {
     type: ""Point"",
     coordinates: [long, latt]
    }
   }
  }
 }).find((error, results) => {
  if (error) console.log(error);
  console.log(JSON.stringify(results, 0, 2));
 });
```
and display on any map library client sided






",2019-01-11T16:20:34Z,23075300
4262,strapi/strapi,393724050,455149641,"@arogyakoirala So if we have to depend on a third-party, we should add a provider system to this new type of field.

About the GeoPoint is nothing more than a pin that you put on Map. It just works like a GPS (lat, lon coordinates).",2019-01-17T12:07:06Z,4144726
4263,strapi/strapi,393724050,455729730,"> @arogyakoirala So if we have to depend on a third-party, we should add a provider system to this new type of field.

I'm assuming this refers to the map tiles, let me do my research on this and get back soon.

> About the GeoPoint is nothing more than a pin that you put on Map. It just works like a GPS (lat, lon coordinates).

Oh that can be easily done through this new content-type. In fact, it is multi-geometry by default, meaning you can add one (or many points), lines and polygons, as part of a single entry. It gets captured as a FeatureCollection ([https://macwright.org/2015/03/23/geojson-second-bite.html#featurecollection](https://macwright.org/2015/03/23/geojson-second-bite.html#featurecollection))      ",2019-01-19T00:29:39Z,24402285
4264,strapi/strapi,393724050,457197334,"@arogyakoirala Is it possible to change the map background in your field? In the beginning, I was thinking about using something like OpenStreetMap ;)",2019-01-24T13:32:06Z,4144726
4265,strapi/strapi,393724050,457789239,@Aurelsicoko this is OpenStreetMap. However I've used vector tiles (mapboxgljs) as opposed to raster (leafletjs). ,2019-01-26T01:38:00Z,24402285
4266,strapi/strapi,393724050,471204882,"@arogyakoirala 
Your fork is only for MongoDB ? Not for PostgreSQL/PostGIS ?
I am starting the same, but with PG and the ""geometry"" data type...",2019-03-09T17:43:00Z,6769414
4267,strapi/strapi,393724050,471249426,"@sign0 I've merely customized the JSON input type to easily take geometries in GeoJSON format. I do see value in extending this functionality to 'geometry' data type, as it would enable backend geospatial querying. ",2019-03-10T05:46:42Z,24402285
4268,strapi/strapi,393724050,471319716,"@arogyakoirala Yes, especially since the latest versions of PostgreSQL efficiently manage the JSON with the JSONB type.

Postgres is also suitable for storing vector or raster tiles (I use that: https://openmaptiles.org/ with SQLite/MBTile or Postgres/gis, and it's compatible with the mapbox-gl workflow like gl-style). 

Unfortunately, I'm not familiar with Knex ORM, especially when it comes to interacting with PostGIS (I more familiar with Sequelize ORM).

And the management of geometry type would mean that it would be an ""exclusive feature"" for Postgres : which would go may be against the philosophy of this project which claims to be a NoSQL / SQL compatible CMS... But for example, Wordpress is ""optimized"" for MySQL).

I have not yet tried your contribution, but the description is impressive ! :)",2019-03-10T16:13:00Z,6769414
4269,strapi/strapi,393724050,471687773,"@arogyakoirala -  Awesome work.  I was pleasantly surprised to see that you are working on this.  I was planning on having to do this on my own.   I look forward to checking out your fork.  I'd love to help if I can.       Assuming the strapi folks can see the obvious benefits of this, a few areas that I'd love to work on:

1. Topojson - an extension to GeoJSON that encodes topology rather than discrete geometries, allowing for smaller files and on-the-fly simplification.
2. GeoJSON model -  I haven't checked out the fork yet and I don't know if it is your intent to fully model the GeoJSON spec. It would be great to have the ability for instance to create relations between GeoJSON `Features` and other strapi models.",2019-03-11T19:31:30Z,550422
4270,strapi/strapi,393724050,472149247,"> Postgres is also suitable for storing vector or raster tiles (I use that: https://openmaptiles.org/ with SQLite/MBTile or Postgres/gis, and it's compatible with the mapbox-gl workflow like gl-style).

There's postGIS as well https://postgis.net/",2019-03-12T19:33:20Z,550422
4271,strapi/strapi,393724050,472179519,"> > Postgres is also suitable for storing vector or raster tiles (I use that: https://openmaptiles.org/ with SQLite/MBTile or Postgres/gis, and it's compatible with the mapbox-gl workflow like gl-style).
> 
> There's postGIS as well https://postgis.net/

Yes it is!
Postgis also supports geometry -> TopoJSON with AsTopoJSON() function.",2019-03-12T21:01:48Z,6769414
4272,strapi/strapi,393724050,472234817,"> @arogyakoirala - Awesome work. I was pleasantly surprised to see that you are working on this. I was planning on having to do this on my own. I look forward to checking out your fork. I'd love to help if I can. Assuming the strapi folks can see the obvious benefits of this, a few areas that I'd love to work on:
> 
>     1. Topojson - an extension to GeoJSON that encodes topology rather than discrete geometries, allowing for smaller files and on-the-fly simplification.
> 
>     2. GeoJSON model -  I haven't checked out the fork yet and I don't know if it is your intent to fully model the GeoJSON spec. It would be great to have the ability for instance to create relations between GeoJSON `Features` and other strapi models.

@cupofnestor  thanks for your appreciation, I'm really glad (first open-source contribution woot woot). I feel both of the features you suggest will ad significant value. TopoJSON will reduce the load, and a GeoJSON model should help facilitate spatial queries which would be awesome. Feel free to fiddle with my fork and make changes. Do let me know how I can help.  

",2019-03-13T00:39:29Z,24402285
4273,strapi/strapi,393724050,625854894,"Just wanted to chime in to say (and maybe up the priority a bit :)), that currently this is the only thing holding me back to fully switch to strapi for a project, since it needs an easy way for editors to input location information. datocms has a similar field which works really well for addresses, I think that would be a great start which could further be extended to more complex geodata. ",2020-05-08T14:54:37Z,861414
4274,strapi/strapi,393724050,636362500,"It would be nice to support this GeoJSON type, so more applications could be developed with Strapi. ",2020-05-30T17:44:15Z,10890471
4275,strapi/strapi,393724050,640604986,"This will come with the custom field feature. With that, every field type will be able to be supported.
I'm locking this issue. We well understood the need.
Thank you all for your contribution.",2020-06-08T13:31:52Z,5716596
4276,strapi/strapi,393724050,1119815567,"Hey all, we are currently moving many feature requests to our new feedback and feature request website to help clean up the GitHub issues and make it easier for us to review bug reports and fix them.

I have moved this feature to the following URL: https://feedback.strapi.io/plugin-requests/p/geo-mapping-plugin
If you are interested in this feature please feel free to go there and add more comments and/or upvote it there.

For now I will close this and lock it so that all new information goes into our new feedback website.
Thanks!",2022-05-06T16:58:54Z,8593673
4277,strapi/strapi,544542483,544542483,"Hi,

**Describe the bug**
I can't access some data via GraphQL versus simple fetch.

**Steps to reproduce the behavior**
1- Create a new _avatar_ media field on users. 
2- Try to get that avatar via the below GraphQL query:
````graphql
query {
  me {
    username
    avatar {url}
  }
}
````
3)- You'll get the below error:
_""Cannot query field \""avatar\"" on type \""UsersPermissionsMe\"".""_ with a _GRAPHQL_VALIDATION_FAILED_ code.

**Code snippets**
The below fetch if returning the avatar object:

http://localhost:1337/users/me/

````json
{
    ""id"": 20,
    ""username"": ""***"",
    ""email"": ""***@gmail.com"",
    ""provider"": ""local"",
    ""confirmed"": true,
    ""blocked"": null,
    ""created_at"": ""2019-12-26T19:17:36.847Z"",
    ""updated_at"": ""2020-01-01T20:11:46.181Z"",
    ""avatar"": {
        ""id"": 12,
        ""url"": ""/uploads/8dd5de998ffa4de09fc264c0fab924ab.jpg""
    }
}
````

**System**
- Node.js version: 10.17.0
- NPM version: 6.11.3
- Strapi version: v3.0.0-beta.18.3
- Database: sqlite
- Operating system: macod

**Expected behavior**
I should be able to get the avatar data from GraphQL",2020-01-02T11:29:06Z,6083654
4278,strapi/strapi,544542483,570182328,"> This is a templated message

Hello, please follow the issue template.

A proper issue submission let's us better understand the origin of your bug and therefore help you.

I will reopen your issue when we receive the issue following the template guidelines and properly fill out the template.

Please update the issue.

Thank you.

> A copy of the bug report template can be found below:

```
<!--
Hello 👋 Thank you for submitting an issue.

Before you start, please make sure your issue is understandable and reproducible.
To make your issue readable make sure you use valid Markdown syntax.

https://guides.github.com/features/mastering-markdown/
-->

**Describe the bug**
A clear and concise description of what the bug is.

**Steps to reproduce the behavior**
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Code snippets**
If applicable, add code samples to help explain your problem.

**System**
- Node.js version: <!-- Please ensure you are using the Node LTS version (v10) -->
- NPM version:
- Strapi version: <!-- Please make sure you are on the latest version -->
- Database:
- Operating system:

**Additional context**
Add any other context about the problem here.
```",2020-01-02T11:29:18Z,8593673
4279,strapi/strapi,544542483,570187202,"@derrickmehaffy I just update the issue template, thanks for re-opening it :)",2020-01-02T11:54:38Z,6083654
4280,strapi/strapi,544542483,570196205,"Thank you for reporting this issue.

The [definition UsersPermissionsMe](https://github.com/strapi/strapi/blob/632bf537ecf624019f2710f6b63dd2a93c6f74f0/packages/strapi-plugin-users-permissions/config/schema.graphql#L22) that is used for the `me` query is static.
We should make it a little bit more dynamic based on the User model structure.

A workaround is to use the [customization concept](https://strapi.io/documentation/3.0.0-beta.x/concepts/customization.html#plugin-extensions) to override the `schema.graphql` file of the **Users & Permissions** plugin.",2020-01-02T12:39:03Z,5716596
4281,strapi/strapi,544542483,570784264,"Since I update to the latest version of Strapi, even the simple fetch doesn't return the avatar object.

I read the documentation about the customization concept but there is no `schema.graphql` in the extension folder. Grateful if you could share some hints.",2020-01-04T13:05:38Z,6083654
4282,strapi/strapi,544542483,572448141,"I think my problem is on relation of this topic.
""users/me"" return only custom fields but not relationships, its for this reason avatar (files, images) is not listed.

Auth.callback (""auth/local"") return all datas correctly, using the same process for User.me (users/me) without the jwt outpout, this would be the solution ? ",2020-01-09T08:27:37Z,53647925
4283,strapi/strapi,544542483,573095680,Have the same problem.,2020-01-10T16:02:39Z,8247995
4284,strapi/strapi,544542483,583307068,"I don't know if overriding the schema of the plugin works in a different way than overriding controllers or services but pasting `node-modules/strapi-plugin-users-permissions/config/schema.graphql` into `extensions/users-permisions/config` for editing the later one doesn't override it but tries to re-register it

```
[2020-02-07T09:15:41.870Z] error Error: There can be only one type named ""UsersPermissionsMe"".

Field ""UsersPermissionsMe.id"" can only be defined once.

Field ""UsersPermissionsMe.username"" can only be defined once.

Field ""UsersPermissionsMe.name"" can only be defined once.

Field ""UsersPermissionsMe.email"" can only be defined once.

Field ""UsersPermissionsMe.confirmed"" can only be defined once.

Field ""UsersPermissionsMe.blocked"" can only be defined once.

Field ""UsersPermissionsMe.role"" can only be defined once.

There can be only one type named ""UsersPermissionsMeRole"".

Field ""UsersPermissionsMeRole.id"" can only be defined once.

Field ""UsersPermissionsMeRole.name"" can only be defined once.

Field ""UsersPermissionsMeRole.description"" can only be defined once.

Field ""UsersPermissionsMeRole.type"" can only be defined once.

There can be only one type named ""UsersPermissionsLoginInput"".

Field ""UsersPermissionsLoginInput.identifier"" can only be defined once.

Field ""UsersPermissionsLoginInput.password"" can only be defined once.

Field ""UsersPermissionsLoginInput.provider"" can only be defined once.

There can be only one type named ""UsersPermissionsLoginPayload"".

Field ""UsersPermissionsLoginPayload.jwt"" can only be defined once.

Field ""UsersPermissionsLoginPayload.user"" can only be defined once.

Field ""Query.me"" can only be defined once.

Field ""Mutation.login"" can only be defined once.

Field ""Mutation.register"" can only be defined once.
    at assertValidSDL (/home/ec2-user/environment/Molecula-dev/node_modules/graphql/validation/validate.js:108:11)
    at Object.buildASTSchema (/home/ec2-user/environment/Molecula-dev/node_modules/graphql/utilities/buildASTSchema.js:71:34)
    at Object.buildSchemaFromTypeDefinitions (/home/ec2-user/environment/Molecula-dev/node_modules/graphql-tools/dist/generate/buildSchemaFromTypeDefinitions.js:23:28)
    at makeExecutableSchema (/home/ec2-user/environment/Molecula-dev/node_modules/graphql-tools/dist/makeExecutableSchema.js:26:29)
    at Object.generateSchema (/home/ec2-user/environment/Molecula-dev/node_modules/strapi-plugin-graphql/services/Schema.js:300:22)
    at Object.initialize (/home/ec2-user/environment/Molecula-dev/node_modules/strapi-plugin-graphql/hooks/graphql/index.js:59:50)
    at Promise.resolve.then (/home/ec2-user/environment/Molecula-dev/node_modules/strapi/lib/hooks/index.js:37:28)
```

But editing the node-modules one doesn't work neither. I have just copied the type UsersPermissionsUser from the autogenerated file `exports/graphql/schema.graphql` into the type UsersPermissionsMe of `node-modules/strapi-plugin-users-permissions/config/schema.graphql` and it doesn't return any error but querying a field which wasn't there (related or not) renders a GraphQL validation error. I suppose this schema is not the one which is being used for the validation...",2020-02-07T09:29:51Z,24911971
4285,strapi/strapi,544542483,583326880,Can you please share the content of the `schema.graphql` file.,2020-02-07T10:21:32Z,5716596
4286,strapi/strapi,544542483,583396052,"Just one line added, the ""name: String"" in line 25, but name cannot be queried through the me modek in graphql

```
const _ = require('lodash');
const { ApolloError } = require('apollo-server-koa');

/**
* Throws an ApolloError if context body contains a bad request
* @param contextBody - body of the context object given to the resolver
* @throws ApolloError if the body is a bad request
*/
function checkBadRequest(contextBody) {
  if (_.get(contextBody, 'output.payload.statusCode', 200) !== 200) {
    const statusCode = _.get(contextBody, 'output.payload.statusCode', 400);
    const message = _.get(contextBody, 'output.payload.message', 'Bad Request');
    throw new ApolloError(message, statusCode, _.omit(contextBody, ['output']));
  }
}

module.exports = {
  type: {
    UsersPermissionsPermission: false, // Make this type NOT queriable.
  },
  definition: `
    type UsersPermissionsMe {
      id: ID!
      username: String!
      name: String
      email: String!
      confirmed: Boolean
      blocked: Boolean
      role: UsersPermissionsMeRole
    }

    type UsersPermissionsMeRole {
      id: ID!
      name: String!
      description: String
      type: String
    }

    input UsersPermissionsLoginInput {
      identifier: String!
      password: String!
      provider: String = ""local""
    }

    type UsersPermissionsLoginPayload {
      jwt: String!
      user: UsersPermissionsMe!
    }
  `,
  query: `
    me: UsersPermissionsMe
  `,
  mutation: `
    login(input: UsersPermissionsLoginInput!): UsersPermissionsLoginPayload!
    register(input: UserInput!): UsersPermissionsLoginPayload!
  `,
  resolver: {
    Query: {
      me: {
        resolverOf: 'User.me',
        resolver: {
          plugin: 'users-permissions',
          handler: 'User.me',
        },
      },
      role: {
        plugin: 'users-permissions',
        resolverOf: 'UsersPermissions.getRole',
        resolver: async (obj, options, { context }) => {
          context.params = {...context.params, ...options.input};

          await strapi.plugins[
            'users-permissions'
          ].controllers.userspermissions.getRole(context);

          return context.body.role;
        },
      },
      roles: {
        description: `Retrieve all the existing roles. You can't apply filters on this query.`,
        plugin: 'users-permissions',
        resolverOf: 'UsersPermissions.getRoles', // Apply the `getRoles` permissions on the resolver.
        resolver: async (obj, options, { context }) => {
          context.params = {...context.params, ...options.input};

          await strapi.plugins[
            'users-permissions'
          ].controllers.userspermissions.getRoles(context);

          return context.body.roles;
        },
      },
    },
    Mutation: {
      createRole: {
        description: 'Create a new role',
        plugin: 'users-permissions',
        resolverOf: 'UsersPermissions.createRole',
        resolver: async (obj, options, { context }) => {
          await strapi.plugins[
            'users-permissions'
          ].controllers.userspermissions.createRole(context);

          return { ok: true };
        },
      },
      updateRole: {
        description: 'Update an existing role',
        plugin: 'users-permissions',
        resolverOf: 'UsersPermissions.updateRole',
        resolver: async (obj, options, { context }) => {
          await strapi.plugins[
            'users-permissions'
          ].controllers.userspermissions.updateRole(
            context.params,
            context.body
          );

          return { ok: true };
        },
      },
      deleteRole: {
        description: 'Delete an existing role',
        plugin: 'users-permissions',
        resolverOf: 'UsersPermissions.deleteRole',
        resolver: async (obj, options, { context }) => {
          await strapi.plugins[
            'users-permissions'
          ].controllers.userspermissions.deleteRole(context);

          return { ok: true };
        },
      },
      createUser: {
        description: 'Create a new user',
        plugin: 'users-permissions',
        resolverOf: 'User.create',
        resolver: async (obj, options, { context }) => {
          context.params = _.toPlainObject(options.input.where);
          context.request.body = _.toPlainObject(options.input.data);

          await strapi.plugins['users-permissions'].controllers.user.create(
            context
          );

          return {
            user: context.body.toJSON ? context.body.toJSON() : context.body,
          };
        },
      },
      updateUser: {
        description: 'Update an existing user',
        plugin: 'users-permissions',
        resolverOf: 'User.update',
        resolver: async (obj, options, { context }) => {
          context.params = _.toPlainObject(options.input.where);
          context.request.body = _.toPlainObject(options.input.data);

          await strapi.plugins['users-permissions'].controllers.user.update(
            context
          );

          return {
            user: context.body.toJSON ? context.body.toJSON() : context.body,
          };
        },
      },
      deleteUser: {
        description: 'Delete an existing user',
        plugin: 'users-permissions',
        resolverOf: 'User.destroy',
        resolver: async (obj, options, { context }) => {
          // Set parameters to context.
          context.params = _.toPlainObject(options.input.where);
          context.request.body = _.toPlainObject(options.input.data);

          // Retrieve user to be able to return it because
          // Bookshelf doesn't return the row once deleted.
          await strapi.plugins['users-permissions'].controllers.user.findOne(
            context
          );
          // Assign result to user.
          const user = context.body.toJSON
            ? context.body.toJSON()
            : context.body;

          // Run destroy query.
          await strapi.plugins['users-permissions'].controllers.user.destroy(
            context
          );

          return {
            user,
          };
        }
      },
      register: {
        description: 'Register a user',
        plugin: 'users-permissions',
        resolverOf: 'Auth.register',
        resolver: async (obj, options, {context}) => {
          context.request.body = _.toPlainObject(options.input);

          await strapi.plugins['users-permissions'].controllers.auth.register(context);
          let output = context.body.toJSON ? context.body.toJSON() : context.body;

          checkBadRequest(output);
          return {
            user: output.user || output, jwt: output.jwt
          };
        }
      },
      login: {
        resolverOf: 'Auth.callback',
        plugin: 'users-permissions',
        resolver: async (obj, options, {context}) => {
          context.params = {...context.params, provider: options.input.provider};
          context.request.body = _.toPlainObject(options.input);

          await strapi.plugins['users-permissions'].controllers.auth.callback(context);
          let output = context.body.toJSON ? context.body.toJSON() : context.body;

          checkBadRequest(output);
          return {
            user: output.user || output, jwt: output.jwt
          };
        }
      }
    }
  }
};

```",2020-02-07T13:48:01Z,24911971
4287,strapi/strapi,544542483,584106410,"Hello! You don't need to copy the entire file.

```js
module.exports = {
  definition: `
    extend type UsersPermissionsMe {
      name: String
    }
  `
}
```",2020-02-10T12:48:58Z,5716596
4288,strapi/strapi,544542483,587533705,"Thanks for your comment @lauriejim, really helpful!",2020-02-18T15:59:00Z,9984086
4289,strapi/strapi,544542483,588137686,"Now I reached the other problem, it works with plain fields but it just returns null with relationships

With this schema in `extensions/users-permisions/config`...
```
module.exports = {
  definition: `
    extend type UsersPermissionsMe {
      name: String
      messages(sort: String, limit: Int, start: Int, where: JSON): [Message]
    }
  `
}
```

... and this query ...
```
{
    me{
        name
        messages{
            _id
        }
    }
}
```

... I'm getting this response
```
{
    ""data"": {
        ""me"": {
            ""name"": ""Mig"",
            ""messages"": null
        }
    }
}
```",2020-02-19T10:04:01Z,24911971
4290,strapi/strapi,544542483,588368200,You also have to add `messages` in your extension.,2020-02-19T18:19:47Z,5716596
4291,strapi/strapi,544542483,588378883,"My fault, I copied it wrong but the code was well, I have edited the post.",2020-02-19T18:40:36Z,24911971
4292,strapi/strapi,544542483,589715945,Ho yes I confirm!,2020-02-21T15:59:49Z,5716596
4293,strapi/strapi,544542483,593090113,"Same for me.

extensions/users-permisions/config/schema.graphql
```
module.exports = {
  definition: `
    extend type UsersPermissionsMe {
      avatar: UploadFile
    }
  `
}
```

the query
```
# Write your query or mutation here
query {
  me {
    username
    email
    avatar {
      url
    }
  }
}
```

the result
```
{
  ""data"": {
    ""me"": {
      ""username"": ""geo"",
      ""email"": ""email@.com"",
      ""avatar"": null
    }
  }
}
```

any workaround to get to get that value?",2020-03-01T12:12:52Z,6083654
4294,strapi/strapi,544542483,593120553,"@geosigno Same for me. 
@lauriejim Please help

the query =>
       
        query {
            me {
                id
                username
                user_info {
                    name
                    birthday
                }
                user_status {
                    securityLevel
                }
            }
        }

its result =>

        {
            ""data"": {
                ""me"": {
                    ""id"": ""1"",
                    ""username"": ""ImUser1"",
                    ""user_info"": {
                        ""name"": null,
                        ""birthday"": null
                    },
                    ""user_status"": {
                        ""securityLevel"": null
                    }
                }
            }
        }

my schema.graphql =>

        module.exports = {
            definition: `
                extend type UsersPermissionsMe {
                    user_status: UserStatus
                    user_info: UserInfo
                }
            `
        }

I am also trying another way, but it doesn't work for me.
https://github.com/strapi/strapi/issues/2627

",2020-03-01T17:15:00Z,134104
4295,strapi/strapi,544542483,596047583,"I solved this issue with #2627 and below query is working well.

    query{

    me {
        id
        username
    }
    self {
        user_info {
            name
            birthday
        }
        user_status {
            securityLevel
        }
    }

    }
",2020-03-07T05:03:02Z,134104
4296,strapi/strapi,544542483,615394058,"I'm commenting here since #2627 is locked, and both issues are very related. I had to make a few adjustments to the fix mentioned by @unolee 

I wanted to query relationship linked to the authenticated user. I created a file named `extensions/users-permissions/config/schema.graphql.js` with this content:

```js
module.exports = {
  query: `
    self: UsersPermissionsUser
  `,
  resolver: {
    Query: {
      self: {
        resolver: 'plugins::users-permissions.user.me'
      },
    },
  }
};
```

Custom fields are now working fine, if you replace `me` by `self` in your queries.",2020-04-17T18:20:51Z,8087692
4297,strapi/strapi,544542483,619978921,"@remi2j 

As @lauriejim said in https://github.com/strapi/strapi/issues/2627#issuecomment-568257332 you shouldn't use `self: UsersPermissionsUser` here out of security reasons.

Instead create a further Type with restricted fields, e.g.: 

```
  definition: `
    type Self {
      id: ID
      username: String
      email: String
      additionalField1: String
      additionalField2: String
      ...
    }
  `,
  query: `
      self: Self
  `,
  resolver: {
      Query: {
          self: {
              resolver: 'plugins::users-permissions.user.me'
          },
      },
  }
```

Or extend UsersPermissionsMe as seen in: https://github.com/strapi/strapi/issues/4878#issuecomment-584106410",2020-04-27T13:15:54Z,18480335
4298,strapi/strapi,544542483,633183761,I confirm the workaround by replacing _me_ with _self_ is working fine.,2020-05-24T05:53:14Z,6083654
4299,strapi/strapi,544542483,999096647,"This issue has been mentioned on **Strapi Community Forum**. There might be relevant details there:

https://forum.strapi.io/t/strapi-v4-cant-access-custom-fields-from-users-me-via-graphql/13744/1
",2021-12-21T21:10:43Z,8593673
4300,strapi/strapi,544542483,1005355651,"This issue has been mentioned on **Strapi Community Forum**. There might be relevant details there:

https://forum.strapi.io/t/get-all-items-related-to-user/14159/1
",2022-01-05T03:37:36Z,8593673
4301,strapi/strapi,544542483,1006130324,should be fixed in v4 I believe,2022-01-05T22:32:15Z,8593673
4302,strapi/strapi,407645854,407645854,"<!-- ⚠️ If you do not respect this template your issue will be closed. -->

<!-- ⚠️ Make sure to browse the opened and closed issues before submitting your issue. -->

<!-- 
Please also submit your idea on the Strapi Product Board:
https://portal.productboard.com/strapi/tabs/2-under-consideration/submit-idea

If your request on the product board is accepted this feature request issue will be closed,
but will still accept public discussion.
-->

- [ ] **I have created my request on the Product Board before I submitted this issue**
- [x] **I have looked at all the other requests on the Product Board before I submitted this issue**

**Please describe your feature request:**

Add auto increment option to the model attributes",2019-02-07T11:05:41Z,1007051
4303,strapi/strapi,407645854,461824057,Hi @abdonrd can you give us more details about your need please.,2019-02-08T14:41:24Z,5716596
4304,strapi/strapi,407645854,461826987,"@lauriejim I mean an integer attribute that auto increment.
Like we have here at GitHub with the issues and the PRs.",2019-02-08T14:50:37Z,1007051
4305,strapi/strapi,407645854,461873957,"Hum something like that ?

```
{
  ...
  ""options"": {
    ""timestamps"": true,
    ""increment"": ""number""
  }
  ...
}
```",2019-02-08T17:07:01Z,5716596
4306,strapi/strapi,407645854,461885533,"Maybe! If we want to replace the current ID.

Or something like this if we want it as another field:

```
""identifier"": {
  ""type"": ""integer"",
  ""autoIncrement"": true
},
```

",2019-02-08T17:42:50Z,1007051
4307,strapi/strapi,407645854,463642775,@abdonrd can you make a PR for that?,2019-02-14T14:17:02Z,4144726
4308,strapi/strapi,407645854,464126953,@Aurelsicoko for now I will not be able to dedicate time. 😕 ,2019-02-15T17:12:14Z,1007051
4309,strapi/strapi,407645854,504814192,"I tried adding these mongoose plugins `mongoose-auto-increment` and `mongoose-sequence` to strapi in `config/functions/mongoose.js`. The counter collections are getting create, but the sequence counts are not getting updated. Is there a way to get these plugins working or is there a way to implement this myself?",2019-06-24T02:11:44Z,4073294
4310,strapi/strapi,407645854,521971960,"I just found out that this was indeed an option in version 1.x.x of strapi.
Link: https://strapi.io/documentation/1.x.x/models.html#autoincrement

I suppose this is no more a part of the current version.
Any way to add it back?",2019-08-16T11:00:45Z,10728448
4311,strapi/strapi,407645854,618680493,+1,2020-04-23T21:25:23Z,6853444
4312,strapi/strapi,407645854,1119825737,"Hey all, we are currently moving many feature requests to our new feedback and feature request website to help clean up the GitHub issues and make it easier for us to review bug reports and fix them.

I have moved this feature to the following URL: https://feedback.strapi.io/developer-experience/p/add-auto-increment-option-to-the-model-attributes
If you are interested in this feature please feel free to go there and add more comments and/or upvote it there.

For now I will close this and lock it so that all new information goes into our new feedback website.
Thanks!",2022-05-06T17:12:17Z,8593673
4313,strapi/strapi,524307759,524307759,"
- [x] **I have created my request on the Product Board before I submitted this issue**
- [X] **I have looked at all the other requests on the Product Board before I submitted this issue**

About the GraphQL plugin:

If a model has a unique field defined in it, we should be able to filter for that field.
In this moment only IDs can be used for filtering

Example:
model Article with title, content and slug. Slug is required and unique
I would like to run this query

`
query {
  article(slug: ""my-awesome-article"") {
    title
    id
    content
  }
}
`
",2019-11-18T11:31:42Z,12105333
4314,strapi/strapi,524307759,556004678,Thank you for this feedback!,2019-11-20T13:32:24Z,5716596
4315,strapi/strapi,524307759,557201249,I'm also interested on this!,2019-11-21T17:56:59Z,1007051
4316,strapi/strapi,524307759,557243728,Hi @nicecatch great suggestion. You can achieve that by overridding the query and the action to handle it quickly on your own :),2019-11-21T19:47:38Z,6065744
4317,strapi/strapi,524307759,563453290,@lauriejim Maybe is a good example for another guide?,2019-12-09T21:42:19Z,1007051
4318,strapi/strapi,524307759,564720705,"> Hi @nicecatch great suggestion. You can achieve that by overridding the query and the action to handle it quickly on your own :)

@alexandrebodin Where I can override the query? Thanks!",2019-12-11T20:33:26Z,1007051
4319,strapi/strapi,524307759,564736162,"@abdonrd You can override your graphql creating a `schema.graphql.js` file in the `./api/{api}/config` folder

This would look something like that:

```js
const { sanitizeEntity } = require('strapi-utils');

module.exports = {
  query: `
    articleBySlug(id: ID slug: String): Address
  `,
  resolver: {
    Query: {
      articleBySlug: {
        resolverOf: 'Article.findOne',
        resolver(_, args) {
          const entity = strapi.services.article.findOne(args);
          return sanitizeEntity(entity, { model: strapi.models.article });
        },
      },
    },
  },
};

```",2019-12-11T21:15:48Z,6065744
4320,strapi/strapi,524307759,564759972,"I just created the `api/competition/config/schema.graphql.js` file with:

```javascript
const { sanitizeEntity } = require('strapi-utils');

module.exports = {
  query: `
    competitionBySlug(id: ID slug: String): Competition
  `,
  resolver: {
    Query: {
      competitionBySlug: {
        resolverOf: 'Competition.findOne',
        resolver(_, args) {
          const entity = strapi.services.competition.findOne(args);
          return sanitizeEntity(entity, { model: strapi.models.competition });
        }
      }
    }
  }
};
```

But if I tried the query:

```graphql
{
  competitionBySlug(slug: ""fms"") {
    id
    name
  }
}
```

I get this error:

`""message"": ""Cannot return null for non-nullable field Competition.id.""`",2019-12-11T22:21:42Z,1007051
4321,strapi/strapi,524307759,564893327,"Here is a working example. You need to await your service call and only pass the right params

```js
const { sanitizeEntity } = require('strapi-utils');

module.exports = {
  query: `
    articleBySlug(id: ID name: String): Article
  `,
  resolver: {
    Query: {
      articleBySlug: {
        resolverOf: 'Article.findOne',
        async resolver(_, { slug }) {
          const entity = await strapi.services.article.findOne({ slug });
          return sanitizeEntity(entity, { model: strapi.models.article });
        },
      },
    },
  },
};

```",2019-12-12T07:57:38Z,6065744
4322,strapi/strapi,524307759,564926905,"Thanks @alexandrebodin! It works now! 👏 

And how would it be with a plugin?
I've tried this, but it doesn't work for me.


`extensions/users-permissions/config/schema.graphql.js`

```javascript
const { sanitizeEntity } = require('strapi-utils');

module.exports = {
  query: `
    userByUsername(username: String): UsersPermissionsUser
  `,
  resolver: {
    Query: {
      userByUsername: {
        resolverOf: 'User.findOne',
        async resolver(_, { username }) {
          const entity = await strapi.plugins['users-permissions'].services.user.findOne({ username });
          return sanitizeEntity(entity, { model: strapi.plugins['users-permissions'].models.user });
        },
      },
    },
  },
};
```",2019-12-12T09:37:28Z,1007051
4323,strapi/strapi,524307759,564952912,"@alexandrebodin it works with:

`extensions/users-permissions/config/schema.graphql.js`
```javascript
const { sanitizeEntity } = require('strapi-utils');

module.exports = {
  query: `
    userByUsername(username: String): UsersPermissionsUser
  `,
  resolver: {
    Query: {
      userByUsername: {
        plugin: 'users-permissions',
        resolverOf: 'User.findOne',
        async resolver(_, { username }) {
          const entity = await strapi.plugins['users-permissions'].services.user.fetch({ username });
          return sanitizeEntity(entity, { model: strapi.plugins['users-permissions'].models.user });
        },
      },
    },
  },
};
```

It's the right way?",2019-12-12T10:43:38Z,1007051
4324,strapi/strapi,524307759,1006127590,"This can be done via customization, marking as closed. 

We have filters that would work just as easily also.",2022-01-05T22:27:04Z,8593673
4325,termux/termux-packages,1208013839,1208013839,"**Problem description**

The window comes out for a few seconds and then disappears

![Screenshot_20210726-122626](https://user-images.githubusercontent.com/64093255/127032377-f4cc25ec-1f05-49ea-aeb5-eefc9f4f893d.png)



Info:
Android 8.0 32 bits
2GB of ram
DE: lxqt
WM: Openbox 
Using vnc

In proot it works, but in termux anything related to OpenGL does not want to work

**Steps to reproduce**
<!--
Exact steps to reproduce the behavior.
Please post all commands that are necessary to reproduce the issue.
-->

**Expected behavior**
<!--
A clear and concise description of what you expected to happen.
-->


<!--
Please, post output of command `termux-info`. If it is not working for you
for whatever reason, post Android OS version and what CPU architecture you have.
Markdown code formatting should be applied.
-->
",2021-07-26T17:22:34Z,64093255
4326,termux/termux-packages,1208013839,1102331488,@suhan-paradkar ,2021-07-26T17:28:48Z,64093255
4327,termux/termux-packages,1208013839,1102331497,VNC doesn't have GLX extension. So forget about everything that uses GLX at least until someone manages to get it working.,2021-07-26T18:46:55Z,10137
4328,termux/termux-packages,1208013839,1102331506,"> VNC doesn't have GLX extension. So forget about everything that uses GLX at least until someone manages to get it working.

I've seen people in the reddit group who do it, and it works well for them.
",2021-07-26T18:50:51Z,64093255
4329,termux/termux-packages,1208013839,1102331508,They probably used XSDL.,2021-07-26T18:54:08Z,10137
4330,termux/termux-packages,1208013839,1102331512,"No, is vnc:

https://www.reddit.com/r/termux/comments/on4fkp/openglglfw_gears_on_termux_x11_no_root/?utm_medium=android_app&utm_source=share",2021-07-26T18:56:27Z,64093255
4331,termux/termux-packages,1208013839,1102331516,"Here is an output of `xdpyinfo`:

<p align=""center""><img src=""https://user-images.githubusercontent.com/25881154/127044060-f1611ab3-7460-4c25-af43-1251de352be5.png"" width=""50%""/></p>

There no GLX and I even has disabled it intentionally for tigervnc which uses same build options as `xorg-xvfb-server`.",2021-07-26T19:03:06Z,10137
4332,termux/termux-packages,1208013839,1102331522,What is on reddit is GLFW examples and not `glxgears`.,2021-07-26T19:04:25Z,10137
4333,termux/termux-packages,1208013839,1102331526,The GLFW examples also happen to you,2021-07-26T19:05:50Z,64093255
4334,termux/termux-packages,1208013839,1102331530,even freeglut fails me,2021-07-26T19:06:10Z,64093255
4335,termux/termux-packages,1208013839,1102331537,@Yisus7u7 I really have no idea whether freeglut works or not. These packages were added as requested and since then I didn't get any reports about whether they actually work or not.,2021-07-26T19:08:05Z,10137
4336,termux/termux-packages,1208013839,1102331540,Oh ok :(,2021-07-26T19:10:43Z,64093255
4337,termux/termux-packages,1208013839,1102331545,"I just got `glxgears` working ([sources](https://github.com/JoakimSoderberg/mesademos/blob/master/src/xdemos/glxgears.c)). So it may rather depend on where you got it and how it was compiled, but not on GLX extension presence.

<p align=""center""><img src=""https://user-images.githubusercontent.com/25881154/127045923-4595e77e-a788-4630-9445-1888447e21e6.png"" width=""50%""/></p>
",2021-07-26T19:17:25Z,10137
4338,termux/termux-packages,1208013839,1102331551,Thanks,2021-07-26T20:40:15Z,64093255
4339,termux/termux-packages,1208013839,1102331557,@Yisus7u7 maybe because you're on arm32,2021-07-28T02:22:43Z,85779483
4340,termux/termux-packages,1208013839,1102331560,"> @Yisus7u7 maybe because you're on arm32

Why? ",2021-07-28T03:36:41Z,64093255
4341,termux/termux-packages,1208013839,1102331562,"![Screenshot_20210928-114239](https://user-images.githubusercontent.com/64093255/135129936-02cb98e3-e66c-4b52-9eeb-75dd4dc0dbc4.png)

@xeffyr I tried the one in that repository and it doesn't work either.
",2021-09-28T16:43:38Z,64093255
4342,termux/termux-packages,1208013839,1102331569,"> > @Yisus7u7 maybe because you're on arm32
> 
> Why?

mesa-demos does work on aarch ... it just doesn't work on 32bit arm cpu",2021-09-28T17:08:21Z,39275638
4343,termux/termux-packages,1208013839,1102331579,"> > > @Yisus7u7 maybe because you're on arm32
> > 
> > 
> > Why?
> 
> mesa-demos does work on aarch ... it just doesn't work on 32bit arm cpu

I guess patches should be made for arm32",2021-09-28T17:11:01Z,64093255
4344,termux/termux-packages,1208013839,1102331583,"> > > > @Yisus7u7 maybe because you're on arm32
> > > 
> > > 
> > > Why?
> > 
> > 
> > mesa-demos does work on aarch ... it just doesn't work on 32bit arm cpu
> 
> Then you have to see what the problem is and create patches.

If I was smart enough to do that I would've already done that 🥴 ...

someone did help me to native compile the mesa library and I think it worked probably ... but no way to test it ...

can you give me a test program to check wheather OpenGL works or not ?

btw ... i have those scripts on my GitHub repo ...
just go to Termux-Faint-Hope repo and copy the patches folder in /sdcard/ and run the myScripts/build-zink.sh or myScripts/build-mesa.sh",2021-09-28T23:17:46Z,39275638
4345,termux/termux-packages,1208013839,1102331588,"> > > > > @Yisus7u7 maybe because you're on arm32
> > > > 
> > > > 
> > > > Why?
> > > 
> > > 
> > > mesa-demos does work on aarch ... it just doesn't work on 32bit arm cpu
> > 
> > 
> > Then you have to see what the problem is and create patches.
> 
> If I was smart enough to do that I would've already done that 🥴 ...
> 
> someone did help me to native compile the mesa library and I think it worked probably ... but no way to test it ...
> 
> can you give me a test program to check wheather OpenGL works or not ?

https://github.com/davidanthonygardner/glxgears

Try compiling and running that ",2021-09-28T23:20:50Z,64093255
4346,termux/termux-packages,1208013839,1102331592,"> > > > > > @Yisus7u7 maybe because you're on arm32
> > > > > 
> > > > > 
> > > > > Why?
> > > > 
> > > > 
> > > > mesa-demos does work on aarch ... it just doesn't work on 32bit arm cpu
> > > 
> > > 
> > > Then you have to see what the problem is and create patches.
> > 
> > 
> > If I was smart enough to do that I would've already done that 🥴 ...
> > someone did help me to native compile the mesa library and I think it worked probably ... but no way to test it ...
> > can you give me a test program to check wheather OpenGL works or not ?
> 
> https://github.com/davidanthonygardner/glxgears
> 
> Try compiling and running that

haha wth ... now even linking doesn't work ? I'll have to recompile it then check
btw link for the script is is previous comment, could you check it works or not ?
![Screenshot_2021-09-29-05-29-06-812_com termux](https://user-images.githubusercontent.com/39275638/135179179-3a98d6e5-6deb-4963-8f78-491d89363b3e.jpg)
",2021-09-28T23:31:47Z,39275638
4347,termux/termux-packages,1208013839,1102331597,"I also had that mistake once I compiled mesa... 

I think bookstores are broken. ",2021-09-28T23:35:25Z,64093255
4348,termux/termux-packages,1208013839,1102331601,"> I also had that mistake once I compiled mesa...
> 
> I think bookstores are broken.
ah nvm ... removing ndk-multilib fixed it ...
![Screenshot_2021-09-29-05-37-32-411_com termux](https://user-images.githubusercontent.com/39275638/135179704-87b36ee6-58e5-4489-a8eb-84a93397bace.jpg)

",2021-09-28T23:38:43Z,39275638
4349,termux/termux-packages,1208013839,1102331606,"it works on proot-distro!
",2021-09-29T00:11:37Z,87801676
4350,termux/termux-packages,1208013839,1102331610,"> it works on proot-distro!

maybe Termux problem?",2021-09-29T00:12:02Z,87801676
4351,termux/termux-packages,1208013839,1102331612,"> it works on proot-distro!

how exactly ? on a 32 bit arm device ? can you give me exact instructions ?",2021-09-29T00:20:40Z,39275638
4352,termux/termux-packages,1208013839,1102331617,"> > it works on proot-distro!
> 
> how exactly ? on a 32 bit arm device ? can you give me exact instructions ?

no, i use arm64 but you can view this vid:

https://www.youtube.com/watch?v=PO75bU3zGm0&t=709s",2021-09-29T01:33:13Z,87801676
4353,termux/termux-packages,1208013839,1102331622,"> > > it works on proot-distro!
> > 
> > 
> > how exactly ? on a 32 bit arm device ? can you give me exact instructions ?
> 
> no, i use arm64 but you can view this vid:
> 
> https://www.youtube.com/watch?v=PO75bU3zGm0&t=709s

on arm64 OpenGL works even normally ... no need for proot-distro",2021-09-29T01:35:27Z,39275638
4354,termux/termux-packages,1208013839,1102331628,"> > > > it works on proot-distro!
> > > 
> > > 
> > > how exactly ? on a 32 bit arm device ? can you give me exact instructions ?
> > 
> > 
> > no, i use arm64 but you can view this vid:
> > https://www.youtube.com/watch?v=PO75bU3zGm0&t=709s
> 
> on arm64 OpenGL works even normally ... no need for proot-distro

Very true, the problem is 32-bit. ",2021-09-29T02:23:31Z,64093255
4355,termux/termux-packages,1208013839,1102331636,"i'm sumitted a [problem](https://github.com/ptitSeb/gl4es/issues/342) in gl4es project

let's wait for respnse.",2021-09-29T03:31:25Z,87801676
4356,termux/termux-packages,1208013839,1102331659,"
> umm ... can i have the wallpaper link ?

https://github.com/Yisus7u7/monas_chinas_uwu_uwu

",2021-09-29T03:35:16Z,64093255
4357,termux/termux-packages,1208013839,1102331667,@yisus7u7 do your phone has access son gl4es hardware?,2021-09-29T06:16:45Z,87801676
4358,termux/termux-packages,1208013839,1102331671,"> termux/termux-packages#10226

Here is how I compiled `glxgears` (I intentionally mentioned only the .c file and not the whole repo):
```
~ $ curl -LO https://github.com/JoakimSoderberg/mesademos/raw/master/src/xdemos/glxgears.c
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   154  100   154    0     0   1029      0 --:--:-- --:--:-- --:--:--  1033
100 21237  100 21237    0     0  86261      0 --:--:-- --:--:-- --:--:-- 86261
~ $ clang glxgears.c -o glxgears -lGL -lX11 -lm
```

Running it:
```
~ $ export DISPLAY=:1
~ $ ./glxgears 
7265 frames in 5.0 seconds = 1452.936 FPS
```

P.S. I don't use custom mesa with these experimentals with gallium, zink, adreno drivers and other attempts to enable hardware acceleration. Simply installed `build-essential xorgproto libx11 mesa tigervnc`.",2021-09-29T08:46:45Z,10137
4359,termux/termux-packages,1208013839,1102331673,"![Screenshot_20210929-081316](https://user-images.githubusercontent.com/64093255/135275350-30101c43-2ee3-4a93-b4ac-7ba7ea3f1a77.png)

@xeffyr just doesn't work on arm (32-bit)",2021-09-29T13:14:17Z,64093255
4360,termux/termux-packages,1208013839,1102331676,"According to this comment

https://github.com/ptitSeb/gl4es/issues/342#issuecomment-931310418

Could it be that disabling glx and mesa-dri has to do with this? 

",2021-09-30T13:17:09Z,64093255
4361,termux/termux-packages,1208013839,1102331681,"@Yisus7u7 It **works** on AArch64, i686 and x86_64. So nothing to do with it. But note that gl4es is broken completely, see https://github.com/termux/x11-packages/issues/442 for details.",2021-09-30T13:26:52Z,10137
4362,termux/termux-packages,1208013839,1102331685,"> @Yisus7u7 It **works** on AArch64, i686 and x86_64. So nothing to do with it. But note that gl4es is broken completely, see termux/x11-packages#442 for details.

It's not just gl4es, glu, freeglut, glew and other openGL-related libraries not working ",2021-09-30T13:30:00Z,64093255
4363,termux/termux-packages,1208013839,1102331692,"I try glxgears on xwayland (use termux x11 app) and it crash too (Segmentation Fault)

I use 32 bit prefix termux bootstrap (my device actually 64 bit)

On 64 bit termux bootstrap (default) glxgears works fine in xwayland",2021-10-06T15:36:33Z,11465849
4364,termux/termux-packages,1208013839,1102331695,Then we can confirm that in arm (32-bit) OpenGL does not work ,2021-10-06T16:55:39Z,64093255
4365,termux/termux-packages,1208013839,1102331699,"> Then we can confirm that in arm (32-bit) OpenGL does not work

![Screenshot_20211007-161825_Termux_X11](https://user-images.githubusercontent.com/11465849/136356578-1e6b30d0-4d37-424f-830b-48c48695bb68.png)

btw, i just use Xephyr to make nested x server in xwayland, it somewhat bad performance, but it can software renderer and glxgears work in Termux X11 (i user 32 bit proot debian) 

my command:
(on termux)
termux-x11 &
(I login to proot debian 32 bit, then use this command)
env DISPLAY=:0 Xephyr :1 -fullscreen &
env DISPLAY=:1 xfce4-session

Then, run glxgears from that session...",2021-10-07T09:26:58Z,11465849
4366,termux/termux-packages,1208013839,1102331707,https://github.com/termux/termux-x11/releases,2021-10-08T06:42:34Z,11465849
4367,termux/termux-packages,1208013839,1102331717,"Use termux from fdroid is fine, i also from fdroid too, its not problem...  you need to install termux-x11.deb inside termux ",2021-10-08T13:07:49Z,11465849
4368,termux/termux-packages,1208013839,1102331724,"> > Then we can confirm that in arm (32-bit) OpenGL does not work
> 
> btw, i just use Xephyr to make nested x server in xwayland, it somewhat bad performance, but it can software renderer and glxgears work in Termux X11 (i user 32 bit proot debian)
> 
> my command: (on termux) termux-x11 & (I login to proot debian 32 bit, then use this command) env DISPLAY=:0 Xephyr :1 -fullscreen & env DISPLAY=:1 xfce4-session
> 
> Then, run glxgears from that session...

@xeffyr Based on this, will OpenGL in arm not be able to access any driver or library? ",2021-10-08T13:26:39Z,64093255
4369,termux/termux-packages,1208013839,1102331730,"@Yisus7u7 x11-packages **never** supported any kind of hw acceleration (officially) and therefore doesn't require any driver to work. In fact, mesa here is provided to fullfill dependency needs of gtk and qt5 libs and maybe for some other packages. I don't know how deeply OpenGL is broken but on my device `glxgears` [works](termux/termux-packages#10226) as was shown previously.",2021-10-08T13:52:50Z,10137
4370,termux/termux-packages,1208013839,1102331733,"Well, I don't know if glxgears use opengl, but it still fails in arm. ",2021-10-08T13:57:17Z,64093255
4371,termux/termux-packages,1208013839,1102331739,"`glxgears` use exactly OpenGL (not OpenGLES) and yes, it still fails because not fixed.",2021-10-08T14:02:01Z,10137
4372,termux/termux-packages,1208013839,1102331742,"> Well, I don't know if glxgears use opengl, but it still fails in arm.

Are you in proot? If yes, you can just run Xephyr on top of Termux Wayland (Termux X11) or Xserver-xsdl.... and you can run glxgears there and some 3D software rendering too.. but performance is very low fps",2021-10-08T14:09:30Z,11465849
4373,termux/termux-packages,1208013839,1102331750,"> > Use termux from fdroid is fine, i also from fdroid too, its not problem... you need to install termux-x11.deb inside termux
> 
> why this error ?
> 
> ![Screenshot_2021-10-09-06-27-15-493_com termux](https://user-images.githubusercontent.com/39275638/136637231-580fac1b-f033-4623-830c-77ffc4e2f7a6.jpg)

Download from github action, use the latest build
https://github.com/termux/termux-x11/actions",2021-10-09T00:30:52Z,11465849
4374,termux/termux-packages,1208013839,1102331755,"aftef you run xephyr :1, then just

`env DISPLAY=:1 glxgears`

Then view it in your termux X11 app

Btw, this is for proot distro, the xephyr unavailable in termux repo

You need to inside proot (proot-distro) for this method",2021-10-09T01:07:45Z,11465849
4375,termux/termux-packages,1208013839,1102331760,"I try adding `TERMUX_DEBUG_BUILD=true` into https://github.com/termux/x11-packages/blob/master/packages/mesa/build.sh

Rebuild, install and use gdb to get these outputs
I tested this on aarch64 but installing arm32 Termux app from https://github.com/termux/termux-app/actions and it's reproducible
So pretty much confirm arm32 Mesa is broken
```
Application version:
0.117
Packages CPU architecture:
arm
Subscribed repositories:
# sources.list
deb https://packages.termux.org/apt/termux-main/ stable main
# science-repo (sources.list.d/science.list)
deb https://packages.termux.org/apt/termux-science science stable
# x11-repo (sources.list.d/x11.list)
deb https://packages.termux.org/apt/termux-x11 x11 main
# game-repo (sources.list.d/game.list)
deb https://packages.termux.org/apt/termux-games games stable
Updatable packages:
ed/stable 1.17-4 arm [upgradable from: 1.17]
nano/stable 5.9 arm [upgradable from: 5.8]
pcre2/stable 10.38 arm [upgradable from: 10.37]
termux-tools/stable 0.141 all [upgradable from: 0.140]
Android version:
11
Kernel build information:
Linux localhost 4.4.177-22274331 termux/x11-packages#1 SMP PREEMPT Tue Sep 7 18:27:31 +07 2021 armv8l Android
Device manufacturer:
samsung
Device model:
SM-A307GN
```
```
Starting program: /data/data/com.termux/files/usr/bin/glxdemo 
[New LWP 16252]

Thread 1 ""glxdemo"" received signal SIGSEGV, Segmentation fault.
glShadeModel (mode=7424)
    at /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h:2981
2981	/home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h: (undocumented errno 2).
Continuing.
[New LWP 16253]
[New LWP 16254]
[LWP 16254 exited]
[LWP 16253 exited]

Thread 1 ""glxdemo"" received signal SIGSEGV, Segmentation fault.
glShadeModel (mode=7424)
    at /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h:2981
2981	in /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h
#0  glShadeModel (mode=7424)
    at /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h:2981
        _tbl = 0x0
        _func = <optimized out>
termux/x11-packages#1  0xaaaaa97e in main (argc=<optimized out>, argv=<optimized out>)
    at /home/builder/.termux-build/mesa-demos/src/src/xdemos/glxdemo.c:120
        dpy = 0xf742fa00
        win = 2097154
```
```
Starting program: /data/data/com.termux/files/usr/bin/glinfo 

Program received signal SIGSEGV, Segmentation fault.
glDrawBuffer (mode=1028)
    at /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h:3156
3156	/home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h: (undocumented errno 2).
Continuing.
[New LWP 4808]
[New LWP 4809]
[LWP 4809 exited]
[LWP 4808 exited]

Thread 1 ""glinfo"" received signal SIGSEGV, Segmentation fault.
glDrawBuffer (mode=1028)
    at /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h:3156
3156	in /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h
#0  glDrawBuffer (mode=1028)
    at /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h:3156
        _tbl = 0x0
        _func = <optimized out>
termux/x11-packages#1  0xf6aea19c in fgOpenWindow () from /data/data/com.termux/files/usr/lib/libglut.so
No symbol table info available.
termux/x11-packages#2  0xf6ae87b0 in fgCreateWindow () from /data/data/com.termux/files/usr/lib/libglut.so
No symbol table info available.
termux/x11-packages#3  0xf6aea236 in glutCreateWindow ()
   from /data/data/com.termux/files/usr/lib/libglut.so
No symbol table info available.
termux/x11-packages#4  0xaaaaa674 in main (argc=1, argv=0xfffef2c4)
    at /home/builder/.termux-build/mesa-demos/src/src/demos/glinfo.c:18
No locals.
```
```
Starting program: /data/data/com.termux/files/usr/bin/glxinfo 

Program received signal SIGSEGV, Segmentation fault.
glGetString (name=7936)
    at /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h:3667
3667	/home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h: (undocumented errno 2).
Continuing.
[New LWP 5387]
[New LWP 5389]
[LWP 5389 exited]
[LWP 5387 exited]

Thread 1 ""glxinfo"" received signal SIGSEGV, Segmentation fault.
glGetString (name=7936)
    at /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h:3667
3667	in /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h
#0  glGetString (name=7936)
    at /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h:3667
        _tbl = 0x0
        _func = <optimized out>
termux/x11-packages#1  0xaaaac34e in print_screen_info (dpy=0xf742fa00, scrnum=0, opts=0xfffef258, 
    coreProfile=<optimized out>, es2Profile=0, limits=0, coreWorked=0)
    at /home/builder/.termux-build/mesa-demos/src/src/xdemos/glxinfo.c:468
        serverVersion = 0xf70766b0 <Fake_glXQueryServerString.version> ""1.4 Mesa 17.3.9""
        clientVersion = 0xf70762c8 <Fake_glXGetClientString.version> ""1.4 Mesa 17.3.9""
        glRenderer = <optimized out>
        displayName = <optimized out>
        glVendor = <optimized out>
        glExtensions = <optimized out>
        period = <optimized out>
        glxExtensions = 0xf6fe74fb ""GLX_MESA_copy_sub_buffer GLX_MESA_pixmap_colormap GLX_MESA_release_buffers GLX_ARB_create_context GLX_ARB_get_proc_address GLX_EXT_texture_from_pixmap GLX_EXT_visual_info GLX_EXT_visual_rating GLX_SGI""...
        glVersion = <optimized out>
        colon = <optimized out>
        extfuncs = {GetProgramivARB = <optimized out>, GetStringi = <optimized out>, 
          GetConvolutionParameteriv = <optimized out>}
        serverVendor = 0xf6fe74d9 ""Brian Paul""
        serverExtensions = 0xf6fe74fb ""GLX_MESA_copy_sub_buffer GLX_MESA_pixmap_colormap GLX_MESA_release_buffers GLX_ARB_create_context GLX_ARB_get_proc_address GLX_EXT_texture_from_pixmap GLX_EXT_visual_info GLX_EXT_visual_rating GLX_SGI""...
        clientVendor = 0xf6fe74d9 ""Brian Paul""
        clientExtensions = 0xf6fe74fb ""GLX_MESA_copy_sub_buffer GLX_MESA_pixmap_colormap GLX_MESA_release_buffers GLX_ARB_create_context GLX_ARB_get_proc_address GLX_EXT_texture_from_pixmap GLX_EXT_visual_info GLX_EXT_visual_rating GLX_SGI""...
        glxVersionMajor = <optimized out>
        glxVersionMinor = <optimized out>
        height = 100
        width = 100
        ctx = 0xf70b61c0
        root = <optimized out>
        oglstring = 0xaaaae58c ""OpenGL""
        fbconfigs = <optimized out>
        visinfo = <optimized out>
        mask = 10250
        attr = {background_pixmap = 2863353856, background_pixel = 0, 
          border_pixmap = 1, border_pixel = 0, bit_gravity = 40, 
          win_gravity = -147447808, backing_store = 0, backing_planes = 4294898232, 
          backing_pixel = 4147188316, save_under = -69032, event_mask = 163840, 
          do_not_propagate_mask = 1, override_redirect = -148363805, 
          colormap = 2097153, cursor = 4148361728}
        win = 2097154
termux/x11-packages#2  0xaaaac014 in main (argc=<optimized out>, argv=<optimized out>)
    at /home/builder/.termux-build/mesa-demos/src/src/xdemos/glxinfo.c:1261
        opts = {mode = Normal, findBest = 0 '\000', limits = 0 '\000', 
          singleLine = 0 '\000', displayName = 0x0, allowDirect = 1 '\001'}
        dpy = 0xf742fa00
        numScreens = 1
        scrnum = 0
        coreWorked = <optimized out>
```
```
Starting program: /data/data/com.termux/files/usr/bin/glxgears 
[New LWP 5771]

Thread 1 ""glxgears"" received signal SIGSEGV, Segmentation fault.
glLightfv (light=16384, pname=4611, params=0xaaaaf000 <init.pos>)
    at /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h:2862
2862	/home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h: (undocumented errno 2).
Continuing.
[New LWP 5773]
[New LWP 5774]
[LWP 5774 exited]
[LWP 5773 exited]

Thread 1 ""glxgears"" received signal SIGSEGV, Segmentation fault.
glLightfv (light=16384, pname=4611, params=0xaaaaf000 <init.pos>)
    at /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h:2862
2862	in /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h
#0  glLightfv (light=16384, pname=4611, params=0xaaaaf000 <init.pos>)
    at /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h:2862
        _tbl = 0x0
        _func = <optimized out>
termux/x11-packages#1  0xaaaab8d8 in init ()
    at /home/builder/.termux-build/mesa-demos/src/src/xdemos/glxgears.c:398
        pos = {5, 5, 10, 0}
        red = {0.800000012, 0.100000001, 0, 1}
        green = {0, 0.800000012, 0.200000003, 1}
        blue = {0.200000003, 0.200000003, 1, 1}
termux/x11-packages#2  0xaaaab4c4 in main (argc=<optimized out>, argv=<optimized out>)
    at /home/builder/.termux-build/mesa-demos/src/src/xdemos/glxgears.c:790
        winWidth = <optimized out>
        winHeight = <optimized out>
        x = <optimized out>
        y = <optimized out>
        i = <optimized out>
        printInfo = 0 '\000'
        dpyName = <optimized out>
        dpy = 0xf742fa00
        visId = <optimized out>
        ctx = 0xf70f21c0
        win = 2097154
#0  glLightfv (light=16384, pname=4611, params=0xaaaaf000 <init.pos>)
    at /home/builder/.termux-build/mesa/src/src/mapi/glapi/glapi_mapi_tmp.h:2862
        _tbl = 0x0
        _func = <optimized out>
termux/x11-packages#1  0xaaaab8d8 in init ()
    at /home/builder/.termux-build/mesa-demos/src/src/xdemos/glxgears.c:398
        pos = {5, 5, 10, 0}
        red = {0.800000012, 0.100000001, 0, 1}
        green = {0, 0.800000012, 0.200000003, 1}
        blue = {0.200000003, 0.200000003, 1, 1}
termux/x11-packages#2  0xaaaab4c4 in main (argc=<optimized out>, argv=<optimized out>)
    at /home/builder/.termux-build/mesa-demos/src/src/xdemos/glxgears.c:790
        winWidth = <optimized out>
        winHeight = <optimized out>
        x = <optimized out>
        y = <optimized out>
        i = <optimized out>
        printInfo = 0 '\000'
        dpyName = <optimized out>
        dpy = 0xf742fa00
        visId = <optimized out>
        ctx = 0xf70f21c0
        win = 2097154
```
Not sure how useful are these logs. Might be a red herring.",2021-10-13T08:19:13Z,12658589
4376,termux/termux-packages,1208013839,1102331769,"![Screenshot_20211016-152508](https://user-images.githubusercontent.com/64093255/137601417-26228ba0-6245-4aee-9acf-50b9b9840cc4.png)
![Screenshot_20211016-152816](https://user-images.githubusercontent.com/64093255/137601418-e74a9f84-467b-4376-93a3-efdfdcc3dc39.png)

@xeffyr Posh Shell (armhf alpine, proot) has no problem running openGL, so the error will be that any specific hardware/driver cannot be accessed? ",2021-10-16T20:33:01Z,64093255
4377,termux/termux-packages,1208013839,1102331773,"Apparently Xephyr and Posh Shell have no problem with openGL apart from its very poor performance. 

Note that both use wayland ",2021-10-16T20:37:25Z,64093255
4378,termux/termux-packages,1208013839,1102331779,"![Screenshot_20211017-165820](https://user-images.githubusercontent.com/64093255/137646590-bb603e86-08e9-4608-9e17-3c286ae5e1f3.png)
![Screenshot_20211017-165749](https://user-images.githubusercontent.com/64093255/137646591-b6ebcdf9-7850-4340-a955-d522f453c868.png)

@xeffyr Solved! 
Turn on mesa hardware acceleration

https://gxmersam.blogspot.com/p/getting-hardware-acceleration-on-termux.html?m=1
",2021-10-17T22:05:34Z,64093255
4379,termux/termux-packages,1208013839,1102331785,"![Screenshot_20211017-170718](https://user-images.githubusercontent.com/64093255/137646658-dd076f78-042e-4c55-95bd-493757346a93.png)

![Screenshot_20211017-170626](https://user-images.githubusercontent.com/64093255/137646663-771e4da0-2b70-4158-9dc6-875a0002969f.png)

Enabling on mesa hardware acceleration ",2021-10-17T22:07:59Z,64093255
4380,termux/termux-packages,1208013839,1102331788,"The performance is excellent, there is no lag, everything goes very fast ",2021-10-17T22:08:19Z,64093255
4381,termux/termux-packages,1208013839,1102331792,"> The performance is excellent, there is no lag, everything goes very fast

Does it run on debian proot/chroot? ",2021-10-18T00:45:44Z,11465849
4382,termux/termux-packages,1208013839,1102331796,"> > The performance is excellent, there is no lag, everything goes very fast
> 
> Does it run on debian proot/chroot?

No, it is fully native in termux x11

https://youtu.be/hsO-Os9jr8g",2021-10-18T00:47:17Z,64093255
4383,termux/termux-packages,1208013839,1102331802,"> > > The performance is excellent, there is no lag, everything goes very fast
> > 
> > 
> > Does it run on debian proot/chroot?
> 
> No, it is fully native in termux x11
> 
> https://youtu.be/hsO-Os9jr8g

what really !!! have to check right now !!! 😃",2021-10-18T04:36:38Z,39275638
4384,termux/termux-packages,1208013839,1102331807,"> ![Screenshot_20211017-165820](https://user-images.githubusercontent.com/64093255/137646590-bb603e86-08e9-4608-9e17-3c286ae5e1f3.png) ![Screenshot_20211017-165749](https://user-images.githubusercontent.com/64093255/137646591-b6ebcdf9-7850-4340-a955-d522f453c868.png)
> 
> @xeffyr Solved! Turn on mesa hardware acceleration
> 
> https://gxmersam.blogspot.com/p/getting-hardware-acceleration-on-termux.html?m=1

this is currently using the llvmpipe driver unless you specified GALLIUM_DRIVER=zink",2021-10-18T04:55:22Z,74525818
4385,termux/termux-packages,1208013839,1102331811,"Actually its better to run in chroot linux because it hss many 3D app/games , also box86 wine.. But if it still in termux desktop, i think there is no app/games there..",2021-10-18T04:57:39Z,11465849
4386,termux/termux-packages,1208013839,1102331815,"![IMG_20211018_105206](https://user-images.githubusercontent.com/74525818/137673723-4d6d7d68-d5a5-4f19-ad5c-944f2bb04180.jpg)
![IMG_20211018_105117](https://user-images.githubusercontent.com/74525818/137673745-d22a9a13-4a74-48c4-8aa7-a83e4005fe51.jpg)
this was using the Zink driver ",2021-10-18T05:26:58Z,74525818
4387,termux/termux-packages,1208013839,1102331818,"> ![IMG_20211018_105206](https://user-images.githubusercontent.com/74525818/137673723-4d6d7d68-d5a5-4f19-ad5c-944f2bb04180.jpg) ![IMG_20211018_105117](https://user-images.githubusercontent.com/74525818/137673745-d22a9a13-4a74-48c4-8aa7-a83e4005fe51.jpg) this was using the Zink driver

Can it work on Adreno 506?",2021-10-18T05:28:38Z,11465849
4388,termux/termux-packages,1208013839,1102331824,"> > ![IMG_20211018_105206](https://user-images.githubusercontent.com/74525818/137673723-4d6d7d68-d5a5-4f19-ad5c-944f2bb04180.jpg) ![IMG_20211018_105117](https://user-images.githubusercontent.com/74525818/137673745-d22a9a13-4a74-48c4-8aa7-a83e4005fe51.jpg) this was using the Zink driver
> 
> Can it work on Adreno 506?

it should work on all GPUs that support vulkan ",2021-10-18T05:29:36Z,74525818
4389,termux/termux-packages,1208013839,1102331833,"> > > ![IMG_20211018_105206](https://user-images.githubusercontent.com/74525818/137673723-4d6d7d68-d5a5-4f19-ad5c-944f2bb04180.jpg) ![IMG_20211018_105117](https://user-images.githubusercontent.com/74525818/137673745-d22a9a13-4a74-48c4-8aa7-a83e4005fe51.jpg) this was using the Zink driver
> > 
> > 
> > Can it work on Adreno 506?
> 
> it should work on all GPUs that support vulkan

Can you give me tutorial link? I tried few days ago, i read tutorial somewhere, but in the end, i compile and not work.. Is it like turnip? do you have tutorial?

I will try in chroot..",2021-10-18T05:31:42Z,11465849
4390,termux/termux-packages,1208013839,1102331839,"> > > > ![IMG_20211018_105206](https://user-images.githubusercontent.com/74525818/137673723-4d6d7d68-d5a5-4f19-ad5c-944f2bb04180.jpg) ![IMG_20211018_105117](https://user-images.githubusercontent.com/74525818/137673745-d22a9a13-4a74-48c4-8aa7-a83e4005fe51.jpg) this was using the Zink driver
> > > 
> > > 
> > > Can it work on Adreno 506?
> > 
> > 
> > it should work on all GPUs that support vulkan
> 
> Can you give me tutorial link? I tried few days ago, i read tutorial somewhere, but in the end, i compile and not work.. Is it like turnip? do you have tutorial?
> 
> I will try in chroot..

you could try the instructions in https://github.com/suhan-paradkar/tewmux-disabled/releases/download/mesa-hw/instructions.tar.gz

i dont know weather it will work in an chroot envionment 
",2021-10-18T05:41:53Z,74525818
4391,termux/termux-packages,1208013839,1102331842,You can try [this](https://youtu.be/PO75bU3zGm0) in chroot environment,2021-10-18T05:43:34Z,87801676
4392,termux/termux-packages,1208013839,1102331847,"> > > The performance is excellent, there is no lag, everything goes very fast
> > 
> > 
> > Does it run on debian proot/chroot?
> 
> No, it is fully native in termux x11
> 
> https://youtu.be/hsO-Os9jr8g

ok i couldn't get it to work ... i'll ask you in telegram ... please respond me there",2021-10-18T06:33:28Z,39275638
4393,termux/termux-packages,1208013839,1102331849,"![Screenshot_20211022-204551](https://user-images.githubusercontent.com/64093255/138537854-b5a31685-bf01-4a29-88ae-a10bc90c3dc9.png)
![Screenshot_20211022-204516](https://user-images.githubusercontent.com/64093255/138537855-5e4f04cb-2a7e-451b-bd4e-8a2cb1da4918.png)

@xeffyr @suhan-paradkar 

I just killed two birds with one stone

### 1 - OpenGL crash in 32 bits arm devices 

**How to fix it?**

- Enable opengl in mesa, vulkan drivers and galium drivers. 

- I haven't tried zink, but you don't need to enable zink 

- Add wayland, libsha1 mesa-gl, mesa-dri packages 

- Enable opengl in libepoxy package 

- Enable opengl, Xephyr, dri1 dri2 dri3, in xorg-xserver 

And voila!, everything will work smoothly in both Xwayland and vnc. 

### 2 - How to fix gl4es is slow 

Just follow the steps in step 1, and voila, gl4es will go fast and smooth. 

### Additional Notes 

Do not use ndk-multilib for compilation, leave OpenGL broken for compilation of more packages in the future 

It works perfectly, I don't get segmentation failures and the optimization is excellent, I don't know if this works also in 64 bits. 

The slowness of gl4es must be because the graphics are forced, since mesa does not provide a correct rendering, I think that explains why gl4es is slow 

I was guided by this page :  

https://gxmersam.blogspot.com/p/getting-hardware-acceleration-on-termux.html?m=1


And I go back and repeat, I'm not using zink, you don't need zink to work. 

I hope you find all this information useful 
",2021-10-23T02:34:21Z,64093255
4394,termux/termux-packages,1208013839,1102331856,"from what i have seen before mesa has some issues when compiling in ci, mainly saying llvm-config not found ",2021-10-23T02:45:44Z,74525818
4395,termux/termux-packages,1208013839,1102331861,"> @Yonle

Please **__do not mention me__** as __i'm not interested with this__. I repeat, **__do not mention random user__**. ",2021-10-23T05:45:18Z,63401712
4396,termux/termux-packages,1208013839,1102331866,"a little help ?
i just can't seem to get it work 😥",2021-10-23T07:56:41Z,39275638
4397,termux/termux-packages,1208013839,1102331871,"> > @Yonle
> 
> Please ****do not mention me**** as **i'm not interested with this**. I repeat, ****do not mention random user****.

Sorry I mentioned it by mistake",2021-10-23T13:13:54Z,64093255
4398,termux/termux-packages,1208013839,1102331875,"Lots of off-topic and overquoting.

I'm locking issue now. If you know how to fix - submit a pull request with changes to package versions, build configuration and patches (whatever is required).",2021-10-23T13:53:24Z,10137
4399,termux/termux-packages,1208013839,1103041678,"Note that `mesa-demos`, the package that contains `glxgears`, has been disabled now. This issue is not worth being open.",2022-04-19T19:47:51Z,69125751
4400,google/blockly,1212965768,1212965768,"_This issue was automatically created by [Allstar](https://github.com/ossf/allstar/)._

**Security Policy Violation**
Project is out of compliance with Binary Artifacts policy: binaries present in source code

**Rule Description**
Binary Artifacts are an increased security risk in your repository. Binary artifacts cannot be reviewed, allowing the introduction of possibly obsolete or maliciously subverted executables. For more information see the [Security Scorecards Documentation](https://github.com/ossf/scorecard/blob/main/docs/checks.md#binary-artifacts) for Binary Artifacts.

**Remediation Steps**
To remediate, remove the generated executable artifacts from the repository.

**Artifacts Found**

- demos/mobile/android/gradle/wrapper/gradle-wrapper.jar
- demos/plane/soy/SoyMsgExtractor.jar
- demos/plane/soy/SoyToJsSrcCompiler.jar

**Additional Information**
This policy is drawn from [Security Scorecards](https://github.com/ossf/scorecard/), which is a tool that scores a project's adherence to security best practices. You may wish to run a Scorecards scan directly on this repository for more details.

---

Allstar has been installed on all Google managed GitHub orgs. Policies are gradually being rolled out and enforced by the GOSST and OSPO teams. Learn more at http://go/allstar

This issue will auto resolve when the policy is in compliance.

Issue created by Allstar. See https://github.com/ossf/allstar/ for more information. For questions specific to the repository, please contact the owner or maintainer.",2022-04-22T23:33:43Z,85575025
4401,google/blockly,1212965768,1107072071,We're planning on deleting this demo anyway.,2022-04-23T00:33:37Z,25440652
4402,google/blockly,1212965768,1107893154,"Reopening issue. Status:
Project is out of compliance with Binary Artifacts policy: binaries present in source code

**Rule Description**
Binary Artifacts are an increased security risk in your repository. Binary artifacts cannot be reviewed, allowing the introduction of possibly obsolete or maliciously subverted executables. For more information see the [Security Scorecards Documentation](https://github.com/ossf/scorecard/blob/main/docs/checks.md#binary-artifacts) for Binary Artifacts.

**Remediation Steps**
To remediate, remove the generated executable artifacts from the repository.

**Artifacts Found**

- demos/mobile/android/gradle/wrapper/gradle-wrapper.jar
- demos/plane/soy/SoyMsgExtractor.jar
- demos/plane/soy/SoyToJsSrcCompiler.jar

**Additional Information**
This policy is drawn from [Security Scorecards](https://github.com/ossf/scorecard/), which is a tool that scores a project's adherence to security best practices. You may wish to run a Scorecards scan directly on this repository for more details.",2022-04-24T18:26:26Z,85575025
4403,google/blockly,1212965768,1109216746,"Updating issue after ping interval. Status:
Project is out of compliance with Binary Artifacts policy: binaries present in source code

**Rule Description**
Binary Artifacts are an increased security risk in your repository. Binary artifacts cannot be reviewed, allowing the introduction of possibly obsolete or maliciously subverted executables. For more information see the [Security Scorecards Documentation](https://github.com/ossf/scorecard/blob/main/docs/checks.md#binary-artifacts) for Binary Artifacts.

**Remediation Steps**
To remediate, remove the generated executable artifacts from the repository.

**Artifacts Found**

- demos/mobile/android/gradle/wrapper/gradle-wrapper.jar
- demos/plane/soy/SoyMsgExtractor.jar
- demos/plane/soy/SoyToJsSrcCompiler.jar

**Additional Information**
This policy is drawn from [Security Scorecards](https://github.com/ossf/scorecard/), which is a tool that scores a project's adherence to security best practices. You may wish to run a Scorecards scan directly on this repository for more details.",2022-04-26T01:57:42Z,85575025
4404,google/blockly,1212965768,1112604934,"Updating issue after ping interval. Status:
Project is out of compliance with Binary Artifacts policy: binaries present in source code

**Rule Description**
Binary Artifacts are an increased security risk in your repository. Binary artifacts cannot be reviewed, allowing the introduction of possibly obsolete or maliciously subverted executables. For more information see the [Security Scorecards Documentation](https://github.com/ossf/scorecard/blob/main/docs/checks.md#binary-artifacts) for Binary Artifacts.

**Remediation Steps**
To remediate, remove the generated executable artifacts from the repository.

**Artifacts Found**

- demos/mobile/android/gradle/wrapper/gradle-wrapper.jar
- demos/plane/soy/SoyMsgExtractor.jar
- demos/plane/soy/SoyToJsSrcCompiler.jar

**Additional Information**
This policy is drawn from [Security Scorecards](https://github.com/ossf/scorecard/), which is a tool that scores a project's adherence to security best practices. You may wish to run a Scorecards scan directly on this repository for more details.",2022-04-28T20:02:42Z,85575025
4405,google/blockly,1212965768,1460651097,"Status update:
The plane demo is gone.  But the mobile/android directory remains.",2023-03-08T18:23:22Z,250480
4406,google/WebFundamentals,669419291,669419291,"Hello!

Umm, we don't really care for icons and web animations... Instead, when are you going to add REAL support for streaming?? Meaning, Dolby ATMOS, DTS:X, Dolby Vision, etc?
Microsoft beat you to it, with their new Edge browser. And to be honest, I love it MORE than Chrome now!! It's awesome!!!!!!!!!!!!!!!!!!!!
Oh, and not to forget: give us the option to switch internally between the Java Script Player and HTML5 Player, for streaming as well? Hmm? When?

I am dead serious, WHEN??? You are not adding/doing anything useful to it, nothing beneficial for the end user.
Please DO actual relevant and useful stuff for/with it! I'm almost 40, and I bet I'm gonna die BEFORE you even get the idea to implement all of this!!! Pfffff................😭😭😭💔💔",2020-07-31T05:10:00Z,59364663
4407,google/glog,1075227076,1075227076,"My simple program links to the `glog` library, but it seems cannot find some symbols:

```
...
Undefined symbols for architecture x86_64:
  ""void google::MakeCheckOpValueString<std::nullptr_t>(std::__1::basic_ostream<char, std::__1::char_traits<char> >*, std::nullptr_t const&)"", referenced from:
      std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >* google::MakeCheckOpString<void*, std::nullptr_t>(void* const&, std::nullptr_t const&, char const*) in foo.cpp.o
      std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >* google::MakeCheckOpString<void*, std::nullptr_t>(void* const&, std::nullptr_t const&, char const*) in bar.cpp.o
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)

```

### HOW I compile glog

<details>

```bash
        git clone --depth=1 https://github.com/google/glog.git
        pushd glog
        cmake -S . -B build -DBUILD_SHARED_LIBS=OFF
        cmake --build build --target install
        popd
```

```
Cloning into 'glog'...
remote: Enumerating objects: 97, done.
remote: Counting objects: 100% (97/97), done.
remote: Compressing objects: 100% (86/86), done.
remote: Total 97 (delta 25), reused 38 (delta 6), pack-reused 0
Unpacking objects: 100% (97/97), done.

-- The CXX compiler identification is AppleClang 11.0.0.11000033
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Could NOT find GTest (missing: GTest_DIR)
CMake Warning at CMakeLists.txt:90 (find_package):
  By not providing ""Findgflags.cmake"" in CMAKE_MODULE_PATH this project has
  asked CMake to find a package configuration file provided by ""gflags"", but
  CMake did not find one.

  Could not find a package configuration file provided by ""gflags"" (requested
  version 2.2.0) with any of the following names:

    gflagsConfig.cmake
    gflags-config.cmake

  Add the installation prefix of ""gflags"" to CMAKE_PREFIX_PATH or set
  ""gflags_DIR"" to a directory containing one of the above files.  If ""gflags""
  provides a separate development package or SDK, be sure it has been
  installed.


-- Looking for C++ include pthread.h
-- Looking for C++ include pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Could NOT find Unwind (missing: Unwind_LIBRARY Unwind_PLATFORM_LIBRARY) 
-- Looking for C++ include unwind.h
-- Looking for C++ include unwind.h - found
-- Looking for C++ include dlfcn.h
-- Looking for C++ include dlfcn.h - found
-- Looking for C++ include execinfo.h
-- Looking for C++ include execinfo.h - found
-- Looking for C++ include glob.h
-- Looking for C++ include glob.h - found
-- Looking for C++ include inttypes.h
-- Looking for C++ include inttypes.h - found
-- Looking for C++ include memory.h
-- Looking for C++ include memory.h - found
-- Looking for C++ include pwd.h
-- Looking for C++ include pwd.h - found
-- Looking for C++ include stdint.h
-- Looking for C++ include stdint.h - found
-- Looking for C++ include strings.h
-- Looking for C++ include strings.h - found
-- Looking for C++ include sys/stat.h
-- Looking for C++ include sys/stat.h - found
-- Looking for C++ include sys/syscall.h
-- Looking for C++ include sys/syscall.h - found
-- Looking for C++ include sys/time.h
-- Looking for C++ include sys/time.h - found
-- Looking for C++ include sys/types.h
-- Looking for C++ include sys/types.h - found
-- Looking for C++ include sys/utsname.h
-- Looking for C++ include sys/utsname.h - found
-- Looking for C++ include sys/wait.h
-- Looking for C++ include sys/wait.h - found
-- Looking for C++ include syscall.h
-- Looking for C++ include syscall.h - not found
-- Looking for C++ include syslog.h
-- Looking for C++ include syslog.h - found
-- Looking for C++ include ucontext.h
-- Looking for C++ include ucontext.h - not found
-- Looking for C++ include unistd.h
-- Looking for C++ include unistd.h - found
-- Looking for C++ include ext/hash_map
-- Looking for C++ include ext/hash_map - found
-- Looking for C++ include ext/hash_set
-- Looking for C++ include ext/hash_set - found
-- Looking for C++ include ext/slist
-- Looking for C++ include ext/slist - not found
-- Looking for C++ include tr1/unordered_map
-- Looking for C++ include tr1/unordered_map - not found
-- Looking for C++ include tr1/unordered_set
-- Looking for C++ include tr1/unordered_set - not found
-- Looking for C++ include unordered_map
-- Looking for C++ include unordered_map - found
-- Looking for C++ include unordered_set
-- Looking for C++ include unordered_set - found
-- Looking for C++ include stddef.h
-- Looking for C++ include stddef.h - found
-- Check size of unsigned __int16
-- Check size of unsigned __int16 - failed
-- Check size of u_int16_t
-- Check size of u_int16_t - done
-- Check size of uint16_t
-- Check size of uint16_t - done
-- Looking for dladdr
-- Looking for dladdr - found
-- Looking for fcntl
-- Looking for fcntl - found
-- Looking for pread
-- Looking for pread - found
-- Looking for pwrite
-- Looking for pwrite - found
-- Looking for sigaction
-- Looking for sigaction - found
-- Looking for sigaltstack
-- Looking for sigaltstack - found
-- Performing Test HAVE_NO_DEPRECATED
-- Performing Test HAVE_NO_DEPRECATED - Success
-- Performing Test HAVE_NO_UNNAMED_TYPE_TEMPLATE_ARGS
-- Performing Test HAVE_NO_UNNAMED_TYPE_TEMPLATE_ARGS - Success
-- Looking for pthread_threadid_np
-- Looking for pthread_threadid_np - found
-- Looking for snprintf
-- Looking for snprintf - found
-- Looking for UnDecorateSymbolName in dbghelp
-- Looking for UnDecorateSymbolName in dbghelp - not found
-- Performing Test HAVE___ATTRIBUTE__
-- Performing Test HAVE___ATTRIBUTE__ - Success
-- Performing Test HAVE___ATTRIBUTE__VISIBILITY_DEFAULT
-- Performing Test HAVE___ATTRIBUTE__VISIBILITY_DEFAULT - Success
-- Performing Test HAVE___ATTRIBUTE__VISIBILITY_HIDDEN
-- Performing Test HAVE___ATTRIBUTE__VISIBILITY_HIDDEN - Success
-- Performing Test HAVE___BUILTIN_EXPECT
-- Performing Test HAVE___BUILTIN_EXPECT - Success
-- Performing Test HAVE___SYNC_VAL_COMPARE_AND_SWAP
-- Performing Test HAVE___SYNC_VAL_COMPARE_AND_SWAP - Success
-- Performing Test HAVE_RWLOCK
-- Performing Test HAVE_RWLOCK - Success
-- Performing Test HAVE___DECLSPEC
-- Performing Test HAVE___DECLSPEC - Failed
-- Performing Test STL_NO_NAMESPACE
-- Performing Test STL_NO_NAMESPACE - Failed
-- Performing Test STL_STD_NAMESPACE
-- Performing Test STL_STD_NAMESPACE - Success
-- Performing Test HAVE_USING_OPERATOR
-- Performing Test HAVE_USING_OPERATOR - Success
-- Performing Test HAVE_NAMESPACES
-- Performing Test HAVE_NAMESPACES - Success
-- Performing Test HAVE_GCC_TLS
-- Performing Test HAVE_GCC_TLS - Success
-- Performing Test HAVE_MSVC_TLS
-- Performing Test HAVE_MSVC_TLS - Failed
-- Performing Test HAVE_CXX11_TLS
-- Performing Test HAVE_CXX11_TLS - Failed
-- Performing Test HAVE_ALIGNED_STORAGE
-- Performing Test HAVE_ALIGNED_STORAGE - Failed
-- Performing Test HAVE_CXX11_ATOMIC
-- Performing Test HAVE_CXX11_ATOMIC - Success
-- Performing Test HAVE_CXX11_CONSTEXPR
-- Performing Test HAVE_CXX11_CONSTEXPR - Failed
-- Performing Test HAVE_CXX11_CHRONO
-- Performing Test HAVE_CXX11_CHRONO - Success
-- Performing Test HAVE_CXX11_NULLPTR_T
-- Performing Test HAVE_CXX11_NULLPTR_T - Success
-- Performing Test HAVE_LOCALTIME_R
-- Performing Test HAVE_LOCALTIME_R - Success
-- Performing Test COMPILER_HAS_HIDDEN_VISIBILITY
-- Performing Test COMPILER_HAS_HIDDEN_VISIBILITY - Success
-- Performing Test COMPILER_HAS_HIDDEN_INLINE_VISIBILITY
-- Performing Test COMPILER_HAS_HIDDEN_INLINE_VISIBILITY - Success
-- Performing Test COMPILER_HAS_DEPRECATED_ATTR
-- Performing Test COMPILER_HAS_DEPRECATED_ATTR - Success
-- Configuring done
-- Generating done
-- Build files have been written to: /Users/neo/work/build/glog/build
[  3%] Building CXX object CMakeFiles/glogbase.dir/src/demangle.cc.o
[  7%] Building CXX object CMakeFiles/glogbase.dir/src/logging.cc.o
[ 11%] Building CXX object CMakeFiles/glogbase.dir/src/raw_logging.cc.o
[ 14%] Building CXX object CMakeFiles/glogbase.dir/src/symbolize.cc.o
[ 18%] Building CXX object CMakeFiles/glogbase.dir/src/utilities.cc.o
[ 22%] Building CXX object CMakeFiles/glogbase.dir/src/vlog_is_on.cc.o
[ 25%] Building CXX object CMakeFiles/glogbase.dir/src/signalhandler.cc.o
[ 25%] Built target glogbase
[ 29%] Linking CXX static library libglog.a
[ 29%] Built target glog
[ 33%] Linking CXX static library libglogtest.a
[ 33%] Built target glogtest
[ 37%] Building CXX object CMakeFiles/logging_unittest.dir/src/logging_unittest.cc.o
[ 40%] Linking CXX executable logging_unittest
[ 40%] Built target logging_unittest
[ 44%] Building CXX object CMakeFiles/stl_logging_unittest.dir/src/stl_logging_unittest.cc.o
[ 48%] Linking CXX executable stl_logging_unittest
[ 48%] Built target stl_logging_unittest
[ 51%] Building CXX object CMakeFiles/symbolize_unittest.dir/src/symbolize_unittest.cc.o
[ 55%] Linking CXX executable symbolize_unittest
[ 55%] Built target symbolize_unittest
[ 59%] Building CXX object CMakeFiles/demangle_unittest.dir/src/demangle_unittest.cc.o
[ 62%] Linking CXX executable demangle_unittest
[ 62%] Built target demangle_unittest
[ 66%] Building CXX object CMakeFiles/stacktrace_unittest.dir/src/stacktrace_unittest.cc.o
[ 70%] Linking CXX executable stacktrace_unittest
[ 70%] Built target stacktrace_unittest
[ 74%] Building CXX object CMakeFiles/utilities_unittest.dir/src/utilities_unittest.cc.o
[ 77%] Linking CXX executable utilities_unittest
[ 77%] Built target utilities_unittest
[ 81%] Building CXX object CMakeFiles/signalhandler_unittest.dir/src/signalhandler_unittest.cc.o
[ 85%] Linking CXX executable signalhandler_unittest
[ 85%] Built target signalhandler_unittest
[ 88%] Building CXX object CMakeFiles/cleanup_immediately_unittest.dir/src/cleanup_immediately_unittest.cc.o
[ 92%] Linking CXX executable cleanup_immediately_unittest
[ 92%] Built target cleanup_immediately_unittest
[ 96%] Building CXX object CMakeFiles/cleanup_with_prefix_unittest.dir/src/cleanup_with_prefix_unittest.cc.o
[100%] Linking CXX executable cleanup_with_prefix_unittest
[100%] Built target cleanup_with_prefix_unittest
Install the project...
-- Install configuration: """"
-- Installing: /usr/local/lib/libglog.a
-- Installing: /usr/local/include/glog/export.h
-- Installing: /usr/local/include/glog/logging.h
-- Installing: /usr/local/include/glog/raw_logging.h
-- Installing: /usr/local/include/glog/stl_logging.h
-- Installing: /usr/local/include/glog/vlog_is_on.h
-- Installing: /usr/local/include/glog/log_severity.h
-- Installing: /usr/local/include/glog/platform.h
-- Installing: /usr/local/lib/pkgconfig/libglog.pc
-- Installing: /usr/local/lib/cmake/glog/glog-modules.cmake
-- Installing: /usr/local/lib/cmake/glog/glog-config.cmake
-- Installing: /usr/local/lib/cmake/glog/glog-config-version.cmake
-- Old export file ""/usr/local/lib/cmake/glog/glog-targets.cmake"" will be replaced.  Removing files [/usr/local/lib/cmake/glog/glog-targets-noconfig.cmake].
-- Installing: /usr/local/lib/cmake/glog/glog-targets.cmake
-- Installing: /usr/local/lib/cmake/glog/glog-targets-noconfig.cmake
```

</details>",2021-12-09T07:08:17Z,38041294
4408,google/glog,1075227076,989577617,"### Env info

```
$ sw_vers
ProductName:	Mac OS X
ProductVersion:	10.14.6
BuildVersion:	18G103

$ clang --version 
Apple clang version 11.0.0 (clang-1100.0.33.17)
Target: x86_64-apple-darwin18.7.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

$ ls -l /usr/local/lib | grep glog
-rw-r--r--  1 neo  admin   391864 Dec  9 15:02 libglog.a

$ mkdir -p /tmp/t && cp /usr/local/lib/libglog.a /tmp/t && ar x /tmp/t/libglog.a
$ ls -1 /tmp/t/*.o | xargs nm | grep MakeCheckOpValueString | cut -d' ' -f2- | sort -u
T __ZN6google22MakeCheckOpValueStringIaEEvPNSt3__113basic_ostreamIcNS1_11char_traitsIcEEEERKT_
T __ZN6google22MakeCheckOpValueStringIcEEvPNSt3__113basic_ostreamIcNS1_11char_traitsIcEEEERKT_
T __ZN6google22MakeCheckOpValueStringIhEEvPNSt3__113basic_ostreamIcNS1_11char_traitsIcEEEERKT_
T __ZN6google22MakeCheckOpValueStringIiEEvPNSt3__113basic_ostreamIcNS1_11char_traitsIcEEEERKT_

$ ls -1 /tmp/t/*.o | xargs nm | grep MakeCheckOpValueString | cut -d' ' -f3 | sort -u | c++filt
void google::MakeCheckOpValueString<signed char>(std::__1::basic_ostream<char, std::__1::char_traits<char> >*, signed char const&)
void google::MakeCheckOpValueString<char>(std::__1::basic_ostream<char, std::__1::char_traits<char> >*, char const&)
void google::MakeCheckOpValueString<unsigned char>(std::__1::basic_ostream<char, std::__1::char_traits<char> >*, unsigned char const&)
void google::MakeCheckOpValueString<int>(std::__1::basic_ostream<char, std::__1::char_traits<char> >*, int const&)

# Build error msg
Undefined symbols for architecture x86_64:
  ""void google::MakeCheckOpValueString<std::nullptr_t>(std::__1::basic_ostream<char, std::__1::char_traits<char> >*, std::nullptr_t const&)"", referenced from:
      std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >* google::MakeCheckOpString<void*, std::nullptr_t>(void* const&, std::nullptr_t const&, char const*) in foo.cpp.o
      std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >* google::MakeCheckOpString<void*, std::nullptr_t>(void* const&, std::nullptr_t const&, char const*) in bar.cpp.o
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)

```",2021-12-09T07:13:14Z,38041294
4409,google/glog,1075227076,989578880,"> My simple program links to the glog library

You need to provide a minimal example.",2021-12-09T07:15:31Z,2170034
4410,google/glog,1075227076,989589652,"### `sample.cpp`

```c++
#include <cstdio>
#include <cstdlib>
#include <glog/logging.h>

int main(void) {
    char *s = (char *) malloc(100);
    CHECK_NE(s, nullptr); // not ok
    //CHECK_NOTNULL(s); // ok
    free(s);
    return 0;
}
```

### Compile

<details>

```console
$ g++ sample.cpp -lglog -Wall -Wextra -g -DDEBUG -O0
In file included from sample.cpp:3:
/usr/local/include/glog/logging.h:741:9: error: use of overloaded operator '<<' is ambiguous (with operand types 'std::ostream'
      (aka 'basic_ostream<char>') and 'const std::__1::nullptr_t')
  (*os) << v;
  ~~~~~ ^  ~
/usr/local/include/glog/logging.h:804:3: note: in instantiation of function template specialization
      'google::MakeCheckOpValueString<std::__1::nullptr_t>' requested here
  MakeCheckOpValueString(comb.ForVar2(), v2);
  ^
/usr/local/include/glog/logging.h:828:1: note: in instantiation of function template specialization
      'google::MakeCheckOpString<char *, std::__1::nullptr_t>' requested here
DEFINE_CHECK_OP_IMPL(Check_NE, !=)  // Use CHECK(x == NULL) instead.
^
/usr/local/include/glog/logging.h:817:17: note: expanded from macro 'DEFINE_CHECK_OP_IMPL'
    else return MakeCheckOpString(v1, v2, exprtext); \
                ^
sample.cpp:7:5: note: in instantiation of function template specialization 'google::Check_NEImpl<char *, std::__1::nullptr_t>'
      requested here
    CHECK_NE(s, nullptr); // not ok
    ^
/usr/local/include/glog/logging.h:899:30: note: expanded from macro 'CHECK_NE'
#define CHECK_NE(val1, val2) CHECK_OP(_NE, !=, val1, val2)
                             ^
/usr/local/include/glog/logging.h:874:3: note: expanded from macro 'CHECK_OP'
  CHECK_OP_LOG(name, op, val1, val2, google::LogMessageFatal)
  ^
/usr/local/include/glog/logging.h:854:18: note: expanded from macro 'CHECK_OP_LOG'
         google::Check##name##Impl(                      \
                 ^
<scratch space>:51:1: note: expanded from here
Check_NEImpl
^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/ostream:195:20: note: 
      candidate function
    basic_ostream& operator<<(basic_ostream& (*__pf)(basic_ostream&))
                   ^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/ostream:199:20: note: 
      candidate function
    basic_ostream& operator<<(basic_ios<char_type, traits_type>&
                   ^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/ostream:204:20: note: 
      candidate function
    basic_ostream& operator<<(ios_base& (*__pf)(ios_base&))
                   ^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/ostream:207:20: note: 
      candidate function
    basic_ostream& operator<<(bool __n);
                   ^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/ostream:219:20: note: 
      candidate function
    basic_ostream& operator<<(const void* __p);
                   ^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/ostream:220:20: note: 
      candidate function
    basic_ostream& operator<<(basic_streambuf<char_type, traits_type>* __sb);
                   ^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/ostream:863:1: note: 
      candidate function [with _Traits = std::__1::char_traits<char>]
operator<<(basic_ostream<char, _Traits>& __os, const char* __str)
^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/ostream:817:1: note: 
      candidate function [with _CharT = char, _Traits = std::__1::char_traits<char>]
operator<<(basic_ostream<_CharT, _Traits>& __os, const char* __strn)
^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/ostream:870:1: note: 
      candidate function [with _Traits = std::__1::char_traits<char>]
operator<<(basic_ostream<char, _Traits>& __os, const signed char* __str)
^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/ostream:878:1: note: 
      candidate function [with _Traits = std::__1::char_traits<char>]
operator<<(basic_ostream<char, _Traits>& __os, const unsigned char* __str)
^
1 error generated.

```

</details>",2021-12-09T07:36:15Z,38041294
4411,google/glog,1075227076,989593918,"If you compile the project using CMake, you should also use CMake to link against glog. There's a [dedicated section](https://github.com/google/glog#consuming-glog-in-a-cmake-project) in the readme how to achieve this.

In case you cannot use CMake to consume glog, it is your responsibility to reconstruct the CMake configuration (flags, compiler and linker options, etc.) because simply linking against glog is generally not sufficient.",2021-12-09T07:44:01Z,2170034
4412,google/glog,1075227076,989596708,"It seems that the problem is not caused by CMake and make.

I do the following patch to my program and minimal reproducible program, they worked.

```diff
-    CHECK_NE(s, nullptr);
+    CHECK_NOTNULL(s);
```

FYI: My project uses CMake to link again glog, the minimal sample use make to link against glog.

The error is much self-explanatory: `error: use of overloaded operator '<<' is ambiguous (with operand types 'std::ostream'`",2021-12-09T07:49:10Z,38041294
4413,google/glog,1075227076,989603203,"> My project uses CMake to link again glog

This was not evident from your description. Make sure to provide a complete minimal example without cherry picking parts that you believe to be relevant.

According to the linker error, there seems to be an ABI mismatch. If you use C++11 features you need to compile glog using the `-std=c++11` flag as well (e.g., by passing -DCMAKE_CXX_STANDARD=11 when configuring glog using CMake).

> The error is much self-explanatory: error: use of overloaded operator '<<' is ambiguous (with operand types 'std::ostream')

You initially reported a linker error. Now it's compiler error. Which is it now?",2021-12-09T08:00:38Z,2170034
4414,google/glog,1075227076,989607805,"> According to the linker error, there seems to be an ABI mismatch. If you use C++11 features you need to compile glog using the -std=c++11 flag as well (e.g., by passing -DCMAKE_CXX_STANDARD=11 when configuring glog using CMake).

Excellent observation.

### Recompile with `-std=c++11`

```bash
$ g++ sample.cpp -std=c++11 -lglog -Wall -Wextra -g -DDEBUG -O0
Undefined symbols for architecture x86_64:
  ""void google::MakeCheckOpValueString<std::nullptr_t>(std::__1::basic_ostream<char, std::__1::char_traits<char> >*, std::nullptr_t const&)"", referenced from:
      std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >* google::MakeCheckOpString<char*, std::nullptr_t>(char* const&, std::nullptr_t const&, char const*) in sample-5ad0b1.o
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)

```",2021-12-09T08:07:56Z,38041294
4415,google/glog,1075227076,989621267,"> Excellent observation.

Given your attitude, it looks like you're not interested in providing a full minimal example including a corresponding `CMakeLists.txt`. Therefore I'm closing this issue.",2021-12-09T08:26:13Z,2170034
